\subsection{Limita\c c\~oes da pesquisa e propostas futuras}

As limitações deste trabalho resultam no tempo e os modelos de aprendizagem de máquina, como vistos durante esta dissertação, têm vários modelos que podem ser trabalhados em conjunto com as séries temporais, por exemplo, os modelos de rede neural LSTM, CNN, RNN... Entre outros modelos que não foram muito bem tratados aqui porque são modelos mais complexos e exigiriam um maior intervalo de tempo para este momento, apenas os modelos que foram trabalhados no início atenderam à questão de pesquisa que foi levantada.

Mas nos próximos passos para um trabalho futuro é abordar melhor estes modelos de previsão tendo com muitos autores na literatura que trabalham com estes modelos, até competição de aprendizagem de máquinas com os modelos mais famosos como o Light GBM em comparação com o XGboost para previsão de curto prazo e para longo prazo cada modelo tem sua relevância LR como um modelo de máximo 3 variáveis para dados com poucas variáveis é muito eficiente e ágil.

No trabalho que se seguirá a este como complemento a este trabalho, tem como abordar toda a literatura, não apenas os últimos 6 anos, e também visa as outras partes que não foram abordadas como dissertações, teses e capítulos de livros, apesar de ter abordado um pequeno grupo de artigos, ainda tinha uma gama muito grande de artigos sobre o assunto. 

A otimização matemática com alguns modelos como floresta aleatória, XGboost, Ligth GBM, que poderia ser usada para aumentar o gradiente e melhorar a precisão da aleatoriedade dos galhos das árvores. Os métodos de otimização para melhorar o modelo foram \textbf{Grid Search, Randomized Search e Bayesian Optimization (Bayes Search)} que vem do inglês para o português seria \textbf{Otimização Bayesiana} para a floresta aleatória o melhor método em hipóteses sérias a busca aleatória (randomized) dos galhos mais rapidamente a árvore predizendo assim melhor o tempo, mas em teoria todos eles em algum modelo falharam em reduzir os erros listados na seção \ref{subsec:metrica} em vez de reduzir, houve um aumento dos erros tornando a previsão ao longo do tempo pior, como por exemplo no apêndice \ref{sec:ararxma24} que teve os melhores resultados se pegar os erros entre os modelos citados anteriormente e em comparação com os modelos de otimização encontrados na literatura teve um aumento dos erros de 6 para 30 \% e para uma previsão mais precisa ela precisa ser próxima de zero.

Nesta parte da otimização é relevante pesquisar ou ter mais profundidade nos hiperparâmetros para ter uma melhor utilização da árvore e modelos de gradiente. 