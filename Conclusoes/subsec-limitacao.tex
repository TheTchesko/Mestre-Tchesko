\subsection{Limita\c c\~oes da pesquisa e propostas futuras}

As limitações deste trabalho estão relacionadas ao tempo disponível e à abordagem dos modelos de aprendizado de máquina. Durante esta dissertação, foram explorados alguns modelos que podem ser aplicados em conjunto com séries temporais, como os modelos de rede neural LSTM, CNN, RNN, entre outros. No entanto, devido à complexidade desses modelos e à necessidade de um maior período de tempo para sua aplicação adequada, eles não foram incluídos neste momento. Os modelos que foram utilizados inicialmente foram escolhidos de forma a atender à pergunta de pesquisa levantada.

Em trabalhos futuros, seria interessante aprofundar a investigação desses modelos de previsão, uma vez que existem muitos autores na literatura que trabalham com eles. Também seria válido realizar comparações entre os modelos mais conhecidos, como o Light GBM e o XGBoost, para previsões de curto prazo. Para previsões de longo prazo, cada modelo possui sua relevância, sendo que a regressão linear (LR) é eficiente e ágil quando se trata de dados com poucas variáveis.

No próximo trabalho, que complementará esta dissertação, é recomendado abordar toda a literatura disponível, não apenas os últimos 6 anos, e considerar outras fontes, como dissertações, teses e capítulos de livros. Apesar de terem sido considerados apenas alguns artigos relevantes, existem muitos outros disponíveis sobre o assunto.

No contexto da otimização matemática, alguns modelos, como a floresta aleatória, XGBoost e Light GBM, poderiam se beneficiar do uso de técnicas para aumentar o gradiente e melhorar a precisão dos resultados. Métodos de otimização como Grid Search, Randomized Search e Bayesian Optimization (Otimização Bayesiana) foram aplicados para melhorar os modelos. Em teoria, esses métodos deveriam reduzir os erros, conforme discutido na seção de métricas (Seção \ref{subsec:metrica}), no entanto, observou-se um aumento nos erros ao longo do tempo. Um exemplo disso pode ser visto no apêndice \ref{sec:ararxma24}, onde os modelos apresentaram um aumento dos erros de 6\% para 30\%. Para obter previsões mais precisas, é necessário minimizar esses erros e torná-los próximos de zero.

Portanto, seria relevante realizar uma pesquisa mais aprofundada sobre os hiperparâmetros e explorar estratégias de otimização para uma melhor utilização dos modelos baseados em árvores e gradientes.