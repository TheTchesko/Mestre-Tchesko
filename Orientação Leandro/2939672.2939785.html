<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>2939672.2939785</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 18pt; }
 .s1 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s2 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s3 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; margin:0pt; }
 .a, a { color: #001472; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s4 { color: #001472; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 4pt; }
 .s6 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s7 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s8 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s9 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s10 { color: #001472; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s11 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s12 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s13 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s14 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .h3, h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s15 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s16 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 4pt; }
 .s17 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s19 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 1pt; }
 .s20 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 1pt; }
 .s21 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 5pt; }
 .s22 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s23 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 1pt; }
 .s24 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s25 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; }
 .s26 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 8pt; }
 .s27 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 6pt; }
 .s28 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s29 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -4pt; }
 .s30 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s31 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s32 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -1pt; }
 .s33 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 2pt; }
 .s34 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -3pt; }
 .s35 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -1pt; }
 .s36 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 2pt; }
 .s37 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 1pt; }
 .s38 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s39 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -2pt; }
 .s40 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -3pt; }
 .s41 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 9pt; vertical-align: 3pt; }
 .s42 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 5pt; }
 .s44 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -1pt; }
 .s46 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 5pt; }
 .s47 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 2pt; }
 .h4 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 6pt; }
 .s48 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 15pt; }
 .s49 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 6pt; }
 .s50 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 4pt; }
 .s51 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 8pt; }
 .s52 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 7pt; }
 .s53 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6pt; }
 .s54 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 6pt; vertical-align: 4pt; }
 .s55 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6pt; vertical-align: 1pt; }
 .s56 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -2pt; }
 .s57 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -6pt; }
 .s58 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; vertical-align: -8pt; }
 .s59 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 9pt; }
 .s60 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 9pt; }
 .s61 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -5pt; }
 .s62 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 6pt; }
 .s63 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 6pt; }
 .s64 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s65 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 5.5pt; vertical-align: 3pt; }
 .s66 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 5.5pt; }
 .s67 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s68 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s69 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 5.5pt; vertical-align: 4pt; }
 .s70 { color: black; font-family:Perpetua, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s71 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s72 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s73 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 19pt; }
 .s74 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s75 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 1pt; }
 .s76 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 1pt; }
 .s77 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -5pt; }
 .s78 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 2pt; }
 .s79 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 1pt; }
 .s80 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; }
 .s81 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; vertical-align: 2pt; }
 .s82 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 4pt; vertical-align: 3pt; }
 .s83 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 4pt; }
 .s84 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; }
 .s85 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; vertical-align: 2pt; }
 .s86 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 4pt; vertical-align: 2pt; }
 .s87 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; vertical-align: -4pt; }
 .s88 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s89 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s90 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s91 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s93 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: 3pt; }
 .s94 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 3pt; }
 .s95 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s96 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 10; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 2; }
 #l2> li>*:first-child:before {counter-increment: c2; content: counter(c2, decimal)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l2> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l3 {padding-left: 0pt;counter-reset: c3 1; }
 #l3> li>*:first-child:before {counter-increment: c3; content: counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l4 {padding-left: 0pt;counter-reset: c3 1; }
 #l4> li>*:first-child:before {counter-increment: c3; content: counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l4> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l5 {padding-left: 0pt;counter-reset: c3 1; }
 #l5> li>*:first-child:before {counter-increment: c3; content: counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l5> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l6 {padding-left: 0pt;counter-reset: c4 1; }
 #l6> li>*:first-child:before {counter-increment: c4; content: "("counter(c4, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l6> li:first-child>*:first-child:before {counter-increment: c4 0;  }
 #l7 {padding-left: 0pt;counter-reset: d1 1; }
 #l7> li>*:first-child:before {counter-increment: d1; content: "("counter(d1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l7> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l8 {padding-left: 0pt;counter-reset: c3 1; }
 #l8> li>*:first-child:before {counter-increment: c3; content: counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l8> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l9 {padding-left: 0pt;counter-reset: e1 1; }
 #l9> li>*:first-child:before {counter-increment: e1; content: "("counter(e1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l9> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 #l10 {padding-left: 0pt;counter-reset: f1 1; }
 #l10> li>*:first-child:before {counter-increment: f1; content: counter(f1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l10> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l11 {padding-left: 0pt;counter-reset: f2 7; }
 #l11> li>*:first-child:before {counter-increment: f2; content: counter(f1, decimal)"."counter(f2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l11> li:first-child>*:first-child:before {counter-increment: f2 0;  }
 #l12 {padding-left: 0pt;counter-reset: f3 8; }
 #l12> li>*:first-child:before {counter-increment: f3; content: "["counter(f3, decimal)"] "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l12> li:first-child>*:first-child:before {counter-increment: f3 0;  }
 li {display: block; }
 #l13 {padding-left: 0pt;counter-reset: g1 1; }
 #l13> li>*:first-child:before {counter-increment: g1; content: "["counter(g1, decimal)"] "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l13> li:first-child>*:first-child:before {counter-increment: g1 0;  }
 li {display: block; }
 #l14 {padding-left: 0pt;counter-reset: f1 1; }
 #l14> li>*:first-child:before {counter-increment: f1; content: counter(f1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l14> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l15 {padding-left: 0pt;counter-reset: f2 7; }
 #l15> li>*:first-child:before {counter-increment: f2; content: counter(f1, decimal)"."counter(f2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l15> li:first-child>*:first-child:before {counter-increment: f2 0;  }
 #l16 {padding-left: 0pt;counter-reset: f3 13; }
 #l16> li>*:first-child:before {counter-increment: f3; content: "["counter(f3, decimal)"] "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l16> li:first-child>*:first-child:before {counter-increment: f3 0;  }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><span><img width="34" height="34" alt="image" src="2939672.2939785/Image_001.jpg"/></span></p><h1 style="padding-top: 4pt;padding-left: 116pt;text-indent: 0pt;text-align: center;">XGBoost: A Scalable Tree Boosting System</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 5pt;padding-left: 237pt;text-indent: 0pt;line-height: 13pt;text-align: center;">Tianqi Chen</p><p class="s2" style="padding-left: 237pt;text-indent: 0pt;line-height: 10pt;text-align: center;">University of Washington</p><p style="padding-left: 237pt;text-indent: 0pt;line-height: 13pt;text-align: center;"><a href="mailto:tqchen@cs.washington.edu" class="s3">tqchen@cs.washington.edu</a></p><p class="s1" style="padding-top: 5pt;padding-left: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">Carlos Guestrin</p><p class="s2" style="padding-left: 1pt;text-indent: 0pt;line-height: 10pt;text-align: center;">University of Washington</p><p style="padding-left: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;"><a href="mailto:guestrin@cs.washington.edu" class="s3">guestrin@cs.washington.edu</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 5pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">ABSTRACT</h2><p style="padding-top: 3pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;">Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end- to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quan- tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compres- sion and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 53pt;text-indent: 0pt;text-align: left;">Keywords</h2><p style="padding-top: 3pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;">Large-scale Machine Learning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 53pt;text-indent: 0pt;text-align: left;"><a name="bookmark0">1. INTRODUCTION</a></h2><p style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;">Machine learning and data-driven approaches are becom- ing very important in many areas. Smart spam classifiers protect our email by learning from massive amounts of s- pam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics. There are two importan- t factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.</p><p class="s4" style="padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark67" class="s64">Among the machine learning methods used in practice, gradient tree boosting [</a>10<a href="#bookmark1" class="s64">]</a>1 <a href="#bookmark84" class="s64">is one technique that shines in many applications. Tree boosting has been shown to give state-of-the-art results on many standard classification benchmarks [</a>16<a href="#bookmark78" class="s64">]. LambdaMART [</a>5<span style=" color: #000;">], a variant of tree boost- ing for ranking, achieves state-of-the-art result for ranking</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="160" height="1" alt="image" src="2939672.2939785/Image_002.png"/></span></p><p class="s5" style="padding-top: 1pt;padding-left: 53pt;text-indent: 0pt;line-height: 87%;text-align: justify;"><a name="bookmark1">1</a><span class="p">Gradient tree boosting is also known as gradient boosting machine (GBM) or gradient boosted regression tree (GBRT)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: justify;"><a href="mailto:permissions@acm.org" class="s7" target="_blank">Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.</a></p><p class="s8" style="padding-top: 1pt;padding-left: 55pt;text-indent: 0pt;text-align: left;">KDD ’16, August 13-17, 2016, San Francisco, CA, USA</p><p class="s9" style="padding-top: 2pt;padding-left: 53pt;text-indent: 0pt;text-align: left;"> <span class="s6">c 2016 ACM. ISBN 978-1-4503-4232-2/16/08. . . $15.00</span></p><p style="padding-top: 2pt;padding-left: 53pt;text-indent: 0pt;text-align: left;"><a href="http://dx.doi.org/10.1145/2939672.2939785" class="s7" target="_blank">DOI: </a><a href="http://dx.doi.org/10.1145/2939672.2939785" class="s10" target="_blank">http://dx.doi.org/10.1145/2939672.2939785</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark83" class="s64">problems. Besides being used as a stand-alone predictor, it is also incorporated into real-world production pipelines for ad click through rate prediction [</a>15<a href="#bookmark76" class="s64">]. Finally, it is the de- facto choice of ensemble method and is used in challenges such as the Netflix prize [</a>3<span style=" color: #000;">].</span></p><p class="s4" style="padding-left: 21pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark2" class="s64">In this paper, we describe XGBoost, a scalable machine learning system for tree boosting. The system is available as an open source package</a>2<a href="#bookmark3" class="s64">. The impact of the system has been widely recognized in a number of machine learning and data mining challenges. Take the challenges hosted by the machine learning competition site Kaggle for example. A- mong the 29 challenge winning solutions </a><a href="#bookmark3" class="a">3</a> <span style=" color: #000;">published at Kag- gle’s blog during 2015, 17 solutions used XGBoost. Among these solutions, eight solely used XGBoost to train the mod- el, while most others combined XGBoost with neural net- s in ensembles. For comparison, the second most popular method, deep neural nets, was used in 11 solutions. The success of the system was also witnessed in KDDCup 2015, where XGBoost was used by every winning team in the top-</span></p><ol id="l1"><li><p class="s4" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark74" class="s64">Moreover, the winning teams reported that ensemble methods outperform a well-configured XGBoost by only a small amount [</a>1<span style=" color: #000;">].</span></p><p style="padding-left: 21pt;text-indent: 8pt;text-align: justify;">These results demonstrate that our system gives state-of- the-art results on a wide range of problems. Examples of the problems in these winning solutions include: store sales prediction; high energy physics event classification; web text classification; customer behavior prediction; motion detec- tion; ad click through rate prediction; malware classification; product categorization; hazard risk prediction; massive on- line course dropout rate prediction. While domain depen- dent data analysis and feature engineering play an important role in these solutions, the fact that XGBoost is the consen- sus choice of learner shows the impact and importance of our system and tree boosting.</p><p style="padding-left: 21pt;text-indent: 8pt;text-align: justify;">The most important factor behind the success of XGBoost is its scalability in all scenarios. The system runs more than ten times faster than existing popular solutions on a single machine and scales to billions of examples in distributed or memory-limited settings. The scalability of XGBoost is due to several important systems and algorithmic optimizations. These innovations include: a novel tree learning algorithm is for handling <span class="s11">sparse data</span>; a theoretically justified weighted quantile sketch procedure enables handling instance weights in approximate tree learning. Parallel and distributed com- puting makes learning faster which enables quicker model ex- ploration. More importantly, XGBoost exploits out-of-core</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="160" height="1" alt="image" src="2939672.2939785/Image_003.png"/></span></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a name="bookmark2">2</a><a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a><a name="bookmark3">&zwnj;</a></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 11pt;text-align: left;">3<span class="p">Solutions come from of top-3 teams of each competitions.</span></p><p style="padding-top: 2pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;">computation and enables data scientists to process hundred millions of examples on a desktop. Finally, it is even more exciting to combine these techniques to make an end-to-end system that scales to even larger data with the least amount of cluster resources. The major contributions of this paper is listed as follows:</p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">•</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 76pt;text-indent: 0pt;text-align: left;">We design and build a highly scalable end-to-end tree boosting system.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">•</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 76pt;text-indent: 0pt;text-align: left;">We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">•</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 76pt;text-indent: 0pt;text-align: left;">We introduce a novel sparsity-aware algorithm for par- allel tree learning.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">•</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 76pt;text-indent: 0pt;text-align: left;">We propose an effective cache-aware block structure for out-of-core tree learning.</p><p class="s4" style="padding-top: 7pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark90" class="s64">While there are some existing works on parallel tree boost- ing [</a>22<a href="#bookmark91" class="s64">, </a>23<a href="#bookmark87" class="s64">, </a>19<a href="#bookmark80" class="s64">], the directions such as out-of-core compu- tation, cache-aware and sparsity-aware learning have not been explored. More importantly, an end-to-end system that combines all of these aspects gives a novel solution for real-world use-cases. This enables data scientists as well as researchers to build powerful variants of tree boosting al- gorithms [</a>7<a href="#bookmark65" class="s64">, </a>8<span style=" color: #000;">]. Besides these major contributions, we also make additional improvements in proposing a regularized learning objective, which we will include for completeness.</span></p><p class="s4" style="padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark8" class="s64">The remainder of the paper is organized as follows. We will first review tree boosting and introduce a regularized objective in Sec. </a>2<a href="#bookmark22" class="s64">. We then describe the split finding meth- ods in Sec. </a>3 <a href="#bookmark37" class="s64">as well as the system design in Sec. </a>4<a href="#bookmark43" class="s64">, including experimental results when relevant to provide quantitative support for each optimization we describe. Related work is discussed in Sec. </a>5<a href="#bookmark51" class="s64">. Detailed end-to-end evaluations are included in Sec. </a>6<a href="#bookmark72" class="s64">. Finally we conclude the paper in Sec. </a>7<span style=" color: #000;">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l2"><li><h2 style="padding-left: 74pt;text-indent: -20pt;text-align: left;"><a name="bookmark4">TREE BOOSTING IN A NUTSHELL</a><a name="bookmark8">&zwnj;</a></h2><p class="s4" style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark69" class="s64">We review gradient tree boosting algorithms in this sec- tion. The derivation follows from the same idea in existing literatures in gradient boosting. Specicially the second order method is originated from Friedman et al. [</a>12<span style=" color: #000;">]. We make mi- nor improvements in the reguralized objective, which were found helpful in practice.</span></p><ol id="l3"><li><h2 style="padding-top: 5pt;padding-left: 80pt;text-indent: -26pt;text-align: left;"><a name="bookmark5">Regularized Learning Objective</a><a name="bookmark9">&zwnj;</a></h2><p style="padding-top: 1pt;padding-left: 62pt;text-indent: 0pt;line-height: 10pt;text-align: left;">For a given data set with <span class="s13">n </span>examples and <span class="s13">m </span>features</p><p class="s14" style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: center;">m</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="268" height="128" alt="image" src="2939672.2939785/Image_004.jpg"/></span></p><h3 style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a name="bookmark10">Figure 1: Tree Ensemble Model. The final predic- tion for a given example is the sum of predictions from each tree.</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="text-indent: 0pt;line-height: 9pt;text-align: left;">                   </p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">it into the leaves and calculate the final prediction by sum- ming up the score in the corresponding leaves (given by <span class="s13">w</span>). To learn the set of functions used in the model, we minimize the following <span class="s11">regularized </span>objective.</p><p style="padding-top: 8pt;padding-left: 85pt;text-indent: 0pt;text-align: center;"><a name="bookmark11"><span class="s12">L</span></a>(<span class="s13">φ</span>) =       <span class="s13">l</span>(<span class="s13">y</span>ˆ<span class="s13">i, yi</span>) +        Ω(<span class="s13">fk</span>)</p><p class="s16" style="padding-left: 115pt;text-indent: 0pt;line-height: 10pt;text-align: left;">i <span class="s17">k </span><span class="p">(2)</span></p><p style="text-indent: 0pt;line-height: 9pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 82pt;text-indent: 0pt;line-height: 15pt;text-align: left;">where Ω(<span class="s13">f </span>) = <span class="s13">γT </span>+ <u>1</u> <span class="s13">λ w </span><span class="s5">2</span></p><p style="padding-top: 10pt;padding-left: 21pt;text-indent: 0pt;text-align: justify;">Here <span class="s13">l </span>is a differentiable convex loss function that measures <span class="s19">t</span>he  difference  between  the  prediction  <span class="s13">y</span>ˆ<span class="s14">i  </span><span class="s19">a</span>nd  the  target  <span class="s13">y</span><span class="s14">i</span><span class="s19">. </span><a href="#bookmark93" class="s64">The  second  term  Ω  penalizes  the  complexity  of  the  model (i.e., the regression tree functions).  The additional regular- ization term helps to smooth the final learnt weights to avoid over-fitting.   Intuitively,  the  regularized  objective  will  tend to select a model employing simple and predictive functions. A  similar  regularization  technique  has  been  used  in  Regu- larized  greedy  forest  (RGF)  [</a><a href="#bookmark93" class="a">2</a><span style=" color: #001472;">5</span>]  model.  Our  objective  and the  corresponding  learning  algorithm  is  simpler  than  RGF and  easier  to  parallelize.  When  the  regularization  parame- ter is set to zero, the objective falls back to the traditional gradient tree boosting.</p></li><li><h2 style="padding-top: 6pt;padding-left: 48pt;text-indent: -26pt;text-align: left;"><a name="bookmark6">Gradient Tree Boosting</a></h2><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 21pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark11" class="s64">The tree ensemble model in Eq. (</a><span style=" color: #001472;">2</span>) includes functions as parameters and cannot be optimized using traditional opti- mization  methods  in  Euclidean  space.   Instead,  the  model is  trained  in  an  additive  manner.   Formally,  let  <span class="s13">y</span>ˆ(<span class="s13">t</span>)  be  the prediction of the <span class="s13">i</span>-th instance at the <span class="s13">t</span>-th iteration, we will</p><p class="s19" style="padding-left: 21pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">need to add <span class="s13">f</span><span class="s14">t </span>to minimize the following objective.</p><p class="s15" style="text-indent: 0pt;line-height: 9pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="padding-top: 5pt;padding-left: 94pt;text-indent: 0pt;line-height: 6pt;text-align: left;">n</p><p class="s14" style="padding-left: 84pt;text-indent: 0pt;line-height: 14pt;text-align: center;"><span class="s20">L</span><span class="s21">(</span>t<span class="s22">)  </span><span class="s19">=</span><span class="p">        </span><span class="s13">l</span><span class="p">(</span><span class="s13">y</span>i<span class="s23">,</span><span class="s13"> y</span><span class="p">ˆ</span>i<span class="s21">(</span>t<span class="s24">−</span><span class="s22">1)  </span><span class="s19">+</span><span class="p"> </span><span class="s13">f</span>t<span class="s19">(</span><span class="h3">x</span>i<span class="s19">))</span><span class="p"> + Ω(</span><span class="s13">f</span>t<span class="s19">)</span></p><p class="s14" style="padding-left: 90pt;text-indent: 0pt;line-height: 7pt;text-align: left;">i<span class="s22">=1</span></p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s20">D </span>= <span class="s12">{</span>(<b>x</b><span class="s14">i</span><span class="s23">, y</span><span class="s14">i</span><span class="s19">)</span><span class="s12">} </span>(<span class="s12">|D| </span>= <span class="s13">n, </span><b>x</b><span class="s14">i </span><span class="s20">∈ </span><span class="s15">R </span><span class="s13">, y</span><span class="s14">i </span><span class="s20">∈ </span><span class="s15">R</span>), a tree ensem-</p><p style="padding-top: 6pt;padding-left: 21pt;text-indent: 0pt;line-height: 5pt;text-align: left;">This means we greedily add the <span class="s13">f </span>that most improves our</p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a href="#bookmark10" class="s64">ble model (shown in Fig. </a><span style=" color: #001472;">1</span>) uses <span class="s13">K </span>additive functions to</p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 10pt;text-align: left;">predict the output.</p><p class="s15" style="text-indent: 0pt;line-height: 9pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="padding-top: 6pt;padding-left: 39pt;text-indent: 0pt;line-height: 7pt;text-align: center;">K</p><p class="s13" style="padding-left: 106pt;text-indent: 0pt;line-height: 14pt;text-align: left;">y<span class="p">ˆ</span>i <span class="p">= </span>φ<span class="p">(</span><span class="h3">x</span>i<span class="p">) =        </span>fk<span class="p">(</span><span class="h3">x</span>i<span class="p">)</span>,   fk <span class="s12">∈ F</span>,               <span class="p">(1)</span></p><p class="s14" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: center;">k<span class="s22">=1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">F { } → ∈</p><p style="padding-left: 81pt;text-indent: 0pt;text-align: left;"/><p class="s14" style="padding-left: 3pt;text-indent: 0pt;line-height: 6pt;text-align: center;">t</p><p class="s4" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark11" class="s64">model according to Eq. (</a>2<a href="#bookmark69" class="s64">). Second-order approximation can be used to quickly optimize the objective in the general setting [</a>12<span style=" color: #000;">].                                               </span><span class="s25">   </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 31pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s12">L</span><span class="s5">(</span><span class="s14">t</span><span class="s22">)  </span><span class="s12">  </span><span class="s26"> </span>[<span class="s13">l</span>(<span class="s13">y , y</span>ˆ(<span class="s13">t</span><span class="s12">−</span>1)) + <span class="s13">g f </span>(<b>x </b>) + <span class="s27">1</span> <span class="s13">h f </span>2(<b>x </b>)] + Ω(<span class="s13">f </span>)</p><p style="padding-left: 53pt;text-indent: 0pt;text-align: left;">where = <span class="s13">f </span>(<b>x</b>) = <span class="s13">wq</span>(<span class="s28">x</span>) (<span class="s13">q </span>: <span class="s15">R</span><span class="s16">m </span><span class="s13">T, w </span><span class="s15">R</span><span class="s16">T </span>) is the</p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 8pt;text-align: left;">space of regression trees (also known as CART). Here <span class="s13">q </span>rep-</p><p class="s14" style="text-indent: 0pt;line-height: 16pt;text-align: right;">i</p><p class="s14" style="padding-left: 53pt;text-indent: 0pt;line-height: 7pt;text-align: left;">i<span class="s22">=1</span></p><p class="s14" style="padding-left: 45pt;text-indent: 0pt;line-height: 16pt;text-align: left;">i t i</p><p class="s29" style="padding-left: 14pt;text-indent: 0pt;text-align: left;">2 <span class="s14">i t i t</span></p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 8pt;text-align: justify;">resents the structure of each tree that maps an example to</p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 2pt;text-align: justify;">the corresponding leaf index. <span class="s13">T </span>is the number of leaves in the</p><p style="text-indent: 0pt;line-height: 9pt;text-align: left;">will use the decision rules in the trees (given by <span class="s13">q</span>) to classify</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">tree. Each <span class="s13">fk </span>corresponds to an independent tree structure <span class="s13">q </span>and leaf weights <span class="s13">w</span>. Unlike decision trees, each regression tree contains a continuous score on each of the leaf, we use <span class="s23">w</span><span class="s14">i </span><span class="s19">to represent score on </span><span class="s13">i</span>-th leaf. For a given example, we</p><p class="s13" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="p">where  </span>gi  <span class="p">=  </span>∂ <span class="s30">(</span><span class="s31">t−</span><span class="s30">1) </span>l<span class="p">(</span>yi, y<span class="p">ˆ(</span>t<span class="s12">−</span><span class="p">1))  and  </span>hi  <span class="p">=  </span>∂<span class="p">2</span><span class="s32">(</span><span class="s31">t−     </span>l<span class="p">(</span>yi, y<span class="p">ˆ        )</span></p><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">y<span class="s22">ˆ</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">y<span class="s22">ˆ</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s30" style="text-indent: 0pt;line-height: 5pt;text-align: left;">1)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="text-indent: 0pt;line-height: 7pt;text-align: left;">(<span class="s14">t</span><span class="s24">−</span>1)</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 21pt;text-indent: 0pt;line-height: 1pt;text-align: left;">are first and second order gradient statistics on the loss func-</p><p style="padding-left: 21pt;text-indent: 0pt;text-align: left;">tion. We can remove the constant terms to obtain the fol- lowing simplified objective at step <span class="s13">t</span>.</p><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;line-height: 9pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">t</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-bottom: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: center;"><a name="bookmark12"><span class="s12">L</span></a><span class="s33">˜</span><span class="s5">(</span><span class="s14">t</span><span class="s22">)  </span>= <span class="s26"> </span>[<span class="s13">g f </span>(<b>x </b>) + <u>1</u> <span class="s13">h f </span>2(<b>x </b>)] + Ω(<span class="s13">f </span>)            (3)</p><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i<span class="s22">=1</span></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 6pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 316pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="315" height="1" alt="image" src="2939672.2939785/Image_005.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="264" height="119" alt="image" src="2939672.2939785/Image_006.jpg"/></span></p><h3 style="padding-bottom: 1pt;padding-left: 321pt;text-indent: 0pt;text-align: left;">Algorithm 1: <span class="p">Exact Greedy Algorithm for Split Finding</span></h3><p style="padding-left: 316pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="315" height="1" alt="image" src="2939672.2939785/Image_007.png"/></span></p><p style="padding-left: 325pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a name="bookmark13"><b>Input</b></a>: <span class="s13">I</span>, instance set of current node</p><p style="padding-left: 325pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><b>Input</b>: <span class="s13">d</span>, feature dimension</p><p class="s13" style="padding-left: 116pt;text-indent: 0pt;line-height: 12pt;text-align: center;">gain <span class="s12">←</span><span class="s34"> </span><span class="p">0</span></p><p class="s13" style="text-indent: 0pt;line-height: 11pt;text-align: left;">G <span class="s12">←</span></p><p class="s14" style="text-indent: 0pt;line-height: 80%;text-align: left;"><span class="s35">i</span><span class="s24">∈</span>I <span class="s23">g</span>i<span class="s19">,</span><span class="p"> </span><span class="s13">H </span><span class="s12">←</span></p><p class="s14" style="text-indent: 0pt;line-height: 12pt;text-align: left;">i<span class="s24">∈</span>I <span class="s36">h</span><span class="s37">i</span></p><p style="padding-left: 325pt;text-indent: 0pt;text-align: left;"><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">		</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 7pt;padding-left: 53pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a name="bookmark14">Figure 2: Structure Score Calculation. We only need to sum up the gradient and second order gra-</a></h3><p class="s13" style="padding-left: 30pt;text-indent: 0pt;line-height: 4pt;text-align: left;"><span class="h3">for </span>k <span class="p">= 1 </span><span class="s38">to </span>m <span class="h3">do</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="88" alt="image" src="2939672.2939785/Image_008.png"/></span></p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">← ←</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="padding-left: 45pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s23">G</span>L <span class="s19">0</span><span class="s13">, H</span>L <span class="s19">0</span></p><p class="s13" style="padding-left: 45pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="h3">for </span>j <span class="s11">in sorted(</span>I<span class="s11">, by </span><span class="h3">x</span>jk<span class="s11">) </span><span class="h3">do</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="46" alt="image" src="2939672.2939785/Image_009.png"/></span></p><p class="s13" style="padding-top: 1pt;padding-left: 59pt;text-indent: 0pt;line-height: 76%;text-align: left;"><span class="s23">G</span><span class="s14">L  </span><span class="s20">←</span><span class="s12"> </span>G<span class="s14">L </span><span class="s19">+</span><span class="p"> </span>g<span class="s14">j</span><span class="s23">,</span>  H<span class="s14">L  </span><span class="s20">←</span><span class="s12"> </span>H<span class="s14">L </span><span class="s19">+</span><span class="p"> </span>h<span class="s14">j </span><span class="s23">G</span><span class="s14">R  </span><span class="s20">←</span><span class="s12"> </span>G <span class="s12">− </span>G<span class="s14">L</span><span class="s23">,</span>  H<span class="s14">R  </span><span class="s20">←</span><span class="s12"> </span>H <span class="s12">− </span>H<span class="s14">L</span></p><p class="s39" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">G<span class="s30">2 </span>G<span class="s30">2 </span><span class="s40">G</span><span class="s32">2</span></p><p style="padding-left: 507pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="22" height="1" alt="image" src="2939672.2939785/Image_010.png"/></span></p><h3 style="padding-left: 116pt;text-indent: 0pt;text-align: center;">dient statistics on each leaf, then apply the scoring</h3><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><span class="s13">score </span><span class="s12">← </span>max(<span class="s13">score,</span><span class="s41"> </span><span class="s42">L</span><span class="s31"> </span>+<u> </u><span class="s42">R</span><span class="s31"> </span><span class="s12">− </span>)</p><h3 style="padding-left: 53pt;text-indent: 0pt;line-height: 3pt;text-align: left;">formula to get the quality score.</h3><h3 style="padding-left: 24pt;text-indent: 0pt;line-height: 2pt;text-align: right;">end</h3><h3 style="padding-top: 1pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">end</h3><p class="s14" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">H<span class="s44">L</span><span class="s22">+</span>λ</p><p class="s14" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">H<span class="s44">R</span><span class="s22">+</span>λ</p><p class="s14" style="padding-left: 11pt;text-indent: 0pt;line-height: 8pt;text-align: left;">H<span class="s22">+</span>λ</p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">{ | }</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: left;"><span class="s19">Define </span>I<span class="s14">j </span><span class="s19">= </span>i q<span class="p">(</span><span class="h3">x</span><span class="s14">i</span><span class="s19">) = </span>j <span class="p">as the instance set of leaf </span>j<a href="#bookmark12" class="s64">. We can rewrite Eq (</a><span class="s4">3</span><span class="p">) by expanding Ω as follows</span></p><h3 style="padding-bottom: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Output<span class="p">: Split with max score</span></h3><p style="padding-left: 21pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="319" height="1" alt="image" src="2939672.2939785/Image_011.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 316pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="319" height="1" alt="image" src="2939672.2939785/Image_012.png"/></span></p><p class="s14" style="padding-left: 97pt;text-indent: 0pt;line-height: 0pt;text-align: left;">n T</p><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;line-height: 9pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;line-height: 9pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">j</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: center;"><span class="s12">L</span><span class="s33">˜</span><span class="s5">(</span><span class="s14">t</span><span class="s22">)  </span>= <span class="s26"> </span>[<span class="s13">g f </span>(<b>x </b>) + <u>1</u> <span class="s13">h f </span>2(<b>x </b>)] + <span class="s13">γT </span>+ <u>1</u> <span class="s13">λ </span><span class="s26"> </span><span class="s15"> </span><span class="s13">w</span>2</p><p class="s25" style="text-indent: 0pt;line-height: 10pt;text-align: center;">  <b>Algorithm 2: </b>Approximate Algorithm for Split Finding    </p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark15"><span class="h3">for </span></a>k <span class="p">= 1 </span><span class="s38">to </span>m <span class="h3">do</span></p><p style="padding-left: 54pt;text-indent: 0pt;line-height: 9pt;text-align: left;"/><p class="s14" style="padding-left: 93pt;text-indent: 0pt;line-height: 4pt;text-align: left;">i<span class="s22">=1</span></p><p class="s15" style="text-indent: 0pt;line-height: 9pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s15" style="text-indent: 0pt;line-height: 9pt;text-align: left;">     </p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="padding-top: 4pt;padding-left: 97pt;text-indent: 0pt;line-height: 1pt;text-align: left;">T</p><p class="s14" style="padding-left: 24pt;text-indent: 0pt;line-height: 4pt;text-align: right;">j<span class="s22">=1</span></p><p style="padding-top: 1pt;padding-left: 22pt;text-indent: 0pt;line-height: 8pt;text-align: left;">(4)</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Propose <span class="s13">Sk</span></p><p style="padding-left: 1pt;text-indent: 0pt;line-height: 10pt;text-align: left;">= <span class="s12">{</span><span class="s13">sk</span>1</p><p class="s13" style="text-indent: 0pt;line-height: 10pt;text-align: left;">, sk<span class="p">2</span></p><p class="s13" style="text-indent: 0pt;line-height: 10pt;text-align: left;">, <span class="s12">· · · </span>skl</p><p style="text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s12">} </span>by percentiles on feature <span class="s13">k</span>.</p><p class="s19" style="padding-top: 6pt;padding-left: 83pt;text-indent: 0pt;text-align: left;">=<span class="p">       [(       </span><span class="s13">g</span><span class="s14">i</span>)<span class="s13">w</span><span class="s14">j</span></p><p class="s14" style="padding-top: 1pt;padding-left: 93pt;text-indent: 0pt;text-align: left;">j<span class="s22">=1    </span>i<span class="s24">∈</span>I<span class="s44">j</span></p><p style="padding-top: 1pt;text-indent: 0pt;line-height: 13pt;text-align: left;">+ <u>1</u> ( <span class="s13">h</span></p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2 <span class="s46">i</span></p><p class="s14" style="padding-left: 20pt;text-indent: 0pt;line-height: 7pt;text-align: left;">i<span class="s24">∈</span>I<span class="s44">j</span></p><p style="padding-top: 6pt;text-indent: 0pt;text-align: left;">+ <span class="s13">λ</span>)<span class="s13">w</span>2] + <span class="s13">γT</span></p><p style="padding-left: 85pt;text-indent: 0pt;text-align: left;">Proposal can be done per tree (global), or per split(local).</p><h3 style="padding-left: 71pt;text-indent: 0pt;line-height: 10pt;text-align: left;">end</h3><p class="s15" style="text-indent: 0pt;line-height: 9pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 71pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="h3">for </span>k <span class="p">= 1 </span><span class="s38">to </span>m <span class="h3">do</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="26" alt="image" src="2939672.2939785/Image_013.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="33" alt="image" src="2939672.2939785/Image_014.png"/></span></p><p style="padding-top: 4pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">For a fixed structure <span class="s13">q</span>(<b>x</b>), we can compute the optimal</p><p class="s14" style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s36">G</span><span class="s37">k</span>v  <span class="s47">←</span><span class="p">=     </span>j<span class="s24">∈{</span>j<span class="s24">|</span>s<span class="s44">k</span><span class="s31">,v </span><span class="s24">≥</span><span class="h4">x</span><span class="s44">j</span><span class="s31">k </span>&gt;s<span class="s44">k</span><span class="s31">,v−</span><span class="s30">1 </span><span class="s24">} </span><span class="s36">g</span><span class="s37">j</span></p><p style="text-indent: 0pt;line-height: 12pt;text-align: left;">weight <span class="s13">wj</span><span class="s12">∗ </span>of leaf <span class="s13">j </span>by</p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 12pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="69" height="1" alt="image" src="2939672.2939785/Image_015.png"/></span></p><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s23" style="text-indent: 0pt;line-height: 9pt;text-align: left;">h<span class="s14">i </span><span class="s19">+</span><span class="p"> </span><span class="s13">λ</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s12" style="padding-top: 6pt;padding-left: 129pt;text-indent: 0pt;text-align: left;"><span class="s13">w</span>∗ <span class="p">= </span>− <span class="s15"> </span><span class="s48"> </span><span class="s49">i</span><span class="s24">∈</span><span class="s14">I</span><span class="s50">j</span><span class="s31">  </span><span class="s51">g</span><span class="s52">i</span><span class="s14">      </span><span class="s13">,                        </span><span class="p">(5)</span></p><p class="s14" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i<span class="s24">∈</span>I<span class="s44">j</span></p><p style="padding-left: 171pt;text-indent: 0pt;line-height: 10pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 14pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Hkv <span class="s12">←</span><span class="p">=</span></p><h3 style="padding-top: 1pt;text-indent: 0pt;text-align: left;">end</h3><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 12pt;text-align: left;">j<span class="s24">∈{</span>j<span class="s24">|</span>s<span class="s44">k</span><span class="s31">,v </span><span class="s24">≥</span><span class="h4">x</span><span class="s44">j</span><span class="s31">k </span>&gt;s<span class="s44">k</span><span class="s31">,v−</span><span class="s30">1 </span><span class="s24">} </span><span class="s36">h</span><span class="s37">j</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-bottom: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Follow same step as in previous section to find max</p><p style="text-indent: 0pt;line-height: 9pt;text-align: left;">score only among proposed splits.</p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="315" height="1" alt="image" src="2939672.2939785/Image_016.png"/></span></p><p style="padding-top: 3pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">and calculate the corresponding optimal value by</p><p class="s25" style="text-indent: 0pt;line-height: 10pt;text-align: left;">   </p><p style="text-indent: 0pt;text-align: left;"/><p class="s53" style="padding-top: 7pt;padding-left: 146pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><a name="bookmark16">    </a><span class="s22">    </span><span class="s14">T</span><span class="s54"> </span><span class="s55">2</span>  </p><p class="s4" style="padding-top: 3pt;padding-left: 93pt;text-indent: 0pt;line-height: 6pt;text-align: left;">13<a href="#bookmark24" class="s64">], It is implemented in a commercial software TreeNet </a><a href="#bookmark24">4</a></p><p class="s22" style="padding-top: 4pt;padding-left: 24pt;text-indent: 0pt;line-height: 4pt;text-align: right;"><span class="s56">˜</span>(<span class="s14">t</span>)</p><p class="s56" style="padding-left: 30pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1<span class="p"> (</span></p><p class="s14" style="padding-left: 7pt;text-indent: 0pt;line-height: 8pt;text-align: left;">i<span class="s24">∈</span>I<span class="s44">j</span><span class="s31">  </span><span class="s36">g</span><span class="s37">i</span><span class="s33">)</span></p><p style="padding-top: 4pt;padding-left: 100pt;text-indent: 0pt;line-height: 4pt;text-align: left;">for gradient boosting, but is not implemented in existing</p><p style="padding-left: 98pt;text-indent: 0pt;line-height: 70%;text-align: left;"><span class="s12">L </span>(<span class="s13">q</span>) = <span class="s12">− </span><span class="s57">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-top: 4pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">j<span class="s22">=1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">i<span class="s24">∈</span>I<span class="s44">j</span></p><p style="padding-left: 29pt;text-indent: 0pt;line-height: 7pt;text-align: left;">+ <span class="s13">γT. </span>(6)</p><p class="s23" style="text-indent: 0pt;line-height: 9pt;text-align: left;">h<span class="s14">i </span><span class="s19">+ </span><span class="s13">λ</span></p><p style="padding-top: 6pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">opensource packages. According to user feedback, using col- umn sub-sampling prevents over-fitting even more so than</p><p class="s4" style="padding-left: 62pt;text-indent: 0pt;line-height: 10pt;text-align: justify;"><a href="#bookmark16" class="s64">Eq (</a>6<span style=" color: #000;">) can be used as a scoring function to measure the</span></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">quality of a tree structure <span class="s13">q</span><a href="#bookmark14" class="s64">. This score is like the impurity score for evaluating decision trees, except that it is derived for a wider range of objective functions. Fig. </a><span style=" color: #001472;">2 </span>illustrates how this score can be calculated.</p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">∪</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 53pt;text-indent: 8pt;text-align: justify;"><span class="p">Normally it is impossible to enumerate all the possible tree structures </span>q<span class="p">. A greedy algorithm that starts from a single leaf and iteratively adds branches to the tree is used </span><span class="s19">instead. Assume </span><span class="p">that </span>I<span class="s14">L </span><span class="s19">and </span>I<span class="s14">R </span><span class="s19">are the instance sets of left and right nodes after the split. Lettting </span>I <span class="p">= </span>I<span class="s14">L </span><span class="s23">I</span><span class="s14">R</span><span class="s19">, </span><span class="p">then the loss reduction after the split is given by</span></p><p style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computa- tions of the parallel algorithm described later.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h2 style="padding-left: 42pt;text-indent: -20pt;text-align: left;"><a name="bookmark17">SPLIT FINDING ALGORITHMS</a><a name="bookmark22">&zwnj;</a></h2><ol id="l4"><li><h2 style="padding-top: 7pt;padding-left: 48pt;text-indent: -26pt;text-align: left;"><a name="bookmark18">Basic Exact Greedy Algorithm</a></h2><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 21pt;text-indent: 8pt;line-height: 11pt;text-align: justify;"><a href="#bookmark23" class="s64">One of the key problems in tree learning is to find the best split as indicated by Eq (</a><span style=" color: #001472;">7</span>). In order to do so, a s- plit finding algorithm enumerates over all the possible splits on all the features. We call this the <span class="s11">exact greedy algorithm</span>.</p><p class="s58" style="padding-left: 24pt;text-indent: 0pt;line-height: 22%;text-align: right;"><a name="bookmark23">1</a><span class="s19"> </span><span class="s57">(</span><span class="s15"> </span></p><p class="s14" style="padding-top: 4pt;text-indent: 0pt;line-height: 4pt;text-align: left;">i<span class="s24">∈</span>I<span class="s44">L</span></p><p class="s23" style="text-indent: 0pt;line-height: 9pt;text-align: left;">g<span class="s14">i</span><span class="s19">)</span><span class="p">2</span></p><p class="s14" style="padding-left: 18pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s33">(</span><span class="s59"> </span>i<span class="s24">∈</span>I<span class="s44">R</span></p><p class="s23" style="text-indent: 0pt;line-height: 9pt;text-align: left;">g<span class="s14">i</span><span class="s19">)</span><span class="p">2</span></p><p class="s14" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="s33">(</span><span class="s59"> </span>i<span class="s24">∈</span>I</p><p class="s23" style="padding-top: 1pt;text-indent: 0pt;line-height: 8pt;text-align: left;">g<span class="s14">i</span><span class="s19">)</span><span class="p">2  </span><span class="s60"> </span></p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Most existing single machine tree boosting implementation-</p><p style="text-indent: 0pt;text-align: left;"><span><img width="71" height="1" alt="image" src="2939672.2939785/Image_017.png"/></span></p><p class="s20" style="padding-left: 53pt;text-indent: 0pt;line-height: 87%;text-align: left;">L<span class="s14">split </span><span class="s19">= </span><span class="s61">2</span></p><p class="s14" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">i<span class="s24">∈</span>I<span class="s44">L </span><span class="s36">h</span><span class="s37">i</span></p><p style="text-indent: 0pt;line-height: 16pt;text-align: left;">+ <span class="s13">λ </span><span class="s27">+</span> <span class="s62"> </span></p><p class="s14" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">i<span class="s24">∈</span>I<span class="s44">R</span><span class="s31"> </span><span class="s36">h</span><span class="s37">i</span></p><p style="text-indent: 0pt;line-height: 16pt;text-align: left;">+ <span class="s13">λ </span><span class="s63">−</span><span class="s12"> </span><span class="s62"> </span></p><p class="s13" style="text-indent: 0pt;line-height: 7pt;text-align: right;">γ</p><p style="text-indent: 0pt;text-align: left;"><span><img width="71" height="1" alt="image" src="2939672.2939785/Image_018.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="65" height="1" alt="image" src="2939672.2939785/Image_019.png"/></span></p><p class="s14" style="text-indent: 0pt;line-height: 85%;text-align: left;"><span class="s35">i</span><span class="s24">∈</span>I <span class="s23">h</span>i <span class="s19">+ </span><span class="s13">λ</span></p><p class="s4" style="padding-top: 1pt;padding-left: 10pt;text-indent: 0pt;text-align: left;"><a href="#bookmark88" class="s64">s, such as scikit-learn [</a>20<a href="#bookmark89" class="s64">], R’s gbm [</a>21<span style=" color: #000;">] as well as the single machine version of XGBoost support the exact greedy algo-</span></p><p style="padding-left: 281pt;text-indent: 0pt;line-height: 6pt;text-align: left;">(7)</p><p style="padding-left: 53pt;text-indent: 0pt;text-align: left;">This formula is usually used in practice for evaluating the split candidates.</p><h2 style="padding-top: 7pt;padding-left: 53pt;text-indent: 0pt;text-align: left;"><a name="bookmark7">2.3 Shrinkage and Column Subsampling</a></h2><p class="s4" style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark9" class="s64">Besides the regularized objective mentioned in Sec. </a>2.1<a href="#bookmark68" class="s64">, two additional techniques are used to further prevent over- fitting. The first technique is shrinkage introduced by Fried- man [</a>11<span style=" color: #000;">]. Shrinkage scales newly added weights by a factor </span><span class="s13">η </span><span style=" color: #000;">after each step of tree boosting. Similar to a learning rate in tochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to im-</span></p><p class="s4" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark13" class="s64">rithm. The exact greedy algorithm is shown in Alg. </a>1<a href="#bookmark23" class="s64">. It is computationally demanding to enumerate all the possible splits for continuous features. In order to do so efficiently, the algorithm must first sort the data according to feature values and visit the data in sorted order to accumulate the gradient statistics for the structure score in Eq (</a>7<span style=" color: #000;">).</span></p></li><li><h2 style="padding-top: 6pt;padding-left: 48pt;text-indent: -26pt;text-align: left;"><a name="bookmark19">Approximate Algorithm</a></h2><p style="padding-top: 1pt;padding-left: 21pt;text-indent: 8pt;line-height: 11pt;text-align: justify;">The exact greedy algorithm is very powerful since it enu- merates over all possible splitting points greedily. However, it is impossible to efficiently do so when the data does not fit entirely into memory. Same problem also arises in the dis-</p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><a name="bookmark24">prove the model. The second technique is column (feature)           </a><u>                                                      </u></p><p class="s4" style="padding-left: 53pt;text-indent: 0pt;text-align: left;"><a href="#bookmark77" class="s64">subsampling. This technique is used in RandomForest [</a>4<span style=" color: #000;">,</span></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 10pt;text-align: left;">4<a href="http://www.salford-systems.com/products/treenet" class="s64" target="_blank">https://www.salford-systems.com/products/treenet</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="200" height="149" alt="image" src="2939672.2939785/Image_020.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s65" style="padding-left: 78pt;text-indent: 0pt;text-align: left;"> <span class="s66">       </span><span class="s67">    </span><span class="s68">exact greedy</span></p><p class="s68" style="padding-left: 95pt;text-indent: 0pt;line-height: 10pt;text-align: left;">global eps=0.3 local eps=0.3</p><p class="s69" style="padding-top: 2pt;padding-left: 78pt;text-indent: 0pt;text-align: left;"> <span class="s66">       </span><span class="s67">    </span><span class="s68">global eps=0.05</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s70" style="padding-top: 3pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">0<span class="s71">.</span>83</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s70" style="padding-left: 94pt;text-indent: 0pt;text-align: left;">0<span class="s71">.</span>82</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s70" style="padding-left: 94pt;text-indent: 0pt;text-align: left;">0<span class="s71">.</span>81</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Test AUC</p><p style="text-indent: 0pt;text-align: left;"/><p class="s70" style="padding-left: 94pt;text-indent: 0pt;text-align: left;">0<span class="s71">.</span>80</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s70" style="padding-left: 94pt;text-indent: 0pt;text-align: left;">0<span class="s71">.</span>79</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s70" style="padding-left: 94pt;text-indent: 0pt;text-align: left;">0<span class="s71">.</span>78</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s70" style="padding-left: 94pt;text-indent: 0pt;text-align: left;">0<span class="s71">.</span>77</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s70" style="padding-left: 94pt;text-indent: 0pt;text-align: left;">0<span class="s71">.</span>76</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s70" style="padding-left: 94pt;text-indent: 0pt;line-height: 5pt;text-align: left;">0<span class="s71">.</span>75</p><p class="s70" style="padding-left: 116pt;text-indent: 0pt;line-height: 4pt;text-align: center;">0          10         20         30         40         50         60         70         80         90</p><p class="s72" style="padding-left: 116pt;text-indent: 0pt;line-height: 6pt;text-align: center;">Number of Iterations</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 53pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark25">Figure 3: Comparison of test AUC convergence on Higgs 10M dataset. The eps parameter corresponds to the accuracy of the approximate sketch. This</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: left;"><span><img width="236" height="95" alt="image" src="2939672.2939785/Image_021.jpg"/></span></p><h3 style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a name="bookmark26">Figure 4: Tree structure with default directions. An example will be classified into the default direction when the feature needed for the split is missing.</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">D { · · · }</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 21pt;text-indent: 0pt;text-align: left;">ta. Formally, let multi-set <span class="s13">k </span>= (<span class="s13">x</span>1<span class="s13">k, h</span>1)<span class="s13">, </span>(<span class="s13">x</span>2<span class="s13">k, h</span>2) (<span class="s13">xnk, hn</span>) represent the <span class="s13">k</span>-th feature values and second order gradient statistics of each training instances. We can define a rank functions <span class="s13">rk </span>: <span class="s15">R </span><span class="s12">→ </span>[0<span class="s13">, </span>+<span class="s12">∞</span>) as</p><h3 style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">roughly translates to 1 / eps buckets in the proposal. We find that local proposals require fewer buckets, because it refine split candidates.</h3><p class="s13" style="padding-top: 2pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">rk<span class="p">(</span>z<span class="p">) =  </span><span class="s15"> </span></p><p style="padding-left: 10pt;text-indent: 0pt;line-height: 7pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="65" height="1" alt="image" src="2939672.2939785/Image_022.png"/></span></p><p class="s22" style="padding-top: 1pt;text-indent: 0pt;text-align: left;">(<span class="s14">x,h</span>)<span class="s24">∈D</span><span class="s44">k </span><span class="s36">h</span></p><p class="s22" style="padding-top: 15pt;text-indent: 0pt;text-align: left;">(<span class="s14">x,h</span>)<span class="s24">∈</span><span class="s73"> </span><span class="s24">D</span><span class="s44">k</span><span class="s31"> </span><span class="s14">,x&lt;z</span></p><p class="s13" style="padding-top: 2pt;text-indent: 0pt;text-align: left;">h, <span class="p">(8)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">tributed setting. To support effective gradient tree boosting in these two settings, an approximate algorithm is needed.</p><p class="s4" style="padding-bottom: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark85" class="s64">We summarize an approximate framework, which resem- bles the ideas proposed in past literatures [</a>17<a href="#bookmark75" class="s64">, </a>2<a href="#bookmark90" class="s64">, </a>22<a href="#bookmark15" class="s64">], in Alg. </a>2<a href="#bookmark27" class="s64">. To summarize, the algorithm first proposes candi- date splitting points according to percentiles of feature dis- tribution (a specific criteria will be given in Sec. </a>3.3<span style=" color: #000;">). The algorithm then maps the continuous features into bucket- s split by these candidate points, aggregates the statistics and finds the best solution among proposals based on the aggregated statistics.</span></p><p style="text-indent: 0pt;line-height: 9pt;text-align: left;">There are two variants of the algorithm, depending on</p><p style="padding-left: 62pt;text-indent: 0pt;line-height: 9pt;text-align: left;"/><p style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">when the proposal is given. The global variant proposes all the candidate splits during the initial phase of tree construc- tion, and uses the same proposals for split finding at all level-</p><p class="s4" style="padding-left: 53pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark25" class="s64">s. The local variant re-proposes after each split. The global method requires less proposal steps than the local method. However, usually more candidate points are needed for the global proposal because candidates are not refined after each split. The local proposal refines the candidates after splits, and can potentially be more appropriate for deeper trees. A comparison of different algorithms on a Higgs boson dataset is given by Fig. </a>3<span style=" color: #000;">. We find that the local proposal indeed requires fewer candidates. The global proposal can be as accurate as the local one given enough candidates.</span></p><p class="s4" style="padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark90" class="s64">Most existing approximate algorithms for distributed tree learning also follow this framework. Notably, it is also possi- ble to directly construct approximate histograms of gradient statistics [</a>22<a href="#bookmark85" class="s64">]. It is also possible to use other variants of bin- ning strategies instead of quantile [</a>17<a href="#bookmark25" class="s64">]. Quantile strategy benefit from being distributable and recomputable, which we will detail in next subsection. From Fig. </a>3<span style=" color: #000;">, we also find that the quantile strategy can get the same accuracy as exact greedy given reasonable approximation level.</span></p><p style="padding-left: 53pt;text-indent: 8pt;text-align: justify;">Our system efficiently supports exact greedy for the single machine setting, as well as approximate algorithm with both local and global proposal methods for all settings. Users can freely choose between the methods according to their needs.</p></li><li><h2 style="padding-top: 7pt;padding-left: 80pt;text-indent: -26pt;text-align: justify;"><a name="bookmark20">Weighted Quantile Sketch</a><a name="bookmark27">&zwnj;</a></h2><p style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;">One important step in the approximate algorithm is to propose candidate split points. Usually percentiles of a fea- ture are used to make candidates distribute evenly on the da-</p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">which represents the proportion of instances whose feature</p><p class="s13" style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;line-height: 87%;text-align: left;"><span class="p">value </span>k <span class="p">is smaller than </span>z<span class="p">. The goal is to find candidate split points </span><span class="s12">{</span>sk<span class="p">1</span>, sk<span class="p">2</span>, <span class="s12">· · · </span>skl<span class="s12">}</span><span class="p">, such that</span></p><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><span class="s12">|</span>rk<span class="p">(</span>sk,j<span class="p">) </span><span class="s12">− </span>rk<span class="p">(</span>sk,j<span class="p">+1)</span><span class="s12">| </span>&lt; E, sk<span class="p">1 = min </span><span class="h3">x</span>ik, skl <span class="p">= max </span><span class="h3">x</span>ik.</p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: right;">(9)</p><p class="s13" style="padding-left: 21pt;text-indent: 3pt;text-align: justify;"><span class="p">Here </span>E <span class="p">is an approximation factor. Intuitively, this means that there is roughly 1</span>/E <span class="p">candidate points. Here each data </span><span class="s19">point is weighted by </span>h<span class="s14">i</span><span class="s19">. To see why </span>h<span class="s14">i </span><span class="s19">represents the weight, </span><a href="#bookmark12" class="s64">we can rewrite Eq (</a><span class="s4">3</span><span class="p">) as</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;line-height: 9pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">t</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-bottom: 1pt;text-indent: 0pt;text-align: center;"><span class="s26"> </span><span class="s15"> </span><u>1</u> <span class="s13">h </span>(<span class="s13">f </span>(<b>x </b>) <span class="s12">− </span><span class="s13">g /h </span>)2 + Ω(<span class="s13">f </span>) + <span class="s13">constant,</span></p><p class="s14" style="text-indent: 0pt;line-height: 6pt;text-align: left;">i<span class="s22">=1</span></p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 6pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 24pt;text-indent: 0pt;text-align: right;"><span class="s19">which is exactly weighted squared loss with labels </span><span class="s13">g</span><span class="s14">i</span><span class="s23">/h</span><span class="s14">i </span><span class="s19">and weights </span><span class="s13">h</span><span class="s14">i</span><span class="s19">. For large datasets, it is non-trivial to find can- </span><a href="#bookmark82" class="s64">didate splits that satisfy the criteria. When every instance has equal weights, an existing algorithm called quantile s- ketch [</a><span style=" color: #001472;">14</span><a href="#bookmark92" class="s64">, </a><span style=" color: #001472;">24</span>] solves the problem. However, there is no existing quantile sketch for the weighted datasets. There- fore, most existing approximate algorithms either resorted to sorting on a random subset of data which have a chance of failure or heuristics that do not have theoretical guarantee. To solve this problem, we introduced a novel distributed weighted quantile sketch algorithm that can handle weighted data with a <span class="s11">provable theoretical guarantee</span>. The general idea is to propose a data structure that supports <span class="s11">merge </span>and <span class="s11">prune </span><a href="#bookmark29" class="s64">operations, with each operation proven to maintain a certain accuracy level. A detailed description of the algorithm as well as proofs are given in the supplementary material</a><span style=" color: #001472;">5</span>(link</p><p style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">in the footnote).</p></li><li><h2 style="padding-top: 7pt;padding-left: 48pt;text-indent: -26pt;text-align: left;"><a name="bookmark21">Sparsity-aware Split Finding</a><a name="bookmark28">&zwnj;</a></h2><p style="padding-top: 1pt;padding-bottom: 4pt;padding-left: 21pt;text-indent: 8pt;text-align: justify;">In many real-world problems, it is quite common for the input <b>x </b><a href="#bookmark26" class="s64">to be sparse. There are multiple possible causes for sparsity: 1) presence of missing values in the data; 2) frequent zero entries in the statistics; and, 3) artifacts of feature engineering such as one-hot encoding. It is impor- tant to make the algorithm aware of the sparsity pattern in the data. In order to do so, we propose to add a default direction in each tree node, which is shown in Fig. </a><span style=" color: #001472;">4</span>. When a value is missing in the sparse matrix <b>x</b>, the instance is</p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="160" height="1" alt="image" src="2939672.2939785/Image_023.png"/></span></p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark29">5</a><span class="p">Link to the supplementary material</span></p><p style="padding-left: 82pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a href="http://homes.cs.washington.edu/%7Etqchen/pdf/xgboost-supp.pdf" class="s10">http://homes.cs.washington.edu/˜tqchen/pdf/xgboost-supp.pdf</a></p><p style="padding-left: 107pt;text-indent: 0pt;text-align: left;"><span><img width="521" height="132" alt="image" src="2939672.2939785/Image_024.jpg"/></span></p><h3 style="padding-top: 5pt;padding-left: 53pt;text-indent: 0pt;text-align: left;"><a name="bookmark30">Figure 6: Block structure for parallel learning. Each column in a block is sorted by the corresponding feature value. A linear scan over one column in the block is sufficient to enumerate all the split points.</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="317" height="1" alt="image" src="2939672.2939785/Image_025.png"/></span></p><h3 style="padding-bottom: 1pt;padding-left: 58pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Algorithm 3: <span class="p">Sparsity-aware Split Finding</span></h3><p style="padding-left: 53pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="317" height="1" alt="image" src="2939672.2939785/Image_026.png"/></span></p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">{ ∈ | / }</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 62pt;text-indent: 0pt;text-align: left;"><a name="bookmark31"><b>Input</b></a>: <span class="s13">I</span>, instance set of current node <b>Input</b>: <span class="s13">Ik </span>= <span class="s13">i I xik </span>= missing <b>Input</b>: <span class="s13">d</span>, feature dimension</p><p class="s11" style="padding-left: 62pt;text-indent: 0pt;text-align: left;">Also applies to the approximate setting, only collect statistics of non-missing entries into buckets</p><p class="s13" style="padding-left: 62pt;text-indent: 0pt;line-height: 12pt;text-align: left;">gain <span class="s12">←</span><span class="s34"> </span><span class="p">0</span></p><p class="s13" style="text-indent: 0pt;line-height: 11pt;text-align: left;">G <span class="s12">←</span></p><p class="s14" style="text-indent: 0pt;line-height: 80%;text-align: left;"><span class="s35">i</span><span class="s24">∈</span>I<span class="s23">,</span><span class="s13"> g</span>i<span class="s19">,</span><span class="s13">H </span><span class="s12">←</span></p><p class="s14" style="text-indent: 0pt;line-height: 12pt;text-align: left;">i<span class="s24">∈</span>I <span class="s36">h</span><span class="s37">i</span></p><p style="padding-left: 62pt;text-indent: 0pt;text-align: left;"><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">		</span></p><p class="s74" style="padding-left: 58pt;text-indent: 0pt;text-align: left;">32</p><p style="text-indent: 0pt;text-align: left;"><span><img width="135" height="34" alt="image" src="2939672.2939785/Image_027.png"/></span></p><p class="s75" style="padding-left: 14pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="0" height="2" alt="image" src="2939672.2939785/Image_028.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_029.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_030.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_031.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_032.png"/></span></p><p class="s74" style="padding-top: 4pt;padding-left: 50pt;text-indent: 0pt;text-align: center;">Basic algorithm</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_033.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_034.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_035.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_036.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_037.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_038.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_039.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_040.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_041.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_042.png"/></span></p><p class="s74" style="padding-top: 2pt;padding-left: 51pt;text-indent: 0pt;text-align: center;">Sparsity aware algorithm</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="135" height="54" alt="image" src="2939672.2939785/Image_043.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_044.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_045.png"/></span></p><p class="s74" style="padding-top: 4pt;padding-left: 58pt;text-indent: 0pt;text-align: left;">16</p><p class="s74" style="padding-top: 4pt;padding-left: 60pt;text-indent: 0pt;text-align: left;">8</p><p class="s74" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Tree(sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s74" style="padding-top: 4pt;padding-left: 60pt;text-indent: 0pt;text-align: left;">4</p><p class="s74" style="padding-top: 4pt;padding-left: 60pt;text-indent: 0pt;text-align: left;">2</p><p class="s74" style="padding-top: 4pt;padding-left: 60pt;text-indent: 0pt;text-align: left;">1</p><p class="s74" style="padding-top: 4pt;padding-left: 56pt;text-indent: 0pt;text-align: left;">0.5</p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_046.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_047.png"/></span></p><p class="s74" style="padding-top: 4pt;padding-bottom: 4pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">0.25</p><p class="s74" style="text-indent: 0pt;text-align: left;">0.125</p><p class="s67" style="padding-left: 50pt;text-indent: 0pt;line-height: 6pt;text-align: left;"> <span><img width="2" height="0" alt="image" src="2939672.2939785/Image_048.png"/></span><span class="s76">	</span><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_049.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="padding-left: 65pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_050.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_051.png"/></span></p><p class="s13" style="padding-left: 62pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span class="h3">for </span>k <span class="p">= 1 </span><span class="s38">to </span>m <span class="h3">do</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="205" alt="image" src="2939672.2939785/Image_052.png"/></span></p><p class="s11" style="padding-left: 76pt;text-indent: 0pt;line-height: 9pt;text-align: left;">// enumerate missing value goto right</p><p class="s14" style="padding-left: 76pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s23">G</span>L <span class="s20">← </span><span class="p">0</span><span class="s13">, H</span>L <span class="s20">← </span><span class="p">0</span></p><p class="s74" style="padding-left: 65pt;text-indent: 0pt;line-height: 2pt;text-align: left;">0.0625</p><p class="s74" style="padding-top: 4pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">0.03125</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="padding-left: 11pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="0" height="2" alt="image" src="2939672.2939785/Image_053.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_054.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_055.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_056.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_057.png"/></span></p><p class="s74" style="padding-left: 19pt;text-indent: 0pt;line-height: 6pt;text-align: center;">1               2               4               8             16</p><p class="s74" style="padding-left: 17pt;text-indent: 0pt;line-height: 6pt;text-align: center;">Number of Threads</p><p class="s13" style="padding-left: 76pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="h3">for </span>j <span class="s11">in sorted(</span>Ik<span class="s11">, ascent order by </span><span class="h3">x</span>jk<span class="s11">) </span><span class="h3">do</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="46" alt="image" src="2939672.2939785/Image_058.png"/></span></p><p class="s14" style="padding-left: 91pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s23">G</span>L  <span class="s20">←</span><span class="s12"> </span><span class="s13">G</span>L <span class="s19">+</span><span class="p"> </span><span class="s13">g</span>j<span class="s23">,</span><span class="s13">  H</span>L  <span class="s20">←</span><span class="s12"> </span><span class="s13">H</span>L <span class="s19">+</span><span class="p"> </span><span class="s13">h</span>j</p><p class="s13" style="padding-left: 91pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s23">G</span><span class="s14">R </span><span class="s20">← </span>G <span class="s12">− </span>G<span class="s14">L</span><span class="s23">, H</span><span class="s14">R </span><span class="s20">← </span>H <span class="s12">− </span>H<span class="s14">L </span><span class="s77">2</span></p><h3 style="padding-top: 3pt;padding-left: 64pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark32">Figure 5: Impact of the sparsity aware algorithm on Allstate-10K. The dataset is sparse mainly due to one-hot encoding. The sparsity aware algorithm</a></h3><p class="s14" style="padding-bottom: 1pt;padding-left: 179pt;text-indent: 0pt;text-align: left;">G<span class="s78">2 </span>G <span class="s35">G</span><span class="s79">2</span></p><p style="padding-left: 244pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="22" height="1" alt="image" src="2939672.2939785/Image_059.png"/></span></p><p style="padding-left: 91pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span class="s13">score </span><span class="s12">← </span>max(<span class="s13">score,</span><span class="s41"> </span><span class="s42">L</span><span class="s31"> </span>+<u> </u><span class="s42">R</span><span class="s31"> </span><span class="s12">− </span>)</p><h3 style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;text-align: right;">end</h3><p class="s14" style="padding-left: 24pt;text-indent: 0pt;line-height: 39%;text-align: right;">H<span class="s44">L</span><span class="s22">+</span>λ</p><p class="s14" style="padding-left: 11pt;text-indent: 0pt;line-height: 39%;text-align: left;">H<span class="s44">R</span><span class="s22">+</span>λ</p><p class="s14" style="padding-left: 11pt;text-indent: 0pt;line-height: 4pt;text-align: left;">H<span class="s22">+</span>λ</p><h3 style="padding-left: 53pt;text-indent: 0pt;line-height: 4pt;text-align: left;">is more than 50 times faster than the naive version</h3><h3 style="padding-left: 53pt;text-indent: 0pt;text-align: left;">that does not take sparsity into consideration.</h3><p class="s11" style="padding-left: 76pt;text-indent: 0pt;line-height: 10pt;text-align: left;">// enumerate missing value goto left</p><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">← ←</p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="padding-left: 76pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s23">G</span>R <span class="s19">0</span><span class="s13">, H</span>R <span class="s19">0</span></p><p class="s13" style="padding-left: 76pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="h3">for </span>j <span class="s11">in sorted(</span>Ik<span class="s11">, descent order by </span><span class="h3">x</span>jk<span class="s11">) </span><span class="h3">do</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="1" height="46" alt="image" src="2939672.2939785/Image_060.png"/></span></p><p class="s14" style="padding-left: 91pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s23">G</span>R  <span class="s20">←</span><span class="s12"> </span><span class="s13">G</span>R <span class="s19">+</span><span class="p"> </span><span class="s13">g</span>j<span class="s23">,</span><span class="s13">  H</span>R  <span class="s20">←</span><span class="s12"> </span><span class="s13">H</span>R <span class="s19">+</span><span class="p"> </span><span class="s13">h</span>j</p><p class="s13" style="padding-left: 91pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s23">G</span><span class="s14">L </span><span class="s20">← </span>G <span class="s12">− </span>G<span class="s14">R</span><span class="s23">, H</span><span class="s14">L </span><span class="s20">← </span>H <span class="s12">− </span>H<span class="s14">R </span><span class="s77">2</span></p></li></ol></li><li><h2 style="padding-top: 8pt;padding-left: 81pt;text-indent: -20pt;text-align: left;"><a name="bookmark33">SYSTEM DESIGN</a><a name="bookmark37">&zwnj;</a></h2><ol id="l5"><li><h2 style="padding-top: 7pt;padding-left: 87pt;text-indent: -26pt;text-align: left;"><a name="bookmark34">Column Block for Parallel Learning</a></h2><p style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 5pt;text-align: center;">The most time consuming part of tree learning is to get</p><p class="s14" style="padding-bottom: 4pt;padding-left: 179pt;text-indent: 0pt;line-height: 4pt;text-align: left;">G<span class="s78">2 </span>G <span class="s35">G</span><span class="s79">2</span></p><p style="padding-left: 244pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="22" height="1" alt="image" src="2939672.2939785/Image_061.png"/></span></p><p style="padding-left: 91pt;text-indent: 0pt;text-align: left;"><span class="s13">score </span><span class="s12">← </span>max(<span class="s13">score,</span><span class="s41"> </span><span class="s42">L</span><span class="s31"> </span>+<u> </u><span class="s42">R</span><span class="s31"> </span><span class="s12">− </span>)</p><p style="padding-left: 112pt;text-indent: 0pt;text-align: center;">the data into sorted order.  In order to reduce the cost of</p><h3 style="padding-left: 24pt;text-indent: 0pt;line-height: 2pt;text-align: right;">end</h3><h3 style="padding-top: 1pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">end</h3><p class="s14" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">H<span class="s44">L</span><span class="s22">+</span>λ</p><p class="s14" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">H<span class="s44">R</span><span class="s22">+</span>λ</p><p class="s14" style="padding-left: 11pt;text-indent: 0pt;line-height: 8pt;text-align: left;">H<span class="s22">+</span>λ</p><p style="padding-left: 116pt;text-indent: 0pt;line-height: 3pt;text-align: center;">sorting, we propose to store the data in in-memory units,</p><p style="padding-left: 116pt;text-indent: 0pt;text-align: center;">which we called <span class="s11">block</span>.  Data in each block is stored in the</p><h3 style="padding-bottom: 1pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">Output<span class="p">: Split and default directions with max gain</span></h3><p style="padding-left: 53pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="317" height="1" alt="image" src="2939672.2939785/Image_062.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark31" class="s64">classified into the default direction. There are two choices of default direction in each branch. The optimal default di- rections are learnt from the data. The algorithm is shown in Alg. </a><span style=" color: #001472;">3</span>. The key improvement is to only visit the non-missing entries <span class="s13">Ik</span>. The presented algorithm treats the non-presence as a missing value and learns the best direction to handle missing values. The same algorithm can also be applied when the non-presence corresponds to a user specified value by limiting the enumeration only to consistent solutions.</p><p class="s4" style="padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark32" class="s64">To the best of our knowledge, most existing tree learning algorithms are either only optimized for dense data, or need specific procedures to handle limited cases such as categor- ical encoding. XGBoost handles all sparsity patterns in a unified way. More importantly, our method exploits the s- parsity to make computation complexity linear to number of non-missing entries in the input. Fig. </a>5 <a href="#bookmark51" class="s64">shows the com- parison of sparsity aware and a naive implementation on an Allstate-10K dataset (description of dataset given in Sec. </a>6<span style=" color: #000;">). We find that the sparsity aware algorithm runs 50 times faster than the naive version. This confirms the importance of the sparsity aware algorithm.</span></p><p style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">compressed column (CSC) format, with each column sorted by the corresponding feature value. This input data layout only needs to be computed once before training, and can be reused in later iterations.</p><p class="s4" style="padding-left: 21pt;text-indent: 8pt;text-align: right;"><a href="#bookmark30" class="s64">In the exact greedy algorithm, we store the entire dataset in a single block and run the split search algorithm by lin- early scanning over the pre-sorted entries. We do the split finding of all leaves collectively, so one scan over the block will collect the statistics of the split candidates in all leaf branches. Fig. </a>6 <span style=" color: #000;">shows how we transform a dataset into the format and find the optimal split using the block structure. The block structure also helps when using the approxi- mate algorithms. Multiple blocks can be used in this case,</span></p><p style="padding-left: 21pt;text-indent: 0pt;text-align: left;">with each block corresponding to subset of rows in the dataset. Different blocks can be distributed across machines, or s- tored on disk in the out-of-core setting. Using the sorted structure, the quantile finding step becomes a <span class="s11">linear scan </span>over the sorted columns. This is especially valuable for lo- cal proposal algorithms, where candidates are generated fre- quently at each branch. The binary search in histogram ag- gregation also becomes a linear time merge style algorithm.</p><p style="padding-left: 21pt;text-indent: 8pt;text-align: justify;">Collecting statistics for each column can be <span class="s11">parallelized</span>, giving us a parallel algorithm for split finding. Importantly, the column block structure also supports column subsam- pling, as it is easy to select a subset of columns in a block.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="124" height="102" alt="image" src="2939672.2939785/Image_063.png"/></span></p><p class="s74" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Basic algorithm</p><p class="s74" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Cache-aware algorithm</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="124" height="102" alt="image" src="2939672.2939785/Image_064.png"/></span></p><p class="s74" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Basic algorithm</p><p class="s74" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Cache-aware algorithm</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="122" height="102" alt="image" src="2939672.2939785/Image_065.png"/></span></p><p class="s74" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Basic algorithm</p><p class="s74" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Cache-aware algorithm</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="122" height="102" alt="image" src="2939672.2939785/Image_066.png"/></span></p><p class="s74" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Basic algorithm</p><p class="s74" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Cache-aware algorithm</p><p style="text-indent: 0pt;text-align: left;"/><p class="s80" style="padding-left: 70pt;text-indent: 0pt;text-align: left;">128 256 8 8</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 195pt;text-indent: 0pt;line-height: 4pt;text-align: left;">128 4 4</p><p class="s80" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Tree(sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s80" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Tree(sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s80" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Tree(sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s80" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Tree(sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s80" style="padding-left: 72pt;text-indent: 0pt;line-height: 4pt;text-align: left;">64</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 197pt;text-indent: 0pt;text-align: left;">64 2 2</p><p class="s80" style="padding-top: 2pt;padding-left: 72pt;text-indent: 0pt;text-align: left;">32</p><p class="s80" style="padding-top: 3pt;padding-left: 197pt;text-indent: 0pt;text-align: left;">32 1 1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 72pt;text-indent: 0pt;line-height: 4pt;text-align: left;">16</p><p class="s80" style="padding-left: 197pt;text-indent: 0pt;line-height: 4pt;text-align: left;">16 0.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 72pt;text-indent: 0pt;text-align: left;">0.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s81" style="padding-top: 5pt;padding-left: 74pt;text-indent: 0pt;line-height: 7pt;text-align: left;">8 <span class="s80">1 2 4 8 16</span></p><p class="s80" style="padding-left: 106pt;text-indent: 0pt;line-height: 5pt;text-align: left;">Number of Threads</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l6"><li><p style="padding-left: 99pt;text-indent: -14pt;text-align: left;">Allstate 10M</p><p class="s81" style="padding-top: 5pt;padding-left: 29pt;text-indent: 0pt;line-height: 7pt;text-align: left;">8 <span class="s80">1 2 4 8 16</span></p><p class="s80" style="padding-left: 61pt;text-indent: 0pt;line-height: 5pt;text-align: left;">Number of Threads</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 58pt;text-indent: -15pt;text-align: left;">Higgs 10M</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">0.25</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 116pt;text-indent: 0pt;line-height: 5pt;text-align: center;">1              2               4               8              16</p><p class="s80" style="padding-left: 116pt;text-indent: 0pt;line-height: 5pt;text-align: center;">Number of Threads</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 20pt;text-indent: -14pt;text-align: left;">Allstate 1M</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">0.25</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 52pt;text-indent: 0pt;line-height: 5pt;text-align: center;">1              2               4               8              16</p><p class="s80" style="padding-left: 51pt;text-indent: 0pt;line-height: 5pt;text-align: center;">Number of Threads</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 25pt;text-indent: -15pt;text-align: left;">Higgs 1M</p><h3 style="padding-top: 3pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;"><a name="bookmark38">Figure 7: Impact of cache-aware prefetching in exact greedy algorithm. We find that the cache-miss effect impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves the performance by factor of two when the dataset is large.</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="222" height="88" alt="image" src="2939672.2939785/Image_067.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="158" height="143" alt="image" src="2939672.2939785/Image_068.png"/></span></p><p class="s74" style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 8pt;text-align: left;">block size=2^12 block size=2^16</p><p class="s82" style="padding-left: 15pt;text-indent: -12pt;line-height: 8pt;text-align: left;"> <span class="s83">       </span><span class="s84">     </span><span class="s74">block size=2^20 block size=2^24</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s80" style="padding-left: 265pt;text-indent: 0pt;text-align: center;">128</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-top: 2pt;padding-left: 268pt;text-indent: 0pt;text-align: center;">64</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Tree(sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s80" style="padding-top: 3pt;padding-left: 268pt;text-indent: 0pt;text-align: center;">32</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 268pt;text-indent: 0pt;text-align: center;"><a name="bookmark39">16</a></p><h3 style="padding-top: 1pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">Figure 8: Short range data dependency pattern</h3><h3 style="padding-left: 53pt;text-indent: 0pt;text-align: left;">that can cause stall due to cache miss. <span class="s80">8</span></h3><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">    </p><p style="text-indent: 0pt;text-align: left;"/><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">                                        </p><p style="text-indent: 0pt;text-align: left;"/><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">             </p><p style="text-indent: 0pt;text-align: left;"/><p class="s12" style="text-indent: 0pt;line-height: 11pt;text-align: left;">                                          </p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 12pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;"><span class="h3">Time Complexity Analysis </span><span class="p">Let </span>d <span class="p">be the maximum depth of the tree and </span>K <span class="p">be total number of trees. For the exac- t greedy algorithm, the time complexity of original spase </span><span class="s19">aware </span><span class="p">algorithm is </span>O<span class="p">(</span>Kd <span class="h3">x </span><span class="s22">0 </span><span class="s19">log </span>n<span class="p">). Here we use </span><span class="h3">x </span><span class="s22">0 </span><span class="s19">to </span><span class="p">denote number of non-missing entries in the training data. On the other hand, tree boosting on the block structure on- </span><span class="s19">ly cost </span>O<span class="p">(</span>Kd <span class="h3">x </span><span class="s22">0 </span><span class="s19">+ </span><span class="h3">x </span><span class="s22">0 </span><span class="s19">log </span>n<span class="p">). Here </span>O<span class="p">( </span><span class="h3">x </span><span class="s22">0 </span><span class="s19">log </span>n<span class="p">) is the one time preprocessing cost that can be amortized. This analysis shows that the block structure helps to save an ad- ditional log </span>n <span class="p">factor, which is significant when </span>n <span class="p">is large. For the approximate algorithm, the time complexity of original </span><span class="s19">algorithm with binary search is </span>O<span class="p">(</span>Kd <span class="h3">x </span><span class="s22">0 </span><span class="s19">log </span>q<span class="p">). Here </span>q <span class="p">is the number of proposal candidates in the dataset. While </span>q <span class="p">is usually between 32 and 100, the log factor still introduces overhead. Using the block structure, we can reduce the time </span><span class="s19">to </span>O<span class="p">(</span>Kd <span class="h3">x </span><span class="s22">0 </span><span class="s19">+ </span><span class="h3">x </span><span class="s22">0 </span><span class="s19">log </span>B<span class="p">), where </span>B <span class="p">is the maximum num- ber of rows in each block. Again we can save the additional</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="text-indent: 0pt;text-align: right;">4</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">512</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">256</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Tree(sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s80" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">128</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">64</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">32</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">16</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="text-indent: 0pt;text-align: right;">8</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="text-indent: 0pt;text-align: right;">4</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 9pt;text-indent: 0pt;line-height: 4pt;text-align: center;">1                     2                     4                     8                 16</p><p class="s80" style="padding-left: 8pt;text-indent: 0pt;line-height: 4pt;text-align: center;">Number of Threads</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="158" height="143" alt="image" src="2939672.2939785/Image_069.png"/></span></p><p class="s74" style="text-indent: 0pt;line-height: 6pt;text-align: left;">block size=2^16</p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-top: 5pt;text-indent: 0pt;text-align: left;"> <span class="s25">   </span><span class="p">  </span><span class="s74">block size=2^20</span></p><p class="s86" style="padding-top: 2pt;text-indent: 0pt;text-align: left;"> <span class="s83">       </span><span class="s84">     </span><span class="s74">block size=2^24</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s87" style="text-indent: 0pt;line-height: 6pt;text-align: left;"> <span class="s25">   </span><span class="p">  </span><span class="s74">block size=2^12</span></p><p style="text-indent: 0pt;text-align: left;"/><ol id="l7"><li><p style="padding-left: 32pt;text-indent: -14pt;text-align: left;">Allstate 10M</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 9pt;text-indent: 0pt;line-height: 4pt;text-align: center;">1                     2                     4                     8                 16</p><p class="s80" style="padding-left: 8pt;text-indent: 0pt;line-height: 4pt;text-align: center;">Number of Threads</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 37pt;text-indent: -15pt;text-align: left;">Higgs 10M</p></li></ol><p style="padding-left: 53pt;text-indent: 0pt;line-height: 9pt;text-align: left;">log <span class="s13">q </span>factor in computation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h2 style="padding-left: 80pt;text-indent: -26pt;text-align: left;"><a name="bookmark35">Cache-aware Access</a></h2><p class="s4" style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark39" class="s64">While the proposed block structure helps optimize the computation complexity of split finding, the new algorithm requires indirect fetches of gradient statistics by row index, since these values are accessed in order of feature. This is a non-continuous memory access. A naive implementation of split enumeration introduces immediate read/write de- pendency between the accumulation and the non-continuous memory fetch operation (see Fig. </a>8<span style=" color: #000;">). This slows down split finding when the gradient statistics do not fit into CPU cache and cache miss occur.</span></p><p class="s4" style="padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark38" class="s64">For the exact greedy algorithm, we can alleviate the prob- lem by a cache-aware prefetching algorithm. Specifically, we allocate an internal buffer in each thread, fetch the gra- dient statistics into it, and then perform accumulation in a mini-batch manner. This prefetching changes the direct read/write dependency to a longer dependency and helps to reduce the runtime overhead when number of rows in the is large. Figure </a>7 <span style=" color: #000;">gives the comparison of cache-aware vs.</span></p><h3 style="padding-top: 3pt;padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a name="bookmark40">Figure 9: The impact of block size in the approxi- mate algorithm. We find that overly small blocks re- sults in inefficient parallelization, while overly large blocks also slows down training due to cache misses.</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">non cache-aware algorithm on the the Higgs and the All- state dataset. We find that cache-aware implementation of the exact greedy algorithm runs twice as fast as the naive version when the dataset is large.</p><p class="s4" style="padding-left: 21pt;text-indent: 8pt;text-align: left;"><a href="#bookmark40" class="s64">For approximate algorithms, we solve the problem by choos- ing a correct block size. We define the block size to be max- imum number of examples in contained in a block, as this reflects the cache storage cost of gradient statistics. Choos- ing an overly small block size results in small workload for each thread and leads to inefficient parallelization. On the other hand, overly large blocks result in cache misses, as the gradient statistics do not fit into the CPU cache. A good choice of block size balances these two factors. We compared various choices of block size on two data sets. The results are given in Fig. </a>9<span style=" color: #000;">. This result validates our discussion and</span></p><h3 style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;text-align: center;"><a name="bookmark41">Table  1:  Comparison of major tree boosting  systems.</a></h3><table style="border-collapse:collapse;margin-left:103.826pt" cellspacing="0"><tr style="height:21pt"><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">System</p></td><td style="width:40pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">exact</p><p class="s88" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">greedy</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">approximate</p><p class="s88" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">global</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">approximate</p><p class="s88" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">local</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">out-of-core</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">sparsity</p><p class="s88" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">aware</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">parallel</p></td></tr><tr style="height:11pt"><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s89" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">XGBoost</p></td><td style="width:40pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td></tr><tr style="height:11pt"><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">pGBRT</p></td><td style="width:40pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td></tr><tr style="height:11pt"><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Spark MLLib</p></td><td style="width:40pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">partially</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td></tr><tr style="height:11pt"><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">H2O</p></td><td style="width:40pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">partially</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td></tr><tr style="height:11pt"><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">scikit-learn</p></td><td style="width:40pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td></tr><tr style="height:11pt"><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">R GBM</p></td><td style="width:40pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">yes</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">partially</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">no</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;">shows that choosing 216 examples per block balances the cache property and parallelization.</p></li><li><h2 style="padding-top: 5pt;padding-left: 80pt;text-indent: -26pt;text-align: left;"><a name="bookmark36">Blocks for Out-of-core Computation</a></h2><p style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;">One goal of our system is to fully utilize a machine’s re- sources to achieve scalable learning. Besides processors and memory, it is important to utilize disk space to handle data that does not fit into main memory. To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk. During computation, it is impor- tant to use an independent thread to pre-fetch the block into a main memory buffer, so computation can happen in con- currence with disk reading. However, this does not entirely solve the problem since the disk reading takes most of the computation time. It is important to reduce the overhead and increase the throughput of disk IO. We mainly use two techniques to improve the out-of-core computation.</p><h3 style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">Block Compression <span class="p">The first technique we use is block compression. The block is compressed by columns, and de- compressed on the fly by an independent thread when load- ing into main memory. This helps to trade some of the computation in decompression with the disk reading cost. We use a general purpose compression algorithm for com- pressing the features values. For the row index, we substract the row index by the begining index of the block and use a 16bit integer to store each offset. This requires 216 examples per block, which is confirmed to be a good setting. In most of the dataset we tested, we achieve roughly a 26% to 29% compression ratio.</span></h3><h3 style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">Block Sharding <span class="p">The second technique is to shard the data onto multiple disks in an alternative manner. A pre-fetcher thread is assigned to each disk and fetches the data into an in-memory buffer. The training thread then alternatively reads the data from each buffer. This helps to increase the throughput of disk reading when multiple disks are available.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h2 style="padding-left: 74pt;text-indent: -21pt;text-align: left;"><a name="bookmark42">RELATED WORKS</a><a name="bookmark43">&zwnj;</a></h2><p class="s4" style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark67" class="s64">Our system implements gradient boosting [</a>10<a href="#bookmark69" class="s64">], which per- forms additive optimization in functional space. Gradient tree boosting has been successfully used in classification [</a>12<a href="#bookmark78" class="s64">], learning to rank [</a>5<a href="#bookmark65" class="s64">], structured prediction [</a>8<a href="#bookmark93" class="s64">] as well as other fields. XGBoost incorporates a regularized model to prevent overfitting. This this resembles previous work on regularized greedy forest [</a>25<a href="#bookmark77" class="s64">], but simplifies the objective and algorithm for parallelization. Column sampling is a simple but effective technique borrowed from RandomForest [</a>4<span style=" color: #000;">]. While sparsity- aware learning is essential in other types of models such as</span></p><p class="s4" style="padding-top: 4pt;padding-left: 21pt;text-indent: 8pt;text-align: right;"><a href="#bookmark90" class="s64">There are several existing works on parallelizing tree learn- ing [</a>22<a href="#bookmark87" class="s64">, </a>19<a href="#bookmark91" class="s64">]. Most of these algorithms fall into the approxi- mate framework described in this paper. Notably, it is also possible to partition data by columns [</a>23<a href="#bookmark41" class="s64">] and apply the ex- act greedy algorithm. This is also supported in our frame- work, and the techniques such as cache-aware pre-fecthing can be used to benefit this type of algorithm. While most existing works focus on the algorithmic aspect of paralleliza- tion, our work improves in two unexplored system direction- s: out-of-core computation and cache-aware learning. This gives us insights on how the system and the algorithm can be jointly optimized and provides an end-to-end system that can handle large scale problems with very limited computing resources. We also summarize the comparison between our system and existing opensource implementations in Table </a>1<a href="#bookmark82" class="s64">. Quantile summary (without weights) is a classical prob- lem in the database community [</a>14<a href="#bookmark92" class="s64">, </a>24<span style=" color: #000;">]. However, the ap- proximate tree boosting algorithm reveals a more general problem – finding quantiles on weighted data. To the best of our knowledge, the weighted quantile sketch proposed in this paper is the first method to solve this problem. The weighted quantile summary is also not specific to the tree learning and can benefit other applications in data science</span></p><p style="padding-left: 21pt;text-indent: 0pt;text-align: left;">and machine learning in the future.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><h2 style="padding-left: 42pt;text-indent: -21pt;text-align: left;"><a name="bookmark44">END TO END EVALUATIONS</a><a name="bookmark51">&zwnj;</a></h2><ol id="l8"><li><h2 style="padding-top: 7pt;padding-left: 48pt;text-indent: -26pt;text-align: left;"><a name="bookmark45">System Implementation</a></h2><p class="s4" style="padding-top: 1pt;padding-left: 21pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark52" class="s64">We implemented XGBoost as an open source package</a>6<a href="#bookmark53" class="s64">. The package is portable and reusable. It supports various weighted classification and rank objective functions, as well as user defined objective function. It is available in popular languages such as python, R, Julia and integrates naturally with language native data science pipelines such as scikit- learn. The distributed version is built on top of the rabit library</a>7 <a href="#bookmark54" class="s64">for allreduce. The portability of XGBoost makes it available in many ecosystems, instead of only being tied to a specific platform. The distributed XGBoost runs natively on Hadoop, MPI Sun Grid engine. Recently, we also enable distributed XGBoost on jvm bigdata stacks such as Flink and Spark. The distributed version has also been integrated into cloud platform Tianchi</a>8 <span style=" color: #000;">of Alibaba. We believe that there will be more integrations in the future.</span></p></li><li><h2 style="padding-top: 6pt;padding-left: 48pt;text-indent: -26pt;text-align: left;"><a name="bookmark46">Dataset and Setup</a></h2><p class="s4" style="padding-top: 1pt;padding-left: 21pt;text-indent: 8pt;line-height: 11pt;text-align: justify;"><a href="#bookmark55" class="s64">We used four datasets in our experiments. A summary of these datasets is given in Table </a>2<span style=" color: #000;">. In some of the experi-</span></p><p class="s4" style="padding-left: 53pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark66" class="s64" name="bookmark52">linear models [</a>9<span style=" color: #000;">], few works on tree learning have considered           </span><span class="s25">                                                      </span></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">this topic in a principled way. The algorithm proposed in this paper is the first unified approach to handle all kinds of sparsity patterns.</p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 89%;text-align: left;"><a name="bookmark53">6</a><span class="s4">https://github.com/dmlc/xgboost </span>7<span class="p">https://github.com/dmlc/rabit </span>8<span class="p">https://tianchi.aliyun.com</span><a name="bookmark54">&zwnj;</a></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:11pt"><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Dataset</p></td><td style="width:33pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s90" style="text-indent: 0pt;line-height: 9pt;text-align: center;">n</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s90" style="text-indent: 0pt;line-height: 9pt;text-align: center;">m</p></td><td style="width:129pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Task</p></td></tr><tr style="height:11pt"><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Allstate</p></td><td style="width:33pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">10 M</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-right: 5pt;text-indent: 0pt;line-height: 9pt;text-align: right;">4227</p></td><td style="width:129pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Insurance claim classification</p></td></tr><tr style="height:11pt"><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Higgs Boson</p></td><td style="width:33pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">10 M</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-right: 9pt;text-indent: 0pt;line-height: 9pt;text-align: right;">28</p></td><td style="width:129pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Event classification</p></td></tr><tr style="height:11pt"><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Yahoo LTRC</p></td><td style="width:33pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">473K</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-right: 7pt;text-indent: 0pt;line-height: 9pt;text-align: right;">700</p></td><td style="width:129pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Learning to Rank</p></td></tr><tr style="height:11pt"><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Criteo</p></td><td style="width:33pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">1.7 B</p></td><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-right: 9pt;text-indent: 0pt;line-height: 9pt;text-align: right;">67</p></td><td style="width:129pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Click through rate prediction</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><h3 style="padding-top: 2pt;padding-left: 74pt;text-indent: 0pt;text-align: left;"><a name="bookmark55">Table 2: Dataset used in the Experiments.</a></h3><h3 style="padding-top: 2pt;padding-left: 42pt;text-indent: 0pt;text-align: left;"><a name="bookmark56">Table 3: Comparison of Exact Greedy Methods with 500 trees on Higgs-1M data.</a></h3><table style="border-collapse:collapse;margin-left:317.062pt" cellspacing="0"><tr style="height:11pt"><td style="width:116pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Method</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Time per Tree (sec)</p></td><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 3pt;padding-right: 3pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Test AUC</p></td></tr><tr style="height:11pt"><td style="width:116pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">XGBoost</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">0.6841</p></td><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 3pt;padding-right: 3pt;text-indent: 0pt;line-height: 9pt;text-align: center;">0.8304</p></td></tr><tr style="height:11pt"><td style="width:116pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">XGBoost (colsample=0.5)</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">0.6401</p></td><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 3pt;padding-right: 3pt;text-indent: 0pt;line-height: 9pt;text-align: center;">0.8245</p></td></tr><tr style="height:11pt"><td style="width:116pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">scikit-learn</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">28.51</p></td><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 3pt;padding-right: 3pt;text-indent: 0pt;line-height: 9pt;text-align: center;">0.8302</p></td></tr><tr style="height:11pt"><td style="width:116pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">R.gbm</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">1.032</p></td><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 3pt;padding-right: 3pt;text-indent: 0pt;line-height: 9pt;text-align: center;">0.6224</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 3pt;padding-left: 52pt;text-indent: 0pt;text-align: right;"><a href="#bookmark57" class="s64">ments, we use a randomly selected subset of the data either due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suffix to denote the size in these cases.  For example Allstate-10K means a subset of the Allstate dataset with 10K instances. The first dataset we use is the Allstate insurance claim dataset</a>9<a href="#bookmark28" class="s64">. The task is to predict the likelihood and cost of an insurance claim given different risk factors. In the exper- iment, we simplified the task to only predict the likelihood of an insurance claim. This dataset is used to evaluate the impact of sparsity-aware algorithm in Sec. </a>3.4<span style=" color: #000;">. Most of the sparse features in this data come from one-hot encoding. We randomly select 10M instances as training set and use the</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s91" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">32</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s91" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Tree(sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s91" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">16</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s91" style="text-indent: 0pt;text-align: right;">8</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s91" style="text-indent: 0pt;text-align: right;">4</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s91" style="text-indent: 0pt;text-align: right;">2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s91" style="text-indent: 0pt;text-align: right;">1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s91" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">0.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="193" height="147" alt="image" src="2939672.2939785/Image_070.png"/></span></p><p class="s91" style="text-indent: 0pt;text-align: left;">pGBRT</p><p style="text-indent: 0pt;text-align: left;"/><p class="s91" style="text-indent: 0pt;text-align: left;">XGBoost</p><p style="text-indent: 0pt;text-align: left;"/><p class="s91" style="padding-left: 25pt;text-indent: 0pt;line-height: 7pt;text-align: center;">1                2                4                8               16</p><p class="s91" style="padding-left: 23pt;text-indent: 0pt;line-height: 7pt;text-align: center;">Number of Threads</p><p style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">rest as evaluation set.</p><p class="s4" style="padding-left: 53pt;text-indent: 8pt;text-align: right;"><a href="#bookmark58" class="s64">The second dataset is the Higgs boson dataset</a><a href="#bookmark58" class="a">10</a> <a href="#bookmark79" class="s64">from high energy physics. The data was produced using Monte Carlo simulations of physics events. It contains 21 kinematic prop- erties measured by the particle detectors in the accelerator. It also contains seven additional derived physics quantities of the particles. The task is to classify whether an event corresponds to the Higgs boson. We randomly select 10M instances as training set and use the rest as evaluation set. The third dataset is the Yahoo! learning to rank challenge dataset [</a>6<span style=" color: #000;">], which is one of the most commonly used bench- marks in learning to rank algorithms. The dataset contains 20K web search queries, with each query corresponding to a list of around 22 documents. The task is to rank the docu- ments according to relevance of the query. We use the official</span></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">train test split in our experiment.</p><p class="s4" style="padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark59" class="s64">The last dataset is the criteo terabyte click log dataset</a>11<span style=" color: #000;">. We use this dataset to evaluate the scaling property of the system in the out-of-core and the distributed settings. The data contains 13 integer features and 26 ID features of user, item and advertiser information. Since a tree based model is better at handling continuous features, we preprocess the data by calculating the statistics of average CTR and count of ID features on the first ten days, replacing the ID fea- tures by the corresponding count statistics during the next ten days for training. The training set after preprocessing contains 1.7 billion instances with 67 features (13 integer, 26 average CTR statistics and 26 counts). The entire dataset is more than one terabyte in LibSVM format.</span></p><p style="padding-left: 53pt;text-indent: 8pt;text-align: justify;">We use the first three datasets for the single machine par- allel setting, and the last dataset for the distributed and out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not specified, all the experiments are run using all the available cores in the machine. The machine settings of the distribut- ed and the out-of-core experiments will be described in the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="162" height="1" alt="image" src="2939672.2939785/Image_071.png"/></span></p><p class="s5" style="padding-left: 50pt;text-indent: 3pt;line-height: 89%;text-align: left;"><a name="bookmark57">9</a><a href="http://www.kaggle.com/c/ClaimPredictionChallenge" class="s64" target="_blank">https://</a><span class="p">www.kaggle.com/c/ClaimPredictionChallenge </span>10<span class="p">https://archive.ics.uci.edu/ml/datasets/HIGGS </span><a href="http://labs.criteo.com/downloads/download-terabyte-" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 4pt;" target="_blank">11</a><a href="http://labs.criteo.com/downloads/download-terabyte-" class="s64" target="_blank">http://labs.criteo.com/downloads/download-terabyte-</a><a name="bookmark58">&zwnj;</a><a name="bookmark59">&zwnj;</a></p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 9pt;text-align: left;">click-logs/</p><h3 style="padding-top: 5pt;padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a name="bookmark60">Figure 10: Comparison between XGBoost and pG- BRT on Yahoo LTRC dataset.</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a name="bookmark61">Table 4: Comparison of Learning to Rank with 500 trees on Yahoo! LTRC Dataset</a></h3><table style="border-collapse:collapse;margin-left:22.1607pt" cellspacing="0"><tr style="height:11pt"><td style="width:116pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Method</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 8pt;text-align: center;">Time per Tree (sec)</p></td><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 4pt;padding-right: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">NDCG@10</p></td></tr><tr style="height:11pt"><td style="width:116pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">XGBoost</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0.826</p></td><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 4pt;padding-right: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0.7892</p></td></tr><tr style="height:11pt"><td style="width:116pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">XGBoost (colsample=0.5)</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0.506</p></td><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 4pt;padding-right: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0.7913</p></td></tr><tr style="height:11pt"><td style="width:116pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="#bookmark90" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">pGBRT [</a><span style=" color: #001472; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt;">22</span>]</p></td><td style="width:92pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 8pt;text-align: center;">2.576</p></td><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s88" style="padding-left: 4pt;padding-right: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0.7915</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">corresponding section. In all the experiments, we boost trees with a common setting of maximum depth equals 8, shrink- age equals 0.1 and no column subsampling unless explicitly specified. We can find similar results when we use other settings of maximum depth.</p></li><li><h2 style="padding-top: 5pt;padding-left: 48pt;text-indent: -26pt;text-align: left;"><a name="bookmark47">Classification</a></h2><p class="s4" style="padding-top: 1pt;padding-left: 21pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark56" class="s64">In this section, we evaluate the performance of XGBoost on a single machine using the exact greedy algorithm on Higgs-1M data, by comparing it against two other common- ly used exact greedy tree boosting implementations. Since scikit-learn only handles non-sparse input, we choose the dense Higgs dataset for a fair comparison. We use the 1M subset to make scikit-learn finish running in reasonable time. Among the methods in comparison, R’s GBM uses a greedy approach that only expands one branch of a tree, which makes it faster but can result in lower accuracy, while both scikit-learn and XGBoost learn a full tree. The results are shown in Table </a>3<span style=" color: #000;">. Both XGBoost and scikit-learn give bet- ter performance than R’s GBM, while XGBoost runs more than 10x faster than scikit-learn. In this experiment, we al- so find column subsamples gives slightly worse performance than using all the features. This could due to the fact that there are few important features in this dataset and we can benefit from greedily select from all the features.</span></p></li><li><h2 style="padding-top: 6pt;padding-left: 48pt;text-indent: -26pt;text-align: left;"><a name="bookmark48">Learning to Rank</a></h2><p class="s4" style="padding-top: 1pt;padding-left: 21pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark90" class="s64">We next evaluate the performance of XGBoost on the learning to rank problem. We compare against pGBRT [</a>22<span style=" color: #000;">], the best previously pubished system on this task. XGBoost</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="190" height="176" alt="image" src="2939672.2939785/Image_072.png"/></span></p><p class="s72" style="text-indent: 0pt;text-align: left;">Block compression</p><p style="text-indent: 0pt;text-align: left;"/><p class="s72" style="text-indent: 0pt;text-align: left;">Basic algorithm</p><p style="text-indent: 0pt;text-align: left;"/><p class="s72" style="padding-left: 18pt;text-indent: 0pt;text-align: left;">Compression+shard</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="text-indent: 0pt;text-align: left;">Out of system file cache start from this point</p><p style="text-indent: 0pt;text-align: left;"/><p class="s72" style="padding-top: 4pt;padding-left: 24pt;text-indent: 0pt;text-align: right;">4096</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">2048</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Tree(sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">1024</p><p class="s74" style="padding-top: 4pt;padding-left: 95pt;text-indent: 0pt;text-align: left;">32768</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="163" height="153" alt="image" src="2939672.2939785/Image_073.png"/></span></p><p class="s75" style="padding-left: 36pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="0" height="2" alt="image" src="2939672.2939785/Image_074.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_075.png"/></span>	<span><img width="0" height="2" alt="image" src="2939672.2939785/Image_076.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">H2O</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_077.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_078.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_079.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_080.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-left: 53pt;text-indent: 0pt;text-align: center;">Spark MLLib</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_081.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_082.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s76" style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_083.png"/></span>	<span><img width="2" height="0" alt="image" src="2939672.2939785/Image_084.png"/></span></p><p class="s74" style="padding-left: 95pt;text-indent: 0pt;text-align: left;">XGBoost</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_085.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_086.png"/></span></p><p class="s74" style="padding-left: 95pt;text-indent: 0pt;text-align: left;">16384</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Total Running Time (sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s74" style="padding-left: 98pt;text-indent: 0pt;text-align: left;">8192</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-left: 98pt;text-indent: 0pt;text-align: left;">4096</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_087.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_088.png"/></span></p><p class="s74" style="padding-left: 98pt;text-indent: 0pt;text-align: left;">2048</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-top: 5pt;padding-left: 99pt;text-indent: 0pt;text-align: left;">512</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 99pt;text-indent: 0pt;text-align: left;">256</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s93" style="padding-left: 99pt;text-indent: 0pt;line-height: 9pt;text-align: left;">1<span class="s72">21</span>8<span class="s72">28                 256</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-top: 5pt;padding-left: 24pt;text-indent: 0pt;line-height: 6pt;text-align: left;">512 1024 2048</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-left: 98pt;text-indent: 0pt;text-align: left;">1024</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-left: 101pt;text-indent: 0pt;text-align: left;">512</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_089.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="0" alt="image" src="2939672.2939785/Image_090.png"/></span></p><p class="s74" style="padding-left: 101pt;text-indent: 0pt;text-align: left;">256</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s94" style="padding-left: 101pt;text-indent: 0pt;line-height: 9pt;text-align: left;">1<span class="s74">21</span>8<span class="s74">28                    256                    512                   1024                  2048</span></p><p class="s74" style="padding-left: 116pt;text-indent: 0pt;line-height: 6pt;text-align: center;">Number of Training Examples (million)</p><p class="s72" style="padding-left: 133pt;text-indent: 0pt;text-align: left;">Number of Training Examples (million)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 53pt;text-indent: 0pt;text-align: justify;"><a name="bookmark62">Figure 11: Comparison of out-of-core methods on different subsets of criteo data. The missing data points are due to out of disk space. We can find that basic algorithm can only handle 200M exam- ples. Adding compression gives 3x speedup, and sharding into two disks gives another 2x speedup. The system runs out of file cache start from 400M examples. The algorithm really has to rely on disk after this point. The compression+shard method has a less dramatic slowdown when running out of file cache, and exhibits a linear trend afterwards.</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 53pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark61" class="s64">runs exact greedy algorithm, while pGBRT only support an approximate algorithm. The results are shown in Table </a>4 <a href="#bookmark60" class="s64">and Fig. </a>10<span style=" color: #000;">. We find that XGBoost runs faster. Interest- ingly, subsampling columns not only reduces running time, and but also gives a bit higher performance for this prob- lem. This could due to the fact that the subsampling helps prevent overfitting, which is observed by many of the users.</span></p></li><li><h2 style="padding-top: 6pt;padding-left: 80pt;text-indent: -26pt;text-align: justify;"><a name="bookmark49">Out-of-core Experiment</a></h2><p class="s4" style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark62" class="s64">We also evaluate our system in the out-of-core setting on the criteo data. We conducted the experiment on one AWS c3.8xlarge machine (32 vcores, two 320 GB SSD, 60 GB RAM). The results are shown in Figure </a>11<span style=" color: #000;">. We can find that compression helps to speed up computation by factor of three, and sharding into two disks further gives 2x speedup. For this type of experiment, it is important to use a very large dataset to drain the system file cache for a real out- of-core setting. This is indeed our setup. We can observe a transition point when the system runs out of file cache. Note that the transition in the final method is less dramatic. This is due to larger disk throughput and better utilization of computation resources. Our final method is able to process</span></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">1.7 billion examples on a single machine.</p><h2 style="padding-top: 7pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;"><a name="bookmark50">6.6 Distributed Experiment</a></h2><p style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;">Finally, we evaluate the system in the distributed setting. We set up a YARN cluster on EC2 with m3.2xlarge ma- chines, which is a very common choice for clusters. Each machine contains 8 virtual cores, 30GB of RAM and two 80GB SSD local disks. The dataset is stored on AWS S3 instead of HDFS to avoid purchasing persistent storage.</p><p class="s4" style="padding-left: 53pt;text-indent: 8pt;text-align: justify;"><a href="#bookmark86" class="s64">We first compare our system against two production-level distributed systems: Spark MLLib [</a>18<a href="#bookmark63" class="s64">] and H2O </a>12<span style=" color: #000;">. We use 32 m3.2xlarge machines and test the performance of the sys-</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="162" height="1" alt="image" src="2939672.2939785/Image_091.png"/></span></p><p class="s5" style="padding-left: 50pt;text-indent: 0pt;text-align: left;"><a name="bookmark63">12</a><a href="http://www.h2o.ai/" class="s64">www.h2o.ai</a></p><ol id="l9"><li><p style="padding-top: 1pt;padding-left: 63pt;text-indent: -14pt;text-align: left;">End-to-end time cost include data loading</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="195" height="181" alt="image" src="2939672.2939785/Image_092.png"/></span></p><p class="s72" style="text-indent: 0pt;text-align: left;">Spark MLLib</p><p style="text-indent: 0pt;text-align: left;"/><p class="s72" style="text-indent: 0pt;text-align: left;">H2O</p><p style="text-indent: 0pt;text-align: left;"/><p class="s72" style="text-indent: 0pt;text-align: left;">XGBoost</p><p style="text-indent: 0pt;text-align: left;"/><p class="s72" style="padding-top: 6pt;padding-left: 24pt;text-indent: 0pt;text-align: right;">4096</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">2048</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Iteration (sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">1024</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">512</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">256</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">128</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">64</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">32</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">16</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 101pt;text-indent: 0pt;line-height: 10pt;text-align: center;">1<span class="s93">8</span>28                  256                  512                1024               2048</p><p class="s72" style="padding-left: 99pt;text-indent: 0pt;line-height: 6pt;text-align: center;">Number of Training Examples (million)</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 71pt;text-indent: -15pt;text-align: justify;">Per iteration cost exclude data loading</p></li></ol><h3 style="padding-top: 3pt;padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a name="bookmark64">Figure 12: Comparison of different distributed sys- tems on 32 EC2 nodes for 10 iterations on different subset of criteo data. XGBoost runs more 10x than spark per iteration and 2.2x as H2O’s optimized ver- sion (However, H2O is slow in loading the data, get- ting worse end-to-end time). Note that spark suffers from drastic slow down when running out of mem- ory. XGBoost runs faster and scales smoothly to the full 1.7 billion examples with given resources by utilizing out-of-core computation.</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark64" class="s64">tems with various input size. Both of the baseline systems are in-memory analytics frameworks that need to store the data in RAM, while XGBoost can switch to out-of-core set- ting when it runs out of memory. The results are shown in Fig. </a>12<a href="#bookmark70" class="s64">. We can find that XGBoost runs faster than the baseline systems. More importantly, it is able to take advantage of out-of-core computing and smoothly scale to all 1.7 billion examples with the given limited computing re- sources. The baseline systems are only able to handle subset of the data with the given resources. This experiment shows the advantage to bring all the system improvement togeth- er and solve a real-world scale problem. We also evaluate the scaling property of XGBoost by varying the number of machines. The results are shown in Fig. </a>13<span style=" color: #000;">. We can find XGBoost’s performance scales linearly as we add more ma- chines. Importantly, XGBoost is able to handle the entire</span></p><ol id="l10"><ol id="l11"><li><p style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">billion data with only four machines. This shows the system’s potential to handle even larger data.</p><p class="s72" style="padding-top: 4pt;padding-left: 24pt;text-indent: 0pt;text-align: right;">2048</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Time per Iteration (sec)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">1024</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">512</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">256</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 24pt;text-indent: 0pt;text-align: right;">128</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s72" style="padding-left: 116pt;text-indent: 0pt;line-height: 6pt;text-align: center;">4                         8                        16                      32</p><p class="s72" style="padding-left: 116pt;text-indent: 0pt;line-height: 6pt;text-align: center;">Number of Machines</p><p class="s95" style="padding-top: 2pt;padding-left: 84pt;text-indent: 0pt;text-align: left;">of 30th International Conference on Machine Learning (ICML’13)<span class="s96">, volume 1, pages 436–444, 2013.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="168" height="154" alt="image" src="2939672.2939785/Image_093.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="192" height="173" alt="image" src="2939672.2939785/Image_094.png"/></span></p><ol id="l12"><li><p class="s96" style="padding-top: 1pt;padding-left: 84pt;text-indent: -13pt;text-align: left;"><a name="bookmark65">T. Chen, S. Singh, B. Taskar, and C. Guestrin. Efficient second-order gradient boosting for conditional random fields. In </a><span class="s95">Proceeding of 18th Artificial Intelligence and Statistics Conference (AISTATS’15)</span>, volume 1, 2015.</p></li><li><p class="s96" style="padding-left: 84pt;text-indent: -13pt;text-align: left;"><a name="bookmark66">R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. </a><span class="s95">Journal of Machine Learning Research</span>, 9:1871–1874, 2008.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 84pt;text-indent: -17pt;text-align: justify;"><a name="bookmark67">J. Friedman. Greedy function approximation: a gradient boosting machine. </a><span class="s95">Annals of Statistics</span>, 29(5):1189–1232, 2001.<a name="bookmark68">&zwnj;</a></p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 84pt;text-indent: -17pt;text-align: justify;">J. Friedman. Stochastic gradient boosting. <span class="s95">Computational Statistics &amp; Data Analysis</span>, 38(4):367–378, 2002.</p></li><li><p class="s96" style="padding-left: 84pt;text-indent: -17pt;text-align: justify;"><a name="bookmark69">J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic</a></p><h3 style="padding-top: 4pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;"><a name="bookmark70">Figure 13: Scaling of XGBoost with different num- ber of machines on criteo full 1.7 billion dataset. Using more machines results in more file cache and makes the system run faster, causing the trend to be slightly super linear. XGBoost can process the entire dataset using as little as four machines, and s- cales smoothly by utilizing more available resources.</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li></ol></ol></li></ol></li><li><h2 style="padding-left: 74pt;text-indent: -21pt;text-align: left;"><a name="bookmark71">CONCLUSION</a><a name="bookmark72">&zwnj;</a></h2><p style="padding-top: 1pt;padding-left: 53pt;text-indent: 8pt;text-align: justify;">In this paper, we described the lessons we learnt when building XGBoost, a scalable tree boosting system that is widely used by data scientists and provides state-of-the-art results on many problems. We  proposed a novel sparsity aware algorithm for handling sparse data and a theoretically justified weighted quantile sketch for approximate learning. Our experience shows that cache access patterns, data com- pression and sharding are essential elements for building a scalable end-to-end system for tree boosting. These lessons can be applied to other machine learning systems as well. By combining these insights, XGBoost is able to solve real- world scale problems using a minimal amount of resources.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 53pt;text-indent: 0pt;text-align: left;">Acknowledgments</h2><p class="s96" style="padding-top: 2pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;">We would like to thank Tyler B. Johnson, Marco Tulio Ribeiro, Sameer Singh, Arvind Krishnamurthy for their valuable feedback. We also sincerely thank Tong He, Bing Xu, Michael Benesty, Yuan Tang, Hongliang Liu, Qiang Kou, Nan Zhu and all other con- tributors in the XGBoost community. This work was supported in part by ONR (PECASE) N000141010672, NSF IIS 1258741</p><p class="s96" style="padding-left: 53pt;text-indent: 0pt;text-align: justify;">and the TerraSwarm Research Center sponsored by MARCO and DARPA.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><h2 style="padding-left: 74pt;text-indent: -21pt;line-height: 14pt;text-align: left;"><a name="bookmark73">REFERENCES</a><a name="bookmark74">&zwnj;</a></h2></li></ol></li></ol><ol id="l13"><li><p class="s96" style="padding-left: 71pt;text-indent: -13pt;text-align: left;">R. Bekkerman. The present and the future of the kdd cup competition: an outsider’s perspective.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 71pt;text-indent: -13pt;text-align: left;"><a name="bookmark75">R. Bekkerman, M. Bilenko, and J. Langford. </a><span class="s95">Scaling Up Machine Learning: Parallel and Distributed Approaches</span>. Cambridge University Press, New York, NY, USA, 2011.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 71pt;text-indent: -13pt;text-align: left;"><a name="bookmark76">J. Bennett and S. Lanning. The netflix prize. In </a><span class="s95">Proceedings of the KDD Cup Workshop 2007</span>, pages 3–6, New York, Aug. 2007.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 71pt;text-indent: -13pt;text-align: left;"><a name="bookmark77">L. Breiman. Random forests. </a><span class="s95">Maching Learning</span>, 45(1):5–32, Oct. 2001.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 71pt;text-indent: -13pt;text-align: left;"><a name="bookmark78">C. Burges. From ranknet to lambdarank to lambdamart: An overview. </a><span class="s95">Learning</span>, 11:23–581, 2010.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 71pt;text-indent: -13pt;text-align: left;"><a name="bookmark79">O. Chapelle and Y. Chang. Yahoo! Learning to Rank Challenge Overview. </a><span class="s95">Journal of Machine Learning Research - W &amp; CP</span>, 14:1–24, 2011.</p></li><li><p class="s96" style="padding-left: 71pt;text-indent: -13pt;text-align: left;"><a name="bookmark80">T. Chen, H. Li, Q. Yang, and Y. Yu. General functional matrix factorization using gradient boosting. In </a><span class="s95">Proceeding</span></p></li></ol><p class="s96" style="padding-left: 39pt;text-indent: 0pt;text-align: left;">regression: a statistical view of boosting. <span class="s95">Annals of Statistics</span>, 28(2):337–407, 2000.</p><ol id="l14"><ol id="l15"><ol id="l16"><li><p class="s96" style="padding-top: 1pt;padding-left: 39pt;text-indent: -17pt;text-align: left;"><a name="bookmark81">J. H. Friedman and B. E. Popescu. Importance sampled learning ensembles, 2003.</a></p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 39pt;text-indent: -17pt;text-align: left;"><a name="bookmark82">M. Greenwald and S. Khanna. Space-efficient online computation of quantile summaries. In </a><span class="s95">Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data</span>, pages 58–66, 2001.</p></li><li><p class="s96" style="padding-left: 39pt;text-indent: -17pt;line-height: 9pt;text-align: left;"><a name="bookmark83">X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi,</a></p><p class="s96" style="padding-left: 39pt;text-indent: 0pt;text-align: left;">A. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela. Practical lessons from predicting clicks on ads at facebook. In <span class="s95">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</span>, ADKDD’14, 2014.</p></li><li><p class="s96" style="padding-left: 39pt;text-indent: -17pt;text-align: left;"><a name="bookmark84">P. Li. Robust Logitboost and adaptive base class (ABC) Logitboost. In </a><span class="s95">Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI’10)</span>, pages 302–311, 2010.</p></li><li><p class="s96" style="padding-left: 39pt;text-indent: -17pt;text-align: left;"><a name="bookmark85">P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank using multiple classification and gradient boosting. In </a><span class="s95">Advances in Neural Information Processing Systems 20</span>, pages 897–904. 2008.</p></li><li><p class="s96" style="padding-left: 39pt;text-indent: -17pt;line-height: 9pt;text-align: left;"><a name="bookmark86">X. Meng, J. Bradley, B. Yavuz, E. Sparks,</a></p><p class="s96" style="padding-left: 39pt;text-indent: 0pt;line-height: 9pt;text-align: left;">S. Venkataraman, D. Liu, J. Freeman, D. Tsai, M. Amde,</p><p class="s96" style="padding-left: 39pt;text-indent: 0pt;line-height: 9pt;text-align: left;">S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh,</p><p class="s96" style="padding-left: 39pt;text-indent: 0pt;text-align: left;">M. Zaharia, and A. Talwalkar. MLlib: Machine learning in apache spark. <span class="s95">Journal of Machine Learning Research</span>, 17(34):1–7, 2016.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 39pt;text-indent: -17pt;text-align: left;"><a name="bookmark87">B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo. Planet: Massively parallel learning of tree ensembles with mapreduce. </a><span class="s95">Proceeding of VLDB Endowment</span>, 2(2):1426–1437, Aug. 2009.</p></li><li><p class="s96" style="padding-left: 39pt;text-indent: -17pt;line-height: 9pt;text-align: left;"><a name="bookmark88">F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,</a></p><p class="s96" style="padding-left: 39pt;text-indent: 0pt;line-height: 9pt;text-align: left;">B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,</p><p class="s96" style="padding-left: 39pt;text-indent: 0pt;line-height: 9pt;text-align: left;">R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,</p><p class="s96" style="padding-left: 39pt;text-indent: 0pt;text-align: left;">D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. <span class="s95">Journal of Machine Learning Research</span>, 12:2825–2830, 2011.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 39pt;text-indent: -17pt;text-align: left;"><a name="bookmark89">G. Ridgeway. </a><span class="s95">Generalized Boosted Models: A guide to the gbm package</span>.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 39pt;text-indent: -17pt;text-align: left;"><a name="bookmark90">S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin. Parallel boosted regression trees for web search ranking. In </a><span class="s95">Proceedings of the 20th international conference on World wide web</span>, pages 387–396. ACM, 2011.</p></li><li><p class="s96" style="padding-left: 39pt;text-indent: -17pt;text-align: left;"><a name="bookmark91">J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic gradient boosted distributed decision trees. In </a><span class="s95">Proceedings of the 18th ACM Conference on Information and Knowledge Management</span>, CIKM ’09.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 39pt;text-indent: -17pt;text-align: justify;"><a name="bookmark92">Q. Zhang and W. Wang. A fast algorithm for approximate quantiles in high speed data streams. In </a><span class="s95">Proceedings of the 19th International Conference on Scientific and Statistical Database Management</span>, 2007.</p></li><li><p class="s96" style="padding-top: 1pt;padding-left: 39pt;text-indent: -17pt;text-align: left;"><a name="bookmark93">T. Zhang and R. Johnson. Learning nonlinear functions using regularized greedy forest. </a><span class="s95">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, 36(5), 2014.</p></li></ol></ol></ol></body></html>
