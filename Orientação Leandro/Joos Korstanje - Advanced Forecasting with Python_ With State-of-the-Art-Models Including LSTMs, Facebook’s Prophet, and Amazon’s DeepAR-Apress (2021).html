<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 body { background-color: #000; }
 .s1 { color: #FDC200; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 84pt; }
 .s2 { color: #FFF; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 24pt; }
 .s3 { color: #FDC200; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 24pt; }
 h1 { color: #2B2A29; font-family:"Arial Narrow", sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 40pt; }
 h3 { color: #9F9F9F; font-family:"Arial Narrow", sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 28pt; }
 .s4 { color: #2B2A29; font-family:"Arial Narrow", sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 18pt; }
 .s5 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s6 { color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s7 { color: #00F; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .p, p { color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; margin:0pt; }
 .s8 { color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s9 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s10 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 14pt; }
 h2 { color: #2B2A29; font-family:"Arial Narrow", sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 30pt; }
 .s11 { color: #00F; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s12 { color: #00F; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s13 { color: #00F; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 .s14 { color: #00F; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 .s16 { color: #00F; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s17 { color: #00F; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s18 { color: #00F; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s19 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s20 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s21 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 30pt; }
 .s22 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 18pt; }
 .s23 { color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s24 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s25 { color: #00F; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 h4 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 20pt; }
 .s26 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 16pt; }
 .s27 { color: #00F; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s28 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s29 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s30 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s31 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s32 { color: #00F; font-family:SimSun; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s33 { color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s34 { color: #2B2A29; font-family:SimSun; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s35 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s36 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s37 { color: #2B2A29; font-family:SimSun; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s38 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 13pt; }
 .s39 { color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; }
 .s40 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s41 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s42 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s43 { color: black; font-family:Symbol, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s44 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s46 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; }
 .s47 { color: black; font-family:Symbol, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 16pt; vertical-align: -1pt; }
 .s48 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 16pt; }
 .s49 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 1pt; }
 .s50 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 7pt; }
 .s51 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -6pt; }
 .s52 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s53 { color: black; font-family:"Felix Titling"; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s54 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; vertical-align: 7pt; }
 .s55 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s56 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s57 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 1pt; }
 .s58 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -3pt; }
 .s59 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s60 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -2pt; }
 .s61 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; vertical-align: 7pt; }
 .s62 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s63 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 5pt; }
 .s64 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 16pt; vertical-align: -1pt; }
 .s65 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -1pt; }
 .s66 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s67 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s68 { color: #00F; font-family:SimSun; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s70 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -2pt; }
 .s71 { color: black; font-family:"Bookman Old Style", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s72 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s73 { color: black; font-family:"Bookman Old Style", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s74 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s75 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 2pt; }
 .s76 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s77 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s78 { color: #2B2A29; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s79 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s80 { color: #2B2A29; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s81 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s82 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s83 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 16.5pt; }
 .s84 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s85 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s86 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s87 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s88 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s89 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 5pt; }
 .s90 { color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s91 { color: black; font-family:"Arial Narrow", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 1pt; }
 .s92 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -2pt; }
 .s93 { color: black; font-family:"Arial Narrow", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s94 { color: black; font-family:"Arial Narrow", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s95 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 1pt; }
 .s96 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -1pt; }
 .s97 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 5pt; }
 .s98 { color: black; font-family:"Arial Narrow", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 5pt; }
 .s99 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -2pt; }
 .s100 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 2pt; }
 .s101 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -7pt; }
 .s102 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 1pt; }
 .s103 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -1pt; }
 .s104 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s105 { color: black; font-family:"Arial Narrow", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -2pt; }
 .s106 { color: black; font-family:"Century Gothic", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s107 { color: black; font-family:"Century Gothic", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 5pt; }
 .s108 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 6pt; }
 .s109 { color: black; font-family:"Lucida Sans", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 3pt; }
 .s110 { color: black; font-family:"Lucida Sans", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s111 { color: black; font-family:"Century Gothic", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 17pt; vertical-align: -1pt; }
 .s112 { color: black; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 6pt; vertical-align: 5pt; }
 .s113 { color: black; font-family:"Century Gothic", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 8pt; }
 .s114 { color: black; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s115 { color: black; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s116 { color: black; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 16.5pt; }
 .s117 { color: black; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s118 { color: #2B2A29; font-family:Cambria, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s119 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -3pt; }
 .s120 { color: black; font-family:"Century Gothic", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s121 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -3pt; }
 .s122 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -8pt; }
 .s123 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -4pt; }
 .s124 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s125 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s126 { color: black; font-family:"Gill Sans MT", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s127 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 16.5pt; vertical-align: -2pt; }
 .s128 { color: black; font-family:"Gill Sans MT", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 2pt; }
 .s129 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -4pt; }
 .s130 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s131 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 3pt; }
 .s132 { color: black; font-family:"Franklin Gothic Medium", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s133 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 7pt; }
 .s134 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 3pt; }
 .s135 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s136 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 16.5pt; vertical-align: -3pt; }
 .s137 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s138 { color: black; font-family:"Courier New", monospace; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11.5pt; }
 .s139 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s140 { color: black; font-family:"Lucida Sans", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 4pt; }
 .s141 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 17pt; vertical-align: -1pt; }
 .s142 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 5pt; }
 .s143 { color: black; font-family:"Courier New", monospace; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11.5pt; vertical-align: 1pt; }
 .s144 { color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 17pt; }
 .s145 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 1pt; }
 .s146 { color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 6pt; }
 .s147 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -3pt; }
 .s148 { color: black; font-family:"Felix Titling"; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s149 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s150 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s151 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 6.5pt; }
 .s152 { color: #2B2A29; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s153 { color: #2B2A29; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s154 { color: black; font-family:"Perpetua Titling MT", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s155 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 4pt; }
 .s156 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 7pt; }
 .s157 { color: #2B2A29; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s158 { color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s159 { color: #2B2A29; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s160 { color: #2B2A29; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s161 { color: #2B2A29; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; }
 .s162 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11.5pt; }
 .s163 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 17pt; vertical-align: -1pt; }
 .s164 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 5pt; }
 .s165 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 8pt; }
 .s166 { color: #2B2A29; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s167 { color: #2B2A29; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s168 { color: black; font-family:"Century Gothic", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s169 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; }
 .s171 { color: #00F; font-family:SimSun; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; }
 .s172 { color: #2B2A29; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s173 { color: #00F; font-family:SimSun; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s174 { color: #2B2A29; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .a, a { color: #00F; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)". "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 li {display: block; }
 #l2 {padding-left: 0pt;counter-reset: d1 23; }
 #l2> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l2> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l3 {padding-left: 0pt;counter-reset: d2 56; }
 #l3> li>*:first-child:before {counter-increment: d2; content: counter(d1, decimal)"."counter(d2, decimal)" "; color: #2B2A29; font-family:"Book Antiqua", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: d2 0;  }
 #l4 {padding-left: 0pt; }
 #l4> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l5 {padding-left: 0pt; }
 #l5> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l6 {padding-left: 0pt; }
 #l6> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l7 {padding-left: 0pt; }
 #l7> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l8 {padding-left: 0pt; }
 #l8> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l9 {padding-left: 0pt;counter-reset: e1 0; }
 #l9> li>*:first-child:before {counter-increment: e1; content: counter(e1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l9> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 #l10 {padding-left: 0pt;counter-reset: e2 4; }
 #l10> li>*:first-child:before {counter-increment: e2; content: counter(e1, decimal)"."counter(e2, decimal)" "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l10> li:first-child>*:first-child:before {counter-increment: e2 0;  }
 #l11 {padding-left: 0pt; }
 #l11> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l12 {padding-left: 0pt;counter-reset: f1 20; }
 #l12> li>*:first-child:before {counter-increment: f1; content: counter(f1, lower-latin)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l12> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l13 {padding-left: 0pt;counter-reset: f2 1; }
 #l13> li>*:first-child:before {counter-increment: f2; content: counter(f1, lower-latin)"-"counter(f2, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l13> li:first-child>*:first-child:before {counter-increment: f2 0;  }
 #l14 {padding-left: 0pt;counter-reset: f3 11; }
 #l14> li>*:first-child:before {counter-increment: f3; content: counter(f1, lower-latin)"-"counter(f2, decimal)"-"counter(f3, lower-latin)" "; color: black; font-family:"Book Antiqua", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 #l14> li:first-child>*:first-child:before {counter-increment: f3 0;  }
 #l15 {padding-left: 0pt; }
 #l15> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l16 {padding-left: 0pt; }
 #l16> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l17 {padding-left: 0pt; }
 #l17> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l18 {padding-left: 0pt; }
 #l18> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l19 {padding-left: 0pt; }
 #l19> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l20 {padding-left: 0pt; }
 #l20> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l21 {padding-left: 0pt; }
 #l21> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l22 {padding-left: 0pt; }
 #l22> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l23 {padding-left: 0pt; }
 #l23> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l24 {padding-left: 0pt; }
 #l24> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l25 {padding-left: 0pt; }
 #l25> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l26 {padding-left: 0pt;counter-reset: i1 12; }
 #l26> li>*:first-child:before {counter-increment: i1; content: "("counter(i1, upper-latin)") "; color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; }
 #l26> li:first-child>*:first-child:before {counter-increment: i1 0;  }
 #l27 {padding-left: 0pt; }
 #l27> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l28 {padding-left: 0pt; }
 #l28> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l29 {padding-left: 0pt; }
 #l29> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l30 {padding-left: 0pt;counter-reset: j1 12; }
 #l30> li>*:first-child:before {counter-increment: j1; content: "("counter(j1, upper-latin)") "; color: black; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; }
 #l30> li:first-child>*:first-child:before {counter-increment: j1 0;  }
 #l31 {padding-left: 0pt; }
 #l31> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l32 {padding-left: 0pt; }
 #l32> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l33 {padding-left: 0pt; }
 #l33> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l34 {padding-left: 0pt; }
 #l34> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l35 {padding-left: 0pt; }
 #l35> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l36 {padding-left: 0pt; }
 #l36> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l37 {padding-left: 0pt; }
 #l37> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l38 {padding-left: 0pt; }
 #l38> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l39 {padding-left: 0pt; }
 #l39> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l40 {padding-left: 0pt; }
 #l40> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l41 {padding-left: 0pt; }
 #l41> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l42 {padding-left: 0pt; }
 #l42> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l43 {padding-left: 0pt; }
 #l43> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l44 {padding-left: 0pt;counter-reset: o1 1; }
 #l44> li>*:first-child:before {counter-increment: o1; content: counter(o1, decimal)". "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l44> li:first-child>*:first-child:before {counter-increment: o1 0;  }
 li {display: block; }
 #l45 {padding-left: 0pt; }
 #l45> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l46 {padding-left: 0pt;counter-reset: q1 1; }
 #l46> li>*:first-child:before {counter-increment: q1; content: counter(q1, decimal)". "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l46> li:first-child>*:first-child:before {counter-increment: q1 0;  }
 #l47 {padding-left: 0pt; }
 #l47> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l48 {padding-left: 0pt; }
 #l48> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l49 {padding-left: 0pt; }
 #l49> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l50 {padding-left: 0pt; }
 #l50> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l51 {padding-left: 0pt; }
 #l51> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l52 {padding-left: 0pt; }
 #l52> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l53 {padding-left: 0pt;counter-reset: u1 1; }
 #l53> li>*:first-child:before {counter-increment: u1; content: counter(u1, decimal)". "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l53> li:first-child>*:first-child:before {counter-increment: u1 0;  }
 #l54 {padding-left: 0pt; }
 #l54> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l55 {padding-left: 0pt; }
 #l55> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l56 {padding-left: 0pt; }
 #l56> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l57 {padding-left: 0pt;counter-reset: w1 72; }
 #l57> li>*:first-child:before {counter-increment: w1; content: counter(w1, decimal)". "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l57> li:first-child>*:first-child:before {counter-increment: w1 0;  }
 #l58 {padding-left: 0pt; }
 #l58> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l59 {padding-left: 0pt; }
 #l59> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l60 {padding-left: 0pt;counter-reset: x1 0; }
 #l60> li>*:first-child:before {counter-increment: x1; content: counter(x1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l60> li:first-child>*:first-child:before {counter-increment: x1 0;  }
 #l61 {padding-left: 0pt;counter-reset: x2 93; }
 #l61> li>*:first-child:before {counter-increment: x2; content: counter(x1, decimal)"."counter(x2, decimal)". "; color: #2B2A29; font-family:Cambria, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l61> li:first-child>*:first-child:before {counter-increment: x2 0;  }
 #l62 {padding-left: 0pt; }
 #l62> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l63 {padding-left: 0pt; }
 #l63> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l64 {padding-left: 0pt; }
 #l64> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l65 {padding-left: 0pt; }
 #l65> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l66 {padding-left: 0pt; }
 #l66> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l67 {padding-left: 0pt; }
 #l67> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l68 {padding-left: 0pt; }
 #l68> li>*:first-child:before {content: "– "; color: #2B2A29; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l69 {padding-left: 0pt; }
 #l69> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l70 {padding-left: 0pt; }
 #l70> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l71 {padding-left: 0pt; }
 #l71> li>*:first-child:before {content: "• "; color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><span><img width="674" height="121" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_001.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="453" height="292" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_002.png"/></span></p><p class="s1" style="padding-top: 16pt;padding-left: 5pt;text-indent: 0pt;line-height: 85%;text-align: left;">Advanced Forecasting with Python</p><p class="s2" style="padding-top: 13pt;padding-left: 65pt;text-indent: 0pt;text-align: left;">With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR</p><p class="s2" style="padding-left: 65pt;text-indent: 0pt;line-height: 20pt;text-align: left;">—</p><p class="s3" style="padding-left: 65pt;text-indent: 0pt;line-height: 28pt;text-align: left;">Joos Korstanje</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 8pt;padding-left: 139pt;text-indent: -81pt;line-height: 92%;text-align: left;">Advanced Forecasting with Python</h1><h3 style="padding-top: 21pt;padding-left: 58pt;text-indent: 18pt;line-height: 93%;text-align: justify;">With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Joos Korstanje</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 363pt;text-indent: 0pt;text-align: left;"><span><img width="78" height="18" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_003.png"/></span></p><p class="s5" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 121%;text-align: left;">Advanced Forecasting with Python: With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR</p><p class="s6" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 109%;text-align: left;">Joos Korstanje Maisons Alfort, France</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">ISBN-13 (pbk): 978-1-4842-7149-0 ISBN-13 (electronic): 978-1-4842-7150-6</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="https://doi.org/10.1007/978-1-4842-7150-6" class="s7">https://doi.org/10.1007/978-1-4842-7150-6</a></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Copyright © 2021 by Joos Korstanje</p><p class="s6" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;line-height: 109%;text-align: left;">This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.</p><p class="s6" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 109%;text-align: left;">Trademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every occurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark.</p><p class="s6" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 109%;text-align: left;">The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.</p><p class="s6" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 109%;text-align: left;">While the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.</p><p class="s6" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;line-height: 109%;text-align: left;">Managing Director, Apress Media LLC: Welmoed Spahr Acquisitions Editor: Celestin Suresh John Development Editor: Matthew Moodie</p><p class="s6" style="padding-left: 8pt;text-indent: 18pt;line-height: 158%;text-align: left;">Coordinating Editor: Aditee Mirashi Cover designed by eStudioCalamar</p><p class="s6" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Cover image designed by Freepik (www.freepik.com)</p><p class="s6" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;line-height: 107%;text-align: left;"><a href="http://www.springeronline.com/" class="s8" target="_blank">Distributed to the book trade worldwide by Springer Science+Business Media New York, 1 New York Plaza, Suite 4600, New York, NY 10004-1562, USA. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@ springer-sbm.com, or visit </a>www.springeronline.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a <span class="s9">Delaware </span>corporation.</p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;line-height: 109%;text-align: left;"><a href="mailto:bookpermissions@springernature.com" class="s8" target="_blank">For information on translations, please e-mail booktranslations@springernature.com; for reprint, paperback, or audio rights, please e-mail bookpermissions@springernature.com.</a></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 109%;text-align: left;"><a href="http://www.apress.com/bulk-sales" class="s8" target="_blank">Apress titles may be purchased in bulk for academic, corporate, or promotional use. eBook versions and licenses are also available for most titles. For more information, reference our Print and eBook Bulk Sales web page at http://www.apress.com/bulk-sales.</a></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 109%;text-align: left;"><a href="http://www.apress.com/978-1-4842-7149-0" class="s8" target="_blank">Any source code or other supplementary material referenced by the author in this book is available to readers on GitHub via the book’s product page, located at </a><a href="http://www.apress.com/source-code" class="s8" target="_blank">www.apress.com/978-1-4842-7149-0. For more detailed information, please visit http://www.apress.com/source-code.</a></p><p class="s6" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Printed on acid-free paper</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 52pt;text-indent: 0pt;line-height: 16pt;text-align: center;">This book is dedicated to my partner, Olivia,</p><p class="s10" style="padding-left: 52pt;text-indent: 0pt;line-height: 16pt;text-align: center;">for the help and support throughout the period of writing.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark0">Table of Contents</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;line-height: 174%;text-align: justify;"><a href="#bookmark2" class="s11">About the Author ��������������������������������������������������������������������������������������������������� </a><a href="#bookmark4" class="s11">xiii About the Technical Reviewer ���������������������������������������������������������������������������������</a><a href="#bookmark6" class="s11">xv Introduction �����������������������������������������������������������������������������������������������������������xvii</a></p><p class="s12" style="padding-top: 7pt;padding-left: 25pt;text-indent: 0pt;line-height: 153%;text-align: right;"><a href="#bookmark7" class="s13">Part I: Machine Learning for Forecasting ���������������������������������������������� </a><span class="s14">1 </span>Chapter 1<span style=" color: #0000F9;">: </span>Models for Forecasting ��������������������������������������������������������������������������� 3 <span class="s16">Reading Guide for This Book��������������������������������������������������������������������������������������������������������� 4</span></p><p class="s16" style="padding-left: 35pt;text-indent: 0pt;line-height: 144%;text-align: right;">Machine Learning Landscape ������������������������������������������������������������������������������������������������������� 4 Univariate Time Series Models ������������������������������������������������������������������������������������������������ 4 Supervised Machine Learning Models ������������������������������������������������������������������������������������ 9 Other Distinctions in Machine Learning Models�������������������������������������������������������������������� 17 Key Takeaways���������������������������������������������������������������������������������������������������������������������������� 18</p><p class="s12" style="padding-top: 4pt;padding-left: 35pt;text-indent: -9pt;line-height: 146%;text-align: right;">Chapter 2<span style=" color: #0000F9;">: </span>Model Evaluation for Forecasting ���������������������������������������������������������21 <span class="s16">Evaluation with an Example Forecast ����������������������������������������������������������������������������������������� 21 Model Quality Metrics ����������������������������������������������������������������������������������������������������������������� 24 Metric 1: MSE ������������������������������������������������������������������������������������������������������������������������ 25</span></p><p class="s16" style="text-indent: 0pt;line-height: 12pt;text-align: right;">Metric 2: RMSE ���������������������������������������������������������������������������������������������������������������������� 26</p><p class="s16" style="padding-top: 5pt;text-indent: 0pt;text-align: right;">Metric 3: MAE ������������������������������������������������������������������������������������������������������������������������ 27</p><p class="s16" style="padding-top: 5pt;text-indent: 0pt;text-align: right;">Metric 4: MAPE ���������������������������������������������������������������������������������������������������������������������� 28</p><p class="s16" style="padding-top: 5pt;padding-left: 35pt;text-indent: 14pt;line-height: 145%;text-align: right;">Metric 5: R2 ��������������������������������������������������������������������������������������������������������������������������� 28 Model Evaluation Strategies ������������������������������������������������������������������������������������������������������� 29 Overfit and the Out-of-Sample Error ������������������������������������������������������������������������������������� 30 Strategy 1: Train-Test Split ���������������������������������������������������������������������������������������������������� 30</p><p class="s16" style="padding-left: 46pt;text-indent: 0pt;line-height: 142%;text-align: right;">Strategy 2: Train-Validation-Test Split ����������������������������������������������������������������������������������� 32 Strategy 3: Cross-Validation for Forecasting ������������������������������������������������������������������������� 34</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 150%;text-align: justify;">Backtesting ��������������������������������������������������������������������������������������������������������������������������������� 39 Which Strategy to Use for Safe Forecasts?��������������������������������������������������������������������������������� 40 Final Considerations on Model Evaluation ���������������������������������������������������������������������������������� 41 Key Takeaways���������������������������������������������������������������������������������������������������������������������������� 42</p><p class="s12" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 153%;text-align: right;"><a href="#bookmark37" class="s13">Part II: Univariate Time Series Models ������������������������������������������������</a><span class="s14">43 </span>Chapter 3<span style=" color: #0000F9;">: </span>The AR Model ���������������������������������������������������������������������������������������� 45 <span class="s16">Autocorrelation: The Past Influences the Present ����������������������������������������������������������������������� 46</span></p><p class="s16" style="text-indent: 0pt;line-height: 11pt;text-align: right;">Compute Autocorrelation in Earthquake Counts�������������������������������������������������������������������� 46</p><p class="s16" style="padding-top: 5pt;padding-left: 17pt;text-indent: 14pt;line-height: 147%;text-align: right;">Positive and Negative Autocorrelation����������������������������������������������������������������������������������� 50 Stationarity and the ADF Test ������������������������������������������������������������������������������������������������������ 51 Differencing a Time Series���������������������������������������������������������������������������������������������������������� 52 Lags in Autocorrelation ��������������������������������������������������������������������������������������������������������������� 55 Partial Autocorrelation����������������������������������������������������������������������������������������������������������� 57 How Many Lags to Include? �������������������������������������������������������������������������������������������������� 58</p><p class="s16" style="padding-left: 16pt;text-indent: 0pt;line-height: 146%;text-align: right;">AR Model Definition �������������������������������������������������������������������������������������������������������������������� 59 Estimating the AR Using Yule-Walker Equations ������������������������������������������������������������������������� 60 The Yule-Walker Method�������������������������������������������������������������������������������������������������������� 60 Train-Test Evaluation and Tuning������������������������������������������������������������������������������������������� 64 Key Takeaways���������������������������������������������������������������������������������������������������������������������������� 69</p><p class="s12" style="padding-top: 4pt;padding-left: 17pt;text-indent: -9pt;line-height: 149%;text-align: justify;">Chapter 4<span style=" color: #0000F9;">: </span>The MA Model ��������������������������������������������������������������������������������������� 71 <span class="s16">The Model Definition ������������������������������������������������������������������������������������������������������������������� 72 Fitting the MA Model������������������������������������������������������������������������������������������������������������������� 73 Stationarity���������������������������������������������������������������������������������������������������������������������������������� 74 Choosing Between an AR and an MA Model ������������������������������������������������������������������������������� 74 Application of the MA Model ������������������������������������������������������������������������������������������������������� 75 Multistep Forecasting with Model Retraining ����������������������������������������������������������������������������� 82 Grid Search to Find the Best MA Order ��������������������������������������������������������������������������������������� 84 Key Takeaways���������������������������������������������������������������������������������������������������������������������������� 86</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-top: 5pt;padding-left: 35pt;text-indent: -9pt;line-height: 149%;text-align: justify;">Chapter 5<span style=" color: #0000F9;">: </span>The ARMA Model ����������������������������������������������������������������������������������� 89 <span class="s16">The Idea Behind the ARMA Model����������������������������������������������������������������������������������������������� 89 The Mathematical Definition of the ARMA Model������������������������������������������������������������������������ 90 An Example: Predicting Sunspots Using ARMA ��������������������������������������������������������������������������� 90 Fitting an ARMA(1,1) Model��������������������������������������������������������������������������������������������������������� 94 More Model Evaluation KPIs�������������������������������������������������������������������������������������������������������� 96 Automated Hyperparameter Tuning �������������������������������������������������������������������������������������������� 99 Grid Search: Tuning for Predictive Performance ����������������������������������������������������������������������� 100 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 104</span></p><p class="s12" style="padding-top: 4pt;padding-left: 35pt;text-indent: -9pt;line-height: 149%;text-align: justify;">Chapter 6<span style=" color: #0000F9;">: </span>The ARIMA Model �������������������������������������������������������������������������������� 105 <span class="s16">ARIMA Model Definition ������������������������������������������������������������������������������������������������������������ 106 Model Definition������������������������������������������������������������������������������������������������������������������������ 106 ARIMA on the CO2 Example ������������������������������������������������������������������������������������������������������ 107 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 113</span></p><p class="s12" style="padding-top: 4pt;padding-left: 35pt;text-indent: -9pt;line-height: 149%;text-align: justify;">Chapter 7<span style=" color: #0000F9;">: </span>The SARIMA Model������������������������������������������������������������������������������ 115 <span class="s16">Univariate Time Series Model Breakdown �������������������������������������������������������������������������������� 115 The SARIMA Model Definition ��������������������������������������������������������������������������������������������������� 116 Example: SARIMA on Walmart Sales ����������������������������������������������������������������������������������������� 117 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 122</span></p><p class="s12" style="padding-top: 4pt;padding-left: 25pt;text-indent: 0pt;line-height: 151%;text-align: right;"><a href="#bookmark103" class="s13">Part III: Multivariate Time Series Models ������������������������������������������ </a><span class="s14">123 </span>Chapter 8<span style=" color: #0000F9;">: </span>The SARIMAX Model���������������������������������������������������������������������������� 125 <span class="s16">Time Series Building Blocks ����������������������������������������������������������������������������������������������������� 125 Model Definition������������������������������������������������������������������������������������������������������������������������ 126 Supervised Models vs� SARIMAX ���������������������������������������������������������������������������������������������� 126 Example of SARIMAX on the Walmart Dataset �������������������������������������������������������������������������� 127 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 131</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-top: 5pt;padding-left: 17pt;text-indent: -9pt;line-height: 146%;text-align: right;">Chapter 9<span style=" color: #0000F9;">: </span>The VAR Model ������������������������������������������������������������������������������������ 133 <span class="s16">The Model Definition ����������������������������������������������������������������������������������������������������������������� 133 Order: Only One Hyperparameter����������������������������������������������������������������������������������������� 134 Stationarity �������������������������������������������������������������������������������������������������������������������������� 134 Estimation of the VAR Coefficients �������������������������������������������������������������������������������������� 135 One Multivariate Model vs� Multiple Univariate Models������������������������������������������������������������ 135 An Example: VAR for Forecasting Walmart Sales ���������������������������������������������������������������������� 136 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 139</span></p><p class="s12" style="padding-top: 4pt;padding-left: 17pt;text-indent: -9pt;line-height: 148%;text-align: justify;">Chapter 10<span style=" color: #0000F9;">: </span>The VARMAX Model ��������������������������������������������������������������������������� 141 <span class="s16">Model Definition������������������������������������������������������������������������������������������������������������������������ 142 Multiple Time Series with Exogenous Variables������������������������������������������������������������������������ 142 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 145</span></p><p class="s12" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 152%;text-align: right;"><a href="#bookmark120" class="s13">Part IV: Supervised Machine Learning Models ���������������������������������� </a><span class="s14">147 </span>Chapter 11<span style=" color: #0000F9;">: </span>The Linear Regression����������������������������������������������������������������������� 149 <span class="s16">The Idea Behind Linear Regression ������������������������������������������������������������������������������������������ 150 Model Definition������������������������������������������������������������������������������������������������������������������������ 150</span></p><p class="s16" style="padding-left: 16pt;text-indent: 0pt;line-height: 146%;text-align: right;">Example: Linear Model to Forecast CO<span class="s17">2</span><span class="s18"> </span>Levels ������������������������������������������������������������������������� 151 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 157</p><p class="s12" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Chapter 12<span style=" color: #0000F9;">: </span>The Decision Tree Model ������������������������������������������������������������������� 159</p><p class="s16" style="padding-top: 6pt;padding-left: 7pt;text-indent: 0pt;text-align: right;">Mathematics ����������������������������������������������������������������������������������������������������������������������������� 160</p><p class="s16" style="padding-top: 5pt;padding-left: 17pt;text-indent: 14pt;line-height: 146%;text-align: right;">Splitting ������������������������������������������������������������������������������������������������������������������������������� 160 Pruning and Reducing Complexity��������������������������������������������������������������������������������������� 160 Example ������������������������������������������������������������������������������������������������������������������������������������ 161</p><p class="s16" style="text-indent: 0pt;text-align: right;">Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 168</p><p class="s12" style="padding-top: 10pt;padding-left: 17pt;text-indent: -9pt;line-height: 145%;text-align: right;">Chapter 13<span style=" color: #0000F9;">: </span>The kNN Model ���������������������������������������������������������������������������������� 169 <span class="s16">Intuitive Explanation������������������������������������������������������������������������������������������������������������������ 169 Mathematical Definition of Nearest Neighbors ������������������������������������������������������������������������� 169 Combining k Neighbors into One Forecast �������������������������������������������������������������������������� 171 Deciding on the Number of Neighbors k ����������������������������������������������������������������������������� 171</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-top: 5pt;padding-left: 35pt;text-indent: 14pt;line-height: 145%;text-align: right;">Predicting Traffic Using kNN������������������������������������������������������������������������������������������������ 172 Grid Search on kNN ������������������������������������������������������������������������������������������������������������� 175 Random Search: An Alternative to Grid Search ������������������������������������������������������������������� 176 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 177</p><p class="s12" style="padding-top: 4pt;padding-left: 35pt;text-indent: -9pt;line-height: 146%;text-align: right;">Chapter 14<span style=" color: #0000F9;">: </span>The Random Forest ��������������������������������������������������������������������������� 179 <span class="s16">Intuitive Idea Behind Random Forests �������������������������������������������������������������������������������������� 179 Random Forest Concept 1: Ensemble Learning ������������������������������������������������������������������������ 180 Bagging Concept 1: Bootstrap ��������������������������������������������������������������������������������������������� 180</span></p><p class="s16" style="padding-left: 35pt;text-indent: 14pt;line-height: 148%;text-align: right;">Bagging Concept 2: Aggregation ����������������������������������������������������������������������������������������� 181 Random Forest Concept 2: Variable Subsets ���������������������������������������������������������������������������� 182 Predicting Sunspots Using a Random Forest���������������������������������������������������������������������������� 182 Grid Search on the Two Main Hyperparameters of the Random Forest ������������������������������������ 184 Random Search CV Using Distributions ������������������������������������������������������������������������������������ 185 Distribution for max_features���������������������������������������������������������������������������������������������� 186 Distribution for n_estimators����������������������������������������������������������������������������������������������� 187 Fitting the RandomizedSearchCV ���������������������������������������������������������������������������������������� 188 Interpretation of Random Forests: Feature Importance ������������������������������������������������������������ 189 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 191</p><p class="s12" style="padding-top: 3pt;padding-left: 35pt;text-indent: -9pt;line-height: 148%;text-align: right;">Chapter 15<span style=" color: #0000F9;">: </span>Gradient Boosting with XGBoost and LightGBM �������������������������������� 193 <span class="s16">Boosting: A Different Way of Ensemble Learning ���������������������������������������������������������������������� 193 The Gradient in Gradient Boosting �������������������������������������������������������������������������������������������� 194 Gradient Boosting Algorithms ��������������������������������������������������������������������������������������������������� 195 The Difference Between XGBoost and LightGBM���������������������������������������������������������������������� 195 Forecasting Traffic Volume with XGBoost���������������������������������������������������������������������������������� 197 Forecasting Traffic Volume with LightGBM ������������������������������������������������������������������������������� 199 Hyperparameter Tuning Using Bayesian Optimization �������������������������������������������������������������� 200 The Theory of Bayesian Optimization ���������������������������������������������������������������������������������� 201 Bayesian Optimization Using scikit-optimize ���������������������������������������������������������������������� 202 Conclusion �������������������������������������������������������������������������������������������������������������������������������� 204</span></p><p class="s16" style="text-indent: 0pt;text-align: right;">Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 205</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 151%;text-align: right;"><a href="#bookmark168" class="s13">Part V: Advanced Machine and Deep Learning Models ��������������������� </a><span class="s14">207 </span>Chapter 16<span style=" color: #0000F9;">: </span>Neural Networks ������������������������������������������������������������������������������� 209 <span class="s16">Fully Connected Neural Networks��������������������������������������������������������������������������������������������� 209 Activation Functions������������������������������������������������������������������������������������������������������������������ 211 The Weights: Backpropagation�������������������������������������������������������������������������������������������������� 211 Optimizers��������������������������������������������������������������������������������������������������������������������������������� 212 Learning Rate of the Optimizer ������������������������������������������������������������������������������������������������� 212 Hyperparameters at Play in Developing a NN ��������������������������������������������������������������������������� 213</span></p><p class="s16" style="padding-left: 16pt;text-indent: 0pt;line-height: 147%;text-align: right;">Introducing the Example Data��������������������������������������������������������������������������������������������������� 214 Specific Data Prep Needs for a NN ������������������������������������������������������������������������������������������� 215 Scaling and Standardization������������������������������������������������������������������������������������������������ 215 Principal Component Analysis (PCA)������������������������������������������������������������������������������������ 216 The Neural Network Using Keras ���������������������������������������������������������������������������������������������� 219 Conclusion �������������������������������������������������������������������������������������������������������������������������������� 225</p><p class="s16" style="text-indent: 0pt;text-align: right;">Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 226</p><p class="s12" style="padding-top: 10pt;padding-left: 17pt;text-indent: -9pt;line-height: 150%;text-align: justify;">Chapter 17<span style=" color: #0000F9;">: </span>RNNs Using SimpleRNN and GRU ������������������������������������������������������ 227 <span class="s16">What Are RNNs: Architecture����������������������������������������������������������������������������������������������������� 227 Inside the SimpleRNN Unit�������������������������������������������������������������������������������������������������������� 228 The Example ����������������������������������������������������������������������������������������������������������������������������� 229 Predicting a Sequence Rather Than a Value ����������������������������������������������������������������������������� 230 Univariate Model Rather Than Multivariable ����������������������������������������������������������������������������� 230 Preparing the Data �������������������������������������������������������������������������������������������������������������������� 230 A Simple SimpleRNN����������������������������������������������������������������������������������������������������������������� 233 SimpleRNN with Hidden Layers ������������������������������������������������������������������������������������������������ 235 Simple GRU ������������������������������������������������������������������������������������������������������������������������������� 237 GRU with Hidden Layers������������������������������������������������������������������������������������������������������������ 240 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 242</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-top: 5pt;padding-left: 35pt;text-indent: -9pt;line-height: 149%;text-align: justify;">Chapter 18<span style=" color: #0000F9;">: </span>LSTM RNNs���������������������������������������������������������������������������������������� 243 <span class="s16">What Is LSTM ���������������������������������������������������������������������������������������������������������������������������� 243 The LSTM Cell ��������������������������������������������������������������������������������������������������������������������������� 243 Example ������������������������������������������������������������������������������������������������������������������������������������ 244 LSTM with One Layer of 8 ��������������������������������������������������������������������������������������������������������� 246 LSTM with Three Layers of 64 �������������������������������������������������������������������������������������������������� 248 Conclusion �������������������������������������������������������������������������������������������������������������������������������� 251</span></p><p class="s16" style="padding-left: 35pt;text-indent: 0pt;text-align: justify;">Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 251</p><p class="s12" style="padding-top: 10pt;padding-left: 35pt;text-indent: -9pt;line-height: 149%;text-align: justify;">Chapter 19<span style=" color: #0000F9;">: </span>The Prophet Model ���������������������������������������������������������������������������� 253 <span class="s16">The Example ����������������������������������������������������������������������������������������������������������������������������� 254 The Prophet Data Format ���������������������������������������������������������������������������������������������������������� 254 The Basic Prophet Model ���������������������������������������������������������������������������������������������������������� 255 Adding Monthly Seasonality to Prophet ������������������������������������������������������������������������������������ 259 Adding Holiday Data to Basic Prophet �������������������������������������������������������������������������������������� 260 Adding an Extra Regressor to Prophet �������������������������������������������������������������������������������������� 263 Tuning Hyperparameters Using Grid Search ����������������������������������������������������������������������������� 266 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 271</span></p><p class="s12" style="padding-top: 4pt;padding-left: 35pt;text-indent: -9pt;line-height: 149%;text-align: justify;">Chapter 20<span style=" color: #0000F9;">: </span>The DeepAR Model ���������������������������������������������������������������������������� 273 <span class="s16">About DeepAR ��������������������������������������������������������������������������������������������������������������������������� 273 Model Training with DeepAR ����������������������������������������������������������������������������������������������������� 274 Predictions with DeepAR����������������������������������������������������������������������������������������������������������� 276 Probability Predictions with DeepAR����������������������������������������������������������������������������������������� 277 Adding Extra Regressors to DeepAR ����������������������������������������������������������������������������������������� 279 Hyperparameters of the DeepAR����������������������������������������������������������������������������������������������� 281 Benchmark and Conclusion ������������������������������������������������������������������������������������������������������ 283 Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 284</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-top: 5pt;padding-left: 17pt;text-indent: -9pt;line-height: 149%;text-align: justify;">Chapter 21<span style=" color: #0000F9;">: </span>Model Selection��������������������������������������������������������������������������������� 285 <span class="s16">Model Selection Based on Metrics�������������������������������������������������������������������������������������������� 285 Model Structure and Inputs ������������������������������������������������������������������������������������������������������ 286 One-Step Forecasts vs� Multistep Forecasts ���������������������������������������������������������������������������� 287 Model Complexity vs� Gain�������������������������������������������������������������������������������������������������������� 287 Model Complexity vs� Interpretability���������������������������������������������������������������������������������������� 288 Model Stability and Variation ���������������������������������������������������������������������������������������������������� 289 Conclusion �������������������������������������������������������������������������������������������������������������������������������� 289</span></p><p class="s16" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Key Takeaways�������������������������������������������������������������������������������������������������������������������������� 290</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark230" class="s11">Index��������������������������������������������������������������������������������������������������������������������� 291</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark1">About the Author</a><a name="bookmark2">&zwnj;</a></h2><p style="text-indent: 0pt;text-align: left;"><span><img width="160" height="212" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_004.jpg"/></span></p><p class="s19" style="padding-top: 27pt;padding-left: 155pt;text-indent: 0pt;text-align: left;">Joos Korstanje <span class="p">is a data scientist, with over five years of</span></p><p style="padding-top: 3pt;padding-left: 155pt;text-indent: 0pt;line-height: 129%;text-align: left;">industry experience in developing machine learning tools, of which a large part is forecasting models. He currently works at Disneyland Paris where he develops machine</p><p style="padding-left: 155pt;text-indent: 0pt;line-height: 129%;text-align: left;">learning for a variety of tools. His experience in writing and teaching has motivated him to write this book,</p><p class="s20" style="padding-left: 155pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Advanced Forecasting with Python<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark3">About the Technical Reviewer</a><a name="bookmark4">&zwnj;</a></h2><p style="text-indent: 0pt;text-align: left;"><span><img width="160" height="184" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_005.jpg"/></span></p><p class="s19" style="padding-top: 27pt;padding-left: 155pt;text-indent: 0pt;line-height: 128%;text-align: left;">Michael Keith <span class="p">is a data scientist working in the public health sector based in Salt Lake City, Utah. He is passionate about using data to improve health and educational outcomes</span></p><p style="padding-left: 155pt;text-indent: 0pt;line-height: 129%;text-align: left;">and is a lead forecaster for the Utah Department of Health, leveraging Python to produce hundreds of forecasts</p><p style="padding-left: 155pt;text-indent: 0pt;line-height: 128%;text-align: left;">every month. He earned a master’s degree from Florida State University and has worked in data-related roles for several organizations, including Disney in Orlando. He has produced data science–themed videos for Apress, writes for <span class="s20">Towards Data Science</span>, performs consultations for Western</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 129%;text-align: left;">Governors University, and lectures annually to graduate students at Florida State. In his free time, he enjoys road biking, hiking, and watching movies with his wife and beautiful 7-month-old daughter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">xv</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark5">Introduction</a><a name="bookmark6">&zwnj;</a></h2><p class="s20" style="padding-top: 27pt;padding-left: 26pt;text-indent: 0pt;line-height: 128%;text-align: left;">Advanced Forecasting with Python <span class="p">covers all machine learning techniques relevant for forecasting problems, ranging from univariate and multivariate time series to supervised learning, to state-of-the-art deep forecasting models like LSTMs, Recurrent Neural Networks (RNNs), Facebook’s open source Prophet model, and Amazon’s DeepAR model.</span></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Rather than focus on a specific set of models, this book presents an exhaustive overview of all techniques relevant to practitioners of forecasting. It begins by explaining the different categories of models that are relevant for forecasting in a high-level language. Next, it covers univariate and multivariate time series models followed by advanced machine learning and deep learning models, such as Recurrent Neural Networks, LSTMs, Facebook’s Prophet, and Amazon’s DeepAR. It concludes with reflections on model selection like benchmark scores vs. understandability of models</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">vs. compute time and automated retraining and updating of models. Each of the models presented in this book is covered in depth, with an intuitive simple explanation of the model, a mathematical transcription of this idea, and Python code that applies the model to an example dataset.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This book is a great resource for those who want to add a competitive edge to their current forecasting skillset. The book is also adapted to those who start working on forecasting tasks and are looking for an exhaustive book that allows them to start with traditional models and gradually move into more and more advanced models.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can follow along with the code using the GitHub repository that contains a Jupyter notebook per chapter. You are encouraged to use Jupyter notebooks for following along, but you can also run the code in any other Python environment of your choice.</p><p class="s21" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark7">PART I</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Machine Learning for Forecasting</h1><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark8">CHAPTER 1</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Models for Forecasting</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Forecasting, grossly translated as the task of predicting the future, has been present in human society for ages. Whether it is through fortune-tellers, weather forecasts, or</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">algorithmic stock trading, man has always been interested in predicting what the future holds.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Yet forecasting the future is not easy. Consider fortune-tellers, stock market gurus, or weather forecasters: many try to predict the future, but few succeed. And for those who succeed, you will never know whether it was luck or skill.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In recent years, the computing power of computers has become much more commonly available than, say, 30 years ago. This has created a great boom in the use of Artificial Intelligence. Artificial Intelligence and especially machine learning can be used for a wide range of tasks, including robotics, self-driving cars, but also forecasting, that is, if you have a reasonable amount of data about the past that you can project into the future.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Throughout this book, you will learn the modern machine learning techniques that are relevant for forecasting. I will present a large number of machine learning models, together with an intuitive explanation of the model, its mathematics, and an applied use case.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The goal of this book is to give you a real insight into the application of those machine learning models. You will see worked examples applied to real datasets together with honest evaluations of the results: some successful, some less successful.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In this way, this book is different than many other resources, which often present perfectly fitting use cases on simulated data. To learn real-life machine learning and forecasting, it is important to know how models work, but it is even more important to know how to evaluate a model honestly and objectively. This pragmatical point of view will be the guideline throughout the chapters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">3</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 8pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_1#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_1#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_1</a></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark9">Reading Guide for This Book</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Before going further into the different models throughout this book, I will first present a general overview of the machine learning landscape: many types and families of models exist. Each of them has its applications. Before starting, it is important to have</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">an overview of the types of models that exist in machine learning and which of them are relevant for forecasting.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">After this, I will cover several strategies and metrics for evaluating forecasting models. It is important to understand objective evaluation before practicing: you need to understand your goal before starting to practice.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The remaining chapters of the book will each cover a specific model with an intuitive explanation of the model, its mathematical definitions, and an application on a real dataset. You will start simple with common but simple methods and work your way up to the most recent and state-of-the-art methods on the market.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Machine Learning Landscape</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Having the bigger picture of machine learning models before getting into detail will help you to understand how the different models compare to each other and will help you</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">to keep the big picture throughout the book. You will first see univariate time series and supervised regression models: the main categories of forecasting models. After that, you will see a shorter description of machine learning techniques that are less relevant for forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Univariate Time Series Models</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">The first category of machine learning models that I want to talk about is time series models. Even though univariate time series have been around for a long time, they are still used. They also form an important basis for several state-of-the-art techniques. They are classical techniques that any forecaster should be familiar with.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Time series models are models that make a forecast of a variable by looking only at historical developments of the variable itself. This means that time series, as opposed to other model families, do not try to describe any “logical” relationships between</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">variables. They do not try to explain the “why” of trends or seasonalities, but they simply put a mathematical formula on the past and try to project it to the future.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark10">Time series modeling is sometimes criticized for this “lack of science.” But time series models have gained an important place in forecasting due to their performances, and they could not be ignored.</a></p><p class="s26" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">A Quick Example of the Time Series Approach</p><p style="padding-top: 11pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Let’s look at a super-simple, purely hypothetical example of forecasting the average price of a cup of coffee in an imaginary city called X. Imagine someone has made the effort of collecting the average price of coffee for 90 years in this town, with intervals of five years, and that this has yielded the data in Table <span style=" color: #00F;">1-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 98pt;text-indent: 0pt;line-height: 117%;text-align: left;">Table 1-1. <span class="s29">A Hypothetical Example: The Price of a Cup of Coffee Over the Years</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:98pt" cellspacing="0"><tr style="height:24pt"><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Year</p></td><td style="width:240pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Average Price</p></td></tr><tr style="height:22pt"><td style="width:30pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">1960</p></td><td style="width:240pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">0.80</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">1965</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">1.00</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">1970</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">1.20</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">1975</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">1.40</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">1980</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">1.60</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">1985</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">1.80</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">1990</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">2.00</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">1995</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">2.20</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">2000</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">2.40</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">2005</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">2.60</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">2010</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">2.80</p></td></tr><tr style="height:20pt"><td style="width:30pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">2015</p></td><td style="width:240pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">3.00</p></td></tr><tr style="height:22pt"><td style="width:30pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">2020</p></td><td style="width:240pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">3.20</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">This fictitious data clearly shows an increase of 20 cents in the price every five years.</p><p style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 127%;text-align: left;">This is a <span class="s19">linear increasing trend</span>: linear because it increases with the same amount every year and increasing because it becomes more rather than less.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 115%;text-align: left;"><a name="bookmark11">Let’s get this data into Python to see how to plot this linear increasing trend using Listing </a><span style=" color: #00F;">1-1</span><a href="http://www.apress.com/978-1-4842-7149-0" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">. The source code for this book is available on GitHub via the book’s product page, located at </a><span class="s68">www.apress.com/978-1-4842-7149-0</span>. Please note that the library imports are done once per chapter.</p><p class="s28" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 1-1. <span class="s33">Getting the coffee example into Python and plotting the trend</span></p><p class="s34" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">import matplotlib.pyplot as plt</p><p class="s34" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">years = [1965, 1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020]</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">prices = [1.00, 1.20, 1.40, 1.60, 1.80, 2.00, 2.20, 2.40, 2.60, 2.80, 3.00, 3.20]</p><p class="s34" style="padding-top: 8pt;padding-left: 30pt;text-indent: -22pt;line-height: 106%;text-align: left;">data = pd.DataFrame({ &#39;year&#39; : years, &#39;prices&#39;: prices</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">})</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">ax = data.plot.line(x=&#39;year&#39;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 106%;text-align: left;">ax.set_title(&#39;Coffee Price Over Time&#39;, fontsize=16) plt.show()</p><p style="padding-top: 10pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">You will obtain the graph displayed in Figure <span style=" color: #00F;">1-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="264" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_006.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 1-1. <span class="s29">The plot of the coffee price example</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 125%;text-align: left;"><a name="bookmark12">To make predictions for the price of coffee in this hypothetical town, you could just put your ruler next to the graph and continue the upward line: the prediction for this variable does not need any </a><span class="s19">explanatory variables </span>other than its past values. The historical data of this example allows you to forecast the future. This is a determining characteristic of <span class="s19">time series models</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now let’s see a comparable example but with the prices of hot chocolate rather than the prices of a cup of coffee and quarterly data rather than data every five years (Table <span style=" color: #00F;">1-2</span>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">Table 1-2. <span class="s29">Hot Chocolate Prices Over the Years</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="319" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_007.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="323" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_008.png"/></span></p><p class="s35" style="padding-top: 5pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">Period Average Price</p><p class="s36" style="padding-top: 10pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">spring 2018 2.80</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">summer 2018 2.60</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">autumn 2018 3.00</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">Winter 2018 3.20</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">spring 2019 2.80</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">summer 2019 2.60</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">autumn 2019 3.00</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">Winter 2019 3.20</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">spring 2020 2.80</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">summer 2020 2.60</p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">autumn 2020 3.00</p><p style="text-indent: 0pt;text-align: left;"><span><img width="323" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_009.png"/></span></p><p class="s36" style="padding-top: 7pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">Winter 2020 3.20</p><p style="padding-top: 24pt;padding-left: 26pt;text-indent: 17pt;line-height: 126%;text-align: left;">Do you see the trend? In the case of hot chocolate, you do not have a year-over-year increase in price, but you do detect <span class="s19">seasonality</span>: in the example, hot chocolate prices follow the temperatures of the seasons. Let’s get this data into Python to see how to plot this seasonal trend (use Listing <span style=" color: #00F;">1-2 </span>to obtain the graph in Figure <span style=" color: #00F;">1-2</span>).</p><p class="s28" style="padding-top: 12pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark13">Listing 1-2. </a><span class="s33">Getting the hot chocolate example into Python and plotting the trend</span></p><p class="s34" style="padding-top: 9pt;text-indent: 0pt;text-align: right;">seasons = [&quot;Spring 2018&quot;, &quot;Summer 2018&quot;, &quot;Autumn 2018&quot;, &quot;Winter 2018&quot;,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">&quot;Spring 2019&quot;, &quot;Summer 2019&quot;, &quot;Autumn 2019&quot;, &quot;Winter 2019&quot;,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">&quot;Spring 2020&quot;, &quot;Summer 2020&quot;, &quot;Autumn 2020&quot;, &quot;Winter 2020&quot;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.5pt" cellspacing="0"><tr style="height:15pt"><td style="width:88pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">prices = [2.80,</p></td><td style="width:33pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">2.60,</p></td><td style="width:33pt"><p class="s37" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">3.00,</p></td><td style="width:33pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">3.20,</p></td></tr><tr style="height:16pt"><td style="width:88pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">2.80,</p></td><td style="width:33pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">2.60,</p></td><td style="width:33pt"><p class="s37" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">3.00,</p></td><td style="width:33pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">3.20,</p></td></tr><tr style="height:15pt"><td style="width:88pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">2.80,</p></td><td style="width:33pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">2.60,</p></td><td style="width:33pt"><p class="s37" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">3.00,</p></td><td style="width:33pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">3.20]</p></td></tr></table><p class="s34" style="padding-top: 10pt;padding-left: 30pt;text-indent: -22pt;line-height: 113%;text-align: left;">data = pd.DataFrame({ &#39;season&#39;: seasons, &#39;price&#39;: prices</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">})</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">ax = data.plot.line(x=&#39;season&#39;)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax.set_title(&#39;Hot Chocolate Price Over Time&#39;, fontsize=16) ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&#39;right&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="294" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_010.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 1-2. <span class="s29">Plot of the hot chocolate prices</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark14">As in the previous example, you can predict the future prices of hot chocolate easily using the past data on hot chocolate prices: the prices depend only on the season and are not influenced by any explanatory variables.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_011.png"/></span></p><p class="s38" style="padding-top: 7pt;padding-left: 33pt;text-indent: 0pt;line-height: 113%;text-align: left;">Note <span class="s39">Univariate time series models make predictions based on trends and seasonality observed in their own past and do not use explanatory variables other than the </span>target variable<span class="s39">: </span>the variable that you want to forecast<span class="s39">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_012.png"/></span></p><p style="padding-top: 11pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can imagine numerous types of combinations of those two processes, for example, have both a quarterly seasonality and a linear increasing trend and so on. There are many types of processes that can be forecasted by modeling the historical values of the target variable. In Chapters <span style=" color: #00F;">3</span>–<span style=" color: #00F;">7</span>, you will see numerous univariate time series models for forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Supervised Machine Learning Models</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">Now that you are familiar with the idea of using the past of one variable, you are going to discover a different approach to making models. You have just seen univariate time series models, which are models that use only the past of a variable itself to predict its future.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 126%;text-align: left;">Sometimes, this approach is not logical: processes do not always follow trends and seasonality. Some predictions that you would want to make may be dependent on other, independent sources of information: <span class="s19">explanatory variables</span>.</p><p style="padding-left: 43pt;text-indent: 0pt;line-height: 13pt;text-align: left;">In those cases, you can use a family of methods called <span class="s19">supervised machine learning</span></p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">that allows you to model relationships between explanatory variables and a target variable.</p><p class="s26" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">A Quick Example of the Supervised Machine Learning Approach</p><p style="padding-top: 11pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">To understand this case, you have the fictitious data in Table <span style=" color: #00F;">1-3</span>: a new example that contains the sales amount of a company per quarter, with three years of historical data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:139.625pt" cellspacing="0"><tr style="height:20pt"><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s40" style="text-indent: 0pt;line-height: 14pt;text-align: left;">Table 1-3.</p></td><td style="width:80pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s41" style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Quarterly Sales</p></td></tr><tr style="height:24pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Period</p></td><td style="width:80pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Quarterly Sales</p></td></tr><tr style="height:22pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Q1 2018</p></td><td style="width:80pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">48,000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q2 2018</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">20,000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q3 2018</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">35,000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q4 2018</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">32,0000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q1 2019</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">16,000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q2 2019</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">58,000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q3 2019</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">40,000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q4 2019</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">30,000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q1 2020</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">32,000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q2 2020</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">31,000</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q3 2020</p></td><td style="width:80pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">63,000</p></td></tr><tr style="height:22pt"><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q4 2020</p></td><td style="width:80pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">57,000</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">To get this data into Python, you can use the following code (Listing <span style=" color: #00F;">1-3</span>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 1-3. <span class="s33">Getting the quarterly sales example into Python and plotting the trend</span></p><p class="s34" style="padding-top: 9pt;text-indent: 0pt;text-align: right;">quarters = [&quot;Q1 2018&quot;, &quot;Q2 2018&quot;, &quot;Q3 2018&quot;, &quot;Q4 2018&quot;,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">&quot;Q1 2019&quot;, &quot;Q2 2019&quot;, &quot;Q3 2019&quot;, &quot;Q4 2019&quot;,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">&quot;Q1 2020&quot;, &quot;Q2 2020&quot;, &quot;Q3 2020&quot;, &quot;Q4 2020&quot;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.5pt" cellspacing="0"><tr style="height:15pt"><td style="width:71pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">sales = [48,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">20,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">42,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">32,</p></td></tr><tr style="height:16pt"><td style="width:71pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">16,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">58,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">40,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">30,</p></td></tr><tr style="height:15pt"><td style="width:71pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">32,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">31,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">53,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">40]</p></td></tr></table><p class="s34" style="padding-top: 10pt;padding-left: 30pt;text-indent: -22pt;line-height: 113%;text-align: left;">data = pd.DataFrame({ &#39;quarter&#39;: quarters, &#39;sales&#39;: sales</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">})</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax = data.plot.line(x=&#39;quarter&#39;) ax.set_title(&#39;Sales Per Quarter&#39;, fontsize=16)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&#39;right&#39;) plt.show()</p><p style="padding-top: 9pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">The graph that you obtain is a line graph that shows the sales over time (Figure <span style=" color: #00F;">1-3</span>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 101pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="291" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_013.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 1-3. <span class="s29">Plot of the quarterly sales</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">What you can see in this graph does not resemble the previous examples: there is no clear linear trend (neither increasing nor decreasing), and there is no clear quarterly seasonality either.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">But as the data is about sales, you could imagine many factors that influence the sales that you’ll realize. Let’s look for explanatory variables that could help in explaining sales. In Table <span style=" color: #00F;">1-4</span>, the data have been updated with two explanatory variables: discount and advertising budget. Both are potential variables that could influence sales numbers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Table 1-4. <span class="s29">Quarterly Sales, Discount, and Advertising Budget</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:8pt" cellspacing="0"><tr style="height:24pt"><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Period</p></td><td style="width:108pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Quarterly Sales</p></td><td style="width:101pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Avg. Discount</p></td><td style="width:146pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Advertising Budget</p></td></tr><tr style="height:22pt"><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Q1 2018</p></td><td style="width:108pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">48,000</p></td><td style="width:101pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">4%</p></td><td style="width:146pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">500</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q2 2018</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">20,000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">2%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">150</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q3 2018</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">35,000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">3%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">400</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q4 2018</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">32,0000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">3%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">300</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q1 2019</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">16,000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">2%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">100</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q2 2019</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">58,000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">6%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">500</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q3 2019</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">40,000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">4%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">380</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q4 2019</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">30,000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">3%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">280</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q1 2020</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">32,000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">3%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">290</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q2 2020</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">31,000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">3%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">315</p></td></tr><tr style="height:20pt"><td style="width:59pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q3 2020</p></td><td style="width:108pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">63,000</p></td><td style="width:101pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">6%</p></td><td style="width:146pt"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">625</p></td></tr><tr style="height:22pt"><td style="width:59pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Q4 2020</p></td><td style="width:108pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">57,000</p></td><td style="width:101pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">6%</p></td><td style="width:146pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">585</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s have a look at whether it would be possible to use those variables for a prediction of sales using Listing <span style=" color: #00F;">1-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 1-4. <span class="s33">Getting the quarterly sales example into Python and plotting the trend</span></p><p class="s34" style="padding-top: 6pt;text-indent: 0pt;text-align: right;">quarters = [&quot;Q1 2018&quot;, &quot;Q2 2018&quot;, &quot;Q3 2018&quot;, &quot;Q4 2018&quot;,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">&quot;Q1 2019&quot;, &quot;Q2 2019&quot;, &quot;Q3 2019&quot;, &quot;Q4 2019&quot;,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">&quot;Q1 2020&quot;, &quot;Q2 2020&quot;, &quot;Q3 2020&quot;, &quot;Q4 2020&quot;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.5pt" cellspacing="0"><tr style="height:15pt"><td style="width:71pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">sales = [48,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">20,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">42,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">32,</p></td></tr><tr style="height:16pt"><td style="width:71pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">16,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">58,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">40,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">30,</p></td></tr><tr style="height:15pt"><td style="width:71pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">32,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">31,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">53,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">40]</p></td></tr></table><p class="s34" style="padding-top: 10pt;text-indent: 0pt;text-align: right;">discounts = [4,2,3,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">3,2,6,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">4,3,3,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">3,6,6]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">advertising = [500,150,400,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">300,100,500,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">380,280,290,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">315,625,585]</p><p class="s34" style="padding-top: 9pt;padding-left: 48pt;text-indent: -22pt;line-height: 113%;text-align: left;">data = pd.DataFrame({ &#39;quarter&#39;: quarters, &#39;sales&#39;: sales, &#39;discount&#39;: discounts, &#39;advertising&#39;: advertising</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">})</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax = data.plot.line(x=&#39;quarter&#39;) ax.set_title(&#39;Sales Per Quarter&#39;, fontsize=16)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&#39;right&#39;) plt.show()</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This gives you the graph that is displayed in Figure <span style=" color: #00F;">1-4</span>: a graph displaying the development of the three variables over time.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 101pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="287" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_014.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 1-4. <span class="s29">Plot of the sales per quarter with correlated variables</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark15">At this point, visually, you’d probably say that there is not a very important relationship between the three variables. But let’s have a more zoomed-in look at the same graph (Listing </a><span style=" color: #00F;">1-5</span>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 1-5. <span class="s33">Zooming in on the correlated variables of the quarterly sales example</span></p><p class="s34" style="padding-top: 6pt;text-indent: 0pt;text-align: right;">quarters = [&quot;Q1 2018&quot;, &quot;Q2 2018&quot;, &quot;Q3 2018&quot;, &quot;Q4 2018&quot;,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">&quot;Q1 2019&quot;, &quot;Q2 2019&quot;, &quot;Q3 2019&quot;, &quot;Q4 2019&quot;,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">&quot;Q1 2020&quot;, &quot;Q2 2020&quot;, &quot;Q3 2020&quot;, &quot;Q4 2020&quot;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.5pt" cellspacing="0"><tr style="height:15pt"><td style="width:71pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">sales = [48,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">20,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">42,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">32,</p></td></tr><tr style="height:16pt"><td style="width:71pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">16,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">58,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">40,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">30,</p></td></tr><tr style="height:15pt"><td style="width:71pt"><p class="s37" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">32,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">31,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">53,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">40]</p></td></tr></table><p class="s34" style="padding-top: 10pt;text-indent: 0pt;text-align: right;">discounts = [4,2,3,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">3,2,6,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">4,3,3,</p><p class="s34" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: right;">3,6,6]</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 24pt;text-align: left;">discounts_scale_adjusted = [x * 10 for x in discounts] advertising = [500,150,400,</p><p class="s34" style="padding-top: 1pt;padding-left: 90pt;text-indent: 0pt;text-align: left;">300,100,500,</p><p class="s34" style="padding-top: 1pt;padding-left: 90pt;text-indent: 0pt;text-align: left;">380,280,290,</p><p class="s34" style="padding-top: 1pt;padding-left: 90pt;text-indent: 0pt;text-align: left;">315,625,585]</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 24pt;text-align: left;">advertising_scale_adjusted = [x / 10 for x in advertising] data = pd.DataFrame({</p><p class="s34" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;quarter&#39;: quarters, &#39;sales&#39;: sales,</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;discount&#39;: discounts_scale_adjusted, &#39;advertising&#39;: advertising_scale_adjusted</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">})</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax = data.plot.line(x=&#39;quarter&#39;) ax.set_title(&#39;Sales Per Quarter&#39;, fontsize=16)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&#39;right&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark16">This gives the graph displayed in Figure </a><span style=" color: #00F;">1-5</span>: you can suddenly observe a very clear relationship between the three variables! The relationship was already there in the previous graph (Figure <span style=" color: #00F;">1-4</span>), but it was just not visually obvious due to the difference in scale of the curves.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 101pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="291" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_015.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 1-5. <span class="s29">Zoomed view of the correlated variables of the quarterly sales example</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Imagine you observe a correlation as strong as in Figure <span style=" color: #00F;">1-5</span>. If you had to do this sales forecast for next month, you could simply ask your colleagues what the average discount is going to be next month and what next month’s advertising budget is, and you would be able to come up with a reasonable guess of the future sales.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This type of relationships is what you are generally looking at when doing supervised machine learning. Intelligent use of those relations is the fundamental idea behind the different techniques that you will see throughout this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark17">Correlation Coefficient</a></p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;line-height: 126%;text-align: left;">The visual way to detect correlation is great. Yet there is a more exact way to investigate relationships between variables: the correlation coefficient. The <span class="s19">correlation coefficient </span>is a very important measure in statistics and machine learning as it determines how much two variables are correlated.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The correlation coefficient between two variables x and y can be computed as follows:</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 13pt;text-align: left;">A <span class="s19">correlation matrix </span>is a matrix that contains the correlations between each pair of</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">variables in a dataset. Use Listing <span style=" color: #00F;">1-6 </span>to obtain a correlation matrix.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 1-6. <span class="s33">Getting the quarterly sales example into Python and plotting the trend</span></p><p class="s34" style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">data.corr()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">It will give you the correlations between each pair of variables in the dataset as shown in Figure <span style=" color: #00F;">1-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 119pt;text-indent: 0pt;text-align: left;"><span><img width="256" height="90" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_016.gif"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 1-6. <span class="s29">Correlation table of the quarterly sales example</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">A correlation coefficient is always <span class="s19">between -1 and 1</span>. A positive value for the correlation coefficient means that two variables are positively correlated: if one is higher, then the other is generally also higher. If the correlation coefficient is negative, there is a negative correlation: if one value is higher, then the other is generally lower. This is the <span class="s19">direction of the correlation</span>.</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 12pt;text-align: left;">There is also a notion of the <span class="s19">strength of the correlation</span>. A correlation that is close to</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">1 or close to -1 is strong. A correlation coefficient that is close to 0 is a weak correlation. Strong correlations are generally more interesting, as an explanatory variable that strongly correlated to your variable can be used for forecasting it.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark18">In the variables in the example, you see a strong positive correlation between sales and discount (0.848) and a strong positive correlation between sales and advertising (0.90). As the correlations are strong, they could be useful in predicting future sales.</a></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can also observe a strong correlation between the two explanatory variables discount and advertising (0.92). This is important to notice, because if two explanatory variables are very correlated, it may be of little added value to use them both: they contain almost the same “information,” just measured on a different scale.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Later, you’ll discover numerous mathematical models that will allow you to make models of the relationships between a target variable and explanatory variables. This will help you choose which correlated variables to use in a predictive model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Other Distinctions in Machine Learning Models</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">To complete the big picture overview of the machine learning landscape, there are a few more groups that need to be mentioned.</p><p class="s26" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Supervised vs. Unsupervised Models</p><p style="padding-top: 11pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">You have just seen what supervised models are all about. But there also unsupervised models. Unsupervised models differ from all approaches before, as there is no target variable in unsupervised models.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Unsupervised models are great for making segmentations. A classic example is regrouping customers of a store, based on the similarity of those customers. There are many great use cases for such segmentations, but the approach is not generally useful for forecasting.</p><p class="s26" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Classification vs. Regression Models</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 124%;text-align: left;">Inside the group of supervised models, there is an important split between <span class="s19">classification </span>and <span class="s19">regression</span>. Regression is supervised modeling in which the target variable is <span class="s19">numeric</span>. In the examples that you have seen in the previous sections, there were only numeric target variables (e.g., amount of sales). They would therefore be considered regressions.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: justify;">In classification, the target variable is <span class="s19">categorical</span>. An example of classification is predicting whether a specific customer will buy a product (yes or no), based on their customer behavior.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark19">There are many important use cases for classification, but when talking about forecasting the future, it is generally numerical problems and therefore regression models. Think about the weather forecast: rather than trying to predict either hot or cold weather (which are categories of weather), you can try to predict the temperature (numeric). And instead of predicting rainy vs. dry weather (categories), you would predict the percentage of rain.</a></p><p class="s26" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Univariate vs. Multivariate Models</p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;line-height: 122%;text-align: left;">A last important split between different approaches in machine learning is the split between <span class="s19">univariate models </span>and <span class="s19">multivariate models</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In the examples you have seen until now, there was always one target variable (sales). But in some cases, you want to predict multiple related variables at the same time. For example, on social media, you may want to forecast how many likes you will receive, but also how many comments.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The first possibility for treating this is to build two models: one model for forecasting likes and another one for forecasting the number of comments. But some models allow benefiting from a correlation between target variables. These are called multivariate models, and they use the correlation between target variables in such a way that the forecast accuracy improves from forecasting the two at the same time.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: justify;">You will see multivariate models in Chapters <span style=" color: #00F;">8 </span>and <span style=" color: #00F;">9</span>. Multivariate models can be great scientific descriptions of reality, but caution needs to be paid to their predictive performance. Multiple models are sometimes more performant than one model that makes multiple predictions. Data-driven model evaluation strategies will help you to make the best choice. This will be the scope of the next chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4><ol id="l1"><li><p style="padding-top: 16pt;padding-left: 54pt;text-indent: -18pt;line-height: 129%;text-align: left;">Univariate time series models use historical data of a target variable to make predictions.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -18pt;line-height: 129%;text-align: left;">Seasonality and trend are important effects that can be used in time series models.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -18pt;line-height: 129%;text-align: left;">Supervised machine learning uses correlations between variables to forecast a target variable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-top: 5pt;padding-left: 72pt;text-indent: -18pt;line-height: 129%;text-align: left;">Supervised machine learning can be split into classification and regression. In classification, the target variable is categorical;</p><p style="padding-left: 72pt;text-indent: 0pt;line-height: 129%;text-align: left;">in regression, the target variable is numeric. Regression is most relevant for forecasting.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -18pt;line-height: 129%;text-align: left;">The correlation coefficient is a KPI of the relationship between two variables. If the value is close to 1 or -1, the correlation is strong; if it is close to 0, it is weak. A strong correlation between an explanatory variable and the target variable is useful in supervised machine learning.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -18pt;line-height: 129%;text-align: left;">Univariate models predict one variable. Multivariate models predict multiple variables. Most forecasting models are univariate, but some multivariate models exist.</p></li></ol><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark20">CHAPTER 2</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Model Evaluation for Forecasting</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">When developing machine learning models, you generally benchmark multiple models during the build phase. Then you estimate the performances of those models and select the model which you consider most likely to perform well. You need objective measures of performance to decide which forecast to retain as your actual forecast.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In this chapter, you’ll discover numerous tools for model evaluation. You are going to see different strategies for evaluating machine learning models in general and specific adaptations and considerations to take into account for forecasting. You are also going to see different metrics for scoring model performances.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;">Evaluation with an Example Forecast</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Let’s look at a purely hypothetical example with stock prices per month of the year 2020 and the forecast that someone has made for this (Table <span style=" color: #00F;">2-1</span>). Assume that this forecast has been made in December 2019 for the complete year and that it has not been updated since.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;line-height: 11pt;text-align: right;">21</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 8pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_2#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_2#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_2</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 122pt;text-indent: 0pt;text-align: left;">Table 2-1. <span class="s29">Stock Price Data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:122.75pt" cellspacing="0"><tr style="height:24pt"><td style="width:52pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Period</p></td><td style="width:63pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Stock Price</p></td><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Forecasted</p></td></tr><tr style="height:22pt"><td style="width:52pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">January</p></td><td style="width:63pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">35</p></td><td style="width:65pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">30</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">February</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">35</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">31</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">March</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">10</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">30</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">April</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">5</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">10</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">May</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">8</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">12</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">June</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">10</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">17</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">July</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">15</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">18</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">August</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">20</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">27</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">September</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">23</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">29</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">October</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">21</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">24</p></td></tr><tr style="height:20pt"><td style="width:52pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">November</p></td><td style="width:63pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">22</p></td><td style="width:65pt"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">23</p></td></tr><tr style="height:22pt"><td style="width:52pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">December</p></td><td style="width:63pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">25</p></td><td style="width:65pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">22</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can already see that there is quite some difference between the actual values and the forecasted values. But that happens. Let’s start with getting the data into Python and plotting the two lines using Listing <span style=" color: #00F;">2-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 2-1. <span class="s33">Getting the stock data example into Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import matplotlib.pyplot as plt</p><p class="s34" style="padding-top: 9pt;padding-left: 57pt;text-indent: -49pt;line-height: 113%;text-align: left;">period = [&#39;January&#39;, &#39;February&#39;, &#39;March&#39;, &#39;April&#39;, &#39;May&#39;, &#39;June&#39;,</p><p class="s34" style="padding-left: 57pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;July&#39;, &#39;August&#39;, &#39;September&#39;, &#39;October&#39;, &#39;November&#39;, &#39;December&#39;]</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">actual = [35, 35, 10,</p><p class="s34" style="padding-top: 1pt;padding-left: 63pt;text-indent: 0pt;text-align: left;">5, 8, 10,</p><p class="s34" style="padding-top: 1pt;padding-left: 63pt;text-indent: 0pt;text-align: left;">15, 20, 23,</p><p class="s34" style="padding-top: 1pt;padding-left: 63pt;text-indent: 0pt;text-align: left;">21, 22, 25]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">forecast = [30, 31, 30,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:100.504pt" cellspacing="0"><tr style="height:15pt"><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">10,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">12,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">17,</p></td></tr><tr style="height:16pt"><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">18,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">27,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">29,</p></td></tr><tr style="height:15pt"><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">24,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">23,</p></td><td style="width:22pt"><p class="s37" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">22]</p></td></tr></table><p class="s34" style="padding-top: 10pt;padding-left: 48pt;text-indent: -22pt;line-height: 113%;text-align: left;">data = pd.DataFrame({ &#39;period&#39;: period, &#39;actual&#39;: actual, &#39;forecast&#39;: forecast</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">})</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax = data.plot.line(x=&#39;period&#39;) ax.set_title(&#39;Forecast vs Actual&#39;, fontsize=16)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&#39;right&#39;) plt.show()</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This should give you the graph in Figure <span style=" color: #00F;">2-1</span>, which displays the actual values against the forecasted values over time.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 101pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="300" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_017.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 2-1. <span class="s29">Stock prices vs. forecasted stock prices over time</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark21">Now the next simple step is to compute the differences between each forecasted value and each actual value, as shown in Table </a><span style=" color: #00F;">2-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Table 2-2. <span class="s29">Adding the Errors of Each Forecasted Value to the Data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:8pt" cellspacing="0"><tr style="height:24pt"><td style="width:76pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Period</p></td><td style="width:112pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Stock Price</p></td><td style="width:111pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Forecasted</p></td><td style="width:115pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">Error</p></td></tr><tr style="height:22pt"><td style="width:76pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">January</p></td><td style="width:112pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">35</p></td><td style="width:111pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">30</p></td><td style="width:115pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">-5</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">February</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">35</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">31</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">-4</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">March</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">10</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">30</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">20</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">April</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">5</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">10</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">5</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">May</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">8</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">12</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">4</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">June</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">10</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">17</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">7</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">July</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">15</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">18</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">3</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">August</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">20</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">27</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">7</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">September</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">23</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">29</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">6</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">October</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">21</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">24</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">3</p></td></tr><tr style="height:20pt"><td style="width:76pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">November</p></td><td style="width:112pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">22</p></td><td style="width:111pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">23</p></td><td style="width:115pt"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">1</p></td></tr><tr style="height:22pt"><td style="width:76pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">December</p></td><td style="width:112pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">25</p></td><td style="width:111pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">22</p></td><td style="width:115pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">-3</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Model Quality Metrics</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">This month-by-month error is useful information for model improvement. Yet although it gives a first impression of the quality of the model, there are some problems with this way of model evaluation. Firstly, this level of detail would be too much to do a model comparison: you cannot look at long lists of errors for each model, but you ideally want one or a few KPIs per model.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Secondly, you should note that it is not simply possible to take the average of this Error column and consider this as your error metric. Since there are positive and negative error measures, they would average each other out. This would be a strong underestimation of your error.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark22">To solve this problem, you need to standardize the errors by taking the absolute values or by squaring the errors. This will make sure that a negative and a positive error do not cancel each other out. You will now see five common metrics that do exactly this.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;">Metric 1: MSE</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">The Mean Squared Error (MSE) is one of the most used metrics in machine learning. It is computed as the average of the squared errors. To compute the MSE, you take the errors per row of data, square those errors, and then take the average of them.</p><p class="s44" style="padding-top: 8pt;padding-left: 52pt;text-indent: 0pt;line-height: 15pt;text-align: center;"><span class="s42">MSE </span><span class="s43"></span> <u> </u><u>1</u> <span class="s43"></span><span class="s47"></span><span class="s48"> </span><span class="s42">y  </span><span class="s43"></span> <span class="s42">y</span><span class="s49">ˆ</span> <span class="s47"></span><span class="s50">2</span></p><p class="s51" style="padding-top: 1pt;padding-left: 52pt;text-indent: 0pt;line-height: 54%;text-align: center;">n       <span class="s52">i          i</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">You can compute it in Python using the scikit-learn library, as is done in Listing <span style=" color: #00F;">2-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 2-2. <span class="s33">Computing the Mean Squared Error in Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import mean_squared_error print(mean_squared_error(data[&#39;actual&#39;], data[&#39;forecast&#39;]))</p><p style="padding-top: 8pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">For the current example, this gives a MSE of <span class="s19">53.7</span>.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The logic behind this error metric is multifold. First, you can understand the reason that the squared errors are used rather than the original errors, as it would be impossible to sum the original errors. Since there are positive and negative values in the original errors, they would cancel each other out. Imagine a case with one large negative error and one large positive error: the sum of the two errors might be close to zero, which is clearly wrong. The square of a value is always positive, which is why this is one possibility to counter this.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">A second part of the formula that you can understand is that it functions as an average. You take the sum of all values and divide by the number of observations. In the MSE, you take the average of the squared errors.</p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;">The MSE error metric is great for comparing different models on the same dataset.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">The scale of the MSE will be the same for each model applied to the same dataset. However, the scale of the metric is not very intuitive, which makes it difficult to interpret outside of benchmarking multiple models.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark23">The MSE is an error metric, so it should be interpreted as follows: the smaller the error, the better the model. It is not possible to convert this into an accuracy measure, because of the lack of a fixed scale of the metric. There are no upper bounds to the error, so it should really be used for comparison only.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Metric 2: RMSE</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">The RMSE, or Root Mean Squared Error, is the square root of the Mean Squared Error. As you can understand, taking the square root of the MSE does not make a difference when you want to use the error metrics for classing performances in order.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="41" height="16" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_018.png"/></span></p><p class="s42" style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: left;">MSE</p><p style="text-indent: 0pt;text-align: left;"/><p class="s42" style="padding-top: 10pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">RMSE <span class="s53">=</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Yet there is an advantage in using the RMSE rather than the MSE. The reason for taking the square root of the MSE is that the scale of the RMSE is the same as the scale of the original variable. In the MSE formula, you take the average of squared errors. This makes the value difficult to interpret. Using the square root will get the scale of the error metric back to the scale of your actual values.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: justify;">If you compute the RMSE using Listing <span style=" color: #00F;">2-3</span>, you will see that the RMSE is <span class="s19">7.3</span>. This is much more in line with the actual stock prices that you are working with. This can have an advantage for explanation and communication purposes.</p><p class="s28" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">Listing 2-3. <span class="s33">Computing the Root Mean Squared Error in Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import mean_squared_error from math import sqrt</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">print(sqrt(mean_squared_error(data[&#39;actual&#39;], data[&#39;forecast&#39;])))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">As the RMSE is an error measure, a lower RMSE indicates a better model.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Although the RMSE is more intuitively understandable, its scale is still dependent on the actual values. This makes it impossible to compare the RMSE values of different datasets with one another, just like the MSE.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;"><a name="bookmark24">Metric 3: MAE</a></p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">The Mean Absolute Error (MAE) is calculated by taking the absolute differences between the predicted and actual values per row. The average of those absolute errors is the Mean Absolute Error.</p><p class="s53" style="padding-top: 9pt;padding-left: 52pt;text-indent: 0pt;line-height: 14pt;text-align: center;"><span class="s42">MAE </span>= <span class="s54">1</span><span class="s55"> </span>∑<span class="s56">∣</span><span class="s42">y  </span>− <span class="s42">y</span><span class="s57">ˆ</span><span class="s56">∣</span></p><p class="s51" style="padding-left: 52pt;text-indent: 0pt;text-align: center;">n       <span class="s52">i         i</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The MAE takes the absolute values of the errors before averaging. Taking the average of absolute errors is a way to make sure that summing the errors will not make them cancel each other out.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You have seen the MSE using the square of the errors to avoid this, and the MAE is an alternative to this. The MAE has a more intuitive formula: it is the error metric that most people intuitively come up with. Yet the RMSE is generally favored over the MAE.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Since the RMSE uses squares rather than absolute values, the RMSE is easier to use in mathematical computations that demand to take derivatives. The derivative of squared errors is much easier to compute than the derivative of absolute errors. Since the derivative is a much used function in optimization and minimization, this is an important criterion.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The interpretation of the MAE is comparable to the interpretation of the RMSE. They both yield scores that are in the same range of values as the actual values. There will always be a difference in the MAE and the MSE. When using the squared errors, if one of the individual errors is very high, this value may weigh stronger in the total evaluation. Yet there is not a definite way to judge whether one of the error measures is better or worse than the other.</p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;">You can compute the Mean Absolute Error in Python using the code in Listing <span style=" color: #00F;">2-4</span>.</p><p style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">You should obtain an MAE of <span class="s19">5.67</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 2-4. <span class="s33">Computing the Mean Absolute Error in Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import mean_absolute_error print(mean_absolute_error(data[&#39;actual&#39;], data[&#39;forecast&#39;]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark25">Metric 4: MAPE</a></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">The MAPE, short for Mean Absolute Percent Error, is computed by taking the error for each prediction, divided by the actual value. This is done to obtain the errors relative to the actual values. This will make for an error measure as a percentage, and therefore it is standardized.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As you’ve understood from the previous error measures, they were not standardized on a scale between zero and one. Yet this standardization is very useful. This makes for very easy communication of the performance results.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 122%;text-align: left;">To compute the MAPE, you take the absolute values of those percentages per row and compute their average. For the example in Listing <span style=" color: #00F;">2-5</span>, you’ll obtain <span class="s19">0.46</span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="42" height="40" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_019.png"/></span></p><p class="s42" style="padding-left: 11pt;text-indent: -8pt;text-align: left;">y<span class="s58">i</span><span class="s52">  </span><span class="s59">- </span>y<span class="s49">ˆ</span><span class="s58">i</span><span class="s52"> </span>y<span class="s60">i</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s59" style="padding-top: 11pt;padding-left: 52pt;text-indent: 0pt;line-height: 18pt;text-align: center;"><span class="s42">MAPE </span>=<u> </u><span class="s46">1</span><span class="s44"> </span>L</p><p class="s42" style="text-indent: 0pt;line-height: 11pt;text-align: center;">n</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 7pt;text-indent: 17pt;line-height: 129%;text-align: left;">The MAPE measures a percentage error. It is an error measure, so lower values for the MAPE are better. Yet you can easily convert the MAPE to a goodness of fit measure by computing 1 – MAPE. In many cases, it is easier to communicate performance in terms of a positive result rather than a negative one.</p><p style="padding-left: 7pt;text-indent: 17pt;line-height: 129%;text-align: left;">Although the MAPE function is intuitive, it has a serious drawback: when the actual value is 0, the formula will divide the error by the actual value. This leads to a division by zero, which is mathematically impossible. This makes it problematic to use.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Listing 2-5. <span class="s33">Computing the Mean Absolute Percent Error in Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: justify;">from sklearn.metrics import mean_absolute_percentage_error print(mean_absolute_percentage_error(data[&#39;actual&#39;], data[&#39;forecast&#39;]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Metric 5: R2</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">The R2 (R squared) metric is a metric that is very close to the 1 – MAPE metric. It is a performance metric rather than an error metric, which makes it great for communication.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The R2 is a value that tends to be between 0 and 1, with 0 being bad and 1 being perfect. It can therefore be easily used as a percentage by multiplying it by 100. The only case where the R2 can be negative is if your forecast is more than 100% wrong.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s59" style="padding-left: 25pt;text-indent: 0pt;text-align: right;"><a name="bookmark26"><span class="s62">R</span></a><span class="s63">2 </span>= <span class="s44">1</span>-</p><p class="s59" style="padding-top: 7pt;padding-bottom: 2pt;text-indent: 0pt;text-align: left;"> <span class="s64"> </span><span class="s62">y </span>- <span class="s62">y</span><span class="s49">ˆ</span><span class="s65"> </span><span class="s50">2</span></p><p style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="65" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_020.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_021.png"/></span></p><p class="s66" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s66" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s59" style="text-indent: 0pt;text-align: left;"> <span class="s67">  </span><span class="s62">y </span>- <span class="s62">y </span><span class="s50">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The formula does an interesting computation. It computes a ratio between the sum of squared errors and the sum of deviations between the forecast and the average.</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">This comes down to a percentage of increase of your model over using the average as a model. If your model is as bad a prediction as using the average, then the R2 will be zero. As the average is often used as a benchmark model, this is a very practical performance metric.</p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;">In Python, you can compute the R2 by using the code listed in Listing <span style=" color: #00F;">2-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 2-6. <span class="s33">Computing the R2 in Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import r2_score r2_score(data[&#39;actual&#39;], data[&#39;forecast&#39;])</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This gives you a value (rounded) of <span class="s19">0.4</span>. As the R squared is measured on a scale between 0 and 1, you could translate this as 40% better performance than the average. Although this exact interpretation will not be very clear for your business partners and managers, the use of percentages to track model performance will be very convincing in many cases.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Model Evaluation Strategies</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now that you have seen five important metrics, let’s look at how to set up tests for model comparison. When you’re doing advanced forecasting, you will often be working with</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">a lot of models at the same time. There are a lot of models that you can use, and the remainder of this book is going to present them.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 124%;text-align: left;">But when working with all those different models, you generally need to make a final forecast: <span class="s20">how to decide which model is most accurate?</span></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In theory, you could of course use multiple models to predict the (short) future and then wait and see which one works best. This does happen in practice, and this is a great way to make sure you deliver quality forecasts. However, you can do more. Rather than waiting, it is much more interesting to try and use past data to estimate errors.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 121%;text-align: left;">In practice, the question is: <span class="s20">how to decide which model is most accurate, without waiting for the future to confirm your model</span>?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark27">Overfit and the Out-of-Sample Error</a></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">When doing forecasting, you are building models on historical data and projecting a forecast into the future. When doing this, it is important to avoid certain biases. A very common bias is to fit a model on historical data, compute errors on this historical data, and, if the error is good, use it for a forecast.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: justify;">Yet, this will not work due to overfitting. Overfitting a model means that your model has learned the past data too specifically. With the advanced methods that are available on the market, models could get to fit almost any historical trend to 100%. However, this is not at all a guarantee for performance in out-of-sample prediction.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">When models overfit, they obtain very high scores on historical data and poor performances on future data. Rather than learning general, true trends, an overfitted model remembers a lot of “useless” and “noisy” variations in the past and will project this noise into the future.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Machine learning models are powerful learning models: they will learn anything that you give them. But when they overfit, they learn too much. This happens often, and many strategies exist to avoid it.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Strategy 1: Train-Test Split</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">A first split that is often deployed is the train-test split. When applying a train-test split, you split the rows of data in two. You would generally keep 20% or 30% of data in a test set.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can then proceed to fit models on the rest of the data: the training data. You can apply many models to the train data and predict on the test data. You compare the performance of the machine learning models on the test set, and you will notice that the models with good performance on the train data do not necessarily have a good performance on the test data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">A model with a higher score on the training data and a lower score on the test data is a model with an overfit. In this case, a model with a slightly lower error on the training data may well outperform the first model on the test set.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The test performances are those that matter, as they best replicate the future case: they try to predict on data that is “unknown” to the model – this replicates the situation of your forecast in the future.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;"><a name="bookmark28">Now, a question that you could ask is how to choose the test set. In most machine learning problems, you do a random selection of data for the test set. But forecasting is quite particular in this case. As you generally have an order in the data, it makes more sense to keep the test set as the 20% last observations: this will replicate the future situation of application of the forecast.</a></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">As an example of applying the train-test split, let’s make a very simple forecasting model: the mean model. It consists of taking the average on the train data and using that as a forecast. Of course, it will not be very performant, but it is often used as a “minimum” benchmark in forecasting models. Any model that is worse than this is not worth being considered at all.</p><p style="padding-left: 44pt;text-indent: 0pt;line-height: 12pt;text-align: left;">So let’s see an example with the stock price data from before using Listing <span style=" color: #00F;">2-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 2-7. <span class="s33">Train-test split in Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 112%;text-align: left;">from sklearn.model_selection import train_test_split y = data[&#39;actual&#39;]</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 112%;text-align: left;">train, test = train_test_split(y, test_size=0.3, shuffle=False) forecast = train.mean() # forecast is 17.25</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 112%;text-align: left;">train = pd.DataFrame(train) train[&#39;forecast&#39;] = forecast</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">train_error = mean_squared_error(train[&#39;actual&#39;], train[&#39;forecast&#39;])</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 112%;text-align: left;">test = pd.DataFrame(test) test[&#39;forecast&#39;] = forecast</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 112%;text-align: left;">test_error = mean_squared_error(test[&#39;actual&#39;], test[&#39;forecast&#39;]) print(train_error, test_error)</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">This will give a train error of 122.9375 and a test error of 32.4375. Attention here: With one year of data, a train-test split will cause a problem. The training data will not have a full year of data, which makes the model to not fully learn any seasonality. In addition, you should strive for having at least three observations per period (in this case, three years of training data).</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">It is generally a good idea to take seasonality into account when selecting the testing period. In non-forecasting situations, you will often see test sets of 20% or 30% of the data. In forecasting, I advise using a full seasonal period. If you have yearly seasonality, you should use a full year as a test period, to avoid having, for example, a model that works very well in summer, but badly in winter (potentially due to different dynamics in what you are forecasting in different seasons).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark29">Strategy 2: Train-Validation-Test Split</a></p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 128%;text-align: left;">When doing a <span class="s19">model comparison</span>, you benchmark the performances of many models. As said before, you can avoid overfitting by training the models on the train set and testing the models on the test set.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Adding to this approach, you can add an extra split: the <span class="s19">validation split</span>. When using the train-validation-test split, you will train models on the training data, then you benchmark the models on the validation data, and this will be the basis for your model selection. Then finally, you use your selected model to compute an error on the test set: this should confirm the estimated error of your model.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In case you have an error on the test set that is significantly worse than the error on the validation set, this will alert you that the model is not as good as you expected: the validation error is underestimated and should be reinvestigated.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">When using the train-validation-test split, there is a serious amount of data dedicated fully to model comparison and testing. Therefore, this approach should be avoided when working with little data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As said before, it is important to consider which periods you leave out for your validation and test data. If, for example, you have five years of monthly data and you decide to use three years for training, one year for validation, and one year for testing, you are missing out on the two most recent years of data: not a good idea.</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 13pt;text-align: left;">In this case, you could select the best model (that is the <span class="s19">model type </span>and its optimal</p><p class="s19" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 127%;text-align: left;">hyperparameters<span class="p">); and, once the model is selected and benchmarked, you retrain the optimal model including the most recent data.</span></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Listing <span style=" color: #00F;">2-8 </span>shows a very basic example in which two models are compared: the mean and the median. Of course, those models should not be expected to be performant: the goal here is to show how to use the validation data for model comparison.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 2-8. <span class="s33">Train-validation-test split in Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># Splitting into 70% train, 15% validation and 15% test</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">train, test = train_test_split(data[&#39;actual&#39;], test_size = 0.3, shuffle = False, random_state=12345)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">val, test = train_test_split(test, test_size = 0.5, shuffle = False, random_state=12345)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Fit (estimate) the two models on the train data forecast_mean = train.mean() # 17.25 forecast_median = train.median() # 12.5</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Compute MSE on validation data for both models val = pd.DataFrame(val)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">val[&#39;forecast_mean&#39;] = forecast_mean val[&#39;forecast_median&#39;] = forecast_median</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">mean_val_mse = mean_squared_error(val[&#39;actual&#39;], val[&#39;forecast_mean&#39;]) median_val_mse = mean_squared_error(val[&#39;actual&#39;], val[&#39;forecast_median&#39;])</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># You observe the following validation mse: mean mse: 23.56, median mse: 91.25</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">print(mean_val_mse, median_val_mse)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># The best performance is the mean model, so verify its error on test data test = pd.DataFrame(test)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">test[&#39;forecast_mean&#39;] = forecast_mean</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">mean_test_mse = mean_squared_error(test[&#39;actual&#39;], test[&#39;forecast_mean&#39;])</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># You observe a test mse of 41.3125, almost double the validation mse print(mean_test_mse)</p><p style="padding-top: 9pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">If you follow the example, you observe a validation MSE for the mean model of</p><ol id="l2"><ol id="l3"><li><p style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">and <span class="s19">91.25 </span>for the median model. This would be a reason to retain the mean model against the median model. As a final evaluation, you verify whether the validation error is not influenced by a selection of the validation set that is (randomly) favorable to the mean model.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">This is done by taking a final error measure on the test set. As both the validation and the test set were unseen data for the mean model, their errors should be close. However, you observe a test MSE of <span class="s19">41.3125</span>, almost double the validation error. The fact that the test error is far off from the validation error tells you that there is a bias in your error estimate. In this particular case, the bias is rather easy to find: there are too few data points in the validation set (only two data points), which makes the error estimate not reliable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark30">The train-validation-test set would have given you an early warning on the error metrics of your model. It would therefore have prevented you from relying on this forecast for future estimates. Although an evaluation strategy cannot improve a model, it can definitely improve your choice of forecasting models! It therefore has an indirect impact on your model’s accuracy.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Strategy 3: Cross-Validation for Forecasting</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">A problem with the train-test set may occur when you have very little data. In some cases, you cannot “afford” to keep 20% of the data apart from the model. In this case, cross-validation is a great solution.</p><p class="s26" style="padding-top: 10pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">K-Fold Cross-Validation</p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">The most common type of cross-validation is K-fold cross-validation. K-fold cross- validation is like an addition to the train-test split. In the train-test split, you compute the error of each model one time: on the test data. In K-fold cross-validation, you fit the same model k times, and you evaluate the error each time. You then obtain k error measures, of which you take the average. This is your cross-validation error.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">To do this, a K-fold cross-validation makes a number of different train-test splits. If you choose a k of 10, you will split the data into ten. Then each of the ten parts will be serving as test data one time. This means that the remaining nine parts are used as training data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As values for k, you are generally looking between 3 and 10. If you go much higher, the test data becomes small, which can lead to biased error estimates. If you go too low, you have very little folds, and you lose the added value of cross-validation. Note that when k is 1, you are simply applying a train-test split.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As a hypothetical example, imagine a case with 100 data points and a fivefold cross-validation. You will make five different train-test datasets, as you can see in the schematic illustration in Figure <span style=" color: #00F;">2-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="308" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_022.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 2-2. <span class="s29">K-fold cross-validation in Python</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 121%;text-align: left;">On those five train-test splits, you train the model on the observations that have been selected as training data, and you compute the error on the observations that have been selected as the test data. The cross-validation error is then the average of the five test errors (Listing <span style=" color: #00F;">2-9</span>).</p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 2-9. <span class="s33">K-fold cross-validation in Python</span></p><p class="s34" style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import numpy as np</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 163%;text-align: left;">from sklearn.model_selection import KFold kf = KFold(n_splits=5)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">errors = []</p><p class="s34" style="padding-left: 48pt;text-indent: -22pt;line-height: 106%;text-align: left;">for train_index, test_index in kf.split(data): train = data.iloc[train_index,:]</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;text-align: left;">test = data.iloc[test_index,:]</p><p class="s34" style="padding-top: 8pt;padding-left: 48pt;text-indent: 0pt;line-height: 106%;text-align: left;">pred = train[&#39;actual&#39;].mean() test[&#39;forecast&#39;] = pred</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 106%;text-align: left;">error = mean_squared_error(test[&#39;actual&#39;], test[&#39;forecast&#39;]) errors.append(error)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">print(np.mean(errors))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;"><a name="bookmark31">This example will give you a cross-validation error of </a><span class="s19">106.1</span>. This is a MSE, so it should only be compared with errors of other models on the same dataset.</p><p class="s26" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Time Series Cross-Validation</p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">The fact that you have multiple estimates of the error will improve the error estimate. As you can imagine, when you have only one estimate of the error, your test data may be very favorable: all difficult-to-predict events may have fallen in the training data! Cross- validation reduces this risk, by having an evaluation applied to all the data. This makes the error estimate generally more reliable.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Yet, attention must be paid to the specific case of forecasting. As you can see in the image, K-fold cross-validation creates test splits equally throughout the data. In the example, you can see many cases where the test set is temporally before the train set. This means that you are measuring the error of forecasting the past rather than the future using these methods.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">For the category of supervised models, in which you use relations between your target variable and a set of explanatory variables, this is not necessarily a problem: the relationships between variables can be assumed to be the same in the past and the present.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">However, for the category of time series models, this is a serious problem. Time series models are generally based on making forecasts based on trends and/or seasonality: they use the past of the target variable to forecast the future.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">A first problem you’ll encounter is that many time series models will not work with missing data: if a month is missing in the middle, the methods can simply not be estimated.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">A second problem is that when the models can be estimated, they are often not realistically accurate: estimating a period in between data points is much easier than estimating a period that is totally in the future.</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 13pt;text-align: left;">A solution that can be used is called <span class="s19">the time series split </span>(Figure <span style=" color: #00F;">2-3</span>). In this</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">approach, you take only data that is before the test period for model training.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="273" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_023.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 2-3. <span class="s29">Time series cross-validation in Python</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">You can apply time series cross-validation using the code in Listing <span style=" color: #00F;">2-10</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 2-10. <span class="s33">Time series cross-validation in Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.model_selection import TimeSeriesSplit tscv = TimeSeriesSplit()</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">errors = []</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: -22pt;line-height: 113%;text-align: left;">for train_index, test_index in tscv.split(data): train = data.iloc[train_index,:]</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">test = data.iloc[test_index,:] pred = train[&#39;actual&#39;].mean() test[&#39;forecast&#39;] = pred</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">error = mean_squared_error(test[&#39;actual&#39;], test[&#39;forecast&#39;]) errors.append(error)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">print(np.mean(errors))</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">This method of cross-validation estimates the error to be <span class="s19">194.7</span>. Again, this error should not be used as a stand-alone, but as a way to compare performances of multiple methods on the same data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark32">This method can at least be used on time series models, as there are no gaps in the data. However, with this method, you have a bias. For the first fold, you will have much less historical data than for the last fold. This makes the errors of the folds to not be comparable: if they cannot use enough historical data for fitting the model, the errors of the first folds could be considered unfair. This is an inherent weakness of this method that you should keep in mind if you use it.</a></p><p class="s26" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Rolling Time Series Cross-Validation</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">An alternative that has been proposed is the <span class="s19">rolling time series split</span>. It uses the same period for each fold in the cross-validation. You can see the schematic overview of this method in Figure <span style=" color: #00F;">2-4</span>. This will solve the problem of having unequal errors: they are all trained with the same amount of historical data. Yet a problem remains that you cannot base your model on data very far into the past using this method.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">But it should only be used in situations where you have a lot of data or where you don’t have to look very far into the past for predicting the future.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="273" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_024.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 2-4. <span class="s29">Rolling time series cross-validation in Python</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can use the code in Listing <span style=" color: #00F;">2-11 </span>to execute the rolling time series cross- validation on the stock price example.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark33">Listing 2-11. </a><span class="s33">Rolling time series cross-validation in Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 110%;text-align: left;">from sklearn.model_selection import TimeSeriesSplit tscv = TimeSeriesSplit(max_train_size = 2)</p><p class="s34" style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">errors = []</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: -22pt;line-height: 110%;text-align: left;">for train_index, test_index in tscv.split(data): train = data.iloc[train_index,:]</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;text-align: left;">test = data.iloc[test_index,:]</p><p class="s34" style="padding-top: 9pt;padding-left: 48pt;text-indent: 0pt;line-height: 110%;text-align: left;">pred = train[&#39;actual&#39;].mean() test[&#39;forecast&#39;] = pred</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 110%;text-align: left;">error = mean_squared_error(test[&#39;actual&#39;], test[&#39;forecast&#39;]) errors.append(error)</p><p class="s34" style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">print(np.mean(errors))</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 17pt;line-height: 125%;text-align: left;">This method of model evaluation estimates the error to be <span class="s19">174.0. </span>You have now seen three cross-validation errors that are all different. Yet each of them uses the same model. This tells you two things: Firstly, it insists on only comparing error estimates using the same metric, using the same dataset. Secondly, you have observed a significantly lower error in the rolling time series split. This may be a hint that using the model with very little historical data (rolling) could be the more performant model in this case. Yet to be confident of that conclusion, you would need to study the results a bit further. A great starting point would be to add multiple models into the benchmark. This is left for later, as you will discover numerous models throughout this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Backtesting</h4><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 125%;text-align: left;">As a last strategy, I want to mention backtesting. Backtesting is a term that is not much practiced in data science and machine learning. Yet it is the go-to model validation technique in stock trading and finance. Backtesting builds on the same principles as model evaluation.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 127%;text-align: left;">There is a fundamental difference between backtesting and model evaluation using metrics. In backtesting, rather than measuring the accuracy of a forecasting model, you measure the result of a (stock trading) strategy. Rather than trying to forecast the future prices of stocks, you define at which event or trigger you want to buy or sell. For example, you could define a strategy stating that you will buy a given stock at price x and sell at price y.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark34">To evaluate the performances of this strategy, you run the strategy on historical data of stock prices: what would have happened if you had sold and bought stocks at the prices you indicated. Then you evaluate how much profit you would have obtained.</a></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Although backtesting is applied to trading strategies rather than to forecasts, I found it interesting to list it in this chapter, as an alternative to model evaluation in forecasting.</p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">There are several Python libraries for finance, which propose backtesting solutions.</p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">A very simple-to-use example is <span class="s19">fastquant</span><a href="https://github.com/enzoampil/fastquant" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">, available on GitHub over here: </a><a href="https://github.com/enzoampil/fastquant" class="s32" target="_blank">https:// </a><span class="s68">github.com/enzoampil/fastquant</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Which Strategy to Use for Safe Forecasts?</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">After we’ve seen multiple approaches for forecasting model evaluation, you may wonder which solution you should choose. I’d argue that you do not have to choose. As a forecaster or modeler, your most important task is being confident in the evaluation of your model. A reliable evaluation strategy is the key to success! So rather than choosing one strategy, I’d advise you to use a combination of strategies that makes you feel the most confident about your predictions.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">A <span class="s19">combined strategy </span>that you can use is to train and tune your models using cross- validation on the training data. As a next step, you can measure predictive errors on</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">the validation data: the model that has the best error on the validation data will be your preferred model. As a last verification (you can never be sure enough), you make a prediction on the test data with the selected model, and you make sure that the error is the same as the error observed on the validation data. If this is the case, you can be</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">confident that your model should be delivering the same range of errors on future data.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As you do more predictive forecasts, you will develop a feel for which evaluation metrics are important to consider. Just remember that the goal of model evaluation is twofold: on one hand, you use it to improve model performance, and on the other hand, you use it for objective performance measures.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Be aware that sometimes, predictive modelers are focused so much on the performance improvement part that they start tweaking their evaluation methods to improve their performance scores. This is to be avoided: you want to be as confident as possible about the future performance of your model. Metrics are one important part of this; putting in place an objective model evaluation strategy is even more important.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark35">Final Considerations on Model Evaluation</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Besides looking at metrics, you also need to understand what your model is fitting: Which explanatory variables are taken into account by the model? Do those variables make sense intuitively? Do they fit the business knowledge about your data?</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This scientific, or explanatory, understanding of your model will give you additional confidence, which is an important complement to the numerical minimization of forecasting errors.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">For example, if you predict the stock market, you may obtain a 90% accuracy based on the past. But by reflecting on what you are doing, you may realize that in past data, there has not been a simple market crash present. And you may understand from this that your model will go completely wrong in the case of a market crash, while that is maybe the moment when you are the most at risk ever!</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">A great book that talks about this is <span class="s20">The Black Swan </span>in which author Nicholas Taleb insists on the importance of taking into account very rare events when working on stock market trading. A takeaway here is that extreme crises in the history of stock trading have been very influential on trading success, while methods used for day-to-day trading do not (always) take into account such risk management. You may even prefer a model that is worse on a day-to-day basis, but better in predicting huge crashes.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In short, you do not always have to obtain the lowest possible Mean Squared Error! Sometimes, stable performances are the most important. Other times, you may want to avoid any overestimation, while underestimations are not a problem (e.g., in a restaurant, it may be worse to buy too much food due to an overestimated demand forecast, rather than too little). To get to this level of understanding, it is important to</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">understand your models deeply. A model is not a black box that makes predictions for you: it is a mathematical formula that you are building. For good results, you need to evaluate the performances of the formula using evaluation criteria that fit your use case: many strategies and metrics are available, but no one size fits all. Only you can know which evaluation method fits the success criteria of your particular problem statement.</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark36">Key Takeaways</a></h4><ul id="l4"><li><p style="padding-top: 10pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Metrics</p><ul id="l5"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The most suitable metrics for regression problems are R squared (R2), Root Mean Squared Error, and Mean Squared Error.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The R2 gives a percentage-like value. The RMSE gives a value on the scale of the actuals. The MSE gives a value on a scale that is difficult to interpret.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Metrics should be used for benchmarking different models on one and the same dataset.</p></li></ul></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Model evaluation strategies</p><ul id="l6"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Cross-validation gives you a very reliable error estimate. Adaptations are necessary to make it work for time series.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">A train, test, and validation split can be used for benchmarking.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">A combined strategy will give you the safest estimate: use cross-validation on the training data, validation data for model selection, and test data for a last estimate of the error.</p></li></ul></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Overfitting models</p><ul id="l7"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">If your model learns too much from the training data and will not generalize into the future, it is overfitting.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Overfitting is identified by a good performance on the train data, but a bad performance on the test data.</p></li></ul></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Underfitting models</p><ul id="l8"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">If your model does not learn enough from the training data, it is underfitting.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Underfitting is determined by a bad performance on the training data.</p></li></ul></li></ul></li></ol></ol><p class="s21" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark37">PART II</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Univariate Time Series Models</h1><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark38">CHAPTER 3</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The AR Model</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this chapter, you will discover the AR model: the autoregressive model. The AR model is the most basic building block of univariate time series. As you have seen before, univariate time series are a family of models that use only information about the past</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 121%;text-align: left;">of the <span class="s20">target variable </span>to forecast its future, and they do not rely on other <span class="s20">explanatory variables</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Univariate time series models can be intuitively understood as building blocks: they add up from the simplest model to complex models that combine the different effects described by individual models. The AR model is the simplest model of the univariate time series models.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Throughout the following chapters, you will discover more building blocks to add to this basis. This builds up from the AR model to the SARIMA model. You can see a schematic overview of the building blocks in univariate time series in Table <span style=" color: #00F;">3-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Table 3-1. <span class="s29">The Building Blocks of Univariate Time Series</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:26pt" cellspacing="0"><tr style="height:24pt"><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Name</p></td><td style="width:307pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Explanation</p></td><td style="width:58pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Chapter</p></td></tr><tr style="height:22pt"><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">AR</p></td><td style="width:307pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Autoregression</p></td><td style="width:58pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">3</p></td></tr><tr style="height:20pt"><td style="width:46pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">MA</p></td><td style="width:307pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Moving Average</p></td><td style="width:58pt"><p class="s31" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">4</p></td></tr><tr style="height:20pt"><td style="width:46pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARMA</p></td><td style="width:307pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Combination of AR and MA models</p></td><td style="width:58pt"><p class="s31" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">5</p></td></tr><tr style="height:20pt"><td style="width:46pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARIMA</p></td><td style="width:307pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Adding differencing (I) to the ARMA model</p></td><td style="width:58pt"><p class="s31" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">6</p></td></tr><tr style="height:20pt"><td style="width:46pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMA</p></td><td style="width:307pt"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Adding seasonality (S) to the ARIMA model</p></td><td style="width:58pt"><p class="s31" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">7</p></td></tr><tr style="height:38pt"><td style="width:46pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMAX</p></td><td style="width:307pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 8pt;padding-right: 16pt;text-indent: 0pt;line-height: 127%;text-align: left;">Adding external variables (X) to the SARIMA model <i>(note that external variables make the model not univariate anymore)</i></p></td><td style="width:58pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">8</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;line-height: 11pt;text-align: right;">45</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 8pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_3#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_3#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_3</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark39">I must give you an alert here. As I stated in Chapter </a><span style=" color: #00F;">1</span>, the strong point of this book is that it contains real-life examples and honest and objective model evaluations. I cannot insist enough on the importance of model evaluation, as the goal of any forecasting model should be to obtain the best predictive performance.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">With the AR model being the simplest building block, it would be unrealistic to expect great predictive performances on most real-life datasets. Although I could have tweaked the example to fit perfectly or work with a simulated dataset, I prefer to show you the weaknesses of certain models as well.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The AR model is great to start learning about univariate time series, but it is unlikely that you will use an AR model without its brothers and sisters in practice. You would generally work with one of the combined models (SARIMA or SARIMAX) and test which are the building blocks that improve predictive performance on your forecast.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">Autocorrelation: The Past Influences the Present</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">The autoregressive model describes a relationship between the present of a variable and its past. It is therefore suitable for variables in which the past and present values correlate.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">As an intuitive example, consider the waiting line at the doctor. Imagine that the doctor has a plan in which each patient has 20 minutes with them. If every patient takes exactly 20 minutes, this works fine. But what if a patient takes a bit more time? An <span class="s19">autocorrelation </span>could be present if the duration of a consultation has an impact on the duration of the next consultation. So, if the doctor needs to speed up a consultation</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">because the previous consultation took too long, you observe a correlation between past and present. Past values influence future values.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Compute Autocorrelation in Earthquake Counts</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 118%;text-align: left;">Throughout this chapter, you will see examples applied to the well-known <span class="s19">Earthquake dataset</span><a href="http://www.kaggle.com/usgs/earthquake-database" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">. This dataset is collected by the US National Earthquake Information Center. You can find a copy of the dataset on Kaggle (</a><span class="s68">www.kaggle.com/usgs/earthquake-database</span>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_025.png"/></span></p><p class="s38" style="padding-top: 7pt;padding-left: 33pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark40">Note   </a><span class="s39">to get a very quick description of a dataset, you can use </span>df.describe() <span class="s39">to get quick statistics for each column of your dataframe: count, mean, standard deviation, minimum, maximum, and quantiles. For a more thorough description, you can use the </span>pandas profiling package<span class="s39">. examples of both are given in the Github of this book.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_026.png"/></span></p><p style="padding-top: 11pt;padding-left: 26pt;text-indent: 17pt;line-height: 123%;text-align: left;">If you import the data using pandas, you can retrieve a description of the dataframe using the <span class="s19">describe </span>method, as is shown in Listing <span style=" color: #00F;">3-1</span>. Attention: This code is best executed in a Jupyter notebook or Jupyter lab. These are great tools for exploratory data analysis. You can install them directly, or you can consider installing Anaconda, which comes with many useful Python and data science tools.</p><p class="s28" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 3-1. <span class="s33">Describing a dataframe</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Import the dataframe</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">eq = pd.read_csv(&#39;Ch03_Earthquake_database.csv&#39;)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 110%;text-align: left;"># Describe the dataframe eq.describe()</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 120%;text-align: left;">Feel free to scroll through this descriptive table to get a better understanding of what the data is about. If you want a more detailed description of the data, you can use the <span class="s19">pandas profiling package</span>, which automatically creates a very detailed description of the variables of a dataframe. You can use Listing <span style=" color: #00F;">3-2 </span>to get a <span class="s19">profile report</span>.</p><p class="s28" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 3-2. <span class="s33">Profiling a dataframe</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Import the pandas profiling package</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from pandas_profiling import ProfileReport</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 110%;text-align: left;"># Get the pandas profiling report eq.profile_report()</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 123%;text-align: left;">The task that you’ll be performing is forecasting the <span class="s19">number of strong earthquakes per year</span>. The dataset is currently not in the right format to do this, as it has one line per earthquake and not one line per year. To prepare the data for further analysis, you will need to aggregate the data using Listing <span style=" color: #00F;">3-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark41">Listing 3-3. </a><span class="s33">Convert the earthquake data to the yearly number of earthquakes</span></p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 23pt;text-align: left;">import matplotlib.pyplot as plt # Convert years to dates</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">eq[&#39;year&#39;] = pd.to_datetime(eq[&#39;Date&#39;]).dt.year</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Filter on earthquakes with magnitude of 7 or higher eq = eq[eq[&#39;Magnitude&#39;] &gt;= 7]</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Compute a count of earthquakes per year earthquakes_per_year = eq.groupby(&#39;year&#39;).count()</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># Remove erroneous values for year</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">earthquakes_per_year = earthquakes_per_year.iloc[1:-2, 0]</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Make a plot of earthquakes per year ax = earthquakes_per_year.plot() ax.set_ylabel(&quot;Number of Earthquakes&quot;) plt.show()</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 123%;text-align: left;">This code should give you the graph displayed in Figure <span style=" color: #00F;">3-1</span>. You can see that there might be some relationship between years. One year it’s higher and one year it’s lower. This could be due to a negative autocorrelation. Remember that a negative correlation implies that a higher value in one variable corresponds to a lower value in the other variable. For autocorrelation, this means that a higher value this year corresponds with a lower value next year.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="349" height="232" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_027.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 3-1. <span class="s29">Evaluation of the number of earthquakes per year</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 26pt;text-indent: 17pt;line-height: 124%;text-align: left;"><a name="bookmark42">Now that you obtained the number of earthquakes per year, you should </a><span class="s19">check for autocorrelation numerically</span>. You need to find out whether there is a correlation between the number of earthquakes in any given year and the year before it.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">To do this, you can use the same method for computing correlation as seen in Chapter <span style=" color: #00F;">2</span>. Correlation is computed pairwise, on two columns. But for now, you just have one column (the data per year). You’ll need to add a column in the data containing the data for a previous year. This can be obtained by applying a <span class="s19">shift </span>to the original data and concatenating the shifted data with the original data. Listing <span style=" color: #00F;">3-4 </span>shows you how to do this. Figure <span style=" color: #00F;">3-2 </span>shows you how the data has shifted 1 year back.</p><p class="s28" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 3-4. <span class="s33">Plotting the shifted data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">shifts = pd.DataFrame(</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">{</p><p class="s34" style="padding-top: 1pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">&#39;this year&#39;: earthquakes_per_year,</p><p class="s34" style="padding-top: 1pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">&#39;past year&#39;: earthquakes_per_year.shift(1)</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">}</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax = shifts.plot() ax.set_ylabel(&#39;Number of Earthquakes&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 101pt;text-indent: 0pt;text-align: left;"><span><img width="350" height="231" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_028.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 3-2. <span class="s29">Shifted data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;"><a name="bookmark43">This works well, except for one thing. As you can see in the data, there are observations from 1966 to 2014. This means that you don’t have a value for the past year of 1966: this introduces a </a><span class="s19">nan value </span>(a <span class="s19">missing value</span>). This type of border effect is very common in time series analysis, and the best solution, in this case, is to leave the year 1966 out of consideration. You can delete all rows with missing data using Listing <span style=" color: #00F;">3-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 3-5. <span class="s33">Drop missing data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">shifts = shifts.dropna()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: justify;">The next step is to compute the Pearson correlation coefficient using the code that you’ve seen in Chapter <span style=" color: #00F;">2</span>. In case you forgot, you can find it in Listing <span style=" color: #00F;">3-6</span>. And you can see the result in Figure <span style=" color: #00F;">3-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 3-6. <span class="s33">Compute a correlation matrix for the shifts dataframe</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">shifts.corr()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 142pt;text-indent: 0pt;text-align: left;"><span><img width="194" height="63" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_029.gif"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 3-3. <span class="s29">Correlation matrix</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">In the following paragraph, you will discover how to interpret this autocorrelation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Positive and Negative Autocorrelation</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Just like “regular” correlation, autocorrelation can be positive or negative. Positive autocorrelation means that a high value now will likely give a high value in the next period. This can, for example, be observed in stock trading: as soon as many people want to buy a stock, its price goes up. This positive trend makes people want to buy this stock even more as it has positive results. The more people buy the stock, the more it goes up and the more people may want to buy it.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark44">A positive correlation also works on downtrends. If today’s stock value is low, then it is likely that tomorrow’s value would be even lower, as people start selling. When a lot of people sell, the value drops, and even more people will want to sell. This is also a case of positive autocorrelation as the past and the present go in the same direction. If the past is low, the present is low; and if the past is high, the present is high.</a></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Negative autocorrelation exists if two trends are oppositive. This is the case in the doctor’s consultation durations example. If one consultation takes longer, the next one will take shorter. If one consultation takes less time, the doctor may take a bit more time on the next one.</p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;">If you have applied the code in Listing <span style=" color: #00F;">3-6</span>, you have observed a correlation of around</p><p class="s19" style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">0.31 <span class="p">between current year and past year. This correlation is positive rather than negative as was expected based on the graph. This correlation is of medium strength. It seems that the correlation has captured the trend in the data. If you look at the graph, you can see that there is a trend in the number of earthquakes: a bit higher in the earliest years, then a bit lower for a certain period, and in the last years a bit higher again. This is a problem, as the trend is not the relationship that you want to capture here.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Stationarity and the ADF Test</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 121%;text-align: left;">The problem of having a trend in your data is general in univariate time series modeling. The <span class="s19">stationarity </span>of a time series means that a time series does not have a (long-term) trend: it is stable around the same average. If not, you say that a time series is <span class="s19">non- stationary</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">AR models can theoretically have a trend coefficient in the model, yet since stationarity is an important concept in the general theory of time series, it is better to learn how to deal with it right away. A lot of models can only work on stationary time series.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 127%;text-align: left;">A time series that is strongly growing or diminishing over time is obvious to spot. But sometimes it is difficult to tell whether a time series is stationary. This is where the <span class="s19">Augmented Dickey Fuller (ADF) test </span>comes in useful. The Augmented Dickey Fuller test is a hypothesis test that allows you to test whether a time series is stationary. It is applied as shown in Listing <span style=" color: #00F;">3-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark45">Listing 3-7. </a><span class="s33">Augmented Dickey Fuller test</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from statsmodels.tsa.stattools import adfuller result = adfuller(earthquakes_per_year.dropna()) print(result)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">pvalue = result[1] if pvalue &lt; 0.05:</p><p class="s34" style="padding-left: 8pt;text-indent: 22pt;line-height: 113%;text-align: left;">print(&#39;stationary&#39;) else:</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 14pt;text-align: left;">print(&#39;not stationary&#39;)</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">In the earthquake data case, you will see a <span class="s19">p-value </span>that is smaller than <span class="s19">0.05 </span>(the reference value), and this means that the series is theoretically stationary: you don’t have to change anything to apply the AR model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_030.png"/></span></p><p class="s38" style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;line-height: 113%;text-align: left;">Note    <span class="s39">If you’re not familiar with hypothesis testing, you should know that the p-value is the conclusive value for a hypothesis test. In a hypothesis test, you try to prove an alternative hypothesis against a null hypothesis. the p-value</span></p><p class="s39" style="padding-left: 15pt;text-indent: 0pt;line-height: 113%;text-align: left;">indicates the probability that you would observe the data that you have observed if the null hypothesis were true. If this probability is low, you conclude that</p><p class="s39" style="padding-left: 15pt;text-indent: 0pt;line-height: 113%;text-align: justify;">the null hypothesis must be wrong and therefore the alternative must be true. the reference value for the p-value is generally 0.05 but may differ for certain applications.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_031.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Differencing a Time Series</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Even though the ADF test tells you that the data is stationary, you have seen that the autocorrelation is positive where you would expect a negative one. The hypothesis was that this could be caused by a trend.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">You now have two opposing indicators. One tells you that the data is stationary, and the other one tells you that there is a trend. To be safe, it is better to remove the trend anyway. This can be done by <span class="s19">differencing</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark46">Differencing means that rather than modeling the original values of the time series, you model the differences between each value and the next. Even though there is a trend in the actual data, there will probably not be a trend in the differenced data. To confirm, you can do an ADF test again. If it is still not good, you can difference the differenced time series again, until you obtain a correct result.</a></p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;">Listing <span style=" color: #00F;">3-8 </span>shows you how you can easily difference your data in pandas.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 3-8. <span class="s33">Differencing in pandas</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Difference the data</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">differenced_data = earthquakes_per_year.diff().dropna()</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Plot the differenced data ax = differenced_data.plot()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax.set_ylabel(&#39;Differenced number of Earthquakes&#39;) plt.show()</p><p style="padding-top: 9pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">You can see what the differenced data looks like in Figure <span style=" color: #00F;">3-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 101pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="232" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_032.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 3-4. <span class="s29">Differenced data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now to see whether this has switched your autocorrelation to the right direction, you can compute the correlation coefficient as you did before. The code for this can be seen in Listing <span style=" color: #00F;">3-9</span>. You can see what this shifted and differenced data looks like in Figure <span style=" color: #00F;">3-5</span>. The correlation matrix (Figure <span style=" color: #00F;">3-6</span>) shows the correlation coefficient between the shifted differenced data and the nonshifted differenced data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark47">Listing 3-9. </a><span class="s33">Autocorrelation of the differenced data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">shifts_diff = pd.DataFrame(</p><p class="s34" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">{</p><p class="s34" style="padding-top: 1pt;padding-left: 52pt;text-indent: 0pt;text-align: left;">&#39;this year&#39;: differenced_data,</p><p class="s34" style="padding-top: 1pt;padding-left: 52pt;text-indent: 0pt;text-align: left;">&#39;past year&#39;: differenced_data.shift(1)</p><p class="s34" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">}</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">ax = shifts_diff.plot()</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax.set_ylabel(&#39;Differenced number of Earthquakes&#39;) plt.show()</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">shifts_diff.corr()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;text-align: left;"><span><img width="353" height="232" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_033.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 3-5. <span class="s29">Shifted and differenced data</span></p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You now obtain a correlation coefficient of <span class="s19">-0.37</span>, as can be seen in Figure <span style=" color: #00F;">3-6</span>: you have successfully removed the trend from the data. Your correlation is now the negative correlation that you expected. If there are more earthquakes one year, then you generally have fewer earthquakes the next year.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 143pt;text-indent: 0pt;text-align: left;"><span><img width="192" height="61" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_034.gif"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 3-6. <span class="s29">Autocorrelation on differenced data</span></p><h4 style="padding-top: 10pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a name="bookmark48">Lags in Autocorrelation</a></h4><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 127%;text-align: left;">Besides the direction of the autocorrelation, the notion of <span class="s19">lag </span>is important. This notion is applicable only in autocorrelation and not in “regular” correlation.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The number of lags of an autocorrelation means the number of steps back in time that have an impact on the present value. In many cases, if autocorrelation is present, it is not just the previous time step that has an impact, but it can be many time steps.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Applied to the doctor’s consultation durations example, you could imagine that a doctor has a very long delay in one consultation and that they need to speed up for the next three consultations. This would mean that the third “speedy” consultation is still impacted by the delayed one. So the lag of autocorrelation would be at least three: three steps back in time.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The doctor’s consultation durations example is very intuitive, as it is very logical that a doctor has a limited amount of time. In the example of earthquakes per year, there is not such a clear logic that explains relationships between past values and current values. This makes it difficult to intuitively identify the number of lags that should be included.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 122%;text-align: left;">A great tool to investigate autocorrelation on multiple lags at the same time is to use the autocorrelation plot from the <span class="s19">statsmodels package</span>. This code is shown in Listing <span style=" color: #00F;">3-10</span>. <span class="s19">ACF </span>is short for <span class="s19">autocorrelation function</span>: the autocorrelation as a function of lag.</p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;">Note that the choice for 20 lags is arbitrary: it is for plotting purposes only.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Twenty lags mean 20 years back in time, and this should be enough to show relevant autocorrelations, but 40 or even more would be an acceptable option too.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 3-10. <span class="s33">Autocorrelation of the differenced data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from statsmodels.graphics.tsaplots import plot_acf import matplotlib.pyplot as plt</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plot_acf(differenced_data, lags=20) plt.show()</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 125%;text-align: left;">You will obtain the plot in Figure <span style=" color: #00F;">3-7</span>. This plot shows which autocorrelations are important to retain. On the y-axis, you see the <span class="s19">correlation coefficient </span>between the non- lagged data (original data) and the lagged data. The correlation coefficient is between -1 and 1. On the x-axis, you see the <span class="s19">number of lags </span>applied, in this case from 0 to 20. The first correlation is 1, as this is the data with 0 lag: the original data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 95pt;text-indent: 0pt;text-align: left;"><span><img width="320" height="219" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_035.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark49">Figure 3-7. </a><span class="s29">Autocorrelation plot</span></p><p style="padding-top: 12pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">In this plot, you should look for <span class="s19">autocorrelation </span>values that are at least higher than 0.2 or lower than -0.2. Anything below that can be considered noise. Using 0.3 or</p><ol id="l9"><ol id="l10"><li><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">as a minimum value is also possible: there is no strict guideline in interpreting the correlation coefficient, as in some domains data are noisier by nature. For example, in social studies, when working with measurements from questionnaire studies, you’ll often see much more noise than when working with precise physics measurements.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The blue area in the graph allows you to detect visually which autocorrelations are just noise: if the spike goes outside of the blue area, there is a significant autocorrelation, but if the spike stays within the blue area, the observed autocorrelation is insignificant. You can generally expect the autocorrelation to be higher in lags that are closer to the present and diminish toward further-away moments.</p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">In the graph in Figure <span style=" color: #00F;">3-7</span>, you observe a first spike (at lag 0), which is very strong.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">This is normal, as it is the original data. The correlation between a variable and itself is always 1 (100%). The second spike is the correlation with lag 1. This is the autocorrelation that you have computed before using the correlation matrix. The correlation coefficient was -0.37, and you see this again in this graph.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The remaining autocorrelations are less strong. This means that to know the number of earthquakes today, we should mainly look at the values of one lag back; in the</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">current example, this means the number of earthquakes last year. If there were many earthquakes last year, we can expect fewer earthquakes this year: this is indicated by the negative autocorrelation for lag 1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark50">For the sake of understanding how to interpret further lags, imagine a different (hypothetical) ACF plot with only one positive spike at lag 5 and the other lags being all 0. This would mean that to understand this year’s number of earthquakes, you need to look what happened 5 years ago: if there were many earthquakes five years ago, there will be many earthquakes this year. What happened in between is not relevant: since there is no autocorrelation with the previous four years, the number of earthquakes in the previous four years will not help you determine the number of earthquakes this year. This could, for example, be a case of a seasonality over 5-year periods.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 25pt;text-indent: 0pt;text-align: left;">Partial Autocorrelation</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 128%;text-align: left;">The <span class="s19">partial autocorrelation plot </span>is another plot that you should also look at. The difference between autocorrelation and partial autocorrelation is that partial autocorrelation makes sure that any correlation is not counted multiple times</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 121%;text-align: left;">for multiple lags. Therefore, the partial autocorrelation for each lag is <span class="s19">additional autocorrelation </span>to each inferior lag. More mathematically correct is to say that partial autocorrelation is <span class="s19">autocorrelation conditional on earlier lags.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="292" height="23" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_036.png"/></span></p><p class="s62" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">var <span class="s67">( </span>y<span class="s70">i</span><span class="s66"> </span><span class="s71">|</span>y<span class="s70">i</span><span class="s72"> </span><span class="s73">1 </span><span class="s71">, </span><span class="s59"> </span><span class="s71">, </span>y<span class="s70">i</span><span class="s72"> </span><span class="s66">h</span><span class="s72"> </span><span class="s73">1 </span><span class="s67">) </span><span class="s59">  </span>var <span class="s67">( </span>y<span class="s70">i</span><span class="s72"> </span><span class="s66">h </span><span class="s71">|</span>y<span class="s70">i</span><span class="s72"> </span><span class="s73">1 </span><span class="s71">, </span><span class="s59"> </span><span class="s71">, </span>y<span class="s70">i</span><span class="s72"> </span><span class="s66">h</span><span class="s72"> </span><span class="s73">1 </span><span class="s67">)</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s62" style="padding-top: 13pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">PAC <span class="s67">( </span>y<span class="s70">i </span><span class="s71">, </span>y<span class="s70">i </span><span class="s66">h </span><span class="s67">) </span><span class="s59">=</span></p><p class="s62" style="padding-top: 5pt;padding-left: 54pt;text-indent: 0pt;text-align: left;">cov<span class="s67">( </span>y<span class="s70">i</span><span class="s66"> </span><span class="s71">, </span>y<span class="s70">i</span><span class="s72"> </span><span class="s66">h </span><span class="s71">|</span>y<span class="s70">i</span><span class="s72"> </span><span class="s73">1 </span><span class="s71">, </span><span class="s59"> </span><span class="s71">, </span>y<span class="s70">i</span><span class="s72"> </span><span class="s66">h </span><span class="s67">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This has an added value, as the correlations in the autocorrelation plot may be redundant between one another.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 127%;text-align: left;">The idea is easier to understand on a hypothetical example. Imagine a hypothetical case in which the values for the present, lag 1 and lag 5, are exactly the same. An autocorrelation would tell you that there is perfect autocorrelation with both lag 1 and lag 5. Partial autocorrelation would tell you that there is perfect autocorrelation with lag 1, but it would not show autocorrelation for lag 5. As you should always try to make models with the lowest number of variables necessary (often referred to as <span class="s19">Occam’s razor </span>or the <span class="s19">parsimony principle</span>), it would be better to include only lag 1 and not lag 5 in this case.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 125%;text-align: left;">You can compute the partial autocorrelation plot using statsmodels, as is shown in Listing <span style=" color: #00F;">3-11</span>. <span class="s19">PACF </span>is short for <span class="s19">partial autocorrelation function</span>: the autocorrelation as a function of lag.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark51">Listing 3-11. </a><span class="s33">Partial autocorrelation of the differenced data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from statsmodels.graphics.tsaplots import plot_pacf plot_pacf(differenced_data, lags = 20)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This code will give you the plot shown in Figure <span style=" color: #00F;">3-8</span>. As you can see based on the blue background shading in the graph, the PACF shows the first and the second lag outside of the shaded area. This means that it would be interesting to also include the second lag in the AR model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;"><span><img width="321" height="218" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_037.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 3-8. <span class="s29">Partial autocorrelation</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">How Many Lags to Include?</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 122%;text-align: justify;">Now the big question in time series analysis is always how many lags to include. This is called the <span class="s19">order of the time series</span>. The notation is <span class="s19">AR(1) </span>for <span class="s19">order 1 </span>and <span class="s19">AR(p) </span>for an <span class="s19">order p</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The order is up to you to decide. Theoretically speaking, you can base your order on the PACF graph. The theory tells you to take the number of lags before obtaining an autocorrelation of 0. All the other lags should be 0.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In the theory, you often see great graphs where the first spike is very high and the rest is equal to zero. In those cases, the choice is easy: you are working with a very “pure” example of AR(1). Another common case is when your autocorrelation starts high and slowly diminishes to zero. In this case, you should use all of the lags where the PACF is not yet zero.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark52">Yet, in practice, it is not always this simple. Remember the famous saying “all models are wrong, but some are useful.” It is very rare to find cases that perfectly fit an AR model. In general, the autoregression process can help to explain part of the variation in a variable, but not all of it.</a></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">In the earthquake example, there is still a bit of partial autocorrelation on further lags, and this goes on until very far lags. There is even a spike of PACF on lag 18. This could mean many things: maybe there really is autocorrelation with lags far away. Or maybe a hidden process is underlying that the AR model is not capturing well.</p><p style="padding-left: 44pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">In practice, you will try to select the number of lags that gives your model the <span class="s19">best</span></p><p class="s19" style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 125%;text-align: left;">predictive performance<span class="p">. Best predictive performance is often not defined by looking at autocorrelation graphs: those graphs give you a theoretical estimate. Yet predictive performance is best defined by doing </span>model evaluation and benchmarking<span class="p">, using the</span></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">techniques that you have seen in Chapter <span style=" color: #00F;">2</span>. Later in this chapter, you will see how to use model evaluation to choose a performant order for the AR model. But before getting into that, it is time to go deeper into the exact model definition of the AR model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;">AR Model Definition</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Until now, you have seen many aspects of the autoregressive model intuitively. As seen before, autocorrelations show for each lag whether there is a correlation with the present. The AR model uses those correlations to make a predictive model.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">But the most important part is still missing: the mathematical definition of the AR model. To make a prediction based on autocorrelation, you need to express the future values of the target variable as a function of the lagged variables. For the AR model, this gives the following model definition:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s66" style="text-indent: 0pt;line-height: 7pt;text-align: left;">p</p><p style="padding-left: 220pt;text-indent: 0pt;line-height: 7pt;text-align: left;"/><p class="s66" style="padding-left: 52pt;text-indent: 0pt;text-align: center;"><span class="s74">X</span>t    <span class="s75">     </span>  <span class="s75">   </span>i <span class="s74">X</span>t <span class="s72">-</span>i   <span class="s75">     </span>t</p><p class="s66" style="padding-left: 41pt;text-indent: 0pt;text-align: center;">i <span class="s76">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 26pt;text-indent: 17pt;line-height: 121%;text-align: left;">In this formula, <span class="s20">X</span><span class="s77">t </span>is the current value, and it is computed as the sum of each lagged value <span class="s20">X</span><span class="s77">t </span><span class="s78">− </span><span class="s79">i </span>multiplied by a coefficient <span class="s80">φ</span><span class="s77">i </span>for this specific lag. The error <span class="s80">ε</span><span class="s77">t </span>at the end is random noise that you cannot predict, but you can estimate.</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark53">Estimating the AR Using Yule-Walker Equations</a></h4><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 123%;text-align: left;">You have now seen <span class="s19">the definition of the AR model</span>. That is, you have defined its form. However, you can’t use it as long as you don’t have the values to plug into the formula. The next step is to <span class="s19">fit the model</span>: you need to find the optimal values for each coefficient to be able to use this. This is also called <span class="s19">estimating the coefficients</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Once you have values for the parameters, it is relatively easy to use the model: just plug in the lagged values. But as in any machine learning theory, the difficult part is always in estimating the model.</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 13pt;text-align: left;">In the formula, you see that the past values (<span class="s20">X</span><span class="s77">t </span><span class="s78">− </span><span class="s79">i</span>) are multiplied by a coefficient (<span class="s80">φ</span><span class="s77">i</span>).</p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 123%;text-align: left;">The sum of this will give you the current value <span class="s20">X</span><span class="s77">t</span>. When starting to fit the model, you already know the past values and the current value. The only thing you don’t know is the phis (<span class="s80">φ</span><span class="s77">i</span>). The phis are what needs to be found mathematically to define the model. Once you estimate those phis, you will be able to compute tomorrow’s value of <span class="s20">X</span><span class="s77">t </span>using the estimated phis and the known values of today and before.</p><p style="padding-left: 7pt;text-indent: 17pt;line-height: 129%;text-align: justify;">For the AR model, different methods have been found to estimate the coefficients (the phis), but the Yule-Walker method is generally accepted as the most appropriate method.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The Yule-Walker Method</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The <span class="s19">Yule-Walker method </span>consists of a set of equations:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">p</p><p style="padding-left: 190pt;text-indent: 0pt;line-height: 7pt;text-align: left;"/><p class="s52" style="padding-left: 187pt;text-indent: -23pt;line-height: 83%;text-align: left;"><span class="s81">y</span>m  <span class="s82">=</span><span class="s59"> </span><span class="s83">L</span><span class="s81">c</span>k<span class="s81">y</span><span class="s84"> </span>m<span class="s72">-</span>k k<span class="s72">=</span><span class="s85">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s59" style="text-indent: 0pt;text-align: left;">+<span class="s84">cr 8</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s85" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">2</p><p class="s86" style="padding-left: 15pt;text-indent: 0pt;text-align: left;">i::  </p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">m <span class="s85">,0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s87" style="text-indent: 0pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 7pt;text-indent: 17pt;line-height: 112%;text-align: left;">In this formula, <span class="s80">γ</span><span class="s77">m </span>is the <span class="s19">autocovariance function </span>of <span class="s20">X</span><span class="s77">t</span>. <span class="s20">m </span>is the number of lags from 0 to p (p is the maximum lag). There are therefore p + 1 equations. <span class="s88">cr </span><span class="s89">2 </span>is the standard</p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">deviation of the errors.</p><p class="s87" style="text-indent: 0pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 18pt;line-height: 124%;text-align: left;"><span class="s80">δ</span><span class="s77">m</span><span class="s90">, 0 </span>is the <span class="s19">Kronecker delta function</span>: it returns 1 if both values are equal (so if m is equal to 0) and 0 if m is not 0. You can see that this means that everything after the + sign is equal to <span class="s88">cr </span><span class="s89">2 </span>if m is 0 and the whole is equal to 0 otherwise. Therefore, there is only one equation that has the part after the +, while the others (from m = 1 to m = p) do not have this part.</p><p style="padding-left: 7pt;text-indent: 17pt;line-height: 122%;text-align: left;">The clue to solving this is to start with only the equations from m = 1 to m = p (p equations) written in <span class="s19">matrix format</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s59" style="padding-top: 6pt;padding-left: 25pt;text-indent: 0pt;line-height: 10pt;text-align: right;"><a name="bookmark54">( </a><span class="s91">y</span><span class="s92">1 </span>\</p><p class="s59" style="padding-top: 6pt;padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">( <span class="s91">y</span><span class="s93"> </span><span class="s92">0</span></p><p class="s94" style="padding-top: 6pt;padding-left: 14pt;text-indent: 0pt;line-height: 10pt;text-align: left;">y<span class="s93"> </span><span class="s72">-</span><span class="s85">1</span></p><p class="s94" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;line-height: 10pt;text-align: left;">y<span class="s93"> </span><span class="s72">-</span><span class="s85">2</span></p><p class="s59" style="padding-top: 5pt;padding-left: 10pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s95">.</span>\( <span class="s91">c</span><span class="s93">p</span><span class="s96">1</span><span class="s85">  </span>\</p><p class="s93" style="padding-top: 5pt;padding-left: 9pt;text-indent: 0pt;line-height: 5pt;text-align: center;"><span class="s97"> </span><span class="s59"> </span>y        <span class="s98">  </span>y        y       y       <span class="s59">.</span><span class="s97"> </span>cp</p><p class="s85" style="padding-top: 2pt;padding-left: 166pt;text-indent: 0pt;text-align: left;">2<span class="s92"> </span>1 0 <span class="s72">-</span>1 2  <span class="s99"> </span></p><p class="s93" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><span class="s82"> </span><span class="s59"> </span>y <span class="s92">3   </span><span class="s100">   </span><span class="s59">= </span><span class="s82">  </span>y <span class="s92">2            </span>y<span class="s92">1          </span>y <span class="s92">0          </span><span class="s59">.</span><span class="s82"> </span>cp<span class="s92">3 </span><span class="s85"> </span><span class="s82"> </span></p><p class="s59" style="padding-top: 7pt;padding-left: 19pt;text-indent: 0pt;line-height: 3pt;text-align: center;"> <span class="s101">.</span>  <span class="s101">. . . </span>.  <span class="s101">.</span>  </p><p class="s59" style="text-indent: 0pt;line-height: 12pt;text-align: right;">     </p><p class="s93" style="padding-top: 1pt;text-indent: 0pt;text-align: left;">y</p><p style="text-indent: 0pt;text-align: left;"/><p class="s59" style="text-indent: 0pt;line-height: 11pt;text-align: right;">     </p><p class="s59" style="padding-left: 25pt;text-indent: 0pt;line-height: 6pt;text-align: right;">   <span class="s102">p</span><span class="s52">  </span> </p><p class="s59" style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: left;"> </p><p class="s93" style="padding-top: 1pt;text-indent: 0pt;text-align: left;">y</p><p style="text-indent: 0pt;text-align: left;"/><p class="s59" style="padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: left;"> </p><p class="s103" style="padding-left: 7pt;text-indent: 0pt;line-height: 6pt;text-align: left;"> <span class="s59">  </span><span class="s52">p</span><span class="s72">-</span><span class="s85">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s104" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">y <span class="s52">p</span><span class="s72">-</span><span class="s85">2 </span><span class="s94">y</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="text-indent: 0pt;text-align: left;">p<span class="s72">-</span><span class="s85">3</span></p><p class="s59" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">       </p><p class="s59" style="text-indent: 0pt;text-align: left;">  </p><p style="text-indent: 0pt;text-align: left;"/><p class="s52" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">p  <span class="s103"> </span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s59" style="padding-left: 8pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s99">.</span> <span class="s105">cp   </span> </p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">This can be written in <span class="s19">matrix notation </span>as</p><p class="s106" style="padding-top: 11pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">γ= Γ<span class="s107">−</span><span class="s108">1 </span>Φ<span class="s109"></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">To obtain the values for phis, you can use the <span class="s19">Ordinary Least Squares </span>(OLS) method. The Ordinary Least Squares method is a matrix computation that is applied widely throughout statistics to solve problems like this one. It allows estimating coefficients in cases where you have a matrix with historical data and a vector with historical outcomes. The solution to finding the best estimates for the phis is the following:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="10" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_038.png"/></span></p><p class="s106" style="padding-top: 7pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">Φ<span class="s109"></span><span class="s110"> </span>= <span class="s111">(</span>Γ<span class="s112">T</span><span class="s108"> </span>Γ<span class="s111">)</span><span class="s113">−</span><span class="s108">1 </span>Γ<span class="s112">T</span><span class="s108"> </span>γ</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You will see the OLS method also in Chapter <span style=" color: #00F;">11 </span>and other chapters throughout this book.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Once you’ve applied OLS, you will have obtained the estimates for phis, and you can then compute the sigma by using the estimated phis for solving:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">p</p><p style="padding-left: 216pt;text-indent: 0pt;line-height: 7pt;text-align: left;"/><p class="s86" style="text-indent: 0pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p class="s81" style="padding-left: 213pt;text-indent: -20pt;line-height: 81%;text-align: left;">y<span class="s84"> </span><span class="s85">0  </span><span class="s114">=</span><span class="s115"> </span><span class="s116">í:</span>c<span class="s84">p</span><span class="s52">k</span>y<span class="s84"> </span><span class="s52">m</span><span class="s117">-</span><span class="s52">k k</span><span class="s117">=</span><span class="s85">1</span></p><p class="s115" style="padding-top: 8pt;text-indent: 0pt;text-align: left;">+<span class="s84">cr </span><span class="s63">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now you’ll start fitting the AR model in Python, using Listing <span style=" color: #00F;">3-12</span>. Fitting the model in this case means to identify the best possible values for the phis. As you’re working with real data and not with a simulated sample, note that you’ll be working toward the best fit possible of the AR model throughout this example.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 3-12. <span class="s33">Estimate Yule-Walker AR coefficients with order 3</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from statsmodels.regression.linear_model import yule_walker coefficients, sigma = yule_walker(differenced_data, order = 3)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark55">print(&#39;coefficients: &#39;, -coefficients) print(&#39;sigma: &#39;, sigma)</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">The <span class="s19">coefficients </span>that you obtain with this are the coefficients for each of the lagged variables. In this case, the order is 3, so that will need to be applied for the three last values to compute the new value. You can make a forecast by computing the next <span class="s19">steps</span>, based on the current coefficients. The code for this is shown in Listing <span style=" color: #00F;">3-13</span>. Attention: The more steps forward you want to forecast, the more difficult it becomes – your error will likely increase with the number of steps.</p><p class="s28" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 24pt;text-align: left;">Listing 3-13. <span class="s33">Make a forecast with the AR coefficients </span><span class="s34">coefficients, sigma = yule_walker(differenced_data, order = 3) # Make a list of differenced values</span></p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">val_list = list(differenced_data)</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: -11pt;line-height: 113%;text-align: left;"># Reverse the list so that the order corresponds with the order of the coefficients</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">val_list.reverse()</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Define the number of years to predict n_steps = 10</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># For each year to predict for i in range(n_steps):</p><p class="s34" style="padding-top: 7pt;padding-left: 41pt;text-indent: -11pt;line-height: 113%;text-align: left;"># Compute the new value as the sum of lagged values multiplied by their corresponding coefficient</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 14pt;text-align: left;">new_val = 0</p><p class="s34" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">for j in range(len(coefficients)):</p><p class="s34" style="padding-top: 9pt;padding-left: 52pt;text-indent: 0pt;text-align: left;">new_val += coefficients[j] * val_list[j]</p><p class="s34" style="padding-top: 9pt;padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Insert the new value at the beginning of the list val_list.insert(0, new_val)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Redo the reverse to have the order of time val_list.reverse()</p><p class="s34" style="padding-top: 7pt;padding-left: 19pt;text-indent: -11pt;line-height: 113%;text-align: left;"># Add the original first value back into the list and do a cumulative sum to undo the differencing</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark56">val_list = [earthquakes_per_year.values[0]] + val_list new_val_list = pd.Series(val_list).cumsum()</a></p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Plot the newly obtained list plt.plot(range(1966, 2025), new_val_list) plt.ylabel(&#39;Number of earthquakes&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 101pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="218" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_039.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 3-9. <span class="s29">The AR(3) model has not captured anything: flat line forecast</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You will obtain the plot in Figure <span style=" color: #00F;">3-9</span>. The first values are actual data, and the last ten steps are forecasted data. Unfortunately, you can see that the model has not captured anything interesting: it is just a flat line! But you haven’t finished yet. Let’s try with an order of 20 this time and see whether anything improves.</p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;">Just change the order in the first line, and you will obtain the plot in Figure <span style=" color: #00F;">3-10</span>.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">It obtains a very different forecast than the one in Figure <span style=" color: #00F;">3-9</span>. Your model seems to be learning something much more interesting when you add an order 20: it has captured some variation.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">Although this seems to fit well with the data visually, you still need to verify whether the model was correct: did it forecast something close to reality? To do this, you will use some strategies and metrics from Chapter <span style=" color: #00F;">2 </span>and apply them to this forecast.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="218" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_040.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark57">Figure 3-10. </a><span class="s29">The forecast with order 20 has captured something, but is it correct?</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Train-Test Evaluation and Tuning</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this case, let’s apply a train-test split. You will cut off the last 10 years of the dataset and use them as a test set. This allows you to fit your model using the rest of the data (the train set). Once you have fitted on the train set, you can compute the error between the test set and your prediction. The code in Listing <span style=" color: #00F;">3-14 </span>does just that.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 3-14. <span class="s33">Fit the model on a train set and evaluate it on a test set</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.metrics import r2_score</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">train = list(differenced_data)[:-10] test = list(earthquakes_per_year)[-10:]</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">coefficients, sigma = yule_walker(train, order = 3)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Make a list of differenced values val_list = list(train)</p><p class="s34" style="padding-left: 19pt;text-indent: -11pt;line-height: 113%;text-align: left;"># Reverse the list so that the order corresponds with the order of the coefficients</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">val_list.reverse()</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Define the number of years to predict n_steps = 10</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># For each year to predict for i in range(n_steps):</p><p class="s34" style="padding-top: 7pt;padding-left: 59pt;text-indent: -11pt;line-height: 113%;text-align: left;"># Compute the new value as the sum of lagged values multiplied by their corresponding coefficient</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 14pt;text-align: left;">new_val = 0</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">for j in range(len(coefficients)):</p><p class="s34" style="padding-top: 9pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">new_val += coefficients[j] * val_list[j]</p><p class="s34" style="padding-top: 9pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Insert the new value at the beginning of the list val_list.insert(0, new_val)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Redo the reverso to have the order of time val_list.reverse()</p><p class="s34" style="padding-top: 7pt;padding-left: 37pt;text-indent: -11pt;line-height: 113%;text-align: left;"># Add the original first value back into the list and do a cumulative sum to undo the differencing</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">val_list = [earthquakes_per_year[0]] + val_list new_val_list = pd.Series(val_list).cumsum()</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Plot the newly obtained list validation = pd.DataFrame({</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;original&#39;: earthquakes_per_year.reset_index(drop=True), &#39;pred&#39;: new_val_list })</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">print(&#39;Test R2:&#39;, r2_score(validation.iloc[-10:, 0], validation.iloc[-10:, 1]))</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Plot the newly obtained list plt.plot(range(1966, 2015), validation) plt.legend(validation.columns) plt.ylabel(&#39;Number of earthquakes&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="218" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_041.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark58">Figure 3-11. </a><span class="s29">Evaluation plot of the AR model with order 3</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 123%;text-align: left;">Unfortunately, the model with order 3 is quite bad. So bad that the R2 is negative (<span class="s19">-0.04</span>). You can see from the graph (Figure <span style=" color: #00F;">3-11</span>) that the model suffers from <span class="s19">underfitting</span>: it did not capture anything and predicts only an average value into the future. This confirms what you’ve seen before with order 3.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">In the previous example, you have applied the order 20 to see whether that would give a better result. Yet the choice for order 20 is quite random. To find an order that works well, you can apply a <span class="s19">grid search</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">A grid search consists of doing a model evaluation for each value of a <span class="s19">hyperparameter</span>. Hyperparameters are parameters that are not estimated by the model but chosen by the modeler. The order of the model is an example of this. Other models often have more hyperparameters, which makes choosing hyperparameters a nontrivial decision.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">A grid search for one parameter is very simple to code: you simply fit the model for each possible value for the hyperparameter, and you select the best-performing one. You can use the example in Listing <span style=" color: #00F;">3-15</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 3-15. <span class="s33">Apply a grid search to find the order that gives the best R2 score on the test data</span></p><p class="s34" style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">def evaluate(order):</p><p class="s34" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">train = list(differenced_data)[:-10] test = list(earthquakes_per_year)[-10:]</p><p class="s34" style="padding-top: 7pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">coefficients, sigma = yule_walker(train, order = order)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Make a list of differenced values val_list = list(train)</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Reverse the list to corresponds with the order of coefs val_list.reverse()</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Define the number of years to predict n_steps = 10</p><p class="s34" style="padding-top: 7pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;"># For each year to predict for i in range(n_steps):</p><p class="s34" style="padding-top: 7pt;padding-left: 70pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Compute the new value new_val = 0</p><p class="s34" style="padding-left: 70pt;text-indent: 0pt;line-height: 14pt;text-align: left;">for j in range(len(coefficients)):</p><p class="s34" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">new_val += coefficients[j] * val_list[j]</p><p class="s34" style="padding-top: 9pt;padding-left: 70pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Insert the new value at the beginning of the list val_list.insert(0, new_val)</p><p class="s34" style="padding-top: 7pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Redo the reverse to have the order of time val_list.reverse()</p><p class="s34" style="padding-top: 7pt;padding-left: 48pt;text-indent: 0pt;text-align: left;"># Undo the differencing with a cumsum</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">val_list = [earthquakes_per_year[0]] + val_list new_val_list = pd.Series(val_list).cumsum()</p><p class="s34" style="padding-top: 7pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Plot the newly obtained list validation = pd.DataFrame({</p><p class="s34" style="padding-left: 70pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;original&#39;: earthquakes_per_year.reset_index(drop=True), &#39;pred&#39;: new_val_list })</p><p class="s34" style="padding-top: 7pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">return r2_score(validation.iloc[-10:, 0], validation.iloc[-10:, 1])</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># For each order between 1 and 30, fit and evaluate the model orders = []</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">r2scores = []</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: -22pt;line-height: 113%;text-align: left;">for order in range(1, 31): orders.append(order) r2scores.append(evaluate(order))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark59"># Create a results data frame</a></p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">results =pd.DataFrame({&#39;orders&#39;: orders,</p><p class="s34" style="padding-top: 1pt;padding-left: 129pt;text-indent: 0pt;text-align: left;">&#39;scores&#39;: r2scores})</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Show the order with best R2 score results[results[&#39;scores&#39;] == results.max()[&#39;scores&#39;]]</p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">This gives the highest R2 score for order 19, with an R2 score of <span class="s19">0.13</span>. This means that the AR(19) model explains about 13% of the variation on the test data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can reuse the code in Listing <span style=" color: #00F;">3-13 </span>to recreate the graph with the forecast using order 19. Just change the order in the line that fits the Yule-Walker coefficients. The graph is shown in Figure <span style=" color: #00F;">3-12</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This code will also make a plot for the model with order 19 and observe that, although not perfect, the model indeed fits a bit better than the model with order 3.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 122%;text-align: left;">The AR model is one of the basic building blocks of univariate time series. It can be used as a <span class="s19">stand-alone </span>model only in very rare and specific cases. In this real-life</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">example of predicting earthquakes, the R2 score of 0.13 is not very good: you can surely do much better by including other building blocks of univariate time series. You will discover these building blocks throughout the next chapters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="352" height="218" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_042.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 3-12. <span class="s29">Forecast on the test set for the AR model with order 19</span></p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4><ul id="l11"><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The AR model predicts the future of a variable by leveraging correlations between a variable’s past and present values.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Autocorrelation is correlation between a time series and its previous values.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Partial autocorrelation is autocorrelation conditional on earlier lags: it prevents double counting correlations.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The number of lags to include in the AR model can be based on theory (ACF and PACF plots) or can be determined by a grid search.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">A grid search consists of doing a model evaluation for each value of the hyperparameters of a model. This is an optimization method for the choice of hyperparameters.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: justify;">Hyperparameters are different from model coefficients. Model coefficients are optimized by a model, while hyperparameters are chosen by the modeler. Yet the model can use optimization techniques like a grid search to find the best hyperparameters.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: justify;">Yule-Walker equations are used to fit the AR model. Fitting the model means finding the coefficients of the model.</p></li></ul></li></ol></ol><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark60">CHAPTER 4</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The MA Model</h1><p style="padding-top: 21pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">The MA model, short for the <span class="s118">Moving Average model</span>, is the second important building block in univariate time series (see Table <span style=" color: #00F;">4-1</span>). Like the AR model, it is a building block that is more often used as a part of more complex time series, but it can also be used as a stand-alone.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:26pt" cellspacing="0"><tr style="height:19pt"><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s40" style="text-indent: 0pt;line-height: 14pt;text-align: left;">Table 4-1.</p></td><td style="width:300pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s41" style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;">The Building Blocks of Univariate Time Series</p></td><td style="width:56pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:24pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Name</p></td><td style="width:300pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Explanation</p></td><td style="width:56pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Chapter</p></td></tr><tr style="height:22pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">AR</p></td><td style="width:300pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Autoregression</p></td><td style="width:56pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">3</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">MA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Moving Average</p></td><td style="width:56pt"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">4</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARMA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Combination of AR and MA models</p></td><td style="width:56pt"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">5</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARIMA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Adding differencing (I) to the ARMA model</p></td><td style="width:56pt"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">6</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Adding seasonality (S) to the ARIMA model</p></td><td style="width:56pt"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">7</p></td></tr><tr style="height:38pt"><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMAX</p></td><td style="width:300pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;padding-right: 14pt;text-indent: 0pt;line-height: 127%;text-align: left;">Adding external variables (X) to the SARIMA model <i>(note that external variables make the model not univariate anymore)</i></p></td><td style="width:56pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">8</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 17pt;line-height: 123%;text-align: left;">Throughout this chapter, you will see the MA model applied to a forecasting example. Please note that, as in the previous chapter, the model is optimized as <span class="s118">a stand- alone model. </span>When applying univariate time series in practice, you will generally use the SARIMA or SARIMAX model directly: after all, the MA model is a component of</p><p style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">this model. Yet there is added value in seeing the MA model applied separately, as it is important to understand each building block of the SARIMAX model separately.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 11pt;text-align: right;">71</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 8pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_4#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_4#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_4</a></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark61">The Model Definition</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 124%;text-align: left;">The mathematical model underlying the MA model is quite like that of the AR model in form but very different in intuitive understanding. Where the AR model looks at lagged values of the target variable to predict the future, the MA model looks at the <span class="s118">model error of past predictions</span>. The model definition is as follows:</p><p class="s120" style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;text-align: center;"><span class="s62">X</span><span class="s119">t </span><span class="s59">= </span>µ <span class="s59">+ </span>8<span class="s119">t </span><span class="s59">+</span>8<span class="s121">1</span>8<span class="s119">t </span><span class="s72">-</span><span class="s76">1 </span><span class="s59">+.+</span>8<span class="s119">q</span>8<span class="s119">t </span><span class="s72">-</span><span class="s66">q</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 124%;text-align: left;">In this equation, <span class="s118">Xt </span>are the current values (unlagged), <span class="s80">μ </span>(mu) is the average of the series, <span class="s80">ε </span>(epsilon) is the error of the model, and <span class="s80">θ </span>(thetas) are the coefficients to multiply with each of the past values.</p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: justify;">As you can see, the model has only past errors in it and no past values. This means that to predict the future, you don’t look at what happened in the past, but only at how wrong you were in the past!</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This may seem strange, but it has some logic in it. Imagine a case in which you try to make a prediction based on the past. At some points, the process differs strongly from what you expected, and you do not understand why. Let’s say you had a huge unexpected and temporary drop in stock prices at some point.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Both an AR model and an MA model would be able to take this drop into account over time, but in a different way. The AR model would model it as a past low value. The MA model would model it as a large error, which can be interpreted as a large impulse. In this case, it makes sense to see the error as the impulse rather than the value itself, because the fact that the impulse was unexpected is what is going to be a driver for the stock prices the next days as stock traders will be reacting to this unexpected impulse.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 121%;text-align: justify;">In this way, <span class="s20">the model error in the past influences the future, and past errors can help you to make a forecast</span>. This is the idea behind the MA model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="546" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_043.png"/></span></p><p class="s38" style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;line-height: 113%;text-align: justify;">Note <span class="s39">The term moving average is also used for taking the average of the most recent values. In time series, this is often used for smoothing a time series. This has nothing to do with the MA model.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="546" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_044.png"/></span></p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;"><a name="bookmark62">Fitting the MA Model</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">Fitting the MA model is more complicated than the AR model. From a high level, the MA model seems relatively comparable to an AR model: MA depends on past errors, whereas the AR model depends on past values.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Yet there is a big difference here. When fitting the MA model, you do not yet have the past errors of the model. Since you have not yet defined the model, you cannot estimate the error of the model. And you need the error of the model to estimate the coefficients.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">There is some sort of a circular dependency here, which makes it impossible to find a straightforward method to fit the model at once. Yet different techniques can be used for fitting an MA model, of which the most common is the Nonlinear Least Squares. It assumes that the error at time 0 is negligible. The thetas can then be estimated using the following formula:</p><p class="s122" style="padding-top: 12pt;padding-left: 25pt;text-indent: 0pt;line-height: 4pt;text-align: right;">ˆ <span class="s52">T </span><span class="s123">(</span></p><p class="s52" style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: center;">t <span class="s72">-</span><span class="s124">2                                         </span><span class="s125">2</span></p><p class="s59" style="text-indent: 0pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p class="s52" style="text-indent: 0pt;line-height: 3pt;text-align: center;">k</p><p class="s59" style="padding-left: 52pt;text-indent: 0pt;line-height: 7pt;text-align: center;"><span class="s126">0</span><span class="s60">nl</span><span class="s52">s  </span>= <span class="s56">argmin </span><span class="s127">I</span><span class="s99">/</span> <span class="s42">x</span><span class="s60">t</span><span class="s52">  </span>-<span class="s126">0 </span><span class="s127">I</span><span class="s67">(</span>-<span class="s126">0 </span><span class="s67">) </span><span class="s126">q,</span><span class="s60">t</span><span class="s52"> </span><span class="s72">-</span><span class="s124">1</span><span class="s72">-</span><span class="s52">k  </span><span class="s99"> </span></p><p class="s52" style="padding-left: 184pt;text-indent: 0pt;text-align: left;"><span class="s128">0 </span>t <span class="s72">=</span><span class="s124">2 </span>k<span class="s72">=</span><span class="s124">0 </span><span class="s59">)</span></p><p style="padding-top: 12pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">Since this formula uses an <span class="s118">argmin</span>, it is a numerical search for the optimal values for thetas. This means that fitting the model is simply done by iteratively searching through different values for thetas and identifying which values for thetas lead to the lowest value for the rest of the formula, that is, the lowest of the following:</p><p class="s52" style="padding-top: 9pt;padding-left: 52pt;text-indent: 0pt;line-height: 6pt;text-align: center;">T                      t <span class="s72">-</span><span class="s85">2                                 </span><span class="s129"> </span><span class="s130">2</span></p><p class="s83" style="padding-top: 7pt;padding-left: 174pt;text-indent: 0pt;line-height: 53%;text-align: left;">l:I <span class="s131">x</span><span class="s52">t t </span><span class="s72">=</span><span class="s85">2 </span><span class="s59">\</span></p><p class="s132" style="padding-left: 1pt;text-indent: 0pt;line-height: 19pt;text-align: left;"><span class="s59">-</span>e <span class="s127">l</span><span class="s83">:</span><span class="s67">e</span><span class="s59">-</span>e <span class="s67">)</span><span class="s133">k</span><span class="s52"> </span>cp</p><p class="s52" style="padding-left: 15pt;text-indent: 0pt;line-height: 7pt;text-align: left;">k<span class="s72">=</span><span class="s85">0</span></p><ol id="l12"><ol id="l13"><ol id="l14"><li><p class="s52" style="padding-top: 3pt;padding-left: 16pt;text-indent: -16pt;line-height: 12pt;text-align: left;">k <span class="s134">.</span></p><p class="s59" style="padding-left: 16pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-top: 13pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">An example of such a minimization can be found in Figure <span style=" color: #00F;">4-1</span>. The figure shows a curve of error, which depends on the parameters. The goal is to find the parameters that give you the lowest model error. This is not an easy task, as you generally have no clue what those parameters are.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">I won’t go into more depth in the optimization methods that can be used for this minimization task, as that would be out of scope for this book. But you should know that there are many algorithms that do such optimization tasks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><span><img width="510" height="223" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_045.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark63">Figure 4-1. </a><span class="s29">Parameters that minimize an error</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Stationarity</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Important to note is that the MA model has an absolute need for stationarity. Where the AR model can adapt to non-stationary situations, the MA model cannot. The Augmented Dickey Fuller test and differencing are notions that have been covered in Chapter <span style=" color: #00F;">3 </span>and can be used here as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Choosing Between an AR and an MA Model</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Just like the AR model, the lag of the MA model can be decided by looking at the PACF plot (the partial autocorrelation function). But there is more use to the PACF curve than just selecting the best theoretical lag.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The PACF plot can also help you to choose the correct type of model. For instance, you have now seen the AR and the MA model, but you have no idea which one to use. The type of autocorrelation can be a factor in this decision.</p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">The following are indicators for the type of model to use:</p><ul id="l15"><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Autoregression (typical PACF plot in Figure <span style=" color: #00F;">4-2</span>):</p><ul id="l16"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Autocorrelation decays to 0.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Autocorrelation decays exponentially.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Autocorrelation alternates between positive and negative and decays to 0.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 113pt;text-indent: 0pt;text-align: left;"><span><img width="319" height="225" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_046.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark64">Figure 4-2. </a><span class="s29">Typical AR model PACF</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Moving average (typical ACF plot in Figure <span style=" color: #00F;">4-3</span>):</p><ul id="l17"><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">Many values of (almost) zero and a few spikes</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 112pt;text-indent: 0pt;text-align: left;"><span><img width="321" height="225" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_047.jpg"/></span></p><p class="s28" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 4-3. <span class="s29">Typical MA model ACF</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Application of the MA Model</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Let’s now apply this to an example. The data that you will use for this example are Microsoft stock closing prices. You can get those data through the Yahoo Finance Python package as you can see in Listing <span style=" color: #00F;">4-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_048.png"/></span></p><p class="s38" style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark65">Note </a><span class="s39">The Yahoo Finance package is a python package that allows downloading stock data from the Yahoo Finance website.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_049.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 4-1. <span class="s33">Importing stock price data using the Yahoo Finance package</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from pandas_datareader import data as pdr import yfinance</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = pdr.get_data_yahoo(&#39;MSFT&#39;, start=&#39;2019-01-01&#39;, end=&#39;2019-12-31&#39;) data = data[&#39;Close&#39;]</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As you know, it is important to get a first idea of the data, so let’s make a plot of the closing prices. This is done in Listing <span style=" color: #00F;">4-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 4-2. <span class="s33">Plotting the stock price data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt ax = data.plot() ax.set_ylabel(&quot;Stock Price&quot;) plt.show()</p><p style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">This will output the graph in Figure <span style=" color: #00F;">4-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;"><span><img width="323" height="226" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_050.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 4-4. <span class="s29">Plot of the original data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 121%;text-align: left;"><a name="bookmark66">In this graph, you can clearly see an upward trend. It is not even necessary to apply an Augmented Dickey Fuller (ADF) test here to see that this data is </a><span class="s118">not stationary</span>.</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Since the MA model cannot function without stationarity, let’s apply the go-to solution: differencing. This is done in Listing <span style=" color: #00F;">4-3 </span>and will show the plot in Figure <span style=" color: #00F;">4-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 4-3. <span class="s33">Computing the differenced data and plotting it</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Need to difference</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = data.diff().dropna() ax = data.plot()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax.set_ylabel(&quot;Daily Difference in Stock Price&quot;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 112pt;text-indent: 0pt;text-align: left;"><span><img width="323" height="228" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_051.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 4-5. <span class="s29">Plot of the differenced data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">This differenced data seems stationary. For consistency’s sake, Listing <span style=" color: #00F;">4-4 </span>shows how to use an Augmented Dickey Fuller test on these differences, although by visual inspection, there is really no doubt about it: the data is clearly varying around a fixed mean of 0.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 4-4. <span class="s33">Applying an ADF test to the differenced data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from statsmodels.tsa.stattools import adfuller result = adfuller(data)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark67">pvalue = result[1] if pvalue &lt; 0.05:</a></p><p class="s34" style="padding-left: 8pt;text-indent: 22pt;line-height: 113%;text-align: left;">print(&#39;stationary&#39;) else:</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 14pt;text-align: left;">print(&#39;not stationary&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This confirms stationarity of the differenced series. Let’s have a look at the autocorrelation and partial autocorrelation functions to see whether there is an obvious choice for the lag using Listing <span style=" color: #00F;">4-5 </span>to obtain the ACF in Figure <span style=" color: #00F;">4-6 </span>and the PACF in Figure <span style=" color: #00F;">4-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 4-5. <span class="s33">Plotting the autocorrelation function and the partial autocorrelation function</span></p><p class="s34" style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from statsmodels.graphics.tsaplots import plot_acf, plot_pacf plot_acf(data, lags=20)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plot_pacf(data, lags=20) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 95pt;text-indent: 0pt;text-align: left;"><span><img width="319" height="223" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_052.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 4-6. <span class="s29">Autocorrelation function</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 112pt;text-indent: 0pt;text-align: left;"><span><img width="321" height="219" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_053.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark68">Figure 4-7. </a><span class="s29">Partial autocorrelation function</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Unfortunately, there is not a very clear pattern in the autocorrelation nor partial autocorrelation that would confirm a choice for the order. You can observe some decay in the partial autocorrelation function, which is a positive sign for using a time series approach. But nothing too obvious on the choice of lag. Let’s fit the MA model with order 1 and see what it does.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">To fit the MA model in Python, you need to use the ARIMA function from statsmodels. ARIMA is a model that you will see in a next chapter. It is a model that contains multiple building blocks of univariate time series, including AR for the AR model and MA for the MA model. You can specify the order for each of the “smaller” models separately, so by setting everything except MA to 0, you obtain the MA model. This can be done using Listing <span style=" color: #00F;">4-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 4-6. <span class="s33">Fitting the MA model and plotting the forecast</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from sklearn.metrics import r2_score</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from statsmodels.tsa.arima.model import ARIMA</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Forecast the first MA(1) model</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = ARIMA(data.diff().dropna(), order=(0,0,1)) res = mod.fit()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark69">orig_data = data.diff().dropna() pred = res.predict()</a></p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(orig_data) plt.plot(pred) plt.show()</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">print(r2_score(orig_data, pred))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">This gives you a plot of the fit (Figure <span style=" color: #00F;">4-8</span>) and an R2 score on the full period.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;text-align: left;"><span><img width="353" height="259" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_054.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 4-8. <span class="s29">Actuals vs. forecast on a train dataset</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: left;">This plot looks not too bad: the orange curve (predicted) follows the general trend in the blue curve (actual values). You can observe that the predicted curve is less extreme in its predictions (the highs are less high, and the lows are less low). The R2 score is <span class="s118">0.51</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As an interpretation, you could observe that there is an underfit in this model: it does capture the basics, but it does not capture enough of the trend to be a very good model. At this stage, you can already have a check of its out-of-sample performance by creating a train and a test set. This can be done as follows using Listing <span style=" color: #00F;">4-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: left;"><a name="bookmark70">Listing 4-7. </a><span class="s33">Fitting the MA model on train data and evaluating the R2 score on train and test data</span></p><p class="s34" style="padding-top: 6pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">train = data.diff().dropna()[0:240] test = data.diff().dropna()[240:250]</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Forecast the first MA(1) model mod = ARIMA(train, order=(0,0,1)) res = mod.fit()</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">orig_data = data.diff().dropna() pred = res.predict()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">fcst = res.forecast(steps = len(test))</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(r2_score(train, pred)) print(r2_score(test, fcst))</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This should give you a train R2 of <span class="s118">0.51 </span>and a test R2 of <span class="s118">0.13</span>. This performance on the training data is not good, and the performance on the test data is even worse (only 13% of the variation in the test data can be explained by the MA model). A visual of the test data actuals vs. the forecast should confirm this bad fit (using Listing <span style=" color: #00F;">4-8 </span>to obtain Figure <span style=" color: #00F;">4-9</span>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 4-8. <span class="s33">Plotting the out-of-sample forecast of the MA(1) model (MA with order 1)</span></p><p class="s34" style="padding-top: 6pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(list(test)) plt.plot(list(fcst))</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;Actual Prices&#39;, &#39;Predicted Prices&#39;]) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;"><span><img width="323" height="232" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_055.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark71">Figure 4-9. </a><span class="s29">Out-of-sample performance of the MA(1) model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">But when looking at this visual, you can see that there is something weird going on with the forecast on the test set. The MA(1) forecast has forecasted the average for every value except for the first step into the future.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">And there is a mathematical reason for this: the MA(q) model uses the model error of the past q steps to predict the future. When predicting one step into the future, there is no problem: the model has the actual values and the fitted model of each time step before the prediction. But when doing a forecast for a second time step, the model does not know the actual values for time t + 1, while this is an input that is needed for forecasting t + 2.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Multistep Forecasting with Model Retraining</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">In the previous chapter, the forecast was made for ten steps forward. Yet no attention has yet been paid to an essential difference in forecasting. This is the difference between one-step forecasting and multistep forecasting.</p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">One-step forecasting is the basis. It is relatively easy to predict one step forward.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Multistep forecasting is not an easy task using time series, as errors will accumulate with every step forward you take.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In many cases, the most accurate solution is to retrain the model each time step. In fact, it is possible to do this if you update the model as soon as you obtain a new data point. This means that you would do repeated one-step forecasts. In some cases, this can work, while in other cases it is necessary to forecast further in the future.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">It is important to consider the duration of your forecast in model evaluation. If you do multistep forecasts, you should do the evaluation on a multistep train-test split. If you do one-step forecasts, you should do the evaluation on a one-step train-test split.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">If you do one-step forecasts, you will find yourself in a lack of a test dataset: you cannot compute a prediction error on a test data of one data point only. A solution that you can use is iteratively fitting the one-step model and constituting a multistep forecast based on multiple one-step forecasts. This is done in Listing <span style=" color: #00F;">4-9</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 4-9. <span class="s33">Estimating the error of the MA(1) model for ten refitted one-step forecasts</span></p><p class="s34" style="padding-top: 6pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">train = data.diff().dropna()[0:240] test = data.diff().dropna()[240:250]</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Import the ARMA module from statsmodels from statsmodels.tsa.arima.model import ARIMA fcst = []</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">for step in range(len(test)):</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;"># Forecast the first MA(1) model</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = ARIMA(train.reset_index(drop=True), order=(0,0,1)) res = mod.fit()</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">orig_data = data.diff().dropna() pred = res.predict()</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 14pt;text-align: left;">fcst += list(res.forecast(steps = 1))</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 22pt;line-height: 113%;text-align: left;">train = train.append(pd.Series(test[step])) print(r2_score(list(test), fcst)) plt.plot(list(test))</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.plot(fcst)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;Actual Prices&#39;, &#39;Predicted Prices&#39;]) plt.show()</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">The R2 score that you obtain using this method is <span class="s118">0.48</span>: not perfect, but this starts to look like something useful. The graph of the forecasted values vs. the actual values is shown in Figure <span style=" color: #00F;">4-10</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;"><span><img width="323" height="232" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_056.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark72">Figure 4-10. </a><span class="s29">Forecasting one step multiple times yields a relatively acceptable forecast</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: justify;">Of course, the error estimate that you obtain this way is only the error estimate for the forecast of the next time step, and this will not be valid for a longer period. How to use this in practice depends on your specific use case.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">For this specific example, you can consider that if you can perfectly predict the stock market of the next day, this is already quite the accomplishment: let’s retain the one-step error in this case.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">Grid Search to Find the Best MA Order</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">As a next step, let’s try to do a grid search to find the order for the MA model that obtains the lowest error on the test set. Remember the grid search from the previous chapter: the basic version of the grid search simply consists of looping through each possible value of the order and evaluating the model on the test data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: justify;">There are other and better automated ways of optimizing hyperparameters: you will see them throughout the next chapters, so make sure that you understand the idea</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">behind this basic version now. Now let’s find out whether there is a specific order for the MA that delivers us better predictive performances using Listing <span style=" color: #00F;">4-10</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark73">Listing 4-10. </a><span class="s33">Grid search to obtain the MA order that optimizes forecasting R2</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">def evaluate2(order):</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">train = data.diff().dropna()[0:240] test = data.diff().dropna()[240:250]</p><p class="s34" style="padding-top: 7pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">fcst = []</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">for step in range(len(test)):</p><p class="s34" style="padding-top: 1pt;padding-left: 70pt;text-indent: 0pt;text-align: left;"># Forecast the first MA(1) model</p><p class="s34" style="padding-top: 1pt;padding-left: 70pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = ARIMA(train.reset_index(drop=True), order=(0,0,order)) res = mod.fit()</p><p class="s34" style="padding-left: 70pt;text-indent: 0pt;line-height: 113%;text-align: left;">orig_data = data.diff().dropna() pred = res.predict()</p><p class="s34" style="padding-left: 70pt;text-indent: 0pt;line-height: 14pt;text-align: left;">fcst += list(res.forecast(steps = 1))</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 22pt;line-height: 170%;text-align: left;">train = train.append(pd.Series(test[step])) return r2_score(list(test), fcst)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">scores = []</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: -22pt;line-height: 113%;text-align: left;">for i in range(1, 21): scores.append((i, evaluate2(i)))</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># observe best order is 4 with R2 of 0.566 scores = pd.DataFrame(scores) print(scores[scores[1] == scores.max()[1]])</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: justify;">This will give you a best order of 4 with an out-of-sample R2 of <span class="s118">0.57</span>. You can have a look at the final forecast that this yields using the code in Listing <span style=" color: #00F;">4-11</span>. The plot that you will obtain is shown in Figure <span style=" color: #00F;">4-11</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 4-11. <span class="s33">Obtaining the final forecast</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">train = data.diff().dropna()[0:240] test = data.diff().dropna()[240:250]</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">fcst = []</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">for step in range(len(test)):</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;"># Forecast the first MA(1) model</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = ARIMA(train.reset_index(drop=True), order=(0,0,4)) res = mod.fit()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark74">orig_data = data.diff().dropna() pred = res.predict()</a></p><p class="s34" style="padding-top: 7pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">fcst += list(res.forecast(steps = 1))</p><p class="s34" style="padding-left: 8pt;text-indent: 22pt;line-height: 24pt;text-align: left;">train = train.append(pd.Series(test[step])) print(r2_score(list(test), fcst)) plt.plot(list(test))</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">plt.plot(fcst)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;Actual Prices&#39;, &#39;Forecasted Prices&#39;]) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;"><span><img width="323" height="232" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_057.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 4-11. <span class="s29">The final result: the MA(4) forecast</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4></li></ul></li></ul></li><li><p style="padding-top: 10pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">The Moving Average model predicts the future based on impulses in the past.</p><ul id="l18"><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Those impulses are measured as model errors.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The idea behind this is that unexpected impacts can actually have a large impact on the future.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l19"><li><p style="padding-top: 5pt;padding-left: 90pt;text-indent: -17pt;line-height: 129%;text-align: justify;"><a name="bookmark75">There is no one-shot computation for the MA model coefficients, so it takes a bit longer to estimate this model compared to the AR model.</a></p></li><li><p style="padding-top: 6pt;padding-left: 90pt;text-indent: -17pt;line-height: 129%;text-align: left;">The MA model is the second building block of the SARIMA model. The AR and MA models together form the ARMA model, the topic of the next chapter.</p></li></ul></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Multistep predictions are predictions of multiple time steps into the future. This is difficult with MA models, especially when the order is low. A solution can sometimes be to do multiple one-step predictions and retrain the model every time that you receive the new data.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The autocorrelation function and the partial autocorrelation function can help you decide whether you are looking at an AR or an MA forecast. AR forecasts see their autocorrelation exponentially decay to 0 and alternate between positive and negative. MA autocorrelation functions are characterized by many values of almost zero and a few spikes.</p><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark76">CHAPTER 5</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The ARMA Model</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this chapter, you will discover the ARMA model. It is a combination of the AR model and the MA model, which you have seen in the two previous chapters. Since the theory of this chapter should be already relatively familiar to you, this chapter will also introduce a few additional model quality indicators that are valid for the ARMA model but can also be used for the AR and MA models separately.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">The Idea Behind the ARMA Model</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">The ARMA model is a simple combination of the AR and MA models. The idea behind combining the AR and MA models into one model is that the models together are more performant than one model. The development over time of one variable can follow multiple processes at the same time.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 125%;text-align: left;">Let’s do a short recap of the AR and MA models separately. The AR model tries to predict the future by <span class="s118">multiplying past values by a coefficient</span>. This AR process is based on the presence of autocorrelation: the target variable’s present value is correlated to a past value. The MA model does not use past values of the target variable to predict its future, but rather uses the <span class="s118">error of past predictions </span>as an impulse for forecasted values.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The ARMA model is nothing more than a model that allows both these processes to be in place at the same time:</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Part of the future of a variable is explained by past values (the AR effect).</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Part of it is explained by past errors (the MA effect).</p></li></ul></li></ul></li></ol></ol></ol><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The fact that both are combined into one model makes it simply easier to use: you can imagine that working with two separate models would be cumbersome.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;line-height: 11pt;text-align: right;">89</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 8pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_5#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_5#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_5</a></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark77">The Mathematical Definition of the ARMA Model</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The ARMA model is defined by the following equation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s66" style="padding-top: 4pt;padding-left: 52pt;text-indent: 0pt;line-height: 3pt;text-align: center;">p                        q</p><p class="s66" style="padding-top: 3pt;padding-left: 25pt;text-indent: 0pt;text-align: center;"><span class="s74">X</span>t <span class="s82">= </span><span class="s62">c     </span>t     <span class="s75">     </span>   <span class="s75">   </span>i <span class="s74">X</span>t <span class="s72">-</span>i     <span class="s75">     </span>   <span class="s75">   </span>i<span class="s75"> </span>t <span class="s72">-</span>i</p><p class="s66" style="padding-left: 52pt;text-indent: 0pt;text-align: center;">i<span class="s72">=</span><span class="s76">1                        </span>i<span class="s72">=</span><span class="s76">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 7pt;text-indent: 18pt;line-height: 118%;text-align: left;">In this equation, you clearly observe the AR part and the MA part. The AR part uses coefficients <span class="s80">φ</span><span class="s77">i </span>(phis) multiplied by past observations <span class="s20">X</span><span class="s77">t </span><span class="s78">− </span><span class="s79">i </span>for a <span class="s118">number of lags </span>defined as <span class="s118">p, </span>also called <span class="s118">the AR order</span>. The MA part uses coefficients <span class="s80">θ</span><span class="s77">i </span>(thetas) multiplied by past errors <span class="s80">ε</span><span class="s77">t </span><span class="s78">− </span><span class="s79">i </span>(epsilon) for a <span class="s118">number of lags q</span>, <span class="s118">the MA order</span>.</p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 17pt;line-height: 121%;text-align: left;">Those p and q are the same hyperparameters that you have encountered before. For model notation, you use <span class="s20">AR(p) for an AR model of order p </span>and <span class="s20">MA(q) for an MA model of order q</span>. In the ARMA model, p and q remain the same. This gives you the following notation: <span class="s118">ARMA(p, q)</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You should note here that there is no dependency between p and q, as there is no dependency between the AR and MA definitions in the equation. This means that an ARMA(p, q) model can take any order for p and q. Special cases are when p = 0, which means you have an MA model, and when q = 0, which means you have an AR model.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: left;">As p and q are hyperparameters, they are parameters that should be chosen by the modeler, while c, <span class="s80">φ</span>, and <span class="s80">θ </span>are coefficients of the model and will therefore be estimated by the model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">An Example: Predicting Sunspots Using ARMA</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now, let’s get to work on an example and see whether the combined power of the AR and the MA model can deliver a good forecast. You will be working with the sunspot dataset, a relatively famous dataset that contains very long historical observations of the number of spots on the sun. You can get the data into Python using Listing <span style=" color: #00F;">5-1 </span>and see the head of the dataframe in Figure <span style=" color: #00F;">5-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 5-1. <span class="s33">Getting the sunspot data into Python</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">data = pd.read_csv(&#39;Ch05_Sunspots_database.csv&#39;, usecols = [1, 2])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark78">The sunspot dataset is well known for having a specific pattern of seasonal effects over 11-year periods. The dataset is not very detailed: it just contains the data counts per</a></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">month. Since there is a lot of data to work with, let’s set the goal to do a forecast with yearly data. To do this, the data need to be aggregated to yearly data. You can use Listing <span style=" color: #00F;">5-2</span></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;">to do this.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 5-2. <span class="s33">Aggregating the sunspot data to yearly data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data[&#39;year&#39;] = data.Date.apply(lambda x: x[:4]) data = data[[&#39;Monthly Mean Total Sunspot Number&#39;, &#39;year&#39;]].groupby(&#39;year&#39;).sum()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">data.head()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 161pt;text-indent: 0pt;text-align: left;"><span><img width="191" height="121" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_058.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 5-1. <span class="s29">An extract of the sunspot data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">Let’s also make a plot over time to see what type of variation you are working with.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">You can do this using Listing <span style=" color: #00F;">5-3</span>. Using this code, you should obtain the graph in Figure <span style=" color: #00F;">5-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 5-3. <span class="s33">Plotting the yearly sunspot data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt ax = data.plot() ax.set_ylabel(&#39;Sunspots&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 93pt;text-indent: 0pt;text-align: left;"><span><img width="323" height="212" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_059.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark79">Figure 5-2. </a><span class="s29">A plot of the sunspot data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Looking at this plot, the pattern is already strikingly clear: high peaks on regular intervals. Let’s see how to use the ARMA model to capture this pattern and make predictions for the future. The first step, as always, is to verify whether the data is stationary or not.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Remember that stationarity means that there is no long-term trend in the data: the average is constant over time. When looking at the data, there is no obvious long-term trend, yet you can use the ADF test (Augmented Dickey Fuller test) to confirm this for you. Listing <span style=" color: #00F;">5-4 </span>shows you how you can do this.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 5-4. <span class="s33">Applying the ADF test to the sunspot yearly totals</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from statsmodels.tsa.stattools import adfuller</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">result = adfuller(data[&#39;Monthly Mean Total Sunspot Number&#39;]) print(result)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">pvalue = result[1]</p><p class="s34" style="padding-top: 9pt;padding-left: 30pt;text-indent: -22pt;line-height: 113%;text-align: left;">if pvalue &lt; 0.05: print(&#39;stationary&#39;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">else:</p><p class="s34" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">print(&#39;not stationary&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 124%;text-align: left;"><a name="bookmark80">The result that you obtain when applying this test is unexpected: the ADF test tells you that the data is </a><span class="s118">not stationary</span>. In this case, the intuitive decision would be to assume stationarity, while the ADF tells you not to. A choice has to be made.</p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;">For now, let’s keep it the easiest possible and stay with the original data.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now the second thing to look at is the autocorrelation function (ACF) and the partial autocorrelation function (PACF). Remember that the ACF and PACF plots can help you to define whether you are working with AR or MA processes. You can obtain the ACF and PACF using Listing <span style=" color: #00F;">5-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 5-5. <span class="s33">Creating the ACF and PACF plots</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from statsmodels.graphics.tsaplots import plot_acf, plot_pacf import matplotlib.pyplot as plt</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">plot_acf(data[&#39;Monthly Mean Total Sunspot Number&#39;], lags=40) plot_pacf(data[&#39;Monthly Mean Total Sunspot Number&#39;], lags=40) plt.show()</p><p style="padding-top: 1pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">You will obtain the graphs in Figures <span style=" color: #00F;">5-3 </span>and <span style=" color: #00F;">5-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 111pt;text-indent: 0pt;text-align: left;"><span><img width="324" height="221" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_060.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 5-3. <span class="s29">The autocorrelation function (ACF)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 93pt;text-indent: 0pt;text-align: left;"><span><img width="324" height="217" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_061.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark81">Figure 5-4. </a><span class="s29">The partial autocorrelation function (PACF)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The ACF and PACF plots in Figures <span style=" color: #00F;">5-3 </span>and <span style=" color: #00F;">5-4 </span>are great cases to study time series patterns; it is rare to observe such strong patterns on real-life data. Remember from the previous chapters which type of patterns you should be looking at:</p><ul id="l20"><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">AR processes are recognized by</p><ul id="l21"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Exponentially decaying partial autocorrelation</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Partial autocorrelation decaying toward zero</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Swings between negative and positive partial autocorrelation</p></li></ul></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">MA processes are identified by</p><ul id="l22"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Sudden spikes in the ACF and PACF</p><p style="padding-top: 9pt;padding-left: 7pt;text-indent: 17pt;line-height: 129%;text-align: justify;">This means that in the sunspot case, you have a strong indicator for observing an AR pattern: there is exponential decay in the PACF, together with switching from negative to positive. There is no real evidence for an MA process when looking at those plots.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Fitting an ARMA(1,1) Model</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">The ARMA(p,q) model being simply the combination of the AR and MA models, it is important to realize that there is an optimization going on that finds the coefficients of the model that minimize the Mean Squared Error of the model. This is the same approach that has been discussed in Chapter <span style=" color: #00F;">4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark82">Remember that there is an important difference between coefficients and hyperparameters. Coefficients are estimated by the model, whereas hyperparameters must be chosen by the modeler. In the ARMA(p,q) model, p and q are the hyperparameters that you must decide on using plots, performance metrics, and more.</a></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">To get started, let’s see how to fit an ARMA(1,1) model in Python. An ARMA(1,1) model means an ARMA model with an AR component with order 1 and an MA component with order 1. Remember that the order refers to the number of historical values that are used to explain the current value.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">For this first trial with order (1,1), the choice is just to start with the simplest ARMA model possible. And throughout the example, you’ll see how to fine-tune this decision.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Applying an ARMA(1,1) model for the sunspot data means that you will explain tomorrow’s value by looking at today’s actual value (AR part) and today’s error</p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;">(MA part). You are not looking further back into the past, as the order is only 1.</p><p style="padding-top: 3pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">This model can be created in Python using the code in Listing <span style=" color: #00F;">5-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 5-6. <span class="s33">Fitting the ARMA(1,1) model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from sklearn.metrics import r2_score</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from statsmodels.tsa.arima.model import ARIMA</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Forecast the first ARMA(1,1) model</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = ARIMA(list(data[&#39;Monthly Mean Total Sunspot Number&#39;]), order=(1,0,1)) res = mod.fit()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">pred = res.predict() print(r2_score(data, pred))</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(list(data[&#39;Monthly Mean Total Sunspot Number&#39;])) plt.plot(pred)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;Actual Sunspots&#39;, &#39;Predicted Sunspots&#39;]) plt.xlabel(&#39;Timesteps&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 127%;text-align: left;">This code will also generate a plot of the actual values vs. the fitted values (Figure <span style=" color: #00F;">5-5</span>). It will also give you an R2 score of the model fit. Since you have not yet created a train- test split, this R2 score is not representative of any future performance. Yet it can be used to see how wrong the current model is: this gives an intuition on how to improve the model. In this case, with an R2 score of <span class="s118">0.76</span>, you can conclude that the model is already fitting not too badly.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 77pt;text-indent: 0pt;text-align: left;"><span><img width="368" height="275" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_062.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark83">Figure 5-5. </a><span class="s29">The actual values vs. the fitted values of the ARMA(1,1)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">More Model Evaluation KPIs</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">As an addition to performance metrics, I want to take the opportunity to inspect a few more model fit KPIs. Until now, you have seen how to use the Augmented Dickey Fuller test to help you decide on differencing, and you have seen the ACF and PACF plots to help you decide on the type of model needed (AR vs. MA) and to help you choose the best order. You have also seen the train-test split combined with a hyperparameter search to identify the order that maximizes out-of-sample performance.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">An additional KPI that can help you evaluate model fit is to look at the residuals of your model. If a model has a good fit, you will generally observe that the residuals follow a normal distribution. If you find the opposite, your model is generally missing out on important information.</p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">You can make a histogram of the residuals of your model using the code in Listing <span style=" color: #00F;">5-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 5-7. <span class="s33">Plotting a histogram of the residuals</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax = pd.Series(res.resid).hist() ax.set_ylabel(&#39;Number of occurences&#39;) ax.set_xlabel(&#39;Residual&#39;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><a name="bookmark84">You should obtain the graph in Figure </a><span style=" color: #00F;">5-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 106pt;text-indent: 0pt;text-align: left;"><span><img width="337" height="243" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_063.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 5-6. <span class="s29">Histogram of the residuals of the ARMA(1,1) model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 127%;text-align: left;">You can observe visually that the histogram of the residuals is not following the bell- shaped curve that is the <span class="s118">normal distribution</span>. This means that there are probably some lags to be added, which is logical: you already know that the sunspot data has an 11-year seasonality, and for now there is only one lagged period (1 year back) included in the model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_064.png"/></span></p><p class="s38" style="padding-top: 7pt;padding-left: 33pt;text-indent: 0pt;line-height: 113%;text-align: left;">Note   <span class="s39">there are many different approaches for testing the distribution of a variable. this includes histograms, QQ plots, and numerous hypothesis tests. although hypothesis tests for normality are useful in many cases, the goal of a forecast is generally to maximize future performance. It is not necessary to have residuals that are perfectly normally distributed for this purpose, and the graphical way of identifying the normality of residuals is generally sufficient in practice.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_065.png"/></span></p><p style="padding-top: 11pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">When you’ve evaluated the normality of the residuals of your model, you can also look at the summary table of your model. You can obtain this summary table using the code in Listing <span style=" color: #00F;">5-8</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 3pt;text-indent: 0pt;text-align: center;">Listing 5-8. <span class="s33">Obtaining the summary table of your model’s fit</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">res.summary()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 9pt;text-indent: 0pt;text-align: center;">You should obtain the summary table shown in Figure <span style=" color: #00F;">5-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;"><span><img width="321" height="324" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_066.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 5-7. <span class="s29">Summary table of the ARMA(1,1) model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">Important pieces of information that you can get from this table are <span class="s118">the estimates of the coefficients </span>and the corresponding <span class="s118">p-values</span>. Remember that you are working with an ARMA(1,1) model and that you therefore will have one coefficient for the AR model (for the data point one time step back) and one coefficient for the MA model (for the error of one time step back).</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 124%;text-align: left;">When you fit the model in the data, your model is estimating the best possible values for those coefficients. You can see the estimates of the coefficients in the table on the line that says <span class="s118">ar.L1 </span>(AR coefficient for the first lag is <span class="s118">0.7193</span>) and on the line that says <span class="s118">ma.L1 </span>(MA coefficient for the first lag is <span class="s118">0.5254</span>).</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: left;">You can interpret those by looking back to the formula. Basically, those estimates state that you can predict a future data point, by filling in the ARMA formula using the <span class="s118">const </span>as c, the ar.L1 as coefficient <span class="s80">φ</span><span class="s77">i </span>(phi), and the ma.L1 as coefficient <span class="s80">θ</span><span class="s77">i </span>(tetha).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;line-height: 128%;text-align: justify;"><a name="bookmark85">Filling in those coefficients would </a><span class="s118">allow you to compute the next value</span>. This is great information for understanding what information is used by your model to predict the future.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">Now that you know the estimated values of your coefficients, you can also look at the hypothesis tests for those coefficients. For each coefficient, the summary table shows</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 12pt;text-align: left;">a hypothesis test that tells you whether the coefficient is <span class="s118">significantly different from</span></p><p class="s118" style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">zero<span class="p">. If a coefficient were to be zero, this would mean that it is not useful to include the parameter. Yet, if a hypothesis test proves that a coefficient is significantly different from zero, this means that it is useful to add the coefficient in your model. This helps you in deciding which order to use for your ARMA forecast.</span></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 123%;text-align: left;">The p-values of the hypothesis tests can be found on the line of the coefficient, under the column <span class="s118">P&gt;|z|</span>. This column gives you the p-value of the hypothesis test. The p-value must be <span class="s118">smaller than 0.05 </span>to prove that the coefficient is significantly different from zero.</p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;">In this case, the interpretation is as follows:</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The p-value for the ar.L1 coefficient is 0.000, so significant.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The p-value for the ma.L1 coefficient is also 0.000, so significant.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: justify;">Both are smaller than 0.05, and therefore you can conclude that both coefficients should be retained in the model.</p><p style="padding-top: 6pt;padding-left: 25pt;text-indent: 17pt;line-height: 121%;text-align: justify;">As you can imagine, this is very useful information when creating a model. You can go back and forward between <span class="s118">increasing and decreasing the order of AR and MA independently </span>and try to find at which point the coefficients start to become</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 129%;text-align: left;">insignificant (p-values higher than 0.05) in which case you take the order of the highest lag that was still significant as the best model. Of course, keep in mind that significance is one indicator between multiple indicators.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Automated Hyperparameter Tuning</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Although this theoretical approach can be very interesting, you should combine it with tests of predictive performance, as you have done already in Chapters <span style=" color: #00F;">3 </span>and <span style=" color: #00F;">4 </span>using very basic approaches to hyperparameter optimization.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark86">If you remember, what you did was creating a train-test split and then looping through all possible values of the hyperparameter and evaluating the performance on the test set. The order that gave the best performance on the test set was retained.</a></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In this part, you will make an addition to this approach by officializing the notions of grid search and cross-validation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Grid Search: Tuning for Predictive Performance</h4><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 128%;text-align: left;">I’ve purposely waited explaining <span class="s118">grid search </span>until here because it is better explained with an example with two hyperparameters. In the ARMA model, two hyperparameters need to be optimized at the same time: p and q.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The goal of a grid search for hyperparameter optimization is to make the task of choosing hyperparameters. You have seen multiple statistical tools that can help you to make the choice:</p></li></ul></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Autocorrelation function (ACF)</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Partial autocorrelation function (PACF)</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Model residuals</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Summary table</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">With multiple indicators, it is difficult to decide on the best parameters. A grid search is a method that will test every combination of <span class="s118">hyperparameters </span>and evaluate the predictive error of this combination of hyperparameters. This will give you an objective estimate of the hyperparameters that will obtain the best performance.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: left;">A basic approach for a grid search is to make a <span class="s118">train-test split </span>and to fit a model with each combination of hyperparameters on the train set and evaluate the model on the test set. In practice, a grid search is often combined with <span class="s118">cross-validation</span>. As you’ve seen in Chapter <span style=" color: #00F;">2</span>, cross-validation is an augmentation of the train-test split approach</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">in which the train-test split is repeated multiple times. This yields a more reliable error estimate.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">To apply a grid search with cross-validation, you need to implement a loop through each combination of hyperparameters (in this case, p and q). For each combination, the code will split the data in multiple <span class="s118">cross-validation splits</span>: multiple combinations of train-test splits. Those splits are also called folds. For each of those folds, you train the model on the train set and test the model on the test set. This yields an error for</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;"><a name="bookmark87">each fold, of which you take the average to obtain an error for each hyperparameter combination. The hyperparameter combination with the best error will be the model that you retain for your forecast.</a></p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;">Listing <span style=" color: #00F;">5-9 </span>shows how this is done in code.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 5-9. <span class="s33">Grid search with cross-validation for optimal p and q</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import numpy as np</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.model_selection import TimeSeriesSplit data_array = data.values</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">avg_errors = []</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">for p in range(13):</p><p class="s34" style="padding-top: 1pt;padding-left: 70pt;text-indent: -22pt;line-height: 156%;text-align: left;">for q in range(13): errors = []</p><p class="s34" style="padding-top: 2pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">tscv = TimeSeriesSplit(test_size=10)</p><p class="s34" style="padding-top: 9pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">for train_index, test_index in tscv.split(data_array):</p><p class="s34" style="padding-top: 9pt;padding-left: 89pt;text-indent: 0pt;text-align: left;">X_train, X_test = data_array[train_index], data_array[test_index]</p><p class="s34" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">X_test_orig = X_test</p><p class="s34" style="padding-top: 9pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">fcst = []</p><p class="s34" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">for step in range(10):</p><p class="s34" style="padding-top: 9pt;padding-left: 114pt;text-indent: 0pt;text-align: left;">try:</p><p class="s34" style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = ARIMA(X_train, order=(p,0,q)) res = mod.fit()</p><p class="s34" style="padding-top: 7pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">fcst.append(res.forecast(steps=1))</p><p class="s34" style="padding-top: 9pt;padding-left: 114pt;text-indent: 0pt;text-align: left;">except:</p><p class="s34" style="padding-top: 1pt;padding-left: 136pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(&#39;errorred&#39;) fcst.append(-9999999.)</p><p class="s34" style="padding-top: 7pt;padding-left: 114pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train = np.concatenate((X_train, X_test[0:1,:])) X_test = X_test[1:]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 52pt;text-indent: 22pt;line-height: 170%;text-align: left;">errors.append(r2_score(X_test_orig, fcst)) pq_result = [p, q, np.mean(errors)]</p><p class="s34" style="padding-left: 52pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(pq_result) avg_errors.append(pq_result)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">avg_errors = pd.DataFrame(avg_errors) avg_errors.columns = [&#39;p&#39;, &#39;q&#39;, &#39;error&#39;]</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">result = avg_errors.pivot(index=&#39;p&#39;, columns=&#39;q&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The result of the grid search is a table with an estimated error for each combination of p and q in the limits that you have specified (between 0 and 11). Thanks to the cross- validation approach, you can be confident in this error estimate: after all, it is the average of five separate error measurements, and this makes it unlikely that any bias has been caused by a favorable test set selection. You can see the results table in Figure <span style=" color: #00F;">5-8</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="212" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_067.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 5-8. <span class="s29">Table with the R2 scores for each combination of p and q</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">The combination that has yielded the best R2 score is the combination of <span class="s118">p = 10 and q = 9</span>, which yields an estimated R2 score on the test set of <span class="s118">0.84</span>. As a final step, you can refit the model on a train-test split to plot this on a train-test set to see what this error looks like. This is done in Listing <span style=" color: #00F;">5-10 </span>and should give you the graph displayed in Figure <span style=" color: #00F;">5-9</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark88">Listing 5-10. </a><span class="s33">Showing the test prediction of the final model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data_array = data.values</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">X_train, X_test = data_array[:-10], data_array[-10:]</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">X_test_orig = X_test</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">fcst = []</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">for step in range(10):</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = ARIMA(X_train, order=(10,0,9)) res = mod.fit() fcst.append(res.forecast(steps=1))</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train = np.concatenate((X_train, X_test[0:1,:])) X_test = X_test[1:]</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(X_test_orig) plt.plot(fcst)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;Actual Sunspots&#39;, &#39;Predicted Sunspots&#39;]) plt.xlabel(&#39;Time steps of test data&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 108pt;text-indent: 0pt;text-align: left;"><span><img width="331" height="252" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_068.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 5-9. <span class="s29">Prediction on the test set by the final model</span></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4></li><li><p style="padding-top: 10pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">The ARMA model is the combination of the autoregressive model and the Moving Average model.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">The hyperparameters of ARMA are p and q: p for the autoregressive order and q for the moving average order.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">A grid search is a common tool for optimizing the choice of hyperparameters. It can be combined with cross-validation to yield more reliable error estimates.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">You can use the distribution of residuals to evaluate the fit of a model. If the residuals do not follow a normal distribution, there is generally something wrong with the model specification.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">You can use the model summary to look at detailed indicators of model fit. You can also find the estimates of model coefficients and a hypothesis test that tests the model coefficients against zero.</p></li></ul><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark89">CHAPTER 6</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The ARIMA Model</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Having seen several building blocks of univariate time series, in this chapter, you’re going to see a model that combines even more of the time series components: the ARIMA model.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">To keep track of the different building blocks, let’s get back to the table of time series components (Table <span style=" color: #00F;">6-1</span>) to see where the ARIMA model is at.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Table 6-1. <span class="s29">The Building Blocks of Univariate Time Series</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:26pt" cellspacing="0"><tr style="height:24pt"><td style="width:45pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Name</p></td><td style="width:308pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Explanation</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Chapter</p></td></tr><tr style="height:22pt"><td style="width:45pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">AR</p></td><td style="width:308pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Autoregression</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">3</p></td></tr><tr style="height:20pt"><td style="width:45pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">MA</p></td><td style="width:308pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Moving Average</p></td><td style="width:60pt"><p class="s31" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">4</p></td></tr><tr style="height:20pt"><td style="width:45pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARMA</p></td><td style="width:308pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Combination of AR and MA models</p></td><td style="width:60pt"><p class="s31" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">5</p></td></tr><tr style="height:20pt"><td style="width:45pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARIMA</p></td><td style="width:308pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Adding differencing (I) to the ARMA model</p></td><td style="width:60pt"><p class="s31" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">6</p></td></tr><tr style="height:20pt"><td style="width:45pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMA</p></td><td style="width:308pt"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Adding seasonality (S) to the ARIMA model</p></td><td style="width:60pt"><p class="s31" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">7</p></td></tr><tr style="height:38pt"><td style="width:45pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMAX</p></td><td style="width:308pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 7pt;padding-right: 19pt;text-indent: 0pt;line-height: 127%;text-align: left;">Adding external variables (X) to the SARIMA model <i>(note that external variables make the model not univariate anymore)</i></p></td><td style="width:60pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">8</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">As you can see, ARIMA is getting close to the final model. The only parts that are not yet included are S (seasonality) and X (external variables). Some time series processes do not have seasonality nor external variables, which makes ARIMA frequently used as a stand-alone model.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In practice, if you are doing univariate time series, you will generally go directly to ARIMA if you have no expectation of seasonality, or if you expect seasonality, you will go directly to SARIMA (which will be covered in the next chapter).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_6#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_6#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_6</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">105</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;"><a name="bookmark90">ARIMA Model Definition</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 126%;text-align: justify;">The ARIMA model is a model that combines the AR and MA building blocks, just as you have seen with ARMA in the previous chapter. The addition in ARIMA is the “I” building block, which stands for <span class="s19">automatic differencing of non-stationary time series</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: justify;">You should remember that during the past chapters, stationarity has been presented as an important concept in time series. A stationary time series is a time series that</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">has no long-term trend. If a time series is not stationary, you can make it stationary by applying differencing: replacing the actual values by the difference between the actual and the previous value.</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">The “I” in ARIMA is for <span class="s19">integrating</span>. Integrating is a more mathematical synonym</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">for differencing a non-stationary time series. In ARIMA, this differencing is not anymore done in advance of the modeling phase, but it is done during the model fit.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">Model Definition</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">The ARIMA model is short for the Autoregressive, Integrated Moving Average model. The difference from the ARMA model is small: there is just an additional effect that makes the time series non-stationary. A simple example of this would be a linear increasing trend, as shown in the following equation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s66" style="padding-left: 41pt;text-indent: 0pt;line-height: 4pt;text-align: center;">p                            q</p><p class="s66" style="padding-left: 26pt;text-indent: 0pt;line-height: 18pt;text-align: center;"><span class="s62">X</span><span class="s119">t</span>   <span class="s59">= </span><span class="s62">c </span><span class="s59">+ </span><span class="s135">s</span><span class="s119">t</span>  <span class="s59">+ </span><span class="s136">L</span><span class="s135">&lt;</span><span class="s119">i</span> <span class="s62">X</span><span class="s119">t</span> <span class="s72">-</span>i  <span class="s59">+ </span><span class="s136">L</span><span class="s135">e</span><span class="s119">i</span><span class="s135">s</span><span class="s119">t</span> <span class="s72">-</span>i  <span class="s59">+ </span><span class="s135">i5 </span><span class="s62">t</span></p><p class="s66" style="padding-left: 41pt;text-indent: 0pt;text-align: center;">i <span class="s72">=</span><span class="s85">1                           </span>i <span class="s72">=</span><span class="s85">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">When differencing a time series, you actually start to model the differences from one step to another rather than the original values. If the actual values of a variable are not stable over time, it is still possible that the differences are stable over time.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The linear trend is a great example of this. Imagine a linear trend that starts from 0 and increments by 2 every time step. The values will not be stationary at all: they will augment infinitely. Yet the difference between each value and the next is always 2, so the differenced time series is perfectly stationary.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;"><a name="bookmark91">ARIMA on the CO2 Example</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">The fact that the differencing is part of the hyperparameters has a great added value for model building. This makes it possible to do automated hyperparameter tuning on the number of times that differencing should be applied. Let’s see this with an example.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">The data in this example are weekly CO2 data that are available through the statsmodels library. You can obtain the data using the code in Listing <span style=" color: #00F;">6-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;">Listing 6-1. <span class="s33">Importing the data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import statsmodels.api as sm</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = sm.datasets.co2.load_pandas() data = data.data</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">data.head()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">To get an idea of the data that you’re working with, as always, it is good to make a plot of the data over time. Use the code in Listing <span style=" color: #00F;">6-2 </span>to obtain the graph in Figure <span style=" color: #00F;">6-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;">Listing 6-2. <span class="s33">Plotting the data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt ax = data.plot() ax.set_ylabel(&#39;CO2 level&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 111pt;text-indent: 0pt;text-align: left;"><span><img width="324" height="202" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_069.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 6-1. <span class="s29">Plot of the CO2 data over time</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark92">This data shows a very obvious sign of an upward trend, which is fairly constant. And there is also a very clear seasonality pattern in this data (up and down).</a></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now remember, from the previous chapters, the first thing that you should generally look at is autocorrelation and partial autocorrelation functions. Remember that the autocorrelation (ACF) and partial autocorrelation (PACF) plots are relevant only when applied to stationary data. You can use Listing <span style=" color: #00F;">6-3 </span>to create ACF and PACF plots directly on the differenced data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 6-3. <span class="s33">ACF and PACF plots</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from statsmodels.graphics.tsaplots import plot_acf, plot_pacf plot_acf(data.diff().dropna(), lags=40) plot_pacf(data.diff().dropna(), lags=40)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">You should obtain the ACF plot shown in Figure <span style=" color: #00F;">6-2 </span>and the PACF plot in Figure <span style=" color: #00F;">6-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 93pt;text-indent: 0pt;text-align: left;"><span><img width="324" height="221" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_070.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 6-2. <span class="s29">Autocorrelation function plot</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 111pt;text-indent: 0pt;text-align: left;"><span><img width="324" height="226" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_071.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark93">Figure 6-3. </a><span class="s29">Partial autocorrelation function plot</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">It is interesting to note that there are many lagged values in the autocorrelation and partial autocorrelation. So what does this mean? In general, if there is no decay of the correlations toward zero, this means the data is not stationary. Yet the data has been differenced and seems stationary.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The answer here is that the decay occurs relatively late in the autocorrelation. The ACF and PACF plots have 40 lags, but that is not enough for the current example. You can use Listing <span style=" color: #00F;">6-4 </span>to obtain ACF and PACF plots that go further back, and you will observe that there is a decay toward zero. The respective plots are shown in Figures <span style=" color: #00F;">6-4 </span>and <span style=" color: #00F;">6-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 6-4. <span class="s33">ACF and PACF plots with more lags</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plot_acf(data.diff().dropna(), lags=600) plot_pacf(data.diff().dropna(), lags=600) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 93pt;text-indent: 0pt;text-align: left;"><span><img width="324" height="221" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_072.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark94">Figure 6-4. </a><span class="s29">Autocorrelation function plot with 600 lags</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 93pt;text-indent: 0pt;text-align: left;"><span><img width="324" height="226" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_073.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 6-5. <span class="s29">Partial autocorrelation function plot with 600 lags</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This makes it an interesting case for the question of the order of the model. Let’s see how to augment the previous chapter’s grid search cross-validation by adding the I as a third hyperparameter to use in the optimization. The code for this can be seen in Listing <span style=" color: #00F;">6-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 6-5. <span class="s33">Hyperparameter tuning</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from statsmodels.tsa.arima.model import ARIMA from sklearn.metrics import r2_score</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import numpy as np</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.model_selection import TimeSeriesSplit data_array = data[[&#39;co2&#39;]].values</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">avg_errors = []</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">for p in range(6):</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">for q in range(6):</p><p class="s34" style="padding-top: 1pt;padding-left: 92pt;text-indent: -22pt;line-height: 113%;text-align: left;">for i in range(3): errors = []</p><p class="s34" style="padding-top: 7pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">tscv = TimeSeriesSplit(test_size=10)</p><p class="s34" style="padding-top: 9pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">for train_index, test_index in tscv.split(data_array):</p><p class="s34" style="padding-top: 9pt;padding-left: 114pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test = data_array[train_index], data_array[test_index]</p><p class="s34" style="padding-left: 114pt;text-indent: 0pt;line-height: 14pt;text-align: left;">X_test_orig = X_test</p><p class="s34" style="padding-top: 9pt;padding-left: 114pt;text-indent: 0pt;text-align: left;">fcst = []</p><p class="s34" style="padding-top: 1pt;padding-left: 114pt;text-indent: 0pt;text-align: left;">for step in range(10):</p><p class="s34" style="padding-top: 9pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">try:</p><p class="s34" style="padding-top: 1pt;padding-left: 158pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = ARIMA(X_train, order=(p,i,q)) res = mod.fit()</p><p class="s34" style="padding-top: 7pt;padding-left: 158pt;text-indent: 0pt;text-align: left;">fcst.append(res.forecast(steps=1))</p><p class="s34" style="padding-top: 9pt;padding-left: 136pt;text-indent: 0pt;text-align: left;">except:</p><p class="s34" style="padding-top: 1pt;padding-left: 158pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(&#39;errorred&#39;) fcst.append(-9999999.)</p><p class="s34" style="padding-top: 7pt;padding-left: 136pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train = np.concatenate((X_train, X_test[0:1,:])) X_test = X_test[1:]</p><p class="s34" style="padding-top: 7pt;padding-left: 92pt;text-indent: 22pt;line-height: 170%;text-align: left;">errors.append(r2_score(X_test_orig, fcst)) pq_result = [p, i, q, np.mean(errors)]</p><p class="s34" style="padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(pq_result) avg_errors.append(pq_result)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">avg_errors = pd.DataFrame(avg_errors) avg_errors.columns = [&#39;p&#39;, &#39;i&#39;, &#39;q&#39;, &#39;error&#39;] avg_errors.sort_values(&#39;error&#39;, ascending=False)</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">The result of this code will give you a dataframe ordered by R2 scores. The best R2 score is <span class="s19">0.741</span>, and it is obtained for the combination of <span class="s19">p = 4, q = 4, </span>and <span class="s19">I = 1</span>. This means that there is an autoregressive effect of order 4 and a moving average effect of order 4. The data must be differenced one time. Listing <span style=" color: #00F;">6-6 </span>shows you how to show the test fit of the model, and the graph is shown in Figure <span style=" color: #00F;">6-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 6-6. <span class="s33">Plot the final result</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">X_train, X_test = data_array[:-10], data_array[-10:]</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">X_test_orig = X_test</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">fcst = []</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">for step in range(10):</p><p class="s34" style="padding-top: 9pt;padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = ARIMA(X_train, order=(4,1,4)) res = mod.fit() fcst.append(res.forecast(steps=1))</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train = np.concatenate((X_train, X_test[0:1,:])) X_test = X_test[1:]</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(fcst) plt.plot(X_test_orig) plt.legend([&#39;Predicted&#39;, &#39;Actual&#39;]) plt.ylabel(&#39;CO2 Level&#39;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.xlabel(&#39;Time Step of Test Data&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 106pt;text-indent: 0pt;text-align: left;"><span><img width="338" height="244" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_074.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark95">Figure 6-6. </a><span class="s29">Result of the best forecast</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4><ul id="l23"><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The ARIMA model combines three effects:</p><ul id="l24"><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;line-height: 129%;text-align: left;">The AR process, based on autocorrelations between past and present values</p></li><li><p style="padding-top: 6pt;padding-left: 90pt;text-indent: -17pt;line-height: 129%;text-align: left;">The MA process, based on correlations between past errors and present values</p></li><li><p style="padding-top: 6pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">Automatic integration of a time series if it is not stationary</p></li></ul></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The ARIMA(p,I,q) model has three hyperparameters:</p><ul id="l25"><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">The order of the AR process denoted by p</p></li><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">The order of the MA process denoted by q</p></li><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">The order of integration denoted by I (or d in some notations)</p></li></ul></li></ul><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark96">CHAPTER 7</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The SARIMA Model</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this chapter, you are going to discover the final model of univariate time series models: the SARIMA model. This will be the chapter in which everything on univariate time series comes together.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Univariate Time Series Model Breakdown</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 126%;text-align: left;">With the ARIMA components covered in previous chapters, you can now model autoregressive processes, moving average components, and integration (differencing). A common process of time series is still missing from this: <span class="s19">seasonality</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s have a look at the univariate time series breakdown table to see where this fits in (Table <span style=" color: #00F;">7-1</span>). You can see that there is only one model more complicated than the SARIMA model: the SARIMAX model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:26pt" cellspacing="0"><tr style="height:19pt"><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s40" style="text-indent: 0pt;line-height: 14pt;text-align: left;">Table 7-1.</p></td><td style="width:300pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s41" style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;">The Building Blocks of Univariate Time Series</p></td><td style="width:56pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:24pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Name</p></td><td style="width:300pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Explanation</p></td><td style="width:56pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Chapter</p></td></tr><tr style="height:22pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">AR</p></td><td style="width:300pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Autoregression</p></td><td style="width:56pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">3</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">MA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Moving Average</p></td><td style="width:56pt"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">4</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARMA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Combination of AR and MA models</p></td><td style="width:56pt"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">5</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARIMA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Adding differencing (I) to the ARMA model</p></td><td style="width:56pt"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">6</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Adding seasonality (S) to the ARIMA model</p></td><td style="width:56pt"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">7</p></td></tr><tr style="height:38pt"><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMAX</p></td><td style="width:300pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;padding-right: 14pt;text-indent: 0pt;line-height: 127%;text-align: left;">Adding external variables (X) to the SARIMA model <i>(note that external variables make the model not univariate anymore)</i></p></td><td style="width:56pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">8</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_7#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_7#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_7</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">115</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark97">The SARIMAX model is quite different from SARIMA: it uses external variables that correlate with the target variable. This makes SARIMAX a very powerful tool as well, but it can be applied only if you have external variables. Using external variables makes the model not a univariate time series model.</a></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The power of the SARIMA model is that it needs to use only the history of the target variable, which can be applied to very many cases of forecasting. Within univariate time series modeling, the SARIMA model is the most complete model, using AR, MA, integration for modeling trends, and seasonality.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The SARIMA Model Definition</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">The SARIMA model combines many different processes in one and the same model. Therefore, the model definition is going to be relatively complex mathematically. The following equation is the definition of the SARIMA model:</p><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">p</p><p style="text-indent: 0pt;text-align: left;"/><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">s t q t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s58" style="padding-top: 7pt;padding-left: 24pt;text-indent: 0pt;line-height: 13pt;text-align: center;"><span class="s42">y</span>t   <span class="s137">= </span><span class="s42">u</span>t <span class="s137">+</span><span class="s138">η</span>t</p><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">p</p><p style="text-indent: 0pt;text-align: left;"/><p class="s139" style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: right;"><span class="s138">ϕ </span>(<span class="s42">L</span>)<span class="s138">φ</span><span class="s140"></span></p><p class="s142" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;"><span class="s141">(</span><span class="s42">L</span>s<span class="s52">  </span><span class="s141">)</span><span class="s138">∆</span>d<span class="s52"> </span><span class="s138">∆</span>D<span class="s42">u</span></p><p class="s42" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;"><span class="s137">= </span>A<span class="s139">(</span>t <span class="s139">) </span><span class="s137">+</span><span class="s138">θ</span></p><ol id="l26"><li><p class="s52" style="padding-top: 1pt;padding-left: 19pt;text-indent: -15pt;text-align: left;"><span class="s143">θ</span><span class="s140"></span>Q <span class="s144">(</span><span class="s145">L</span><span class="s146">s</span>  <span class="s144">)</span><span class="s143">ζ</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">In this model, you can observe the following parts:</p><ul id="l27"><li><p style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The regular AR part, with the coefficients <span class="s80">φ</span><span class="s77">p</span><span class="s79"> </span>(small phis)</p><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">P</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The seasonal AR part, with the coefficients <span class="s138">φ</span><span class="s109"></span></p><p style="padding-top: 9pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">(capital phis)</p></li><li><p style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The regular MA part, with the coefficients <span class="s80">θ</span><span class="s77">q</span><span class="s79"> </span>(small thetas)</p><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">Q</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The seasonal MA part, with the coefficients <span class="s138">θ</span><span class="s109"></span></p><p style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">(capital thetas)</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The regular integration part, indicated by order d</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The seasonal integration part, indicated by order D</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Coefficient of seasonality s</p><p style="padding-top: 9pt;padding-left: 7pt;text-indent: 17pt;line-height: 129%;text-align: left;">As you can see, adding seasonality to the ARIMA model makes the model much more complex. The seasonality is not just one building block added to the previous model. There is a seasonal part added for each of the building blocks. This makes the lag</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;"><a name="bookmark98">selection of the SARIMA model also much more difficult. The same hyperparameters as before still need to be decided on:</a></p><ul id="l28"><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">p for the AR order</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">q for the MA order</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">I for the differencing order</p><p style="padding-top: 9pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">To this are added three more orders:</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">P for the seasonal AR order</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Q for the seasonal MA order</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">D for the seasonal differencing</p><p style="padding-top: 8pt;padding-left: 25pt;text-indent: 17pt;line-height: 128%;text-align: left;">And lastly, the choice for parameter <span class="s19">s, the seasonal period</span>, must be made. This s is not a hyperparameter like the other orders. s should not be tested using a grid search; rather, it should be decided based on logic. The s is the periodicity of the seasonality. If you work with</p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">monthly data, you should choose 12; if you work with weekly data, you should choose 52; etc.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Example: SARIMA on Walmart Sales</h4><p class="s68" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a href="http://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">In this chapter, you will apply the SARIMA model to a Walmart sales forecasting dataset that is available on Kaggle (</a><a href="http://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/" class="s32" target="_blank">www.kaggle.com/c/walmart-recruiting-store-sales- </a>forecasting/<span class="p">).</span></p><p style="padding-top: 1pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">Let’s get this data in Python and start to see what it looks like using Listing <span style=" color: #00F;">7-1</span>. Note that the Walmart data contains many stores and products, but for this exercise, you will take the sum of the weekly data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 7-1. <span class="s33">Importing the data and creating a plot</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt data = pd.read_csv(&#39;train.csv&#39;) data = data.groupby(&#39;Date&#39;).sum() ax = data[&#39;Weekly_Sales&#39;].plot() ax.set_ylabel(&#39;Weekly sales&#39;) plt.gcf().autofmt_xdate() plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">This will give you the plot shown in Figure <span style=" color: #00F;">7-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 70pt;text-indent: 0pt;text-align: left;"><span><img width="387" height="285" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_075.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 7-1. <span class="s29">Walmart sales over time</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s first try a first model: SARIMA(1,1,1)(1,1,1)52. This notation indicates an order 1 for each of the hyperparameters. The seasonality is 52, because you’re working with weekly data. You will be forecasting ten steps out rather than doing ten one-step forecasts. This is a case of multistep forecasting. Listing <span style=" color: #00F;">7-2 </span>shows how to fit the model</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">and obtain a plot of the performances on the test set (Figure <span style=" color: #00F;">7-2</span>). You will also obtain the R2 score on the test data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 7-2. <span class="s33">Fitting a SARIMA(1,1,1)(1,1,1)52 model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import random random.seed(12345)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">import statsmodels.api as sm</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.metrics import r2_score</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">train = data[&#39;Weekly_Sales&#39;][:-10] test = data[&#39;Weekly_Sales&#39;][-10:]</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = sm.tsa.statespace.SARIMAX(data[&#39;Weekly_Sales&#39;][:-10], order=(1,1,1), seasonal_order=(1,1,1,52))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark99">res = mod.fit(disp=False) fcst = res.forecast(steps=10)</a></p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(list(test)) plt.plot(list(fcst)) plt.legend([&#39;Actual data&#39;, &#39;Forecast&#39;]) plt.ylabel(&#39;Sales&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.xlabel(&#39;Test Data Time Step&#39;) plt.show()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">r2_score(test, fcst)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span><img width="390" height="297" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_076.jpg"/></span></p><p class="s28" style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 7-2. <span class="s29">Predictive performance of the SARIMA model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 125%;text-align: left;">In this version, without any hyperparameter optimization, the SARIMA model obtains an R2 score of <span class="s19">0.73</span>. This means that the model can explain 73% of the variation in the test data. This is a very promising score.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The reason that this score is already quite good is probably the flexibility of the SARIMA model. The fact that it is a combination of multiple effects makes it fit to many different underlying processes. You will now see how this score can improve using a grid search of the hyperparameters.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">To speed up the code, you will not use cross-validation in the grid search here. Speed of execution is also an important aspect of machine learning. As a second optimization,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark100">you will be working with a ten-step forecast rather than with ten updated steps of a</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">one-step forecast. In this way, it will be interesting to see how the seasonal effects may be more efficient in capturing long-term trends. This will be done in Listing <span style=" color: #00F;">7-3</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Note that a try-except block has been added to avoid the code stopping in case an error occurs while fitting the model. Depending on your hardware, some of the fits may be too heavy and cause out-of-memory errors. This is not a problem, as it will still allow you to find the best possible combination of hyperparameters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 7-3. <span class="s33">Grid Search on the SARIMA model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">scores = []</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">for p in range(2):</p><p class="s34" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">for i in range(2):</p><p class="s34" style="padding-top: 1pt;padding-left: 52pt;text-indent: 0pt;text-align: left;">for q in range(2):</p><p class="s34" style="padding-top: 1pt;padding-left: 74pt;text-indent: 0pt;text-align: left;">for P in range(2):</p><p class="s34" style="padding-top: 1pt;padding-left: 96pt;text-indent: 0pt;text-align: left;">for D in range(2):</p><p class="s34" style="padding-top: 1pt;padding-left: 118pt;text-indent: 0pt;text-align: left;">for Q in range(2):</p><p class="s34" style="padding-top: 9pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">try:</p><p class="s34" style="padding-top: 1pt;padding-left: 195pt;text-indent: -33pt;line-height: 113%;text-align: left;">mod = sm.tsa.statespace.SARIMAX(train, order=(p,0,q), seasonal_order=(P,D,Q,52))</p><p class="s34" style="padding-left: 162pt;text-indent: 0pt;line-height: 14pt;text-align: left;">res = mod.fit(disp=False)</p><p class="s34" style="padding-top: 9pt;padding-left: 206pt;text-indent: -44pt;line-height: 113%;text-align: left;">score = [p,i,q,P,D,Q,r2_score(test, res. forecast(steps=10))]</p><p class="s34" style="padding-left: 162pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(score) scores.append(score)</p><p class="s34" style="padding-top: 7pt;padding-left: 162pt;text-indent: 0pt;line-height: 113%;text-align: left;">del mod del res</p><p class="s34" style="padding-top: 7pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">except:</p><p class="s34" style="padding-top: 1pt;padding-left: 162pt;text-indent: 0pt;text-align: left;">print(&#39;errored&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">res = pd.DataFrame(scores)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">res.columns = [&#39;p&#39;, &#39;i&#39;, &#39;q&#39;, &#39;P&#39;, &#39;D&#39;, &#39;Q&#39;, &#39;score&#39;] res.sort_values(&#39;score&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark101">You see that the best order here is (0,1,1)(1,1,1). The zero means that there is no regular AR process. There is a seasonal AR process. A regular and a seasonal MA process are present. Regular and seasonal integration is needed.</a></p><p style="padding-left: 44pt;text-indent: 0pt;line-height: 13pt;text-align: left;">This model gives you an R2 on the test data of <span class="s19">0.734</span>, just slightly better than the</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">nonoptimized model. As the last step, you can check out the plot of this forecast on the test data. This can be done by updating the order in Listing <span style=" color: #00F;">7-2</span>, and this will allow you to obtain the plot in Figure <span style=" color: #00F;">7-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span><img width="390" height="297" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_077.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 7-3. <span class="s29">Predictive performance of the optimized SARIMA model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 124%;text-align: left;">The grid search that was presented here is a good approach to tuning a SARIMA model. At this point, I want to point to a function called <span class="s19">auto_arima </span>from the <span class="s19">pyramid </span><a href="https://pypi.org/project/pyramid-arima/" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">library that automatically tunes ARIMA and SARIMA models. Using this function, you have less code to write for the same optimization. You can check out how to use auto_ arima over here: </a><span class="s68">https://pypi.org/project/pyramid-arima/</span>.</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark102">Key Takeaways</a></h4></li></ul></li><li><p style="padding-top: 10pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The SARIMA model adds a seasonal effect to the ARIMA model.</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">The SARIMA model is the completest version of univariate time series models.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">There are four more hyperparameters in the SARIMA model:</p><ul id="l29"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Seasonal AR order</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Seasonal MA order</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Seasonal integration order</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Periodicity (based on the number of periods that a seasonality would logically return)</p></li></ul></li></ul></li></ol><p class="s21" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark103">PART III</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Multivariate Time Series Models</h1><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark104">CHAPTER 8</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The SARIMAX Model</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this chapter, you will discover the SARIMAX model. This model is the most complete version of classical time series models, as it contains all of the components that you’ve discovered throughout the previous chapters of this book. It adds the X component: external variables.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Time Series Building Blocks</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Let’s have a quick look back at the different components of time series that you have seen throughout the previous chapters using Table <span style=" color: #00F;">8-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:26pt" cellspacing="0"><tr style="height:19pt"><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s40" style="text-indent: 0pt;line-height: 14pt;text-align: left;">Table 8-1.</p></td><td style="width:300pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s41" style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;">The Building Blocks of Univariate Time Series</p></td><td style="width:59pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:24pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Name</p></td><td style="width:300pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Explanation</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Chapter</p></td></tr><tr style="height:22pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">AR</p></td><td style="width:300pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Autoregression</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">3</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">MA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Moving Average</p></td><td style="width:59pt"><p class="s31" style="padding-top: 3pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">4</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARMA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Combination of AR and MA models</p></td><td style="width:59pt"><p class="s31" style="padding-top: 3pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">5</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">ARIMA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Adding differencing (I) to the ARMA model</p></td><td style="width:59pt"><p class="s31" style="padding-top: 3pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">6</p></td></tr><tr style="height:20pt"><td style="width:55pt"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMA</p></td><td style="width:300pt"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">Adding seasonality (S) to the ARIMA model</p></td><td style="width:59pt"><p class="s31" style="padding-top: 3pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">7</p></td></tr><tr style="height:38pt"><td style="width:55pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">SARIMAX</p></td><td style="width:300pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 3pt;padding-right: 16pt;text-indent: 0pt;line-height: 127%;text-align: left;">Adding external variables (X) to the SARIMA model <i>(note that external variables make the model not univariate anymore)</i></p></td><td style="width:59pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">8</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The chapters build up time series models from easy to complex. The SARIMAX model is not considered a perfect example of a univariate time series model. Univariate time series models only use variation in the target variable, while the SARIMAX model uses external variables as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_8#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_8#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_8</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">125</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark105">Model Definition</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The mathematical definition of the SARIMAX model is as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">p</p><p style="text-indent: 0pt;text-align: left;"/><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">s t q t</p><p style="text-indent: 0pt;text-align: left;"/><p class="s58" style="padding-left: 24pt;text-indent: 0pt;line-height: 13pt;text-align: center;"><span class="s42">y</span>t   <span class="s137">= </span><span class="s138">β</span>t <span class="s42">x</span>t  <span class="s137">+ </span><span class="s42">u</span>t</p><p class="s52" style="text-indent: 0pt;line-height: 7pt;text-align: left;">p</p><p style="text-indent: 0pt;text-align: left;"/><p class="s139" style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: right;"><span class="s138">ϕ </span>(<span class="s42">L</span>)<span class="s138">φ</span><span class="s140"></span></p><p class="s142" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;"><span class="s141">(</span><span class="s42">L</span>s<span class="s52">  </span><span class="s141">)</span><span class="s138">∆</span>d<span class="s52"> </span><span class="s138">∆</span>D<span class="s42">u</span></p><p class="s42" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;text-align: left;"><span class="s137">= </span>A<span class="s139">(</span>t <span class="s139">) </span><span class="s137">+</span><span class="s138">θ</span></p><ol id="l30"><li><p class="s52" style="padding-top: 1pt;padding-left: 19pt;text-indent: -15pt;text-align: left;"><span class="s143">θ</span><span class="s140"></span>Q <span class="s144">(</span><span class="s145">L</span><span class="s146">s</span>  <span class="s144">)</span><span class="s143">ζ</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">The <span class="s80">β </span>(beta) part in the first formula represents the external variable. For the rest, the model is very similar to the SARIMA model, but for completeness, let’s relist the hyperparameters of this model:</p><ul id="l31"><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">p for the AR order</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">q for the MA order</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">I for the differencing order</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">P for the seasonal AR order</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Q for the seasonal MA order</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">D for the seasonal differencing</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">s for the seasonal coefficients</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Supervised Models vs. SARIMAX</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">In further chapters, you’ll see many cases of supervised models. As you may recall from the first chapter, supervised models use external variables to predict a target variable. This idea is very similar to the X part in SARIMAX.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The question, of course, is which one of them you should use in practice. From a theoretical point of view, SARIMAX may be preferred in cases where the time series part is more present than the external variables part. If the external variables alone can explain a lot and this is complemented by a part of autocorrelation or seasonality, supervised models may be the better choice.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Yet, as always, the reasonable thing to do is to use multiple models in a model benchmark. The choice for a model should then simply be based on the predictive performance of the model.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark106">Example of SARIMAX on the Walmart Dataset</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In the previous chapter, you’ve seen the SARIMA model used to predict weekly sales at Walmart. Yet the dataset does not just contain weekly sales data: there is also an indicator that tells you whether a week did or did not have a holiday in it.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You did not use the holiday information in the SARIMA model, as it is impossible to add external data to it. And this did not really matter as there were no extreme peaks</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">due to holidays in the test data. Even though the holiday information was missing in the model, it was not represented in the error estimate.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Yet it is important to add such information, as it may be able to explain a large part of the variation in the model. Let’s see how to improve on the Walmart example using external variables.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">As a first step, you should import and prepare the data. You can use Listing <span style=" color: #00F;">8-1 </span>for this. This code also shows you how to create a plot of sales over time.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 8-1. <span class="s33">Preparing the data and making a plot</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import matplotlib.pyplot as plt</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = pd.read_csv(&#39;walmart/train.csv&#39;) data = data.groupby(&#39;Date&#39;).sum() data[&#39;IsHoliday&#39;] = data[&#39;IsHoliday&#39;] &gt; 0</p><p class="s34" style="padding-left: 48pt;text-indent: -22pt;line-height: 113%;text-align: left;">data[&#39;IsHoliday&#39;] = data[&#39;IsHoliday&#39;].apply( lambda x: float(x)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax = data[&#39;Weekly_Sales&#39;].plot() ax.set_ylabel(&#39;Weekly Sales&#39;) plt.gcf().autofmt_xdate() plt.show()</p><p style="padding-top: 9pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">You should obtain the graph displayed in Figure <span style=" color: #00F;">8-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 69pt;text-indent: 0pt;text-align: left;"><span><img width="387" height="285" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_078.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark107">Figure 8-1. </a><span class="s29">Plot of the weekly sales</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The next step is to use correlation analysis to study whether we may expect an improvement from adding the holiday information into the model. If there is no correlation at all between sales and holidays, it would be unwise to add it to the model. Let’s use Listing <span style=" color: #00F;">8-2 </span>to compute the correlation coefficient.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 3pt;text-indent: 0pt;text-align: center;">Listing 8-2. <span class="s33">Is there a correlation between sales and holidays?</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">data[[&#39;Weekly_Sales&#39;, &#39;IsHoliday&#39;]].corr()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 9pt;text-indent: 0pt;text-align: center;">This will output a correlation matrix as shown in Figure <span style=" color: #00F;">8-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 133pt;text-indent: 0pt;text-align: left;"><span><img width="217" height="56" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_079.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 8-2. <span class="s29">The correlation matrix</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: justify;">The correlation coefficient between sales and holidays is <span class="s19">0.17</span>. This is not very high, but enough to consider adding the variable to the model. The model itself will compute the most appropriate coefficient for the variable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 122%;text-align: justify;"><a name="bookmark108">To create a SARIMAX model, you can use Listing </a><span style=" color: #00F;">8-3</span>. It is important to note the terminology of <span class="s19">endog </span>and <span class="s19">exog</span>:</p><ul id="l32"><li><p class="s19" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;line-height: 128%;text-align: justify;">Endogenous variables <span class="p">(endog) are the target variable. This is where all the time series components will be estimated from. In the current case, it is the weekly sales.</span></p></li><li><p class="s19" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;line-height: 128%;text-align: justify;">Exogenous variables <span class="p">(exog) are explanatory variables. This is where the model takes additional correlation from. In the current example, this is the holiday variable.</span></p><p style="padding-top: 6pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">For this particular example, I will not do the hyperparameter search again. If you remember from the previous chapter, the optimal score was found using SARIMA(0,1,1) (1,1,1),52. So for this example, let’s use SARIMAX(0,1,1)(1,1,1)52. The code also computes an R2 score and will show the plot of performance on a test set of 10 weeks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 8-3. <span class="s33">Fitting a SARIMAX model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">import random random.seed(12345)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">import statsmodels.api as sm</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from sklearn.metrics import r2_score</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">train = data[&#39;Weekly_Sales&#39;][:-10] test = data[&#39;Weekly_Sales&#39;][-10:]</p><p class="s34" style="padding-top: 7pt;padding-left: 48pt;text-indent: -22pt;line-height: 113%;text-align: left;">mod = sm.tsa.statespace.SARIMAX( endog=data[&#39;Weekly_Sales&#39;][:-10], exog=data[&#39;IsHoliday&#39;][:-10], order=(0,1,1), seasonal_order=(1,1,1,52),</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">res = mod.fit(disp=False)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">fcst = res.forecast(steps=10, exog = data[&#39;IsHoliday&#39;][-10:])</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(list(test)) plt.plot(list(fcst)) plt.xlabel(&#39;Steps of the test data&#39;) plt.ylabel(&#39;Weekly Sales&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark109">plt.legend([&#39;test&#39;, &#39;forecast&#39;]) plt.show()</a></p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">r2_score(test, fcst)</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">The R2 score obtained by this forecast is <span class="s19">0.734</span>. The performance can be seen visually in Figure <span style=" color: #00F;">8-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 69pt;text-indent: 0pt;text-align: left;"><span><img width="388" height="292" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_080.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 8-3. <span class="s29">Predictive performance on the test data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This chapter has shown you how to use the SARIMAX model using one additional variable. Yet remember that there is no limit for the number of exogenous variables included in a SARIMAX model. As long as you can provide future values for this variable, this may work.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Pay attention here: many variables that can improve model performances are not useful in practice. For example, you may be able to explain sales by using weather data. Yet weather data for the future is not yet known. So while this variable may have an important impact on sales, it cannot be used for improving the forecasting accuracy.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4></li><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The SARIMAX model allows for adding external variables to the SARIMA model.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -72pt;text-align: right;">SARIMAX has two types of variables:</p><ul id="l33"><li><p class="s20" style="padding-top: 8pt;padding-left: 90pt;text-indent: -90pt;text-align: right;">Endogenous<span class="p">: The target variable</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 90pt;text-indent: -90pt;text-align: right;">Exogenous<span class="p">: The external variables</span></p></li></ul></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">You need to specify the future values of the exogenous variable when forecasting. Therefore, when using exogenous variables, it is important to know that you have fixed information for the future about them.</p><ul id="l34"><li><p style="padding-top: 6pt;padding-left: 90pt;text-indent: -17pt;line-height: 129%;text-align: left;">A variable like holidays can work, as you know which holidays will occur in the future.</p></li><li><p style="padding-top: 6pt;padding-left: 90pt;text-indent: -17pt;line-height: 129%;text-align: left;">A variable like weather cannot work, as you will not know which weather will occur in the future.</p><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark110">CHAPTER 9</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The VAR Model</h1><p style="padding-top: 21pt;padding-left: 26pt;text-indent: 0pt;line-height: 124%;text-align: left;">In this chapter, you will discover the VAR model, short for the <span class="s19">Vector Autoregression model</span>. Vector Autoregression models the development over time of multiple variables at the same time.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The term autoregression should sound familiar from the previous chapters. The Vector Autoregression model is part of the family of time series models. Like other models from this category, it predicts the future based on developments in the past of the target variables.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">There is an important specificity to Vector Autoregression. Unlike most other models, Vector Autoregression models multiple target variables at the same time. The multiple variables are used at the same time as target variables and as explanatory variables for each other. A model that uses multiple target variables in one model is called a <span class="s19">multivariate model </span>or, more specifically, <span class="s19">multivariate time series</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">The Model Definition</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Since the VAR model proposes one model for multiple target variables, it regroups those variables as a vector. This explains the name of the model. The model definition is as follows:</p><p class="s84" style="padding-top: 8pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">y<span class="s147">t  </span><span class="s53">= </span>c <span class="s53">+ </span>A<span class="s121">1 </span>y<span class="s147">t </span><span class="s148">−</span><span class="s76">1 </span><span class="s53">+ </span>A<span class="s121">2 </span>y<span class="s147">t </span><span class="s148">−</span><span class="s76">2 </span><span class="s53">+</span><span class="s110"></span><span class="s53">+ </span>A<span class="s147">p </span>y<span class="s147">t </span><span class="s148">−</span><span class="s149">p </span><span class="s53">+ </span>e<span class="s147">t</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">In this formula</p></li></ul></li><li><p class="s5" style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">y<span class="s150">t</span><span class="s151"> </span><span class="p">is a vector that contains the present values of each of the variables.</span></p></li><li><p style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;line-height: 127%;text-align: left;">The order of the VAR model, <span class="s19">p</span>, determines the number of time steps back that are used for predicting the future.</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">c <span class="p">is a vector of constants: one for each target variable.</span></p></li></ul></li></ul></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 9pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s152">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_9#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_9#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_9</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">133</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l35"><li><p style="padding-top: 4pt;padding-left: 54pt;text-indent: -17pt;text-align: left;"><a name="bookmark111">There is a vector of coefficients </a><span class="s5">A </span>for each lag.</p></li><li><p style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The error is denoted as <span class="s5">e</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Order: Only One Hyperparameter</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">So how does the number of lags work in the VAR model? In the previous models, you have seen relatively complex situations with hyperparameters to be estimated for the order of different types of processes.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: justify;">The VAR model is much simpler in this regard. There is only one order to be defined: the order of the model as a whole. The notation for this order is <span class="s19">VAR(p)</span>, in which p is the order.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The order defines how many time steps back in time are taken into account for explaining the present. An order 1 means only the previous time step is taken into account. An order 2 means that the two previous time steps are used. And so on. You can also call this the number of lags that are included.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">If a lag is included, it is always included for all the variables. It is not possible with the VAR model to use different lags for different variables.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Stationarity</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Another concept that you have seen before is stationarity, so let’s find out how that works in the VAR model.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">If you remember, stationarity means that a time series has no trend: it is stable over the long term. You can use the Augmented Dickey Fuller test to test whether a time series is stationary or not.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You have also seen the technique called differencing, or integration, to make a non- stationary time series stationary.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">This theory is also very important for the VAR model. A VAR model can work only if <span class="s153">each of the variables in the model is stationary</span>. When doing VAR models, you generally work on multiple variables at the same time, so it can be a bit cumbersome to handle if some of the time series are stationary and others not. In that case, differencing should be applied only to the non-stationary time series.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">There is one exception to this rule: the Vector Error Correction model is a type of VAR that does not require stationarity. This model can be a good alternative to the VAR</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s68" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a href="http://www.statsmodels.org/stable/generated/statsmodels.tsa.vector_ar.vecm.VECM.html" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank" name="bookmark112">model, so I advise you to have a look at it over here: </a><a href="http://www.statsmodels.org/stable/generated/statsmodels.tsa.vector_ar.vecm.VECM.html" class="s32" target="_blank">www.statsmodels.org/stable/ </a>generated/statsmodels.tsa.vector_ar.vecm.VECM.html<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Estimation of the VAR Coefficients</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">The coefficients of the VAR model can be estimated using a technique called Multivariate Least Squares. The fact that the VAR model uses this technique makes it computationally relatively efficient. To understand this estimation technique, let’s pose the VAR model in matrix notation:</p><p class="s62" style="padding-top: 7pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">Y <span class="s154">= </span>BZ <span class="s154">+</span>U</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">In this case</p><ul id="l36"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Y is a matrix that contains the y values for each variable (each row) and each lag (each column) for future lags.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">B is the coefficient matrix.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Z is a matrix with the past lags of the y variables.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">U is a matrix with the model errors.</p><p style="padding-top: 9pt;padding-left: 25pt;text-indent: 17pt;line-height: 129%;text-align: left;">The Multivariate Least Squares allows you to compute the coefficients using the following matrix computation:</p><p class="s62" style="padding-top: 8pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">B<span class="s155">ˆ</span><span class="s44"> </span><span class="s59">= </span>YZ<span class="s95">&#39;</span><span class="s67">( </span>ZZ<span class="s95">&#39;</span><span class="s67">)</span><span class="s156">-</span><span class="s85">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The coefficients that are estimated this way are what’s underlying the model that you’ll see later on.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">One Multivariate Model vs. Multiple Univariate Models</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Vector Autoregression should be applied to multiple target variables that are correlated. If there is no or very little correlation between the variables, they cannot benefit from being combined in one and the same model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark113">Besides using a VAR model only in cases where it makes sense to combine variables in one and the same model, it is even more important to use objective model evaluation techniques using a train-test set and cross-validation. This can help you make the right choice between using one model for multiple variables and using a separate univariate model for each variable.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">An Example: VAR for Forecasting Walmart Sales</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this example, you will continue to work on the Walmart sales data that have been presented in the previous chapter. Yet in the current example, rather than summing the sales per week, you will sum the weekly data per store. As there are 45 stores in the dataset, this yields 45 weekly time series. You can create this data with a plot using Listing <span style=" color: #00F;">9-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 9-1. <span class="s33">Preparing the Walmart data per store</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import matplotlib.pyplot as plt</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">data = pd.read_csv(&#39;walmart/train.csv&#39;)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = data.pivot_table(index = &#39;Date&#39;, columns = &#39;Store&#39;, values = &#39;Weekly_Sales&#39;)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax = data.plot(figsize=(20,15)) ax.legend([]) ax.set_ylabel(&#39;Sales&#39;) plt.show()</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This code will give you the plot that is shown in Figure <span style=" color: #00F;">9-1</span>. As you can see in this plot, the data per store follows the same pattern. For example, you can see that they almost all peak at the same moment. This shows the interest in using multivariate time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 65pt;text-indent: 0pt;text-align: left;"><span><img width="446" height="321" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_081.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark114">Figure 9-1. </a><span class="s157">The plot of the 45 stores</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now let’s get into the fit of a VAR model. As before, let’s use a ten-step forecast on a ten-step test dataset. The code to create a VAR model can be seen in Listing <span style=" color: #00F;">9-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 9-2. <span class="s33">Fitting the VAR model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import mean_absolute_percentage_error from statsmodels.tsa.api import VAR</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">train = data.iloc[:-10,:]</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">test = data.iloc[-10:,:]</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">model = VAR(train)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">results = model.fit(maxlags=2)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">lag_order = results.k_ar</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">fcst = results.forecast(train.values[-lag_order:], 10)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">model_accuracy = 1 - mean_absolute_percentage_error(test, fcst) print(model_accuracy)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark115">There are a few interesting things to notice in this code, which are different from what you have seen before.</a></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Firstly, note that the data on which the model is fit is a multivariate dataset: it uses all the columns at once. This is as could be expected from a multivariate time series model.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">Secondly, note that rather than doing a grid search, the example shows the use of an argument called <span class="s19">maxlags</span>. This means that the VAR model will optimize the choice for the order of the model itself, all while respecting a maximum lag. In this case, the maxlags has been chosen at two, as going above two would require more coefficients to be estimated than possible using the current data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">The maxlags chooses the ideal order of the model using the <span class="s19">AIC</span>, the <span class="s19">Akaike Information Criterion</span>. The Akaike Information Criterion is a famous KPI for goodness of fit of a model. The AIC is based only on the training data. It is therefore more prone to inducing overfitted models.</p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">AIC is more used in classical statistical models and less in modern machine learning.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 126%;text-align: left;">Yet it is a fast and practical way of choosing order and therefore important to now. The negative aspect of VAR is that it requires very many parameters to be estimated. This makes it <span class="s19">require enormous amounts of data </span>to fit models with higher orders. In the current example, it is impossible to use an order higher than two.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Also, you should notice in the code that when applying the forecast method, it is necessary to give the last part of training data. This is needed so that the model can compute the future by applying coefficients to the lagged variables.</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The model accuracy obtained with the model is <span class="s19">0.89</span>.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Despite the low possibility of model tuning with this model, it must be said that the performances are relatively good. But as always, remember that in practice, the only way to know whether this is good enough is by doing benchmarking and model comparison with other models.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4></li><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The VAR model uses multivariate correlation to make one model for multiple target variables.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The order of the VAR model, p, determines the number of time steps back that are used for predicting the future.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The VAR model implementation can define the ideal number of lags using the maxlags parameter and the Akaike Information Criterion.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The VAR model needs to estimate a large number of parameters, which makes it require a huge amount of historical data. This makes it difficult to estimate higher lags.</p><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark116">CHAPTER 10</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The VARMAX Model</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 127%;text-align: left;">You have discovered the VAR model in the previous chapter. And just like in univariate time series, there are some building blocks that can be built upon the VAR model to account for different types of processes. This can build up from the smaller and more common <span class="s118">VAR </span>to the more complex <span class="s118">VARMAX</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">It must be said that techniques for multivariate time series modeling are a part of the more advanced techniques. They do not always have Python implementations and are less often used than the techniques for univariate time series. While the use of the VAR model is still relatively common, the more advanced models in this branch become less and less documented on the Internet.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 126%;text-align: left;">In this chapter, you will discover the VARMAX model. It is the go-to model for multivariate time series. It adds a <span class="s118">moving average component </span>to the VAR model, and it can allow for external, or exogenous, variables as well. The components in the VARMAX model are therefore</p></li><li><p class="s118" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">V <span class="p">for vector indicating that it’s a multivariate model</span></p></li><li><p class="s118" style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">AR <span class="p">for autoregression</span></p></li><li><p class="s118" style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">MA <span class="p">for moving average</span></p></li><li><p class="s118" style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;line-height: 127%;text-align: justify;">X <span class="p">for the use of exogenous variables (in addition to the endogenous variables)</span></p></li></ul></li></ul><p style="padding-top: 5pt;padding-left: 43pt;text-indent: 0pt;text-align: justify;">It must be noted that the VARMAX model does <span class="s20">not </span>have a seasonal component.</p><p style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: justify;">This is not necessarily a problem, as seasonality can be included through the <span class="s20">exogenous variables</span>. For example, a monthly seasonality can be modeled by adding the exogenous variable month. For a weekly seasonality, you could add the variable week number.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 126%;text-align: left;">Another missing block is the integration block. The VARMAX model does not include the differencing of non-stationary time series. As for the VAR model, all the input time series must already be <span class="s20">stationary</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_10#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_10#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_10</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">141</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark117">Model Definition</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The mathematical model of the VARMAX definition is as follows:</p><p style="padding-top: 10pt;padding-left: 43pt;text-indent: 0pt;text-align: center;"><span class="s20">y</span><span class="s77">t </span>= <span class="s20">v </span>+ <span class="s20">A</span><span class="s158">1</span><span class="s20">y</span><span class="s77">t </span><span class="s159">− </span><span class="s90">1 </span>+ <span class="s160">⋯ </span>+ <span class="s20">A</span><span class="s77">p</span><span class="s20">y</span><span class="s77">t </span><span class="s159">− </span><span class="s79">p </span>+ <span class="s20">Bx</span><span class="s77">t </span>+ <span class="s161">∈</span><span class="s77">t </span>+ <span class="s20">M</span><span class="s158">1</span><span class="s161">∈</span><span class="s77">t </span><span class="s159">− </span><span class="s90">1 </span>+ <span class="s160">⋯ </span>+ <span class="s20">M</span><span class="s77">q</span><span class="s161">∈</span><span class="s77">t </span><span class="s159">− </span><span class="s79">q</span></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 17pt;line-height: 121%;text-align: left;">In this model, <span class="s20">y</span><span class="s77">t </span>is a vector of the values of the present, and the other <span class="s20">y </span>’s are the lagged values. The <span class="s118">A</span>s are the autocorrelation coefficients: for each lag, they are a vector of the same length as the number of time series. <span class="s118">Ms </span>are vectors of coefficients for the lagged model errors. They represent the moving average part of the model.</p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As the VARMAX model is merely a combination of building blocks that you have seen throughout the previous seven chapters, I will spare you the repetition over here. Don’t hesitate to go back to the previous chapters for more details on the intuition and explanations behind different building blocks. Let’s now dive into the Python implementation and the example.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Multiple Time Series with Exogenous Variables</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">As a first step, let’s prepare the Walmart data on a by-store and by-week basis. As you have seen in the previous chapter, this dataset lends itself perfectly to multivariate time series modeling.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 10-1. <span class="s33">Prepare the Walmart data for the VARMAX model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import matplotlib.pyplot as plt</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">data = pd.read_csv(&#39;walmart/train.csv&#39;)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">exog = data.groupby(&#39;Date&#39;)[&#39;IsHoliday&#39;].sum() &gt; 0 exog = exog.apply(lambda x: float(x))</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = data.pivot_table(index = &#39;Date&#39;, columns = &#39;Store&#39;, values = &#39;Weekly_Sales&#39;)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax = data.plot(figsize=(20,15)) ax.legend([]) ax.set_ylabel(&#39;Sales&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark118">Note that in Listing </a><span style=" color: #00F;">10-1</span>, the exogenous data have been created as well. They are the indicator of the presence of holidays per week. They will be the only variable used as exogenous data in this example, but there is no theoretical restriction to use more exogenous variables.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The plot obtained by this code is shown in Figure <span style=" color: #00F;">10-1</span>. You can observe that the time series follow patterns that seem relatively correlated.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 56pt;text-indent: 0pt;text-align: left;"><span><img width="468" height="338" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_082.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">Figure 10-1. <span class="s29">Plot of the time series per store</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In the previous chapter, you have estimated the VAR, using only the order p. This p determines the order of the AR component. In the VARMAX model, there are both an AR component and an MA component. Therefore, there is an order for the AR part and for the MA part to be decided. The hyperparameters of VARMAX(p,q) are</p><ul id="l37"><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The order of the AR component, denoted p</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The order of the MA component, denoted q</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now to estimate a VARMAX model, you may use the code shown in Listing <span style=" color: #00F;">10-2</span>. To avoid the model taking too much computation resources on your hardware, the code example uses the first three stores of the dataset.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark119">Listing 10-2. </a><span class="s33">Running the VARMAX(1,1) model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import statsmodels.api as sm</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.metrics import mean_absolute_percentage_error</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">train = data.iloc[:-10,[0,1,2]]</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">test = data.iloc[-10:,[0,1,2]]</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">train_exog = exog[:-10] test_exog = exog[-10:]</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">mod = sm.tsa.VARMAX(train, order=(1,1), exog=train_exog) res = mod.fit(maxiter=100, disp=False)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">fcst = res.forecast(exog=test_exog.values, steps=10) mape = mean_absolute_percentage_error(test, fcst) model_accuracy = 1 - mape</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">print(model_accuracy)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The VARMAX model, especially when applied to examples with a large number of variables, can be very long to run. What’s taking long here is the estimation of the moving average part of the model. MA models are known to be exceptionally slow to fit, and in the current case, there is the difficulty of having 45 variables to estimate it for.</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The R2 that was obtained on this example was <span class="s118">0.96</span>. This is a great score. This was</p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 122%;text-align: left;">obtained on a <span class="s118">ten-step forecast</span>. As you may remember from Chapter <span style=" color: #00F;">4</span>, the moving average component is <span class="s20">not suited for multistep forecasts</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">With a model training time that is so long already, it would hardly be a solution to add repetitive retraining procedures to the model. The VARMAX model should be used only in cases where training times are not a problem or where the VARMAX obtains performances that cannot be obtained using alternative training methods.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">It is great to have the more advanced modeling technique of VARMAX in your forecasting toolbox. The model can fit more complex processes than many other time series models. Yet it also has its disadvantages: training times are relatively long compared to simpler models, and it needs a relatively large amount of data to estimate correctly.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As a sidenote, you will find that those training time and data availability requirements are almost unavoidable for any of the more complex time series and machine learning techniques. This can partly be countered by computing power, but it is still important to evaluate model choice critically.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4></li><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The VARMAX model consists of</p><ul id="l38"><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;line-height: 129%;text-align: left;">V for vector: it is a multivariate model as it models multiple time series at the same time.</p></li><li><p style="padding-top: 6pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">AR for autoregression.</p></li><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">MA for moving average.</p></li><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">X for the addition of exogenous variables.</p></li></ul></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The VARMAX(p,q) model takes two hyperparameters:</p><ul id="l39"><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">p for the order of the AR part</p></li><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">q for the order of the MA part</p></li></ul></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The time series in a VARMAX have to be stationary.</p><p class="s21" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark120">PART IV</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Supervised Machine Learning Models</h1><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark121">CHAPTER 11</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The Linear Regression</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In the following chapters, you will see the most common supervised machine learning models. As you’ll remember from Chapter <span style=" color: #00F;">1</span>, supervised machine learning algorithms work differently than time series models.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In supervised machine learning models, you try to identify relations between different variables:</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Target variable<span class="p">: The variable that you try to forecast</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;line-height: 126%;text-align: left;">Explanatory variables<span class="p">: Variables that help you to predict the target variable</span></p></li></ul><p style="padding-top: 6pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">For forecasting, it is important to understand which types of explanatory variables you can or cannot use. As an example, let’s say that the sales of hot chocolate strongly depend on the temperature. When the weather is cold, sales are high. When the weather is warm, sales are low.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You could make a model that regroups this basic if/else logic. Yet, when you think about it, this logic could not be used for forecasting in the future. After all, if you want to forecast tomorrow’s sales of hot chocolate, you must know tomorrow’s temperature. And this is not something you know: it would require an additional forecast of temperature!</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Another example is to predict hot chocolate sales not based on the weather, but on the week number. You could try to identify a relationship with the week number based on past data. Then to predict future sales, you would input the next week’s week number into the model and obtain a forecast. This is possible here because the week number is something that you know for certain in advance.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This is important to keep in mind when doing supervised models in general. Now, let’s get into more depth in linear regression.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_11#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_11#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_11</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">149</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark122">The Idea Behind Linear Regression</a></h4><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 125%;text-align: left;">The idea behind linear regression is to define a linear relationship between a target variable and numerous explanatory variables to predict the target variable. Linear regression is widely used, not only for forecasting. Like any supervised model, as long as you put explanatory variables of the future as input to the model, this works perfectly.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Model Definition</h4><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Linear regression is defined as follows:</p><p class="s115" style="padding-top: 6pt;padding-left: 24pt;text-indent: 0pt;text-align: center;"><span class="s42">y </span>= <span class="s162">β</span><span class="s121">0 </span>+ <span class="s162">β</span><span class="s121">1 </span><span class="s42">x</span><span class="s121">1 </span>+<span class="s110"></span>+ <span class="s162">β</span><span class="s58">p </span><span class="s42">x</span><span class="s58">p </span>+ <span class="s162">ε</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">In this formula</p><ul id="l40"><li><p style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">There are <span class="s19">p </span>explanatory variables, called <span class="s19">x</span>.</p></li><li><p style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">There is one target variable called <span class="s19">y</span>.</p></li><li><p style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;line-height: 114%;text-align: left;">The value for y is computed as a constant (<span class="s80">β</span><span class="s158">0</span>) plus the values of the x variables multiplied by their coefficients <span class="s80">β</span><span class="s158">1</span><span class="s90"> </span>to <span class="s80">β</span><span class="s77">p</span>.</p><p style="padding-top: 6pt;padding-left: 8pt;text-indent: 17pt;line-height: 123%;text-align: left;">Figure <span style=" color: #00F;">11-1 </span>shows how to interpret B0 and B1 visually. It shows that for an increase of 1 in the x variable, the increase in the y variable represents <span class="s80">β</span><span class="s158">1</span>. <span class="s80">β</span><span class="s158">0 </span>is the value for y when x is 0.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="266" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_083.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 11-1. <span class="s29">Visual interpretation of linear regression</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark123">To be able to use linear regression, you need to estimate the coefficients (betas) on a training dataset. The coefficients can then be estimated using the following formula, in matrix notation:</a></p><p class="s62" style="padding-top: 7pt;padding-left: 52pt;text-indent: 0pt;text-align: center;"><span class="s88">f3</span><span class="s134">ˆ</span><span class="s44"> </span><span class="s59">= </span><span class="s163">(</span>X<span class="s164">T</span><span class="s66"> </span>X <span class="s163">)</span><span class="s165">-</span><span class="s85">1 </span>X<span class="s164">T</span><span class="s66"> </span>y</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This formula is known as <span class="s19">OLS</span>: the <span class="s19">Ordinary Least Squares method</span>. This model is very fast to fit, as it requires only matrix calculations to compute the betas. Although easy to fit, it is less suited for more complex processes. After all, it is a linear model, and it can therefore only fit linear processes.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">A linear model can fit any type of relationship that goes in one direction. An example of this is “if the temperature goes up, hot chocolate sales go down.” Yet linear models cannot fit anything nonlinear. An example of a nonlinear process is this: “if the temperature is below zero, hot chocolate sales are low; if the temperature is between zero and 10, hot chocolate sales are high; if the temperature is high, hot chocolate sales are low.”</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">As you see, the second example is nonlinear because you could not draw a straight line from low to high hot chocolate sales. Rather, you could better make an if/else statement to capture this logic.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You should keep in mind that linear models are not very good at capturing nonlinear trends. Yet nonlinear models, when tuned correctly, can often approximate linear</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">trends quite well. This is the reason that many of the more advanced machine learning techniques use a lot of nonlinear approaches. You will see this throughout the following chapters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Example: Linear Model to Forecast CO2 Levels</h4><p style="padding-top: 10pt;padding-left: 25pt;text-indent: 0pt;line-height: 123%;text-align: left;">As an example for the linear model, you will work with the CO<span class="s158">2 </span>dataset that you have already discovered in a previous chapter. This dataset has the advantage of being relatively easy to work with, and it will be a perfect case to show how to add <span class="s19">lagged variables, seasonality, and trend </span>into a supervised machine learning model.</p><p style="padding-left: 25pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s get started by importing the data into Python and plotting it. The code for this is shown in Listing <span style=" color: #00F;">11-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark124">Listing 11-1. </a><span class="s33">Importing the data and plotting it</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import statsmodels.api as sm import pandas as pd</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">import matplotlib.pyplot as plt</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = sm.datasets.co2.load_pandas() co2 = data.data</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">co2 = co2.dropna() ax = co2.plot()</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">ax.set_ylabel(&#39;CO2 level&#39;) plt.show()</p><p style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">Figure <span style=" color: #00F;">11-2 </span>shows the graph that you should obtain.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;"><span><img width="323" height="191" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_084.jpg"/></span></p><p class="s29" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><b>Figure 11-2. </b>CO<span class="s166">2 </span>levels over time</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: left;">Now, you only have the dates and the CO<span class="s158">2 </span>values. The interesting step here is to do <span class="s19">feature engineering</span>: creating additional variables, based on the original variables. Even though there is very little information in this dataset, there are a lot of variables that you can create from it.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s start by extracting seasonal variables from the date variable. As we can see from the plot, there is a strong seasonal pattern going on. You could try to capture this by adding a monthly seasonality to the model. For this, it is necessary to create a variable month in your dataset. You can use Listing <span style=" color: #00F;">11-2 </span>to do this.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark125">Listing 11-2. </a><span class="s33">Creating the variable month</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">co2[&#39;month&#39;] = [x.month for x in co2.index]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now that you have this variable for monthly seasonality, let’s see whether you can create a variable that captures the long-term upward trend. The solution to this problem is to add a variable year, by extracting the year from the date variable. As there is a yearly increase in the data, the trend effect could be captured by this variable. This is done in Listing <span style=" color: #00F;">11-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 11-3. <span class="s33">Creating the variable year</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">co2[&#39;year&#39;] = [x.year for x in co2.index]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now for starters, let’s just try to fit a linear regression with only those two explanatory variables month and year. The package scikit-learn, which you have seen before, contains a large number of supervised models and will be used for this exercise. This basic model is created in Listing <span style=" color: #00F;">11-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 11-4. <span class="s33">Fitting a linear regression with two variables</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Create X and y objects</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X = co2[[&#39;year&#39;, &#39;month&#39;]] y = co2[&#39;co2&#39;]</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Create Train test split</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import train_test_split</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=12345,shuffle=False)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Fit model</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">my_lm = LinearRegression() my_lm.fit(X = X_train, y = y_train)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">train_fcst = my_lm.predict(X_train) test_fcst = my_lm.predict(X_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark126">train_r2 = r2_score(y_train, train_fcst) test_r2 = r2_score(y_test, test_fcst)</a></p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">print(train_r2, test_r2)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Plot result plt.plot(list(test_fcst)) plt.plot(list(y_test)) plt.xlabel(&#39;Steps into the test set&#39;) plt.ylabel(&#39;CO2 levels&#39;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.show()</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">The train R2 of this model is <span class="s19">0.96</span>, which is great. The test R2, however, is <span class="s19">0.34</span>, which is relatively bad. In Figure <span style=" color: #00F;">11-3</span>, you can see the fit on the test set and see that there is some improvement to be made.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="353" height="258" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_085.jpg"/></span></p><p class="s28" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 11-3. <span class="s29">Predictive performance plot of the simple model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The next thing that you will add to the model is an autoregressive component. This will be done by feature engineering. As always in supervised models, you need to extract any information as explanatory variables. For this example, let’s see how to use the shift method to create a lagged variable. You can, for example, add five lagged variables easily using Listing <span style=" color: #00F;">11-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark127">Listing 11-5. </a><span class="s33">Adding lagged variables into the data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">co2[&#39;co2_l1&#39;] = co2[&#39;co2&#39;].shift(1)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">co2[&#39;co2_l2&#39;] = co2[&#39;co2&#39;].shift(2)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">co2[&#39;co2_l3&#39;] = co2[&#39;co2&#39;].shift(3)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">co2[&#39;co2_l4&#39;] = co2[&#39;co2&#39;].shift(4)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">co2[&#39;co2_l5&#39;] = co2[&#39;co2&#39;].shift(5)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Note that adding lagged variables creates NA in the dataset. This is a border effect, and it is not a problem. Simply delete any missing data with Listing <span style=" color: #00F;">11-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 11-6. <span class="s33">Drop missing values</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">co2 = co2.dropna()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">As a final step, let’s fit and evaluate the model with the monthly seasonality, the yearly trend, and the five autoregressive lagged variables. You must note here that, since we added the lagged values into the train set, it would be impossible to do this for multiple steps forward. You calculate the first future value using the data of today. You calculate the second future value by using the data of tomorrow. Therefore, the error</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">that you evaluate here should be interpreted as a one-step forecasting error, whereas the previous code block did a multistep forecast. This is done in Listing <span style=" color: #00F;">11-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 11-7. <span class="s33">Fitting the full linear regression model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Create X and y objects</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X = co2[[&#39;year&#39;, &#39;month&#39;, &#39;co2_l1&#39;, &#39;co2_l2&#39;, &#39;co2_l3&#39;, &#39;co2_l4&#39;, &#39;co2_l5&#39;]]</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">y = co2[&#39;co2&#39;]</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Train Test Split</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=12345,shuffle=False)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Fit the model</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">my_lm = LinearRegression() my_lm.fit(X = X_train, y = y_train)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">train_fcst = my_lm.predict(X_train) test_fcst = my_lm.predict(X_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark128">train_r2 = r2_score(y_train, train_fcst) test_r2 = r2_score(y_test, test_fcst)</a></p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">print(train_r2, test_r2)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Plot result plt.plot(list(test_fcst)) plt.plot(list(y_test)) plt.xlabel(&#39;Steps into the test set&#39;) plt.ylabel(&#39;CO2 levels&#39;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.show()</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">The R2 train score of this model is <span class="s19">0.998</span>, and the test R2 score is <span class="s19">0.990</span>. This is a great performance! You can see in the plot (Figure <span style=" color: #00F;">11-4</span>) that the predictions follow the actual values almost perfectly.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="353" height="260" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_086.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 11-4. <span class="s29">Predictive performance plot of the full model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">In conclusion, you have seen how to add time series trends into a supervised model.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">You have used a linear model to obtain this result. In the following chapters, you’ll discover how to apply more and more complex models in the same way.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4><ul id="l41"><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The linear model is the simplest supervised machine learning model.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The linear model finds the best linear combination of external variables to forecast the future.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Linear models cannot easily adapt to nonlinear situations.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: justify;">Feature engineering is the task of creating the best possible variables in your dataset, in order to obtain explanatory variables that can help your model to obtain best performances.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: justify;">Seasonality can be fitted by supervised models when you introduce a seasonal variable.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">A trend can be fitted by supervised models when you introduce a trend variable.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Autoregression can be fitted by supervised models when you add lagged versions of the target variable into the explanatory variables.</p></li></ul></li></ul><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark129">CHAPTER 12</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The Decision Tree Model</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 126%;text-align: left;">As you’ve discovered in the previous chapter, there is a distinction in supervised machine learning models between linear and nonlinear models. In this chapter, you will discover the <span class="s19">Decision Tree </span>model. It is one of the simplest nonlinear machine learning models.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 123%;text-align: left;">The idea behind the Decision Tree model can be intuitively understood as a long list of <span class="s20">if-else statements</span>. Those if-else decisions would be used at the prediction stage: the model predicts some <span class="s19">result x </span>if a certain condition is true, and it will <span class="s19">predict y </span>otherwise. As you see, there is no linear trend in this type of logic, and because of this, <span class="s20">a decision tree model can fit nonlinear trends</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s see an example of this decision tree logic in practice, without yet discussing where the decision tree comes from. Figure <span style=" color: #00F;">12-1 </span>shows an example decision tree that can help you to understand intuitively how a decision tree could work. It shows a decision tree that forecasts the average rainfall depending on climate zone and season.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="243" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_087.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 12-1. <span class="s29">An intuitive example of a decision tree</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_12#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_12#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_12</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">159</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark130">Mathematics</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">You’ll now discover the mathematical and algorithmic components of the decision tree algorithm. There are different ways to fit decision trees, but the most common one is to have two steps: splitting (also called growing) the tree and pruning the tree.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Splitting</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now that you have seen that the decision tree is merely a hierarchical ordination of multiple decision splits, the logical question is where those decisions come from.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">It all starts with a dataset in which you have a target variable and multiple explanatory variables. Now a huge number of splits are possible, but you need to choose one. The first split should be the split that would obtain <span class="s19">the lowest Mean Squared Error</span>. In the end, even a model with only one split would be able to be used as a predictive model.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now that you know the goal of your split, the only thing left to do is to test each possible split and evaluate which of them results in the lowest Mean Squared Error. Once you’ve identified the best first split, you obtain two groups. You then repeat this procedure for each of the two groups, so that you then obtain four groups and so forth.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">At some point, you cannot split any further. This point is whenever there is only one data point in each group. Logically, a single data point cannot be split.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Pruning and Reducing Complexity</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Although it is possible to let the tree continue splitting until further splitting is impossible, it is not necessarily the best thing to do. There is also a possibility to allow having a bit more data points in a group to avoid overfitting.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This can be obtained in multiple ways. Some implementations allow a pruning process, which first forces a tree to split everything completely and then adds the pruning phase to cut off those branches that are the least needed.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Other implementations let you simply add a complexity parameter that will directly prevent the trees from becoming too detailed. This is a parameter that is a great choice for optimizing using a grid search.</p><h4 style="padding-top: 10pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a name="bookmark131">Example</a></h4><p class="s68" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a href="https://archive.ics.uci.edu/ml/datasets/bike%2Bsharing%2Bdataset" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">In this chapter, you’ll be working with data that come from a bike-sharing company. You can download the data from the UCI machine learning archives over here: </a><a href="https://archive.ics.uci.edu/ml/datasets/bike%2Bsharing%2Bdataset" class="s32" target="_blank">https:// </a>archive.ics.uci.edu/ml/datasets/bike+sharing+dataset<span class="p">.</span></p><p style="padding-top: 1pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This will be a difficult example. In this dataset, you’ll be predicting the number of rental bike users per day. And as you can imagine, this will depend strongly on the</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">weather. As I explained in the previous chapter, the weather is hard to predict, and if your forecast has a strong correlation with the weather, you know it will be a big challenge.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s first have a look at the data to know what you’re working with here. This is done in Listing <span style=" color: #00F;">12-1 </span>and will obtain the plot in Figure <span style=" color: #00F;">12-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 12-1. <span class="s33">Import the bike data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import matplotlib.pyplot as plt</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = pd.read_csv(&#39;bikedata/day.csv&#39;) ax = data[&#39;cnt&#39;].plot() ax.set_ylabel(&#39;Number of users&#39;) ax.set_xlabel(&#39;Time&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="342" height="221" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_088.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 12-2. <span class="s29">Plot of the bike-sharing users</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark132">As you can see in the plot, many things are going on here. There is a difficult trend to be estimated. There is also a lot of day-to-day variation. When you look at the dataset, you can see that there are a lot of data on weather. This is going to be the challenge: the number of bike-sharing users depends strongly on the day’s weather, but the weather is not something that you can know in advance.</a></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The only thing you can do in such cases is to try and do a lot of feature engineering to create many valuable variables for the model. In the code block in Listing <span style=" color: #00F;">12-2</span>, you will use some of the existing variables, and you’ll see how to create a number of variables.</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The total list of explanatory values is as follows:</p><ul id="l42"><li><p class="s20" style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Original variable <span class="p">‘season’: (1:spring, 2:summer, 3:fall, 4:winter)</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Original variable <span class="p">‘yr’: The year</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Original variable <span class="p">‘mnth’: The month</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Original variable <span class="p">‘holiday’: Whether the day is a holiday</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Original variable <span class="p">‘weekday’: The day of the week</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;line-height: 126%;text-align: left;">Original variable <span class="p">‘workingday’: Whether the day is a holiday/ weekend or a working day</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;line-height: 126%;text-align: left;">The 7 last days of <span class="p">‘cnt’: An autoregressive component for the number of users</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;line-height: 126%;text-align: left;">The 7 last days of <span class="p">‘weathersit’: The weather, from 4 (very bad weather) to 1 (good weather)</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The 7 last days of <span class="p">‘temperature’</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The 7 last days of <span class="p">‘humidity’</span></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You will also <span class="s19">delete an influential outlier value </span>in the dataset. This is an extreme observation, and it is difficult to understand why it happened. You can see it in Figure <span style=" color: #00F;">12-2</span>, as a very low peak occurring at index 477. As it can influence the model negatively, it is best to not include it in the training data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: left;">There is another low peak somewhere between 650 and 700, but as this part of the data will be our test set, it would be unfair to do a treatment to it. <span class="s19">Removing outliers from the test set would be cheating, </span>as this would make our test score higher than reality. The goal is always to have a model error estimate that is as reliable as possible, to anticipate its behavior when applying the model in practice.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark133">Listing 12-2. </a><span class="s33">Creating the training dataset</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># 7 last days of user count (autoregression) data[&#39;usersL1&#39;] = data[&#39;cnt&#39;].shift(1)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">data[&#39;usersL2&#39;] = data[&#39;cnt&#39;].shift(2)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;usersL3&#39;] = data[&#39;cnt&#39;].shift(3)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;usersL4&#39;] = data[&#39;cnt&#39;].shift(4)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;usersL5&#39;] = data[&#39;cnt&#39;].shift(5)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;usersL6&#39;] = data[&#39;cnt&#39;].shift(6)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;usersL7&#39;] = data[&#39;cnt&#39;].shift(7)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;"># 7 last days of weathersit</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: justify;">data[&#39;weatherL1&#39;] = data[&#39;weathersit&#39;].shift(1) data[&#39;weatherL2&#39;] = data[&#39;weathersit&#39;].shift(2) data[&#39;weatherL3&#39;] = data[&#39;weathersit&#39;].shift(3) data[&#39;weatherL4&#39;] = data[&#39;weathersit&#39;].shift(4) data[&#39;weatherL5&#39;] = data[&#39;weathersit&#39;].shift(5) data[&#39;weatherL6&#39;] = data[&#39;weathersit&#39;].shift(6) data[&#39;weatherL7&#39;] = data[&#39;weathersit&#39;].shift(7)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># 7 last days of temperature data[&#39;tempL1&#39;] = data[&#39;temp&#39;].shift(1)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">data[&#39;tempL2&#39;] = data[&#39;temp&#39;].shift(2)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;tempL3&#39;] = data[&#39;temp&#39;].shift(3)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;tempL4&#39;] = data[&#39;temp&#39;].shift(4)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;tempL5&#39;] = data[&#39;temp&#39;].shift(5)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;tempL6&#39;] = data[&#39;temp&#39;].shift(6)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;tempL7&#39;] = data[&#39;temp&#39;].shift(7)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># 7 last days of humidity data[&#39;humL1&#39;] = data[&#39;hum&#39;].shift(1)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">data[&#39;humL2&#39;] = data[&#39;hum&#39;].shift(2)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;humL3&#39;] = data[&#39;hum&#39;].shift(3)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;humL4&#39;] = data[&#39;hum&#39;].shift(4)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;humL5&#39;] = data[&#39;hum&#39;].shift(5)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;humL6&#39;] = data[&#39;hum&#39;].shift(6)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data[&#39;humL7&#39;] = data[&#39;hum&#39;].shift(7)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark134">data = data.dropna() data = data.drop(477)</a></p><p class="s34" style="padding-top: 7pt;padding-left: 63pt;text-indent: -55pt;line-height: 113%;text-align: left;">X = data[[&#39;season&#39;, &#39;yr&#39;, &#39;mnth&#39;, &#39;holiday&#39;, &#39;weekday&#39;, &#39;workingday&#39;, &#39;weatherL1&#39;, &#39;weatherL2&#39;, &#39;weatherL3&#39;, &#39;weatherL4&#39;, &#39;weatherL5&#39;, &#39;weatherL6&#39;, &#39;weatherL7&#39;,</p><p class="s34" style="padding-left: 57pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;usersL1&#39;,&#39;usersL2&#39;, &#39;usersL3&#39;, &#39;usersL4&#39;, &#39;usersL5&#39;, &#39;usersL6&#39;, &#39;usersL7&#39;,</p><p class="s34" style="padding-left: 57pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;tempL1&#39;, &#39;tempL2&#39;, &#39;tempL3&#39;, &#39;tempL4&#39;, &#39;tempL5&#39;, &#39;tempL6&#39;, &#39;tempL7&#39;,</p><p class="s34" style="padding-left: 8pt;text-indent: 49pt;line-height: 170%;text-align: left;">&#39;humL1&#39;, &#39;humL2&#39;,&#39;humL3&#39;, &#39;humL4&#39;, &#39;humL5&#39;, &#39;humL6&#39;, &#39;humL7&#39;]] y = data[&#39;cnt&#39;]</p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now, let’s move on to the model building. To get started, let’s do a model without any hyperparameter tuning. The example is shown in Listing <span style=" color: #00F;">12-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 12-3. <span class="s33">Fitting the model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># Create Train test split</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import train_test_split</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=12345, shuffle=False)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.tree import DecisionTreeRegressor my_dt = DecisionTreeRegressor(random_state=12345) my_dt.fit(X_train, y_train)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import r2_score print(r2_score(list(y_test), list(my_dt.predict(X_test))))</p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">This first, unoptimized model makes for an R2 of the test set of <span class="s19">0.166</span>. Not great yet, so see what can be done using a grid search. In this grid search, we’ll be tuning a few hyperparameters:</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">min_samples_split: The minimum number of samples required to split a node. Higher values make for fewer splits. Having fewer splits makes a tree less specific, so this can help to prevent overfitting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l43"><li><p style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;"><a name="bookmark135">max_features: The number of features (variables) to consider when searching the best split. When using fewer features, you force the splits to be different from each other, but at the same time, you can be blocking the tree from finding the split that is actually the best split.</a></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;line-height: 128%;text-align: left;">Error criterion<span class="p">: As we’re optimizing our R2, we would potentially want to use the R2 as an error criterion for the decision of the best splits. Yet it is not available. It can therefore be interesting to try the MSE and the MAE and see which one allows us to optimize the R2 of the complete tree.</span></p><p style="padding-top: 6pt;padding-left: 25pt;text-indent: 17pt;line-height: 129%;text-align: justify;">There are many more hyperparameters in the DecisionTreeRegressor. You can refer to the scikit-learn page for the DecisionTreeRegressor to find them. Don’t hesitate to try out and play with those different parameters. The grid search is shown in Listing <span style=" color: #00F;">12-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;">Listing 12-4. <span class="s33">Adding a grid search</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import GridSearchCV</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">my_dt = GridSearchCV(DecisionTreeRegressor(random_state=44),</p><p class="s34" style="padding-top: 1pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">{&#39;min_samples_split&#39;: list(range(20,50, 2)),</p><p class="s34" style="padding-top: 1pt;padding-left: 119pt;text-indent: 0pt;text-align: left;">&#39;max_features&#39;: [0.6, 0.7, 0.8, 0.9, 1.],</p><p class="s34" style="padding-top: 1pt;padding-left: 114pt;text-indent: 5pt;line-height: 113%;text-align: left;">&#39;criterion&#39;: [&#39;mse&#39;, &#39;mae&#39;]}, scoring = &#39;r2&#39;, n_jobs = -1)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">my_dt.fit(X_train, y_train)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">print(r2_score(list(y_test), list(my_dt.predict(X_test))))</p><p style="padding-top: 10pt;padding-left: 43pt;text-indent: 0pt;text-align: justify;">When running this in the example notebook, this created an R2 score of <span class="s19">0.55</span>.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">Although this is still not amazing, it is a large improvement from the previous model. To find out which parameters have been chosen, you can get the best parameters using the code in Listing <span style=" color: #00F;">12-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;">Listing 12-5. <span class="s33">Finding the best parameters</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">print(my_dt.best_estimator_)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 43pt;text-indent: 0pt;text-align: justify;">This will show you the best parameters as follows:</p><p class="s34" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">DecisionTreeRegressor(criterion=&#39;mae&#39;, max_features=0.8, min_samples_split=48,random_state=44)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;"><a name="bookmark136">Apparently, this combination of error criterion </a><span class="s19">MAE rather than MSE</span>, a <span class="s19">max_ features of 0.8</span>, and a <span class="s19">min_samples_split of 48 </span>is the model with the best predictive performance. Now, let’s make a forecast with this model and plot how well it fits, using Listing <span style=" color: #00F;">12-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 12-6. <span class="s33">Plotting the prediction</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">fcst = my_dt.predict(X_test)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(list(fcst)) plt.plot(list(y_test)) plt.ylabel(&#39;Sales&#39;) plt.xlabel(&#39;Time&#39;) plt.show()</p><p style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">The plot that you’ll obtain is the plot shown in Figure <span style=" color: #00F;">12-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="464" height="336" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_089.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 12-3. <span class="s29">Plot of the bike-sharing users forecast</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark137">A last thing that is very practical with the Decision Tree model is that you can obtain a plot that shows you the different splits that have been identified. This type of plot</a></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 13pt;text-align: left;">is sometimes also referred to as a <span class="s19">dendrogram</span>. Despite the lower performances of a</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Decision Tree model, one of its strong points is that it allows for interpretation in detail of the decisions of the model. You can do this using Listing <span style=" color: #00F;">12-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 12-7. <span class="s33">Plotting the dendrogram</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.tree import plot_tree plot_tree(my_dt.best_estimator_, max_depth=1) plt.show()</p><p style="padding-top: 9pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">This code will show you the beginning of the decision tree, as shown in Figure <span style=" color: #00F;">12-4</span>.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">You can make larger extractions, by increasing the max_depth of the plot.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 91pt;text-indent: 0pt;text-align: left;"><span><img width="378" height="273" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_090.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 12-4. <span class="s29">Plot the decision tree</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This type of plot shows one of the real added values of using simpler models like the Decision Tree model. They allow for an interpretation of the model, and this may be required in many circumstances. Models that you’ll see in the next chapters will become less and less interpretable.</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4></li></ul></li><li><p style="padding-top: 10pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">The Decision Tree model is one of the simplest nonlinear supervised machine learning models.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">You can tune the complexity of the decision tree, which defines how long and complex a decision tree becomes.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">Complex trees risk overfitting: they learn too detailed patterns in the training data. You can use a grid search to optimize the complexity of the tree.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">You can create a dendrogram of the fitted decision tree to obtain all the splits and variables that have been used by your model.</p></li></ul><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark138">CHAPTER 13</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The kNN Model</h1><p style="padding-top: 21pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this chapter, you will discover the <span class="s167">kNN model</span>. The kNN model is the third supervised machine learning model that is covered in this book. Like the two previous models, the kNN model is also one of the simpler models. It is also intuitively easy to understand how the model works. As a downside, sometimes it is not performant enough to compete with the more advanced machine learning models that you will see in the following chapters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">Intuitive Explanation</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">So what is the kNN model all about? The intuitive idea behind it is simple: you use the data points closest to a new data point to predict it. How could you best predict tomorrow’s weather? Look at today’s weather! And to predict next month’s sales, it is probably reasonably close to this month’s sales.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Yet it gets more advanced when considering that a nearest data point can be in multiple dimensions. For example, if next month’s sales are in December, the data point could be closest to another month of December in the dimension month.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 122%;text-align: left;">As soon as you have multiple variables, this idea of distance becomes a very powerful concept for predicting new values. In short, the kNN model tries to find the <span class="s167">nearest neighbors </span>to a data point and uses their value as a prediction.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Mathematical Definition of Nearest Neighbors</h4><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 127%;text-align: left;">The definition of nearest neighbors is based on the computation of the <span class="s167">Euclidean distance </span>from the new data point to each of the existing data points. The Euclidean distance is the most common distance measure. You probably use it on a daily basis when talking about your distances from home to work and so on. You can express it mathematically as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 9pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s152">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_13#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_13#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_13</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">169</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="234" height="24" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_091.png"/></span></p><p class="s168" style="text-indent: 0pt;text-align: left;">(</p><p style="text-indent: 0pt;text-align: left;"/><p class="s106" style="text-indent: 0pt;text-align: left;"><span class="s84">p </span>− <span class="s84">q </span>+ <span class="s84">p </span>− <span class="s84">q </span>+<span class="s110"></span>+ <span class="s84">p </span>− <span class="s84">q</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s168" style="text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s76" style="text-indent: 0pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s168" style="text-indent: 0pt;text-align: left;">(</p><p style="text-indent: 0pt;text-align: left;"/><p class="s168" style="text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s76" style="text-indent: 0pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s168" style="text-indent: 0pt;text-align: left;">(</p><p style="text-indent: 0pt;text-align: left;"/><p class="s168" style="text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s76" style="text-indent: 0pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s76" style="text-indent: 0pt;text-align: left;">1 1</p><p style="text-indent: 0pt;text-align: left;"/><p class="s76" style="text-indent: 0pt;text-align: left;">2 2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s149" style="text-indent: 0pt;line-height: 7pt;text-align: left;">n n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s84" style="padding-top: 5pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">d <span class="s168">(</span>p<span class="s55">,</span>q <span class="s168">) </span><span class="s106">=</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: left;">In this formula, <span class="s167">p </span>and <span class="s167">q </span>are two data points each with n dimensions. This would generally mean that there are <span class="s167">n </span>explanatory variables in the dataset. You sum the squared distances for each dimension and finally take the square root.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: left;">The letter <span class="s167">k </span>is used to indicate the number of neighbors to use. To compute the <span class="s153">k nearest neighbors</span>, you simply compute the distance between your new data point and each of the data points in the training data. Depending on which number you have for k, you take the k data points that have the lowest distance.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The graph in Figure <span style=" color: #00F;">13-1 </span>shows how the algorithm works in a two-dimensional situation. All the data points in the training dataset are available to be chosen as a neighbor for any new point. As a dummy example, let’s imagine it’s again about hot chocolate sales based on the temperature.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 10pt;text-indent: 0pt;text-align: left;"><span><img width="548" height="334" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_092.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 13-1. <span class="s157">The six nearest neighbors of a new data point</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark139">The blue points in the graph are real observations from the past, in which we observed temperature and hot chocolate prices. Let’s say we already know tomorrow’s temperature and we want to predict the hot chocolate sales for tomorrow.</a></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">If we use the kNN algorithm for this, we need to identify the closest neighbors: in the graph, they are annotated based on distance. The notion of nearest, in this case, is based only on one variable: temperature. In cases with more variables, this would be the same computation using the formula stated earlier, yet it would be hard to visualize such a multivariate situation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;">Combining k Neighbors into One Forecast</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 128%;text-align: justify;">Once you have identified the <span class="s167">k neighbors </span>that are closest to your new data point, you do not yet have a prediction. There is one step remaining to convert the multiple neighbors into one prediction. There are two prevalent methods for it:</p><ol id="l44"><li><p style="padding-top: 5pt;padding-left: 72pt;text-indent: -18pt;line-height: 128%;text-align: left;">The first method is simply to take <span class="s167">the average </span>of the target value of the k nearest neighbors. This average is then used as the prediction.</p></li><li><p style="padding-top: 5pt;padding-left: 72pt;text-indent: -18pt;line-height: 128%;text-align: left;">The second method is to take <span class="s167">the weighted average </span>of the k nearest neighbors and use their distances as the inverse weight so that closer points are weighted heavier in the prediction.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;">Deciding on the Number of Neighbors k</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">A last thing remains to be decided on, and that is how many nearest neighbors you want to include in the prediction. This is decided by the value of k. To apply this to the previous example, let’s see two different cases – one nearest neighbor and three nearest</p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;">neighbors – and see what the difference in prediction is. The two are given in Figure <span style=" color: #00F;">13-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 9pt;text-indent: 0pt;text-align: left;"><span><img width="548" height="335" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_093.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark140">Figure 13-2. </a><span class="s157">Two alternative predictions with a different number of neighbors used</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The number of neighbors to use, or <span class="s153">k</span>, is a hyperparameter in the kNN model, and it is best optimized using hyperparameter tuning. The method that you’ve seen in this book is a grid search CV, which is one of the go-to methods for hyperparameter tuning. In this chapter, you’ll also discover an alternative method: random search. Let’s first introduce an example to make it more applied.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Predicting Traffic Using kNN</p><p class="s68" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a href="https://archive.ics.uci.edu/ml/datasets/Metro%2BInterstate%2BTraffic%2BVolume" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">For this example, you’ll be working with a dataset of hourly traffic volumes from the Interstate 94. You can find this dataset on the UCI machine learning repository: </a><a href="https://archive.ics.uci.edu/ml/datasets/Metro%2BInterstate%2BTraffic%2BVolume" class="s32" target="_blank">https:// </a>archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume<span class="p">.</span></p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">If you download this data, you’ll find that it has hourly traffic volume, together with some weather data and information about holidays. For the present example, let’s avoid depending on weather data and do a forecast based on seasonality and holidays. After all, we could imagine that traffic depends heavily on the time of day, weekdays, and holidays.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><a name="bookmark141">To import the data into Python, you can use the code in Listing </a><span style=" color: #00F;">13-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 13-1. <span class="s33">Import the traffic data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = pd.read_csv(&#39;Metro_Interstate_Traffic_Volume.csv.gz&#39;, compression=&#39;gzip&#39;)</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">After you’ve imported this data, let’s create the seasonality variables that seem necessary for the modeling exercise for this example. Use the code in Listing <span style=" color: #00F;">13-2 </span>to create the following variables:</p><ul id="l45"><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Year</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Month</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Weekday</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Hour</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">IsHoliday</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 13-2. <span class="s33">Feature engineering to create the additional explanatory variables</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data[&#39;year&#39;] = data[&#39;date_time&#39;].apply(lambda x: x[:4]) data[&#39;month&#39;] = data[&#39;date_time&#39;].apply(lambda x: x[5:7]) data[&#39;weekday&#39;] = pd.to_datetime(data[&#39;date_time&#39;]).apply(lambda x: x.weekday())</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data[&#39;hour&#39;] = pd.to_datetime(data[&#39;date_time&#39;]).apply(lambda x: x.hour) data[&#39;isholiday&#39;] = (data[&#39;holiday&#39;] == &#39;None&#39;).apply(float)</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The next step is to split the data into train and test and fit a default kNN model. This will get you a first feel of the R2 that could be obtained with this type of model. This code is shown in Listing <span style=" color: #00F;">13-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 13-3. <span class="s33">Creating the train-test split and computing the R2 of the default model</span></p><p class="s34" style="padding-top: 6pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Create objects X and y</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X = data[[&#39;year&#39;, &#39;month&#39;, &#39;weekday&#39;, &#39;hour&#39;, &#39;isholiday&#39;]] y = data[&#39;traffic_volume&#39;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># Create Train test split</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import train_test_split</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=12345, shuffle=False)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.neighbors import KNeighborsRegressor my_dt = KNeighborsRegressor()</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 170%;text-align: left;">my_dt.fit(X_train, y_train) fcst = my_dt.predict(X_test)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import r2_score print(r2_score(list(y_test), list(fcst)))</p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">As a positive surprise, the R2 score on this model is very good already: <span class="s167">0.969. </span>This should mean that the forecast is not far from perfect, so let’s try to verify this visually using the code in Listing <span style=" color: #00F;">13-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 13-4. <span class="s33">Creating a plot on the data of the test set</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt plt.figure(figsize=(20,20)) plt.plot(list(y_test)) plt.plot(list(fcst)) plt.legend([&#39;actuals&#39;, &#39;forecast&#39;]) plt.ylabel(&#39;Traffic Volume&#39;) plt.xlabel(&#39;Steps in test data&#39;) plt.show()</p><p style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">This code will generate the graph that is shown in Figure <span style=" color: #00F;">13-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;"><span><img width="464" height="442" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_094.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark142">Figure 13-3. </a><span class="s157">The plot shows the predictive performance on the test dataset</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Grid Search on kNN</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 125%;text-align: left;">The number of neighbors that the scikit-learn implementation uses by default is five. Now as a last step, let’s try to add a grid search to see whether a different number of neighbors might get us a better performance. Let’s test the number of neighbors by increments of 2, in order to speed it up a little. This is done in Listing <span style=" color: #00F;">13-5</span>.</p><p class="s28" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 13-5. <span class="s33">Adding a grid search cross-validation to the kNN model</span></p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 24pt;text-align: left;">from sklearn.model_selection import GridSearchCV my_knn = GridSearchCV(KNeighborsRegressor(),</p><p class="s34" style="padding-left: 114pt;text-indent: 5pt;line-height: 113%;text-align: left;">{&#39;n_neighbors&#39;:[2, 4, 6, 8, 10, 12]}, scoring = &#39;r2&#39;, n_jobs = -1)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark143">my_knn.fit(X_train, y_train)</a></p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(r2_score(list(y_test), list(my_knn.predict(X_test)))) print(my_knn.best_estimator_)</p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The R2 score obtained with this tuned model is <span class="s167">0.9695</span>. This model uses eight nearest neighbors rather than five, and it is better by 0.0002 points on the R2 score. This is very little, and therefore the improvement can hardly be considered existent. In this case, the default model was actually quite good from the start.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Random Search: An Alternative to Grid Search</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">As a last topic for this chapter, I want to introduce an alternative to grid search. This alternative is called random search. Random search is very easy to understand intuitively: rather than checking every combination of hyperparameters on the grid, it will check a random selection of hyperparameters.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The main reason for this is that it is much faster. Also, it has been found that random search is generally able to obtain results that are very close to those of grid search, and the speed improvement is therefore totally worth it. Listing <span style=" color: #00F;">13-6 </span>shows how to replace a grid search by a random search.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 13-6. <span class="s33">Adding a random search cross-validation to the kNN model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import RandomizedSearchCV</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">my_knn = RandomizedSearchCV(KNeighborsRegressor(),</p><p class="s34" style="padding-top: 1pt;padding-left: 96pt;text-indent: 5pt;line-height: 113%;text-align: left;">{&#39;n_neighbors&#39;:list(range(1, 20))}, scoring = &#39;r2&#39;, n_iter=10, n_jobs = -1)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">my_knn.fit(X_train, y_train)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(r2_score(list(y_test), list(my_knn.predict(X_test)))) print(my_knn.best_estimator_)</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">The random search has been allowed to choose any number for k between 1 and 20. The argument <span class="s167">n_iter </span>decides on the number of values that should be <span class="s167">randomly selected </span>within this range. Note that the selection is random, so this may give a different result than the grid search. There is no way to guarantee that a certain value will be included in the test.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">To make a fair comparison with the grid search, we apply a n_iter of 6. As there are six values tested in the grid search, this makes it equivalent. When executing the code, the returned solution is exactly the same as the one returned by grid search.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4></li><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The kNN model bases its predictions on the values of its nearest neighbors.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The neighbors’ values are combined into a prediction by computing the average or a weighted average based on their distance.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The Euclidean distance is generally used for the distance measure.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The number of neighbors to use in the combination is denoted k and is a hyperparameter to the model.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The value of k can be optimized using hyperparameter tuning, for example, through grid search cross-validation.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Random search is an alternative to grid search that is faster and generally gives results that are (almost) as good.</p></li></ul><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark144">CHAPTER 14</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The Random Forest</h1><p style="padding-top: 21pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this chapter, you will discover the <span class="s19">Random Forest model</span>. It is an easy-to-use model, and it is known to be very performant. The Random Forest and the XGBoost model, which you will discover in the next chapter, are two of the most used machine learning algorithms in modern applications.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">A large number of variants on Random Forests and XGBoost exist on the market, yet if you understand the two basics, it will be relatively easy to adapt to any variant.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Intuitive Idea Behind Random Forests</h4><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 128%;text-align: left;">The Random Forest is <span class="s20">strongly based on the Decision Tree model </span>but adds more complexity to it. As the name suggests, a Random Forest consists of a large number of Decision Trees, each of them with a slight variation.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">A Random Forest is much more performant than a Decision Tree. Generally, a Random Forest can combine hundreds or even thousands of Decision Tree models. They will be fitted on slightly different data, as to not be totally equal. So, in short, it’s a large number of Decision Trees making predictions that should be close to each other, yet not exactly the same.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 126%;text-align: left;">Where one machine learning model can sometimes be wrong, the average prediction of a large number of machine learning models is less likely to be wrong. This idea is the foundation of <span class="s19">ensemble learning</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In the Random Forest, ensemble learning is applied to a repetition of many Decision Trees. Ensemble learning can be applied to any combination of a large number of machine learning models. The reason to use Decision Trees is that it has been proven a performant and easy-to-configure model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_14#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_14#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_14</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;">179</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark145">Random Forest Concept 1: Ensemble Learning</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">So how does ensemble learning work exactly? You can understand that having 1000 times the exact same Decision Tree does not have any added value to just using one time this Decision Tree.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 122%;text-align: left;">In the ensemble model, each of the individual models has to be slightly different from another. There are two famous methods for creating ensembles: <span class="s19">bagging </span>and <span class="s19">boosting</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The Random Forest uses bagging to create an ensemble of Decision Trees. In the next chapter, you’ll discover the XGBoost algorithm, which uses the alternative technique called boosting.</p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">Let’s discover the idea behind bagging. Bagging is short for Bootstrap Aggregation.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The idea exists in two parts:</p><ol id="l46"><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -18pt;text-align: left;">Bootstrap</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -18pt;text-align: left;">Aggregation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Bagging Concept 1: Bootstrap</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 125%;text-align: left;">The <span class="s19">bootstrap </span>is one of the essential parts of the algorithm that makes sure that each of the individual learners fits a slightly different Decision Tree. Bootstrapping means that for each individual learner, the dataset is created by a <span class="s19">resampling process</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">Resampling means creating a new dataset based on the original dataset. The new dataset will be created by randomly selecting data points from the original dataset. Yet the important thing to realize here is that the resampling is done <span class="s19">with replacement</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Resampling with replacement puts every sampled data point back into the mother population. This makes it possible for a single data point in the original dataset to be selected multiple times in the bootstrapped dataset. There will also be data points in the original data that are not selected for the bootstrapped dataset. A schematic drawing is shown in Figure <span style=" color: #00F;">14-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: left;"><span><img width="548" height="334" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_095.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark146">Figure 14-1. </a><span class="s29">The bootstrap</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Bagging Concept 2: Aggregation</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 125%;text-align: left;">The <span class="s19">aggregation </span>part describes the fact of using multiple learners. In the Random Forest, the bootstrap is executed many times. The exact number is defined by a hyperparameter called <span class="s19">n_estimators</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">For each n_estimators, a Decision Tree is built using a bootstrapped dataset. This will generate a large number of Decision Trees that are all slightly different due to the differences in the datasets. In the end, you can use each of the Decision Trees to make a prediction. However, you would end up with many slightly different predictions.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The solution to this problem is in the aggregation part. To combine all the individual Decision Tree predictions into one Random Forest prediction, you simply take the average of all the individual predictions. It is relatively straightforward, yet a crucial part of the algorithm.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The idea behind this is that the aggregation of many weak learners will result in errors that average each other out. This makes the Random Forest a very performant machine learning algorithm.</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark147">Random Forest Concept 2: Variable Subsets</a></h4><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 123%;text-align: left;">Besides the bootstrapping approach, the Random Forest has a <span class="s19">second process </span>in place to make sure that the individual learners are not the same. This process does not apply to the data points that are or are not used, but rather <span class="s19">applies to the variables that are or are not used</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">As explained in Chapter <span style=" color: #00F;">12</span>, the Decision Tree checks each of the variables to find the best split to add to the tree. Yet when using a subset of each variable, you only let the tree check out the splits in a randomly selected number of variables. The exact quantity is defined by a hyperparameter called <span class="s19">mtry </span>in most mathematical descriptions and called <span class="s19">max_features </span>in scikit-learn. Although this can sometimes cause a selection of a suboptimal split, it does help to make the individual Decision Trees different from each other.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 124%;text-align: left;">This value is generally around <span class="s19">80%</span>, but it can be higher or lower. Together, <span class="s19">n_estimators </span>and <span class="s19">max_features </span>are the main hyperparameters of the Random Forest model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Predicting Sunspots Using a Random Forest</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 126%;text-align: left;">Let’s develop an example of the Random Forest model. For this example, you’ll again take the sunspot data that you’ve used in Chapter <span style=" color: #00F;">5</span>. The ARMA model in Chapter <span style=" color: #00F;">5 </span>was able to obtain an R2 of <span class="s19">0.84</span>. This time let’s try to forecast the monthly number</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">of sunspots rather than the yearly sum as was done in the previous example. You can import the data using Listing <span style=" color: #00F;">14-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 14-1. <span class="s33">Importing the data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">data = pd.read_csv(&#39;Ch05_Sunspots_database.csv&#39;) data = data.iloc[:,[1,2]]</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now it is necessary to add some feature engineering. Let’s add the variables Year and Month to account for seasonality and the lagged versions of the target variable for the past 12 months. This task can be expected to be a bit harder, as there is a more detailed variation that the model needs to learn. The code in Listing <span style=" color: #00F;">14-2 </span>shows you how this can be done.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 14-2. <span class="s33">Feature engineering</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Seasonality variables</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data[&#39;Date&#39;] = pd.to_datetime(data[&#39;Date&#39;]) data[&#39;Year&#39;] = data[&#39;Date&#39;].apply(lambda x: x.year) data[&#39;Month&#39;] = data[&#39;Date&#39;].apply(lambda x: x.month)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Adding a year of lagged data</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data[&#39;L1&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(1) data[&#39;L2&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(2) data[&#39;L3&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(3) data[&#39;L4&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(4) data[&#39;L5&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(5) data[&#39;L6&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(6) data[&#39;L7&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(7) data[&#39;L8&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(8) data[&#39;L9&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(9) data[&#39;L10&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(10) data[&#39;L11&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(11) data[&#39;L12&#39;] = data[&#39;Monthly Mean Total Sunspot Number&#39;].shift(12)</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now that you have got a dataset, let’s do a train-test split and fit the Random Forest with the default hyperparameters. Use the code in Listing <span style=" color: #00F;">14-3 </span>to fit this default model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 14-3. <span class="s33">Fitting the default Random Forest regressor</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Create X and y object data = data.dropna()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">y = data[&#39;Monthly Mean Total Sunspot Number&#39;]</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X = data[[&#39;Year&#39;, &#39;Month&#39;, &#39;L1&#39;, &#39;L2&#39;, &#39;L3&#39;, &#39;L4&#39;, &#39;L5&#39;, &#39;L6&#39;, &#39;L7&#39;, &#39;L8&#39;, &#39;L9&#39;, &#39;L10&#39;, &#39;L11&#39;, &#39;L12&#39;]]</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Create Train test split</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import train_test_split</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=12345, shuffle=False)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.ensemble import RandomForestRegressor my_rf = RandomForestRegressor()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark148">my_rf.fit(X_train, y_train) fcst = my_rf.predict(X_test)</a></p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import r2_score r2_score(list(y_test), list(fcst))</p><p style="padding-top: 8pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">Using this model, you will obtain an R2 score of the prediction of <span class="s19">0.863</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Grid Search on the Two Main Hyperparameters of the Random Forest</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">This is already a great result, but as always, let’s see if that can be optimized using a grid search hyperparameter tuning. This will be shown in Listing <span style=" color: #00F;">14-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 14-4. <span class="s33">Fitting the Random Forest regressor with hyperparameter tuning</span></p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 24pt;text-align: left;">from sklearn.model_selection import GridSearchCV my_rf = GridSearchCV(RandomForestRegressor(),</p><p class="s34" style="padding-left: 101pt;text-indent: 0pt;text-align: left;">{&#39;max_features&#39;:[0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95],</p><p class="s34" style="padding-top: 1pt;padding-left: 101pt;text-indent: 0pt;text-align: left;">&#39;n_estimators&#39;: [10, 50, 100, 250, 500, 750, 1000]},</p><p class="s34" style="padding-top: 1pt;padding-left: 96pt;text-indent: 0pt;text-align: left;">scoring = &#39;r2&#39;, n_jobs = -1)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">my_rf.fit(X_train, y_train)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(r2_score(list(y_test), list(my_rf.predict(X_test)))) print(my_rf.best_params_)</p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 121%;text-align: left;">This will obtain an R2 score of <span class="s19">0.870</span>, slightly better than the default version. The optimal hyperparameters that have been identified are max_features (mtry) of <span class="s19">0.7 </span>and n_estimators (ntrees) of <span class="s19">750</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">It would be useful now to have a look at the performance plot to see how it looks like on the test data. This can be done using Listing <span style=" color: #00F;">14-5</span>, and this will obtain Figure <span style=" color: #00F;">14-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 14-5. <span class="s33">Obtaining the plot of the forecast on the test data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt plt.plot(list(fcst)) plt.plot(list(y_test))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark149">plt.legend([&#39;fcst&#39;, &#39;actu&#39;]) plt.ylabel(&#39;Sunspots&#39;) plt.xlabel(&#39;Steps into the test data&#39;) plt.show()</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="389" height="285" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_096.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 14-2. <span class="s29">Predictive plot of sunspots per month</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Random Search CV Using Distributions</h4><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">As an additional step in this chapter, let’s go a bit deeper into the <span class="s19">random search CV </span>that you discovered in the previous chapter. As you’ll understand, the more complicated the models become, the heavier the grid search becomes. Alternatives become more and more interesting. You’ll discover more methods for hyperparameter tuning in the next chapters.</p><p style="padding-left: 43pt;text-indent: 0pt;line-height: 12pt;text-align: left;">In this application of random search, you’ll use <span class="s19">distributions </span>rather than a</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">list of possible values to specify which values you want to test for the different hyperparameters. This is a great functionality of RandomizedSearchCV, especially when you have an idea of what the most likely value for your hyperparameters is.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s start by identifying the distributions for max_features and n_estimators and then put them into the RandomizedSearchCV.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark150">Distribution for max_features</a></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 125%;text-align: left;">For the value of max_features, you previously tested values between 0.65 and 0.95 with steps of 0.05. The most used distribution is the <span class="s19">normal distribution</span>, so let’s see how we could use the normal distribution for this.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can use the code in Listing <span style=" color: #00F;">14-6 </span>to try out different normal distributions and check out whether they fit. The normal distribution is defined by a mean and a standard deviation, so those are the two values that you can play around with.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 14-6. <span class="s33">Testing out a normal distribution for the max_features</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import numpy as np</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import scipy.stats as stats import math</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">mu = 0.8</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">variance = 0.005</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">sigma = math.sqrt(variance)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100) plt.plot(x, stats.norm.pdf(x, mu, sigma)) plt.show()</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As shown in Figure <span style=" color: #00F;">14-3</span>, the normal distribution with a mean of 0.8 and a standard deviation of 0.05 covers the range of 0.65–1.0 quite well. The x-axis in this graph shows the value for max_features, and the y-axis shows the corresponding probability for this value being sampled into the random search.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span><img width="343" height="254" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_097.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark151">Figure 14-3. </a><span class="s29">Normal distribution with mean 0.8 and st. dev. 0.05</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">As those values seem quite appropriate for max_features, we’ll use this distribution in the RandomizedSearchCV.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 25pt;text-indent: 0pt;text-align: justify;">Distribution for n_estimators</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">For the distribution of n_estimators, let’s try something different. Let’s say that we want to test values between 50 and 1000 but that we don’t really have a good idea of what the actual value for n_estimators should be.</p><p style="padding-left: 43pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">This means that a <span class="s19">uniform distribution </span>may be appropriate: in a uniform</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;">distribution, you specify a minimum and a maximum, and all values in between are</p><p style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;"><span class="s19">equally probable </span>to be selected. This is shown in Listing <span style=" color: #00F;">14-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;">Listing 14-7. <span class="s33">Testing out a uniform distribution for the n_estimators</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;">x = np.linspace(0, 2000, 100)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(x, stats.uniform.pdf(x, 50, 950)) plt.show()</p><p style="padding-top: 9pt;padding-left: 43pt;text-indent: 0pt;text-align: justify;">The resulting plot is shown in Figure <span style=" color: #00F;">14-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="351" height="246" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_098.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark152">Figure 14-4. </a><span class="s29">Uniform distribution with min 50 and max 1000</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 123%;text-align: left;">Yet there is a small problem. The uniform distribution returns a floating number, while the n_estimators can only take an <span class="s19">integer</span>. The better alternative for a uniform integer is to use the function <span class="s19">scipy.stats.randint</span>, which returns a random integer between a minimum and a maximum.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Fitting the RandomizedSearchCV</p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s now fit the RandomizedSearchCV using the code in Listing <span style=" color: #00F;">14-8</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 14-8. <span class="s33">RandomizedSearchCV with two distributions</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import RandomizedSearchCV</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Specifying the distributions to draw from distributions = {</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;max_features&#39;: stats.norm(0.8, math.sqrt(0.005)), &#39;n_estimators&#39;: stats.randint(50, 1000)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">}</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># Creating the search</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">my_rf = RandomizedSearchCV(RandomForestRegressor(),</p><p class="s34" style="padding-top: 9pt;padding-left: 35pt;text-indent: 0pt;text-align: center;">distributions, n_iter=10,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 169pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark153">scoring = &#39;r2&#39;, n_jobs = -1,</a></p><p class="s34" style="padding-left: 169pt;text-indent: 0pt;line-height: 14pt;text-align: left;">random_state = 12345)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Fitting the search</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">my_rf.fit(X_train, y_train)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Printing the results</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">print(r2_score(list(y_test), list(my_rf.predict(X_test)))) print(my_rf.best_params_)</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">The results that you should obtain are an R2 of <span class="s19">0.86</span>, and the selected hyperparameters are listed in the following. Be aware that due to randomness, you may obtain slightly different results:</p><ul id="l47"><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">‘max_features’: 0.70</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">‘n_estimators’: 819</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The R2 is slightly lower than the one obtained by the grid search. Yet you should observe a very strong increase in execution time. The RandomizedSearchCV is much faster while only losing a very slight amount of performance: a great argument for using RandomizedSearchCV rather than GridSearchCV.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Interpretation of Random Forests: Feature Importance</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">As you have understood by now, the Random Forest is a relatively complex model. When making a prediction, it combines the predictions of many Decision Trees. This gives the model great performance, but it also makes the model relatively difficult to interpret.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">For the Decision Tree model, you have seen how you can plot the tree and follow its decisions based on a new observation. For the Random Forest, you would need to do this many times, making for a very difficult process.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Luckily, an alternative exists for interpreting the Random Forest. This method is called feature importance.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The feature importance of each variable is computed all the way throughout the fitting of the Random Forest. Every time that a variable is used for a split, the error reduction that is brought about by this split is added to the variable’s feature importance. At the end of the algorithm, those added-up errors are standardized so that the sum of the variable importance for all variables is 1.0.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark154">The higher the future importance of a variable, the more important the role it has played in the model and therefore the more predictive value it has for your forecast.</a></p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">You can obtain the feature importances from a Random Forest using Listing <span style=" color: #00F;">14-9</span>.</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 129%;text-align: left;">Note that the Random Forest here is the one fit with RandomizedSearchCV, which is why you call best_estimator first. You also see how to combine the array of feature importances into a more accessible dataframe.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 14-9. <span class="s33">Feature importances</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">fi = pd.DataFrame({</p><p class="s34" style="padding-top: 1pt;padding-left: 52pt;text-indent: 0pt;text-align: left;">&#39;feature&#39;: X_train.columns,</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 44pt;line-height: 170%;text-align: left;">&#39;importance&#39;: my_rf.best_estimator_.feature_importances_}) fi.sort_values(&#39;importance&#39;, ascending=False)</p><p style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">The result looks as shown in Figure <span style=" color: #00F;">14-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 158pt;text-indent: 0pt;text-align: left;"><span><img width="151" height="342" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_099.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 14-5. <span class="s29">Feature importances of the Random Forest</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark155">As you see, the most important feature here is the L1: the 1-lag autoregressive component. The following importances are the other lags. We can understand from this that the model is mainly learning an autoregressive effect with the more recent lags being more important. This is an interesting learning for understanding the model. In some cases, this information can also help you in improving the model even further by</a></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">improving the selection of your variables, or it can give you hints on how to improve your feature engineering.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4></li><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The Random Forest adds bagging to the Decision Tree model.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Bagging is an ensemble method: it fits the same model many times and takes their average as a prediction.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The Random Forest has two main hyperparameters:</p><ul id="l48"><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">The number of trees to use in the forest</p></li><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;line-height: 129%;text-align: left;">The number of randomly selected variables to use in each Decision Tree split</p></li></ul></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Random search is a faster way to tune hyperparameters and often gives results that are not much worse than grid search. You can give probability distributions for each hyperparameter to tune, and the random search will do a certain number of draws in those distributions.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Variable importance is a way to interpret the results of the Random Forest model. It can also hint at how to improve the model further.</p></li></ul></li></ol><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark156">CHAPTER 15</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Gradient Boosting with XGBoost and LightGBM</h1><p style="padding-top: 21pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this chapter, you will discover the <span class="s167">gradient boosting model</span>. In the previous chapter, you discovered the idea behind ensemble methods. As a recap, ensemble methods make powerful predictions by combining predictions of numerous small, less performant models.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Boosting: A Different Way of Ensemble Learning</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Gradient boosting combines numerous small Decision Tree models to make predictions. Of course, those small decision trees are different from each other; else, there wouldn’t be any advantage of using a larger number of them.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 127%;text-align: left;">The important concept to understand here is how those decision trees come to be different from each other. This is achieved by a process called <span class="s167">boosting</span>. Boosting and bagging, which you’ve seen in the previous chapter, are the principal two methods of ensemble learning.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 125%;text-align: left;">Boosting is an <span class="s167">iterative process</span>. It adds more and more weak learners to the ensemble model in an intelligent way. In each step, the individual data points are weighted. Data points that are already predicted well will not be important for the learner to be added. The new weak learners will therefore <span class="s20">focus on learning the things that are not yet understood and therefore improve the ensemble</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can see a schematic overview of the boosting process in Figure <span style=" color: #00F;">15-1</span>. With this approach, you iteratively fit weak models that are focusing on the parts of the data that are not yet understood. While doing this, you keep all the intermediate weak models. The ensemble model is the combination of all those weak learners.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_15#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_15#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_15</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">193</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="221" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_100.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark157">Figure 15-1. </a><span class="s29">The boosting process</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The Gradient in Gradient Boosting</h4><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 128%;text-align: left;">This iterative process is called <span class="s20">gradient </span>boosting for a reason. A gradient is a mathematical term that refers to the vector field of partial derivatives that point in the direction of the steepest slope. In simple terms, we often compare gradients to slopes of uphill roads: the higher the slope, the steeper the hill. Gradients are computed by taking derivatives, or partial derivatives, of a function.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: justify;">In gradient boosting, when adding additional trees to the model, the goal is to add a tree that best explains the variation that was not explained by the previous trees. The target of your new tree is therefore</p><p class="s42" style="padding-top: 8pt;padding-left: 25pt;text-indent: 0pt;text-align: center;">y <span class="s59">− </span>y<span class="s57">ˆ</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This can be denoted rewritten as the negative partial derivative of the loss function against the y predictions:</p><p class="s62" style="padding-top: 7pt;padding-left: 25pt;text-indent: 0pt;line-height: 18pt;text-align: center;">y <span class="s59">- </span>y<span class="s57">ˆ   </span><span class="s59">= -</span><span class="s61"> </span><span class="s169">a</span><u>L</u></p><p class="s59" style="padding-left: 52pt;text-indent: 0pt;line-height: 11pt;text-align: center;">a<span class="s62">y</span><span class="s57">ˆ</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">You set this as the target for the new tree to ensure that adding the tree will explain a maximum amount of additional variation in the overall gradient boosting model. This explains why the model is called <span class="s20">gradient </span>boosting.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark158">Gradient Boosting Algorithms</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">There are many algorithms that each perform slightly different versions of gradient boosting. When the gradient boosting approach was first invented, the algorithms were not very performant, but that changed with the AdaBoost algorithm: the first algorithm that could adapt to weak learners.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Gradient boosting algorithms are among the most performant machine learning tools on the market. After AdaBoost, a long list of slightly different boosting algorithms has been added to the literature, including XGBoost, LightGBM, LPBoost, BrownBoost, MadaBoost, LogitBoost, and TotalBoost. There are still many contributions being made to improve on gradient boosting theory. In this current chapter, two algorithms will be covered: XGBoost and LightGBM.</p><p class="s167" style="padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">XGBoost <span class="p">is one of the most used machine learning algorithms. XGBoost is a quick way to get good performances. As it is easy to use and very performant, it is the first go-to algorithm for many ML practitioners.</span></p><p class="s167" style="padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">LightGBM <span class="p">is another gradient boosting algorithm that is important to know. For the moment, it is a bit less widespread than XGBoost, but it is seriously gaining in popularity. The expected advantage of LightGBM over XGBoost is a gain in speed and memory use.</span></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In this chapter, you will discover the implementations of both of those gradient boosting algorithms.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">The Difference Between XGBoost and LightGBM</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">If you’re going to use those two gradient boosting algorithms, it is important to understand in what way they differ. This can also give you an insight into the types of difference that make for such a large number of models on the market.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The difference here is in the way they identify the best splits inside the weak leaners (the individual decision trees). Remember that the splitting in a decision tree is the moment where your tree needs to find the split that most improves the model.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The intuitively simplest idea for finding the best split is to loop through all the potential fits and find the best one. Yet this takes a lot of time, and better alternatives have been proposed by recent algorithms.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">The alternative proposed by XGBoost is to use a <span class="s167">histogram-based splitting</span>. In this case, rather than looping through all possible splits, the model builds histograms of each of the variables and uses those to find the best split per variable. The best overall split is then retained.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;"><a name="bookmark159">LightGBM was invented by Microsoft, and it has an even more efficient method to define the splits. This method is called </a><span class="s167">Gradient-Based One-Side Sample (GOSS). </span>GOSS computes gradients for each of the data points and uses this to filter out data points with a low gradient. After all, data points with a low gradient are already understood well, whereas individuals with a high gradient need to be learned better. LightGBM also uses</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 128%;text-align: left;">an approach called <span class="s167">Exclusive Feature Bundling (EFB)</span>, which is a feature that allows for a speed-up when having many correlated variables to choose from.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Another difference is that the LightGBM model fits leaf-wise (best-first) tree growth, whereas XGBoost grows the trees tree-wise. You can see the difference in Figure <span style=" color: #00F;">15-2</span>.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">This difference is a feature that would theoretically favor LightGBM in terms of accuracy, yet it comes at a higher risk of overfitting in the case of little data available.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span><img width="460" height="450" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_101.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 15-2. <span class="s29">Leaf-wise growth vs. level-wise growth</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 117%;text-align: left;"><a href="http://www.microsoft.com/en-us/research/publication/lightgbm-a-highly-efficient-gradient-boosting-decision-tree/" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank" name="bookmark160">For more details on the differences between the gradient boosting algorithms, you can check out the paper by Microsoft (</a><a href="http://www.microsoft.com/en-us/research/publication/lightgbm-a-highly-efficient-gradient-boosting-decision-tree/" class="s32" target="_blank">www.microsoft.com/en-us/research/</a></p><p class="s68" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">publication/lightgbm-a-highly-efficient-gradient-boosting-decision-tree/<span class="p">).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Forecasting Traffic Volume with XGBoost</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: left;">For this chapter, we’ll be using the same dataset as the one used for the kNN model: interstate traffic. In the previous example, we were able to obtain an R2 of <span class="s167">0.9695</span>.</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Gradient boosting is a more performant algorithm, so let’s see whether we can use it to improve on this already great score.</p><p style="padding-left: 43pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="https://archive.ics.uci.edu/ml/datasets/Metro%2BInterstate%2BTraffic%2BVolume" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">You can find this dataset on the UCI machine learning repository: </a><a href="https://archive.ics.uci.edu/ml/datasets/Metro%2BInterstate%2BTraffic%2BVolume" class="s32" target="_blank">https://archive.</a></p><p style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 124%;text-align: left;"><span class="s68">ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume</span>. You can import the data using Listing <span style=" color: #00F;">15-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 15-1. <span class="s33">Importing the data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">data = pd.read_csv(&#39;Metro_Interstate_Traffic_Volume.csv&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">To make a fair benchmark between the gradient boosting models and the kNN model, let’s do the exact same feature engineering as the one applied in Chapter <span style=" color: #00F;">13</span>. This feature engineering is shown in Listing <span style=" color: #00F;">15-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 15-2. <span class="s33">Applying the same feature engineering as done previously for the kNN model</span></p><p class="s34" style="padding-top: 6pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data[&#39;year&#39;] = data[&#39;date_time&#39;].apply(lambda x: int(x[:4])) data[&#39;month&#39;] = data[&#39;date_time&#39;].apply(lambda x: int(x[5:7])) data[&#39;weekday&#39;] = pd.to_datetime(data[&#39;date_time&#39;]).apply(lambda x: x.weekday())</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">data[&#39;hour&#39;] = pd.to_datetime(data[&#39;date_time&#39;]).apply(lambda x: x.hour) data[&#39;isholiday&#39;] = (data[&#39;holiday&#39;] == &#39;None&#39;).apply(float)</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s now do a first test with XGBoost, using Listing <span style=" color: #00F;">15-3</span>. You use the XGBoost package for this.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_102.png"/></span></p><p class="s38" style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark161">Note </a><span class="s39">installing libraries using python can sometimes be a pain due to the need for different dependencies. if you have trouble installing those boosting</span></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><a href="https://colab.research.google.com/" style=" color: #2B2A29; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt;" target="_blank">libraries, you could check out Google Colaboratory notebooks (</a><a href="https://colab.research.google.com/" class="s171" target="_blank">https://colab.</a></p><p class="s39" style="padding-left: 15pt;text-indent: 0pt;line-height: 110%;text-align: justify;"><span style=" color: #00F; font-family:SimSun; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt;">research.google.com</span>) or Kaggle notebooks (kaggle.com/notebooks), which are free notebook environments that have all the modern libraries for machine learning at the ready for you.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_103.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 15-3. <span class="s33">Applying the default XGBoost model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># Create objects X and y</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">X = data[[&#39;year&#39;, &#39;month&#39;, &#39;weekday&#39;, &#39;hour&#39;, &#39;isholiday&#39;]] y = data[&#39;traffic_volume&#39;]</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># Create Train test split</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import train_test_split</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=12345, shuffle=False)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from xgboost import XGBRegressor my_xgb = XGBRegressor() my_xgb.fit(X_train, y_train)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">xgb_fcst = my_xgb.predict(X_test)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import r2_score print(r2_score(list(y_test), list(xgb_fcst)))</p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">This results in an R2 score of <span class="s167">0.920. </span>This is quite less good than the 0.9695 that was obtained using kNN.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark162">Forecasting Traffic Volume with LightGBM</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now let’s do the same thing with LightGBM and see how that compares using Listing <span style=" color: #00F;">15-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 15-4. <span class="s33">Applying the default LightGBM model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from lightgbm import LGBMRegressor my_lgbm = LGBMRegressor() my_lgbm.fit(X_train, y_train)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">lgbm_fcst = my_lgbm.predict(X_test) print(r2_score(list(y_test), list(lgbm_fcst)))</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The R2 score of LightGBM is <span class="s167">0.972. </span>This is much better than the default XGBoost model, and it is also better than the tuned kNN model. Let’s create a graph to see where the XGBoost model and the LightGBM model predicted something different. You can use Listing <span style=" color: #00F;">15-5 </span>to create the graph. The graph should be the one in Figure <span style=" color: #00F;">15-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 15-5. <span class="s33">Create a graph to compare the XGBoost and LightGBM forecast to the actuals</span></p><p class="s34" style="padding-top: 6pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt plt.figure(figsize=(20,10)) plt.plot(list(y_test)) plt.plot(list(xgb_fcst)) plt.plot(list(lgbm_fcst))</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;actual&#39;, &#39;xgb forecast&#39;, &#39;lgbm forecast&#39;]) plt.ylabel(&#39;Traffic Volume&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.xlabel(&#39;Steps in test data&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;"><span><img width="555" height="401" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_104.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark163">Figure 15-3. </a><span class="s29">Comparing the two gradient boosting models</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As you can see in this graph, the difference in the two model performances is striking: XGBoost is seriously off at certain places. This indicates that there is some information that it was not able to learn.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Hyperparameter Tuning Using Bayesian Optimization</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">So LightGBM is better for now, but we want to tune the two models to be able to see how the tuned versions perform. This could make a big difference.</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Rather than using GridSearchCV, we’ll be using <span class="s167">Bayesian optimization </span>here. At this</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">stage, you should be comfortable with the two approaches for hyperparameter tuning that were presented earlier: GridSearchCV and RandomizedSearchCV.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark164">The Theory of Bayesian Optimization</a></p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In Bayesian optimization, rather than trying out every combination of hyperparameters or just trying out random guesses, it allows you to make intelligent guesses by modeling the likelihood of a certain value for the hyperparameters to be the optimum. This is done by modeling a probability distribution around the hyperparameters.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The concept is shown in Figure <span style=" color: #00F;">15-4</span>. The picture shows only one hyperparameter, but it can be projected onto a situation with multiple hyperparameters. Bayesian optimization will start sampling a number of values of the hyperparameter and observe the model performance at those points.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 126%;text-align: left;">The goal is not to randomly fall on one value that performs well, but rather to estimate the <span class="s20">gray zone around the curve</span>. Some values of the hyperparameter will give better performance than others. While testing values, Bayesian optimization makes a model of the probability distribution of the optimum being at this location.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In this way, Bayesian optimization is much more <span class="s167">intelligent </span>than the two tuning methods that you have seen until here. It can make intelligent, probability-based guesses about the points that should be tested next. This makes it much more powerful for optimizing hyperparameters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 91pt;text-indent: 0pt;text-align: left;"><span><img width="378" height="250" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_105.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 15-4. <span class="s29">Bayesian optimization</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark165">Bayesian Optimization Using scikit-optimize</a></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">In this part, you will see how to use the scikit-optimize package to apply a Bayesian hyperparameter tuning to XGBoost and LightGBM. After that, you’ll do a comparison of the two models. Let’s start with XGBoost in Listing <span style=" color: #00F;">15-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 15-6. <span class="s33">Applying Bayesian optimization to XGBoost</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from skopt import BayesSearchCV</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from skopt.space import Real, Categorical, Integer import random</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">random.seed(0)</p><p class="s34" style="padding-top: 9pt;padding-left: 30pt;text-indent: -22pt;line-height: 113%;text-align: left;">xgb_opt = BayesSearchCV( XGBRegressor(),</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 14pt;text-align: left;">{</p><p class="s34" style="padding-top: 1pt;padding-left: 52pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;learning_rate&#39;: (10e-6, 1.0, &#39;log-uniform&#39;), &#39;max_depth&#39;: Integer(0, 50, &#39;uniform&#39;), &#39;n_estimators&#39; : (10, 1000, &#39;log-uniform&#39;),</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 14pt;text-align: left;">},</p><p class="s34" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">n_iter=10, cv=3</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">xgb_opt.fit(X_train, y_train)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">xgb_tuned_fcst = opt.best_estimator_.predict(X_test) r2_score(list(y_test), list(xgb_tuned_fcst))</p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 17pt;line-height: 124%;text-align: left;">The resulting R2 score from the tuned model is <span class="s167">0.969</span><span class="s34">. </span>Let’s continue directly with the LightGBM model, as shown in Listing <span style=" color: #00F;">15-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 15-7. <span class="s33">Applying Bayesian optimization to LightGBM</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Random.seed(0)</p><p class="s34" style="padding-top: 1pt;padding-left: 30pt;text-indent: -22pt;line-height: 113%;text-align: left;">lgbm_opt = BayesSearchCV( LGBMRegressor(),</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 14pt;text-align: left;">{</p><p class="s34" style="padding-top: 1pt;padding-left: 52pt;text-indent: 0pt;text-align: left;">&#39;learning_rate&#39;: (10e-6, 1.0, &#39;log-uniform&#39;),</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark166">&#39;max_depth&#39;: Integer(-1, 50, &#39;uniform&#39;), &#39;n_estimators&#39; : (10, 1000, &#39;log-uniform&#39;),</a></p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 14pt;text-align: left;">},</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">n_iter=10, cv=3</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">lgbm_opt.fit(X_train, y_train)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">lgbm_tuned_fcst = lgbm_opt.best_estimator_.predict(X_test) r2_score(list(y_test), list(lgbm_tuned_fcst))</p><p style="padding-top: 7pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">The R2 score from LightGBM is <span class="s167">0.973</span><span class="s34">. </span>So after using Bayesian optimization for the tuning of the two models, the scores have turned around: it is now XGBoost that wins the benchmark. As a final confirmation, let’s plot the predictions that have been made by the two models, using the code in Listing <span style=" color: #00F;">15-8</span>, and you’ll obtain the plot in Figure <span style=" color: #00F;">15-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 15-8. <span class="s33">Plotting the two tuned models</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt plt.figure(figsize=(20,10)) plt.plot(list(y_test)) plt.plot(list(xgb_tuned_fcst)) plt.plot(list(lgbm_tuned_fcst))</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;actual&#39;, &#39;xgb forecast&#39;, &#39;lgbm forecast&#39;]) plt.ylabel(&#39;Traffic Volume&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.xlabel(&#39;Steps in test data&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span><img width="531" height="265" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_106.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark167">Figure 15-5. </a><span class="s29">Compare performances of the two tuned models</span></p><p style="padding-top: 12pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In the current execution, <span class="s167">n_iter</span>, or the number of iterations, was fixed at 20. The execution with this number of iterations is relatively fast. If you want to play around with it, you could test if higher values for n_iter allow the Bayesian optimization to find even higher scores.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Conclusion</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">In conclusion, you have discovered two very performant machine learning algorithms in this chapter. You have also seen throughout this chapter how you could do a benchmark between two models. In practice, you’ll often do benchmarks between even more models. You’ll discover more on model benchmarking in the last chapter of this book.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Yet it is important to realize that model development should always be data-driven: you choose the model for which you can be confident that it has the best performances in the future.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4><ul id="l49"><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Gradient boosting is an ensemble learning technique that lets subsequent individual models from the ensemble learn on the parts that are not yet understood well by the model.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">You have seen two models of gradient boosting:</p><ul id="l50"><li><p class="s20" style="padding-top: 8pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">XGBoost<span class="p">: A more traditional method for gradient boosting</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">LightGBM<span class="p">: A newer but very performant competitor</span></p></li></ul></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Bayesian optimization is a more intelligent method for tuning hyperparameters. It estimates the probability of the optimum being on a certain location and therefore makes intelligent guesses for the optimum.</p></li></ul><p class="s21" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark168">PART V</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Advanced Machine and Deep Learning Models</h1><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark169">CHAPTER 16</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Neural Networks</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 125%;text-align: left;">In the previous five chapters, you have discovered a number of supervised machine learning models, starting from linear regression to gradient boosting. In this chapter, you’ll discover <span class="s118">Neural Networks (NNs)</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 124%;text-align: left;">The scope of Neural Networks is huge. The version that you’ll see in this chapter is a subgroup called <span class="s118">fully connected neural networks</span>. Those neural networks are intuitively quite close to the previous supervised models. In the following chapters, you’ll also discover <span class="s118">Recurrent Neural Networks</span>, and you’ll see a specific network called the <span class="s118">LSTM </span>Neural Network.</p><p style="padding-top: 1pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">The goal of this chapter is to get familiar with the general idea of Neural Networks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Fully Connected Neural Networks</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">So let’s start with a schematic overview of the model in Figure <span style=" color: #00F;">16-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_16#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_16#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_16</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;">209</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 9pt;text-indent: 0pt;text-align: left;"><span><img width="549" height="314" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_107.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark170">Figure 16-1. </a><span class="s29">Fully connected neural networks schema</span></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can read this schema from top to bottom. The model always contains an input layer and an output layer. The input layer of the network contains one node for each variable. The output layer contains the same number of nodes as there are output variables. In the case of univariate regression, there is one output variable (the target variable y), but sometimes there can be multiple output variables at the same time, as you’ve seen, for example, in the VAR model. In this case, there would be multiple nodes in the output layer.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 123%;text-align: left;">After the input, you go to the first hidden layer. Roughly speaking, the values of the hidden layer are computed by <span class="s118">multiplying the input by a weight </span>and then <span class="s118">passing the value through an activation function</span>. This combination of multiplication and activation functions repeats itself in each node until arriving at the output node. This</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">model is called a fully connected model because each node is connected to each node in the following layer. There are other shapes of architectures in which this is not the case: we’ll see some examples in the next two chapters.</p><h4 style="padding-top: 10pt;padding-left: 25pt;text-indent: 0pt;text-align: justify;"><a name="bookmark171">Activation Functions</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">When you choose the architecture of your neural network, you also need to choose the type of activation functions that you want at each location. The activation functions are applied in each node of the network, before going to the next layer.</p><p style="padding-left: 43pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Three relatively common choices of activation functions are <span class="s118">tanh</span>, <span class="s118">ReLU</span>, and</p><p style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 128%;text-align: justify;"><span class="s118">sigmoid</span>. You can see their specific behaviors in Figure <span style=" color: #00F;">16-2</span>. The reason that we use activation functions is that they allow the model to fit more complex problems with fewer nodes, as they add nonlinearity into the computation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;"><span><img width="551" height="156" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_108.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 16-2. <span class="s29">Three common activation layers</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">Without getting into too much detail on the activation layers, it is important to know how to use them. ReLU is often a good choice for starting, whereas tanh is often used for Recurrent Neural Networks that you’ll see in the next chapter.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The choice for activation layers, just like many things in fitting neural networks, is a choice that is not obvious: trial and error to obtain a working neural network is the only way to go. Sometimes, if you’re lucky, you can also find examples in the literature of neural networks that are applied to a similar case as yours. The important thing to remember is that it takes time and effort to come up with something accurate, but if you’re successful, you can obtain great performances.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: justify;">The Weights: Backpropagation</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: justify;">So while moving an input value through your neural network, you encounter something else besides the activation layer: the weights. These weights are estimated at the time of model fitting, through an algorithm called the backpropagation algorithm.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark172">The backpropagation algorithm computes the gradient of the loss function with respect to the weights of the network, one layer at a time. It then iterates backward from the last layer, which is why it is called backpropagation. It is an efficient way for fitting the huge number of weights that is needed for a neural network. Yet it is still a complex algorithm, and it takes time to compute.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Optimizers</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">The backpropagation algorithm’s performances depend on the choice of the optimizer. This is a decision that has to be made again by the modeler. There are many optimizers available. The first available optimizer was gradient descent, as described in the previous paragraph.</p><p class="s118" style="padding-left: 25pt;text-indent: 0pt;line-height: 13pt;text-align: left;">SGD<span class="p">, short for </span>Stochastic Gradient Descent<span class="p">, is an improved version of the gradient</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">descent algorithm. It is more efficient as it computes not on the whole dataset at every iteration, but only a subset of the dataset.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 124%;text-align: left;">Two improvements have been made on SGD, and those have delivered two new optimizers: <span class="s118">RMSProp </span>and <span class="s118">AdaGrad</span>. An even more improved optimizer called Adam uses a combination of those two.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Although I won’t go into the deep mathematical difference between optimizers, it is important to know that there are different optimizers available. Also, a lot of work is still ongoing, and newer and better versions may well arise in the near future. The important thing to retain here is that the choice for an optimizer is a hyperparameter to be chosen by the model developer. Playing around with different optimizers may just help you to add those last points of accuracy to your model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Learning Rate of the Optimizer</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">The next thing that you need to choose as a hyperparameter is the learning rate of your optimizer. You can see it as follows. Your optimizer is going to find the right way to move in, and it is going to take a step in that direction. Yet those steps can be either large or small steps. This depends on the learning rate that you choose.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Choosing large learning rates means that you take large steps in the right direction. Yet a risk is that you take too large steps, and therefore you step over the optimum and then miss it.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark173">A small learning rate on the other hand may let you get stuck into a local optimum and not be able to get out, as your step size is too small. You may also take a long time to converge.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Hyperparameters at Play in Developing a NN</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now until here, the overview is not much different than what we’ve seen before, for example, in Random Forests and XGBoost. Yet there is a big difference in developing Neural Networks: the number of hyperparameters and the time of training the model are so huge that it becomes impossible to simply launch a hyperparameter optimization tool.</p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;">This is all due to the backpropagation algorithm used for fitting the neural network.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 125%;text-align: left;">In short, this algorithm goes back and forth through the network, and it updates the weights. It does not pass all the data at once, but it passes batch by batch until all the data has been passed. When all data has been passed, this is called an <span class="s118">epoch</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">As you have seen, for neural networks, a lot of hyperparameter tuning has to be done. A lot of this is done by hand, using specific tools to judge the quality of the model fit. Building Neural Networks is much more complex than fitting classical machine learning models.</p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;">As an overview, the main neural network’s hyperparameters are</p><ul id="l51"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The number of layers.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The number of nodes in each of the layers.</p></li><li><p style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;line-height: 128%;text-align: left;">The <span class="s118">optimizer </span>is the method to update the weights throughout backpropagation. One of the standard optimizers is Adam, but there are many more.</p></li><li><p style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The <span class="s118">learning rate </span>of the optimizer influences the step size of the optimizer. Too small learning rates make the steps toward the optimum too small and can make it too slow, or you can get blocked in a local optimum.</p></li></ul><p style="padding-top: 6pt;padding-left: 25pt;text-indent: 17pt;line-height: 129%;text-align: left;">Two more hyperparameters that you need to choose are less related to the mathematics, but they are still very important:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l52"><li><p style="padding-top: 4pt;padding-left: 54pt;text-indent: -17pt;line-height: 128%;text-align: left;"><a name="bookmark174">The </a><span class="s118">batch size </span>specifies the number of individuals that will be used for each pass through the algorithm. If it’s too large, you may run out of RAM; but if it’s too small, it may be too slow.</p></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">The number of <span class="s118">epochs </span>is the number of times that the whole dataset is passed through the network. The more epochs, the longer the model continues training. But this should really depend on at which moment you reach the optimum.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Introducing the Example Data</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Before moving on to the fitting and optimization of the neural network, let’s introduce the example data and two data preparation methods. In this chapter, we’ll be using the example data from the Max Planck Institute for Biogeochemistry. The dataset is called the Jena Climate dataset. It contains weather measures like temperature, humidity, and more, recorded every 10 minutes.</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">You can download the data from </a><a href="https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip" class="s32" target="_blank">https://storage.googleapis.com/tensorflow/</a></p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 124%;text-align: left;"><span class="s68">tf-keras-datasets/jena_climate_2009_2016.csv.zip</span>. You can also download and unzip it and use pandas to import it with Listing <span style=" color: #00F;">16-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 16-1. <span class="s33">Importing the data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import keras</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from zipfile import ZipFile import os</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">uri = &quot;https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_ climate_2009_2016.csv.zip&quot;</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">zip_path = keras.utils.get_file(origin=uri, fname=&quot;jena_climate_2009_2016. csv.zip&quot;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">zip_file = ZipFile(zip_path) zip_file.extractall()</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">csv_path = &quot;jena_climate_2009_2016.csv&quot;</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">df = pd.read_csv(csv_path) del zip_file</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark175">df = df.drop(&#39;Date Time&#39;, axis=1)</a></p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">cols = [&#39;p&#39;, &#39;T&#39;, &#39;Tpot&#39;, &#39;Tdew&#39;, &#39;rh&#39;, &#39;VPmax&#39;, &#39;VPact&#39;, &#39;VPdef&#39;, &#39;sh&#39;, &#39;H2OC&#39;, &#39;rho&#39;, &#39;wv&#39;, &#39;mwv&#39;, &#39;wd&#39;]</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">df.columns = cols</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In this example, we’ll do a <span class="s118">forecast of the temperature 12 hours later</span>. To do this, we create lagged variables for the independent variables and make a correct dataframe. We have quite a lot of data, so we can add multiple lagged values to add the most information possible to the model. We’ll use Listing <span style=" color: #00F;">16-2 </span>to add the values for 72 lags (72 times 10 minutes, and then for 84, 96, 108, 120, and 132 steps back in time).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 16-2. <span class="s33">Creating the lagged dataset</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">y = df.loc[2*72:,&#39;T&#39;] lagged_x = []</p><p class="s34" style="padding-left: 37pt;text-indent: -11pt;line-height: 113%;text-align: left;">for lag in range(72,2*72,12): lagged = df.shift(lag)</p><p class="s34" style="padding-left: 37pt;text-indent: 0pt;line-height: 113%;text-align: left;">lagged.columns = [x + &#39;.lag&#39; + str(lag) for x in lagged.columns] lagged_x.append(lagged)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">df = pd.concat(lagged_x, axis=1)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">df = df.iloc[2*72:,:] #drop missing values due to lags</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Specific Data Prep Needs for a NN</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Neural Networks are very sensitive to problems with the input data. Let’s have a look at two tools that are often useful in data preparation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 25pt;text-indent: 0pt;text-align: left;">Scaling and Standardization</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 125%;text-align: left;">A neural network will not be able to learn if you do not standardize the input data. Standardizing means <span class="s20">getting the data onto the same scale</span>. Two examples for this are a standard scaler and a MinMax scaler:</p><ol id="l53"><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -18pt;line-height: 129%;text-align: left;">A standard scaler maps a variable to follow a standard normal distribution. That is, the new mean of the variable is 0, and the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 54pt;text-indent: 0pt;line-height: 129%;text-align: left;"><a name="bookmark176">new standard deviation is 1. It is obtained by taking each value minus the average of the variable and then dividing it through the standard deviation.</a></p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -18pt;line-height: 129%;text-align: left;">The MinMax scaler brings a variable into the range of 0–1 by subtracting the variable’s minimum from each value and then dividing it by the range of the variable.</p></li></ol><p style="padding-top: 6pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">You can apply a scaler using the syntax in Listing <span style=" color: #00F;">16-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 16-3. <span class="s33">Fitting the MinMaxScaler</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># apply a min max scaler</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler()</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">df = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Principal Component Analysis (PCA)</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 127%;text-align: left;">The <span class="s118">PCA </span>is a machine learning model in itself. It comes from the family of dimension reduction models. It allows to take a dataset with a large number of variables and <span class="s20">reduce the number of variables into a projection onto a number of dimensions</span>. Those dimensions will contain the larger part of the information and will contain much less noise.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">Making sure that the input data contains less noise will strongly help during the fitting of your Neural Network. So how does the PCA work? The idea is to make new variables, called <span class="s118">components</span>, based on combinations of strongly correlated variables. In Figure <span style=" color: #00F;">16-3</span>, you can see a hypothetical example with two variables Rain and Humidity that you can expect to be correlated. The first principal component captures the most possible variation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: left;"><span><img width="548" height="450" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_109.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 16-3. <span class="s29">PCA</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The component is a mathematical formula that is a linear combination of the original variables. You can use this score as a new variable. If the principal component is capturing a lot of the original variables, it can be interesting to use the component in your machine learning model rather than the original variables.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">To fit a PCA, you generally start with a PCA with all the components. This is done in Listing <span style=" color: #00F;">16-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 16-4. <span class="s33">Fitting the full PCA</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Fit a PCA with maximum number of components from sklearn.decomposition import PCA</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">mypca = PCA() mypca.fit(df)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark177">You use this PCA to make a </a><span class="s118">scree plot</span>. A scree plot is one of multiple tools used to decide on the number of components to retain. At the point of the elbow, you choose the number of components. You can use Listing <span style=" color: #00F;">16-5 </span>to make a scree plot. It is shown in Figure <span style=" color: #00F;">16-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 16-5. <span class="s33">Making a scree plot</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># Make a scree plot</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt plt.plot(mypca.explained_variance_ratio_) plt.show()</p><p style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">You can see the scree plot in Figure <span style=" color: #00F;">16-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 101pt;text-indent: 0pt;text-align: left;"><span><img width="303" height="198" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_110.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 16-4. <span class="s29">Scree plot</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">In this plot, the x-axis shows the components from the first component to the last. The y variable shows the amount of variation that is captured in those components. So you clearly see that the first <span class="s20">five to ten components </span>have much more information in them than the higher components (those more to the right). You could see the elbow being somewhere around 5, but it seems a good idea to retain ten components so that we retain almost all information but while having ten variables rather than over 80. Finally, you refit the PCA with ten components and transform the data using Listing <span style=" color: #00F;">16-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark178">Listing 16-6. </a><span class="s33">Fitting the PCA with ten components</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">mypca = PCA(10)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">df = mypca.fit_transform(df)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The Neural Network Using Keras</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now that we have made sure that our data are correctly prepared, we can finally move on to the actual neural network.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Building Neural Networks is a lot of work, and I want to find a good balance in showing you the way to get started and to work on improving your network rather than just showing a final performant network.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">A general great first start is to start with a relatively simple network and work your way up from there. In this case, let’s start with a network using two dense layers with 64 nodes. That would make the architecture look as follows.</p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;">For the other hyperparameters, let’s take things that are a little bit standard:</p><ul id="l54"><li><p class="s20" style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Optimizer<span class="p">: Adam</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Learning rate<span class="p">: 0.01</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Batch size<span class="p">: 32 (reduce this if you don’t have enough RAM)</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Epochs<span class="p">: 10</span></p><p style="padding-top: 9pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">Before starting, let’s do a train-test split as shown in Listing <span style=" color: #00F;">16-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 16-7. <span class="s33">Train-test split</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import train_test_split</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.33, random_state=42)</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">Now you build the model using the <span class="s118">keras library </span>using the following code. First, you specify the architecture using Listing <span style=" color: #00F;">16-8</span>. Keras is the go-to library for neural networks in Python.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark179">Listing 16-8. </a><span class="s33">Specify the model and its architecture</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense import random</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">random.seed(42)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">simple_model = Sequential([</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;">Dense(64, activation=&#39;relu&#39;, input_shape=(X_train.shape[1],)), Dense(64, activation=&#39;relu&#39;),</p><p class="s34" style="padding-left: 19pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Dense(1),</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">You can obtain a summary to check if everything is alright using Listing <span style=" color: #00F;">16-9</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 16-9. <span class="s33">Obtain a summary of the model architecture</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">simple_model.summary()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Then you compile the model using Listing <span style=" color: #00F;">16-10</span>. In the compilation part, you specify the optimizer and the learning rate. You also specify the loss, in our case the Mean Absolute Error.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 16-10. <span class="s33">Compile the model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 19pt;text-indent: -11pt;line-height: 113%;text-align: left;">simple_model.compile( optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=&#39;mean_absolute_error&#39;, metrics=[&#39;mean_absolute_error&#39;],</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;">And then you fit the model using Listing <span style=" color: #00F;">16-11</span>. At the fitting call, you specify the</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">epochs and the batch size. You can also specify a validation split so that you obtain a train-validation-test scenario in which you still have the test set for a final check of the R2 score that is not biased by the model development process.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark180">Listing 16-11. </a><span class="s33">Fit the model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 81pt;text-indent: -55pt;line-height: 113%;text-align: left;">smod_history = simple_model.fit(X_train, y_train, validation_split=0.2,</p><p class="s34" style="padding-left: 81pt;text-indent: 0pt;line-height: 113%;text-align: left;">epochs=10, batch_size=32, shuffle = True</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;">Be aware that fitting neural networks can take a lot of time. Running on GPU is</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">generally fast but not always possible depending on your computer hardware.</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now the important part here is to figure out whether or not this model has learned something using those hyperparameters. There is a key graph that is going to help you infinitely while building neural networks. You can obtain this graph using Listing <span style=" color: #00F;">16-12 </span>and see it in Figure <span style=" color: #00F;">16-5</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 126%;text-align: left;"><a href="https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">Be aware that you may get slightly different results. Setting the random seed is not enough to force randomness to be the same in Keras. Although it is possible to force exact reproducibility in Keras, it is quite complex, so I prefer to leave it out and accept that results are not 100% reproducible. You can check out these instructions for more information on how to fix the randomness in Keras: </a><a href="https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development" class="s32" target="_blank">https://keras.io/getting_</a></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development" class="s32">started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-</a></p><p class="s68" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">development<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 16-12. <span class="s33">Plot the training history</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(smod_history.history[&#39;loss&#39;]) plt.plot(smod_history.history[&#39;val_loss&#39;]) plt.title(&#39;model loss&#39;) plt.ylabel(&#39;accuracy&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.xlabel(&#39;epoch&#39;)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="230" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_111.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 16-5. <span class="s29">History plot that shows a model that doesn’t learn too well</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In Figure <span style=" color: #00F;">16-6</span>, you see a few examples of graphs that you are or are not looking for. In graph 1, you see the ideal curve that you are trying to obtain.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In graph 2, you see a neural net that is learning well, and it also shows overfit that happens in practice: the training loss goes down a lot, and the validation loss does too. Yet at some point, you need to reduce the number of epochs, or else your model will start to overfit. If you remember from a previous chapter, overfitting means that the model</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">is learning on specifics from the training data that are actually noise and therefore the learned trends do not generalize.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In the third graph, you see a neural net that is not learning anything. You can see this because the training loss is not going down. In this case, you may either have a problem with your data, or you might want to test out a very different network setup.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="145" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_112.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark181">Figure 16-6. </a><span class="s29">Ideal history plots</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now in our case, what happened? Most likely a case 3: the model is not learning anything. And why is the model not learning? We should try out whether there are just too few neurons to learn it. From here on, you know that you need to increase the number of layers and neurons and see whether this is getting any better.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">From here on, it is honestly a real process of trial and error. The first model that you should try is generally a very simple one, and it will be not learning enough. The next model that you should search for is a model that does learn, even if in the worst case it is overfitting. Then as the last step, you fine-tune until you obtain a graph close enough to the graph on the left in Figure <span style=" color: #00F;">16-6 </span>and you obtain an error score that is good enough to you.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 126%;text-align: left;">After trial and error, a better model architecture that has been found is the one in Listing <span style=" color: #00F;">16-13</span>. Far from saying that this is the best model, at least this model is able to obtain a relatively good R2 score on the test data of <span class="s118">0.908</span>. Feel free to try and tweak this model more and see what happens and whether you can improve it.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 16-13. <span class="s33">A better architecture</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: justify;">random.seed(42) model = Sequential([</p><p class="s34" style="padding-left: 37pt;text-indent: 0pt;line-height: 113%;text-align: justify;">Dense(256, activation=&#39;relu&#39;, input_shape=(X_train.shape[1],)), Dense(256, activation=&#39;relu&#39;),</p><p class="s34" style="padding-left: 37pt;text-indent: 0pt;line-height: 113%;text-align: justify;">Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;),</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: justify;"><a name="bookmark182">Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(256, activation=&#39;relu&#39;), Dense(1), ])</a></p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 13pt;text-align: left;">model.compile(</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;">optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=&#39;mean_absolute_error&#39;, metrics=[&#39;mean_absolute_error&#39;],</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 1pt;padding-left: 63pt;text-indent: -55pt;line-height: 113%;text-align: left;">history = model.fit(X_train, y_train, #validation_data=(X_test, y_test), validation_split=0.2,</p><p class="s34" style="padding-left: 63pt;text-indent: 0pt;line-height: 113%;text-align: left;">epochs=100, batch_size=32, shuffle = True</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;model loss&#39;) plt.ylabel(&#39;accuracy&#39;) plt.xlabel(&#39;epoch&#39;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show()</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">preds = model.predict(X_test) print(r2_score(preds, y_test))</p><p style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">The resulting graph is shown in Figure <span style=" color: #00F;">16-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="234" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_113.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark183">Figure 16-7. </a><span class="s29">History plot of the more complex model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Conclusion</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">The really difficult thing with fitting neural networks is that it is very hard to know if you’ve achieved something that could be improved or whether it’s quite good already. For now, this chapter has shown how to work on improving from a very simple network and trying to get into an optimized version.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">There are some libraries that you could use for hyperparameter optimization. Of course, you could even grid search your architecture. Just know that there is no wonder cure for fitting neural networks. It requires much more effort and dedication than classic machine learning models. Training times are very long, and you will need significant computing power to start thinking about such optimization. Yet on the upside, neural networks can sometimes obtain results that are much more powerful than classical machine learning, and it is an important tool to have in your machine learning toolbox.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">There are a lot of developments in the scientific field of neural networks and AI. In the coming two chapters, you’ll discover neural network structures that are</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">more advanced than the dense structure, but that may apply very well in the case of forecasting.</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4></li></ul></li><li><p style="padding-top: 10pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">Neural Networks are a powerful supervised machine learning model, but it is more difficult to build it than classical machine learning models.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The hyperparameters are</p><ul id="l55"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The number of layers</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The number of nodes in each of the layers</p></li><li><p style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The <span class="s118">optimizer</span></p></li><li><p style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The <span class="s118">learning rate</span></p></li><li><p style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The <span class="s118">batch size</span></p></li><li><p style="padding-top: 8pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The number of <span class="s118">epochs</span></p></li></ul></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">The graph showing the descent of the train and validation loss is a very important aspect in developing neural networks.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">PCA is a model for dimension reduction. It can be also be used for data preparation, especially when there are many correlated variables.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">Scaling the input data is necessary for neural networks. Some commonly used methods are the standard scaler and the MinMax scaler.</p></li></ul><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark184">CHAPTER 17</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">RNNs Using SimpleRNN and GRU</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 128%;text-align: left;">In this chapter, you’ll discover a more advanced version of neural networks called <span class="s19">Recurrent Neural Networks</span>. There are three very common versions of RNNs: SimpleRNN, GRU (Gated Recurrent Unit), and LSTM (Long Short Term Memory). In practice, SimpleRNNs are hardly used anymore for a number of problems that I’ll talk about later. Therefore, I have regrouped SimpleRNN and GRU in this chapter, and LSTMs have their own chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">What Are RNNs: Architecture</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">So what are Recurrent Neural Networks, and what makes them different from the dense neural networks that you’ve seen just before? The following graph shows a node of a fully connected net against an example with a SimpleRNN node.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_17#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_17#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_17</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;">227</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="222" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_114.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark185">Figure 17-1. </a><span class="s29">Recurrent network vs. dense network</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">As you can see in Figure <span style=" color: #00F;">17-1</span>, the big difference in the RNN block is that there is a feedback loop. Where each input of a fully connected network is completely</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">independent, the inputs of an RNN have a feedback relation with each other. This makes</p><p class="s19" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">RNNs great for data that has sequence<span class="p">s, for example:</span></p><ul id="l56"><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Time series</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Written text (sequences of words)</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">DNA sequences</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">uGeolocation sequences</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Inside the SimpleRNN Unit</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Due to this difference in architecture, there is also a difference inside the units. As you can see, there is not just one input in each unit, but there are two inputs. Those two inputs have to be taken into account. As a schematic overview, this looks as shown in Figure <span style=" color: #00F;">17-2</span>, in which X is the input at time t, Y is the target variable at time t, and the a’s are the weights that go from left to right in Figure <span style=" color: #00F;">17-1</span>, so the weights that make the model fit as a sequence through time.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 148pt;text-indent: 0pt;text-align: left;"><span><img width="226" height="229" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_115.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark186">Figure 17-2. </a><span class="s29">RNN cell</span></p><p style="padding-top: 12pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">In the previous chapter, you have mainly seen the <span class="s19">ReLU </span>activation layer being used.</p><p style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 127%;text-align: left;">In Recurrent Neural Networks, however, the <span class="s19">tanh </span>layer is the standard. The reason for this is that for long sequences, the ReLU layer suffers from the exploding gradient</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">problem: the repeated multiplication with weights is acting like an exponentiation that makes it explode. The tanh activation layer does not have this problem, as the values are forced to stay between -1 and 1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">The Example</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">The RNN learns on sequences. Therefore, we will have to adapt the problem statement. In this chapter, let’s work with the same data as in the previous chapter to get a good feel of how the data preparation and use of the data are different from fully connected architectures.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Just to remember quickly: You have a dataset with measures on weather data, and we try to predict the temperature 12 hours (72 time steps of 10 minutes) into the future.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now what we did in the fully connected model was to create lagged variables. There was one y variable (which was not lagged), and the independent variables were lagged values of the y variable and a lot of other variables. The first lag was at 72 time steps, so that the model would use the data from 12 hours ago to predict the now (having a history from now to 12 hours back to predict the now is equivalent to having data from now to predict 12 hours later).</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark187">Predicting a Sequence Rather Than a Value</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">In the RNN, this is not what we should do, as the RNN learns sequences. A big jump of 72 time steps is not really respecting the sequential variation. Yet we do not just want to predict one time step later, as that would mean that we predict the temperature in 10 minutes from now, not really interesting.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">What we will do is <span class="s20">create a matrix of y variables</span>, with lags as well. Before, we wanted to predict one value 72 time steps into the future, but let’s model to predict each of those</p><ol id="l57"><li><p style="padding-left: 8pt;text-indent: 0pt;line-height: 124%;text-align: left;">Even though it might be possible to do it with one y variable, this is also an interesting case of <span class="s20">multistep forecasting</span>, which is often useful.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Univariate Model Rather Than Multivariable</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Another thing that we change from the previous model is that in this case, we will use only the temperature data and not the other variables. This may make the task slightly harder to accomplish, but it’ll be easier to get your head around the use of sequences and RNNs. However, you must know that it is possible to add other explanatory variables into an RNN. For forecasting tomorrow’s temperature, you may want to use not only today’s temperature but also today’s wind direction, wind speed, and humidity, for example. In this case, you could add a third dimension to the input data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Preparing the Data</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">You can prepare the data using the following steps. The first step is to import the data using Listing <span style=" color: #00F;">17-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 17-1. <span class="s33">Importing the data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import keras</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from zipfile import ZipFile import os</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">uri = &quot;https://storage.googleapis.com/tensorflow/tf-keras-datasets/ jena_climate_2009_2016.csv.zip&quot;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark188">zip_path = keras.utils.get_file(origin=uri, fname=&quot;jena_climate_2009_2016. csv.zip&quot;)</a></p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">zip_file = ZipFile(zip_path) zip_file.extractall()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">csv_path = &quot;jena_climate_2009_2016.csv&quot;</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">df = pd.read_csv(csv_path) del zip_file</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The next step is to delete all columns other than the temperature, as we are building a univariate model. This is done in Listing <span style=" color: #00F;">17-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 17-2. <span class="s33">Keep only temperature data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">df = df[[&#39;T (degC)&#39;]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now, as you remember from Chapter <span style=" color: #00F;">16</span>, neural networks need data that has been standardized. So let’s apply a MinMaxScaler as in Listing <span style=" color: #00F;">17-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 17-3. <span class="s33">Apply a MinMaxScaler</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># apply a min max scaler</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">df = pd.DataFrame(scaler.fit_transform(df), columns = [&#39;T&#39;])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now a part that may be harder to get your head around intuitively. We need to split the data into a shape in which we have sequences of past data and sequences of future data. We want to predict 72 steps into the future, and we’ll use 3*72 steps into the past. This is an arbitrary choice, and please feel free to try out using more or less past data. The code in Listing <span style=" color: #00F;">17-4 </span>loops through the data and creates sequences for the model training.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 17-4. <span class="s33">Preparing the sequence data</span></p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 24pt;text-align: left;">ylist = list(df[&#39;T&#39;]) n_future = 72</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">n_past = 3*72 total_period = 4*72</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">idx_end = len(ylist)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">idx_start = idx_end - total_period</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_new = [] y_new = []</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">while idx_start &gt; 0:</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">x_line = ylist[idx_start:idx_start+n_past]</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">y_line = ylist[idx_start+n_past:idx_start+total_period]</p><p class="s34" style="padding-top: 9pt;padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_new.append(x_line) y_new.append(y_line)</p><p class="s34" style="padding-top: 7pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">idx_start = idx_start - 1</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># converting list of lists to numpy array import numpy as np</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_new = np.array(X_new) y_new = np.array(y_new)</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now that we have obtained an X and a Y matrix for the model training, as always we need to split it into a train and test set in order to be able to do a fair model evaluation. This is done in Listing <span style=" color: #00F;">17-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 17-5. <span class="s33">Splitting into train and test</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import train_test_split</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.33, random_state=42)</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">A final step is necessary for fitting the SimpleRNN. This is a step that may be hard to understand intuitively, as there is not really a reason for it. The SimpleRNN layer needs an input format that is 3D, and the shape has to correspond to (n_samples, n_timesteps, n_features). This can be obtained using reshape. This reshape is, unfortunately, necessary sometimes when using Keras, as it is very specific as to the exact shape of the input data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">When working with layer types that you don’t know yet, this can sometimes give real headaches. But it is crucial to learn how to work with it. Listing <span style=" color: #00F;">17-6 </span>shows you how to reshape the data into the right format for Keras to recognize it.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark189">Listing 17-6. </a><span class="s33">Reshape the data to be recognized by Keras</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">batch_size = 32</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">n_samples = X_train.shape[0] n_timesteps = X_train.shape[1] n_steps = y_train.shape[1] n_features = 1</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">X_train_rs = X_train.reshape(n_samples, n_timesteps, n_features ) X_test_rs = X_test.reshape(X_test.shape[0], n_timesteps, n_features )</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">A Simple SimpleRNN</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now let’s start with a very small neural network using the SimpleRNN layer. You can parameterize it with Listing <span style=" color: #00F;">17-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 17-7. <span class="s33">Parameterize a small network with SimpleRNN</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">import random random.seed(42)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from tensorflow.keras.models import Sequential</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">from tensorflow.keras.layers import Dense, SimpleRNN</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">simple_model = Sequential([</p><p class="s34" style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 113%;text-align: left;">SimpleRNN(8, activation=&#39;tanh&#39;,input_shape=(n_timesteps, n_features)), Dense(y_train.shape[1]),</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">])</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 24pt;text-align: left;">simple_model.summary() simple_model.compile(</p><p class="s34" style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 113%;text-align: left;">optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=&#39;mean_absolute_error&#39;, metrics=[&#39;mean_absolute_error&#39;],</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 63pt;text-indent: -55pt;line-height: 113%;text-align: left;">smod_history = simple_model.fit(X_train_rs, y_train, validation_split=0.2,</p><p class="s34" style="padding-left: 63pt;text-indent: 0pt;line-height: 113%;text-align: left;">epochs=5, batch_size=batch_size, shuffle = True</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">preds = simple_model.predict(X_test_rs)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import r2_score print(r2_score(preds, y_test))</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt plt.plot(smod_history.history[&#39;loss&#39;]) plt.plot(smod_history.history[&#39;val_loss&#39;]) plt.title(&#39;model loss&#39;) plt.xlabel(&#39;epoch&#39;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show()</p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The obtained plot is shown in Figure <span style=" color: #00F;">17-3</span>, and the obtained R2 score is <span class="s19">0.66</span>. This is not a great score, and the training history confirms that the model is not correctly specified: it is going down, but it seems to be not learning enough. The loss is what we call the accuracy throughout the model, as the error function of the neural network is</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">often referred to as the loss function. A more complex model should allow to let the loss go down more.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="244" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_116.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark190">Figure 17-3. </a><span class="s29">Training history of the SimpleRNN</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">SimpleRNN with Hidden Layers</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: left;">As an improvement to this, let’s add a second layer to this network. To do this, you need to specify in every layer but the last ‘<span class="s19">return_sequences = True</span>’. Fitting RNNs is even slower than fitting <span class="s19">feedforward neural networks</span>, so be aware that the computation times in this chapter may be long.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You could build the network in Listing <span style=" color: #00F;">17-8 </span>that is slightly more complex and see how it performs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 17-8. <span class="s33">A more complex network with three layers of SimpleRNN</span></p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 24pt;text-align: left;">random.seed(42) simple_model = Sequential([</p><p class="s34" style="padding-left: 37pt;text-indent: 0pt;line-height: 113%;text-align: left;">SimpleRNN(32, activation=&#39;tanh&#39;,input_shape=(n_timesteps, n_features), return_sequences=True),</p><p class="s34" style="padding-left: 37pt;text-indent: 0pt;line-height: 113%;text-align: left;">SimpleRNN(32, activation=&#39;tanh&#39;, return_sequences = True), SimpleRNN(32, activation=&#39;tanh&#39;),</p><p class="s34" style="padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Dense(y_train.shape[1]),</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">])</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">simple_model.summary()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 19pt;text-indent: -11pt;line-height: 113%;text-align: left;">simple_model.compile( optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=&#39;mean_absolute_error&#39;, metrics=[&#39;mean_absolute_error&#39;],</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 63pt;text-indent: -55pt;line-height: 113%;text-align: left;">smod_history = simple_model.fit(X_train_rs, y_train, validation_split=0.2,</p><p class="s34" style="padding-left: 63pt;text-indent: 0pt;line-height: 113%;text-align: left;">epochs=5, batch_size=batch_size, shuffle = True</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 170%;text-align: left;">preds = simple_model.predict(X_test_rs) print(r2_score(preds, y_test))</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(smod_history.history[&#39;loss&#39;]) plt.plot(smod_history.history[&#39;val_loss&#39;]) plt.title(&#39;model loss&#39;) plt.xlabel(&#39;epoch&#39;)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show()</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This gives a multivariate R2 score of 0.90. You will observe the history plot shown in Figure <span style=" color: #00F;">17-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="244" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_117.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark191">Figure 17-4. </a><span class="s29">Training history of the three-layer SimpleRNN</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This network allows the loss to go down earlier and lower, even though the curve is still not looking very impressive. At the same time, the obtained R2 is quite good, so it feels like things are moving in the right direction.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">Simple GRU</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now we could go further into the SimpleRNN and try to optimize it. Yet we won’t do that here. The SimpleRNN is not up to today’s standards anymore. It has some serious problems. One big effect of this is that it is not able to remember a very long time back. There are also some more technical issues with it.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">A more advanced RNN layer has been invented called GRU, for Gated Recurrent Unit. The GRU cell has more parameters as is shown in Figure <span style=" color: #00F;">17-5</span>. This shows that there is an extra passage inside the cell, and this allows for an extra parameter to be estimated. This helps with learning long-term trends.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 130pt;text-indent: 0pt;text-align: left;"><span><img width="227" height="230" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_118.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark192">Figure 17-5. </a><span class="s29">The GRU cell</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Due to those differences, the GRU obtains better general performances than the SimpleRNN, and the SimpleRNN has become very little used. Let’s build a simple network with GRU and see how it performs on the data. This is done in Listing <span style=" color: #00F;">17-9</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 17-9. <span class="s33">A simple architecture with one GRU layer</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">random.seed(42)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from tensorflow.keras.layers import GRU</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">simple_model = Sequential([</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: 5pt;line-height: 113%;text-align: left;">GRU(8, activation=&#39;tanh&#39;,input_shape=(n_timesteps, n_features)), Dense(y_train.shape[1]),</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">])</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 24pt;text-align: left;">simple_model.summary() simple_model.compile(</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;">optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=&#39;mean_absolute_error&#39;, metrics=[&#39;mean_absolute_error&#39;],</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 63pt;text-indent: -55pt;line-height: 113%;text-align: left;">smod_history = simple_model.fit(X_train_rs, y_train, validation_split=0.2,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark193">epochs=10, batch_size=batch_size, shuffle = True</a></p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">preds = simple_model.predict(X_test_rs) print(r2_score(preds, y_test))</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(smod_history.history[&#39;loss&#39;]) plt.plot(smod_history.history[&#39;val_loss&#39;]) plt.title(&#39;model loss&#39;) plt.xlabel(&#39;epoch&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show()</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You will obtain an R2 of <span class="s19">0.88</span>, which is not too bad for a one-layer model, and you obtain the history plot that is shown in Figure <span style=" color: #00F;">17-6</span>. The history plot on the other hand shows that the fitting really did not go very well. The loss is not going down as it is supposed to be, and some changes should be made to the model architecture.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="244" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_119.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 17-6. <span class="s29">The history of the one-layer GRU model</span></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark194">GRU with Hidden Layers</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now as a next step, let’s try to improve on this network by adding some more layers and see if we can get the loss to go down more. To be totally transparent here, it takes a lot of time to work on the optimization of the architecture and hyperparameters. It is a work of trial and error while sticking to a number of model metrics, including loss and R2 score, but also the loss graph.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">It also takes experience to test out different hyperparameters and to develop a feel for the type of changes that you’d need to make. But that’s totally normal; there simply is a learning curve in those models. It is also important to realize that it takes a lot of time for one model to fit. This makes it hard to run grid search or other hyperparameter optimizations, yet at the same time, it can make it frustrating to wait a long time only to see a bad score arriving.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;">The model that you see in Listing <span style=" color: #00F;">17-10 </span>obtains an R2 score of <span class="s19">0.95</span>, so quite a lot better than the simple GRU model.</p><p class="s28" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 17-10. <span class="s33">A more complex network with three layers of GRU</span></p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 24pt;text-align: left;">random.seed(42) simple_model = Sequential([</p><p class="s34" style="padding-left: 24pt;text-indent: 0pt;line-height: 113%;text-align: left;">GRU(64, activation=&#39;tanh&#39;,input_shape=(n_timesteps, n_features), return_sequences=True),</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">GRU(64, activation=&#39;tanh&#39;, return_sequences=True), GRU(64, activation=&#39;tanh&#39;),</p><p class="s34" style="padding-left: 19pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Dense(y_train.shape[1]),</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">])</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 24pt;text-align: left;">simple_model.summary() simple_model.compile(</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;">optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=&#39;mean_absolute_error&#39;, metrics=[&#39;mean_absolute_error&#39;],</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">smod_history = simple_model.fit(X_train_rs, y_train,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark195">validation_split=0.2, epochs=10, batch_size=batch_size, shuffle = True</a></p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">preds = simple_model.predict(X_test_rs) print(r2_score(preds, y_test))</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(smod_history.history[&#39;loss&#39;]) plt.plot(smod_history.history[&#39;val_loss&#39;]) plt.title(&#39;model loss&#39;) plt.xlabel(&#39;epoch&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="244" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_120.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 17-7. <span class="s29">The history of the three-layer GRU model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The history graph of this model is shown in Figure <span style=" color: #00F;">17-7</span>. It is probably not the best model, but at least we see that the training and validation losses are both going down. The obtained R2 is not too bad. I leave it as an exercise for you to try and improve on this architecture using the loss graph and model metrics.</p><h4 style="padding-top: 10pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark196">Key Takeaways</a></h4><ul id="l58"><li><p style="padding-top: 10pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">Recurrent Neural Networks have an integrated feedback loop that makes them great for modeling sequences.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">There are multiple types of RNN cells:</p><ul id="l59"><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The SimpleRNN is a basic RNN cell that is not much used anymore as it has numerous theoretical problems and is not able to learn long-term trends.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The GRU cell is an improvement of the RNN cell, and it has additional parameters that make it easier to remember long-term trends.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The LSTM will be covered in Chapter <span style=" color: #00F;">18</span>.</p></li></ul></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">RNNs, especially when training on long sequences, benefit from using a tanh activation function rather than the ReLU that is common in dense networks.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">You can use RNNs to predict a sequence of multiple steps to make a multistep forecast.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">You have seen a number of practical modeling examples, and this will help you to understand which history plots are good or not.</p></li></ul></li></ol><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark197">CHAPTER 18</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">LSTM RNNs</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 125%;text-align: left;">In the previous chapter, you have discovered two types of Recurrent Neural Network cells called SimpleRNN and GRU (Gated Recurrent Unit). In this chapter, you’ll discover a third type of cell called <span class="s118">LSTM, </span>for <span class="s118">Long Short-Term Memory</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">What Is LSTM</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">LSTMs as a third and last type of RNN cell are even more advanced than the GRU cell. If you remember from the last chapter, the SimpleRNN cell allows having recurrent architectures, by adding a feedback loop between consecutive values. It therefore is an improvement on “simple” feedforward cells that do not allow for this.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">A problem with the SimpleRNN is the longer-term trends, which you could call a longer-term memory. The GRU cell is an improvement on the SimpleRNN that adds a weight to the cell that serves to learn longer-term processes.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The LSTM cell adds long-term memory in an even more performant way because it allows even more parameters to be learned. This makes it the most powerful RNN to do forecasting, especially when you have a longer-term trend in your data. LSTMs are one of the state-of-the-art models for forecasting at this moment.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">The LSTM Cell</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 128%;text-align: left;">Let’s see how the LSTM cell differs from the GRU cell. It has much more components. Notably, there are <span class="s20">three sigmoids and two tanh operations </span>all combined into the same cell. You can see a schematic overview in Figure <span style=" color: #00F;">18-1 </span>where there are two weights coming in from the past cell (c and a of the time t-1) and once transformed another two weights are going (c and a of the time t). The X is the input to this cell, and the Y is the output.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_18#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_18#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_18</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">243</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="272" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_121.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark198">Figure 18-1. </a><span class="s29">The LSTM cell</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In practice, this all comes down to having more parameters to estimate, and the LSTM can therefore better fit on data that have short- and long-term trends.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Example</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">In applied machine learning, it is important to benchmark models against each other. Let’s therefore use the same data for a last time and make it into a benchmark of the three RNN models that we have seen. As a short recap, the performances that were obtained by the models in Chapter <span style=" color: #00F;">17 </span>are repeated in Table <span style=" color: #00F;">18-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-bottom: 2pt;padding-left: 127pt;text-indent: 0pt;line-height: 117%;text-align: left;">Table 18-1. <span class="s29">Performances of the SimpleRNN and GRU Models</span></p><table style="border-collapse:collapse;margin-left:127.25pt" cellspacing="0"><tr style="height:24pt"><td style="width:79pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Model</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">SimpleRNN</p></td><td style="width:35pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">GRU</p></td></tr><tr style="height:22pt"><td style="width:79pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">One layer of 8</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">0.66</p></td><td style="width:35pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">0.88</p></td></tr><tr style="height:22pt"><td style="width:79pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Three layers of 64</p></td><td style="width:60pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">0.90</p></td><td style="width:35pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">0.95</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s see how the LSTM compares to this. You can use the same data preparation as used in Chapter <span style=" color: #00F;">17</span>, which is repeated in Listing <span style=" color: #00F;">18-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 18-1. <span class="s33">Importing the weather data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import keras</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from zipfile import ZipFile import os</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">uri = &quot;https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_ climate_2009_2016.csv.zip&quot;</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">zip_path = keras.utils.get_file(origin=uri, fname=&quot;jena_climate_2009_2016. csv.zip&quot;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">zip_file = ZipFile(zip_path) zip_file.extractall()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">csv_path = &quot;jena_climate_2009_2016.csv&quot;</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">df = pd.read_csv(csv_path) del zip_file</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># retain only temperature df = df[[&#39;T (degC)&#39;]]</p><p class="s172" style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># apply a min max scaler</p><p class="s34" style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">df = pd.DataFrame(scaler.fit_transform(df), columns = [&#39;T&#39;])</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># convert to windowed data sets ylist = list(df[&#39;T&#39;])</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">n_future = 72 n_past = 3*72 total_period = 4*72</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">idx_end = len(ylist)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">idx_start = idx_end - total_period</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark199">X_new = [] y_new = []</a></p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">while idx_start &gt; 0:</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">x_line = ylist[idx_start:idx_start+n_past]</p><p class="s34" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">y_line = ylist[idx_start+n_past:idx_start+total_period]</p><p class="s34" style="padding-top: 9pt;padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_new.append(x_line) y_new.append(y_line)</p><p class="s34" style="padding-top: 7pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">idx_start = idx_start - 1</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import numpy as np X_new = np.array(X_new) y_new = np.array(y_new)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"># train test split</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import train_test_split</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.33, random_state=42)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># reshape data into the right format for RNNs n_samples = X_train.shape[0]</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">n_timesteps = X_train.shape[1] n_steps = y_train.shape[1] n_features = 1</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_train_rs = X_train.reshape(n_samples, n_timesteps, n_features ) X_test_rs = X_test.reshape(X_test.shape[0], n_timesteps, n_features )</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">LSTM with One Layer of 8</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now, as we did in the previous chapter, let’s start with fitting a simple one-layer network. The code is almost the same as Chapter <span style=" color: #00F;">17</span>’s code; you just need to change the name of the layer into LSTM. The full code is shown in Listing <span style=" color: #00F;">18-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 18-2. <span class="s33">One-layer LSTM</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">import random</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, LSTM</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">random.seed(42) batch_size = 32</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">simple_model = Sequential([</p><p class="s34" style="padding-top: 1pt;padding-left: 37pt;text-indent: 5pt;line-height: 113%;text-align: left;">LSTM(8, activation=&#39;tanh&#39;,input_shape=(n_timesteps, n_features)), Dense(y_train.shape[1]),</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">])</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 24pt;text-align: left;">simple_model.summary() simple_model.compile(</p><p class="s34" style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 113%;text-align: left;">optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=&#39;mean_absolute_error&#39;, metrics=[&#39;mean_absolute_error&#39;],</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 81pt;text-indent: -55pt;line-height: 113%;text-align: left;">smod_history = simple_model.fit(X_train_rs, y_train, validation_split=0.2,</p><p class="s34" style="padding-left: 81pt;text-indent: 0pt;line-height: 113%;text-align: left;">epochs=5, batch_size=batch_size, shuffle = True</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">preds = simple_model.predict(X_test_rs) print(r2_score(preds, y_test))</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(smod_history.history[&#39;loss&#39;]) plt.plot(smod_history.history[&#39;val_loss&#39;]) plt.title(&#39;model loss&#39;) plt.xlabel(&#39;epoch&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a name="bookmark200">The results of this model are the following. The R2 score obtained by this model is</a></p><ol id="l60"><ol id="l61"><li><p style="padding-top: 2pt;padding-left: 33pt;text-indent: -25pt;text-align: left;">The plot of the training loss is shown in Figure <span style=" color: #00F;">18-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="244" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_122.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 18-2. <span class="s29">Training history of the one-layer LSTM</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In this graph, you can see that the train loss and val loss both go down a little bit. Yet the drop in validation loss is not very steep. This leads to the conclusion that the model is learning a bit, but that we are probably able to get more out of this model. We’d want to see a bigger drop in validation loss before it plateaus. In the current graph, you’re almost directly on a plateau. In Table <span style=" color: #00F;">18-2</span>, you see the table updated with the new value.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 47pt;text-indent: 0pt;text-align: left;">Table 18-2. <span class="s29">Performances of the SimpleRNN and GRU Models</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:48pt" cellspacing="0"><tr style="height:24pt"><td style="width:93pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Model</p></td><td style="width:84pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">SimpleRNN</p></td><td style="width:61pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">GRU</p></td><td style="width:77pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 27pt;text-indent: 0pt;text-align: left;">LSTM</p></td></tr><tr style="height:22pt"><td style="width:93pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">One layer of 8</p></td><td style="width:84pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.66</p></td><td style="width:61pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">0.88</p></td><td style="width:77pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 27pt;text-indent: 0pt;text-align: left;">0.93</p></td></tr><tr style="height:22pt"><td style="width:93pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Three layers of 64</p></td><td style="width:84pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.90</p></td><td style="width:61pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">0.95</p></td><td style="width:77pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">LSTM with Three Layers of 64</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">As you remember from the previous chapter and as you can see in the table, the GRU model with three layers of 64 cells worked quite well. Before going deeper into the tuning of the LSTM model, let’s also try out the performances of this architecture. You can use Listing <span style=" color: #00F;">18-3 </span>to do this.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 18-3. <span class="s33">Three-layer LSTM</span></p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 24pt;text-align: left;">random.seed(42) simple_model = Sequential([</p><p class="s34" style="padding-left: 42pt;text-indent: 0pt;line-height: 113%;text-align: left;">LSTM(64, activation=&#39;tanh&#39;,input_shape=(n_timesteps, n_features), return_sequences=True),</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">LSTM(64, activation=&#39;tanh&#39;, return_sequences=True), LSTM(64, activation=&#39;tanh&#39;),</p><p class="s34" style="padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Dense(y_train.shape[1]),</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">])</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 24pt;text-align: left;">simple_model.summary() simple_model.compile(</p><p class="s34" style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 113%;text-align: left;">optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=&#39;mean_absolute_error&#39;, metrics=[&#39;mean_absolute_error&#39;],</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 81pt;text-indent: -55pt;line-height: 113%;text-align: left;">smod_history = simple_model.fit(X_train_rs, y_train, validation_split=0.2,</p><p class="s34" style="padding-left: 81pt;text-indent: 0pt;line-height: 113%;text-align: left;">epochs=10, batch_size=batch_size, shuffle = True</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">preds = simple_model.predict(X_test_rs) print(r2_score(preds, y_test))</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.plot(smod_history.history[&#39;loss&#39;]) plt.plot(smod_history.history[&#39;val_loss&#39;]) plt.title(&#39;model loss&#39;) plt.ylabel(&#39;accuracy&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">plt.xlabel(&#39;epoch&#39;)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 127%;text-align: left;"><a name="bookmark201">This model obtains an R2 score of </a><span class="s118">0.94</span>. When comparing this to the other values in Table <span style=" color: #00F;">18-3</span>, you can observe that it functions a bit less than the GRU three-layer model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 24pt;text-indent: 0pt;text-align: center;">Table 18-3. <span class="s29">Performances of the SimpleRNN and GRU Models</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:57.875pt" cellspacing="0"><tr style="height:24pt"><td style="width:99pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Model</p></td><td style="width:91pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">SimpleRNN</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">GRU</p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">LSTM</p></td></tr><tr style="height:22pt"><td style="width:99pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">One layer of 8</p></td><td style="width:91pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">0.66</p></td><td style="width:60pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">0.88</p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s31" style="padding-top: 5pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">0.93</p></td></tr><tr style="height:22pt"><td style="width:99pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">Three layers of 64</p></td><td style="width:91pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">0.90</p></td><td style="width:60pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">0.95</p></td><td style="width:64pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">0.94</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: justify;">Let’s also have a look at the history graph shown in Figure <span style=" color: #00F;">18-3</span>. What we can see in this training graph is that the loss drops more than in the architecture with one layer. The shape of the train loss is looking not too bad: it has the right shape, although it seems that the drop could go on a bit longer, and therefore the plateau would be reached later and at a lower loss. The validation, on the other hand, does</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: justify;">not really have the right shape, which tells us that there is probably some R2 to win by tweaking the model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="244" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_123.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">Figure 18-3. <span class="s29">Training history of the three-layer LSTM</span></p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Conclusion</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">So where does that leave us? The best RNN is the three-layer GRU, very closely followed by the three-layer LSTM and the one-layer LSTM. The differences in scores clearly show why the SimpleRNN layer is not much used anymore. The GRU and the LSTM are two RNN models that are performant for forecasting tasks.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The LSTM model has the advantage over GRU to be able to fit more long-term trends. A hypothetical reason for the better performance of the GRU is that the data did not contain very long-term trends, as the data were reworked to contain only 36 hours of past data for each line of data.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">When doing modeling in practice, it is very common to run benchmarks like the one done in this example. The important thing is to master the different models to be able to tune and benchmark them effectively. After that, the decision should be objective based on performances during the benchmark. In this case, it was not the more recent LSTM that won, but the GRU.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4><ul id="l62"><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">The LSTM is the most advanced type of RNN as it has a large number of components that allow it to remember longer-term trends.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">You have seen an example of how to do a model benchmark between multiple models on the same dataset. This is crucial in applied machine learning.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">You have also seen an example of how to interpret and work with neural network history graphs that show the descent in loss. You can use them to draw conclusions on the way your model is learning and how to improve that.</p></li></ul></li></ol></ol><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark202">CHAPTER 19</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The Prophet Model</h1><p style="padding-top: 21pt;padding-left: 26pt;text-indent: 0pt;line-height: 124%;text-align: left;">In this chapter, you’ll discover the <span class="s19">Prophet model</span>, which was open sourced by Facebook. It is not exactly a model, but rather <span class="s20">an automated procedure </span>for building forecasting models.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You’ll notice that there is a big difference in working with this model than with Keras, as the Prophet model does a lot of the work for you. It has a much higher level of user- friendliness: it does not require any theory to get started, as opposed to the previous chapters.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">A disadvantage of this can be that you do have fewer possibilities to understand exactly how and what the model is learning. There are also fewer parameters to tweak, which may be an advantage if the model works, but it may be a disadvantage if you do not obtain the required model precision.</p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;">A quote from the developers explains the goal of Facebook’s Prophet:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s29" style="padding-left: 47pt;text-indent: 0pt;text-align: justify;">We use a simple, modular regression model that often works well with default parameters, and that allows analysts to select the components that are relevant to their forecasting problem and easily make adjustments as needed. The second component is a system for measuring and tracking forecast accuracy, and flagging forecasts that should be checked manually to help analysts make incremental improvements.</p><p style="padding-top: 5pt;padding-left: 209pt;text-indent: 0pt;text-align: left;"><a href="https://peerj.com/preprints/3190/" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank">—</a><a href="https://peerj.com/preprints/3190/" class="s173" target="_blank">https://peerj.com/preprints/3190/</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">As Facebook’s Prophet is a procedure rather than an actual model, there is not one particular part of mathematical theory that is contributed by it. It is rather the combination of different processes and the automated way of tuning behind this that</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">are useful. Let’s go directly into an example and discover the different components that together make up the Facebook Prophet.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_19#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_19#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_19</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">253</p><h4 style="padding-top: 10pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark203">The Example</a></h4><p class="s68" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="http://www.kaggle.com/c/recruit-restaurant-visitor-forecasting" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">In this example, we’ll be working with a dataset from a Kaggle competition on forecasting the number of restaurant visitors. You can download the data on the Kaggle page of the competition that used this dataset: </a><a href="http://www.kaggle.com/c/recruit-restaurant-visitor-forecasting" class="s32" target="_blank">www.kaggle.com/c/recruit- </a>restaurant-visitor-forecasting<span class="p">. You will need the following datasets from there: ‘air_visit_data.csv.zip’, ‘date_info.csv.zip’, and ‘air_reserve.csv.zip’.</span></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">We’ll be using Prophet, and <span class="s19">Prophet can only make univariate forecasts</span>. If you remember from earlier, this means that Prophet only makes forecasts with only one dependent variable. If you’d want to adapt to multivariate forecasts, you’d need to train a Prophet for each dependent variable separately. Therefore, we will be working on a combined forecast for the sum of all the restaurants. You can prepare the data of the dependent variable using Listing <span style=" color: #00F;">19-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 19-1. <span class="s33">Preparing the dependent variable</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">y = pd.read_csv(&#39;air_visit_data.csv.zip&#39;)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">y = y.pivot(index=&#39;visit_date&#39;, columns=&#39;air_store_id&#39;)[&#39;visitors&#39;] y = y.fillna(0)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">y = pd.DataFrame(y.sum(axis=1))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The Prophet Data Format</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">When building a Prophet model, you are always obliged to have a dataset with the columns ‘ds’ and ‘y’. This is how the model recognizes the date column (‘ds’) and the dependent variable (‘y’). You can create this format using Listing <span style=" color: #00F;">19-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 19-2. <span class="s33">Preparing the modeling dataframe</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">y = y.reset_index(drop=False) y.columns = [&#39;ds&#39;, &#39;y&#39;]</p><p style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">We will also add a train-test split made by putting the last 28 days into the test set.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">This can be done using Listing <span style=" color: #00F;">19-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark204">Listing 19-3. </a><span class="s33">Creating a train-test split</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">train = y.iloc[:450,:] test = y.iloc[450:,:]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">The Basic Prophet Model</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Having a dataframe with the two correct column names is all you need to start creating a basic Prophet model. The modeling is not too different from scikit-learn, so it should be quite familiar to you. You can create and fit a basic model using Listing <span style=" color: #00F;">19-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-4. <span class="s33">Creating a basic Prophet model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from fbprophet import Prophet m = Prophet()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">m.fit(train)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">After the model is fit, you need to make a prediction. To make the prediction, you can use a Prophet function to create a future dataframe that will be an input to the predict function. You can then use the predict method to make the prediction. Listing <span style=" color: #00F;">19-5 </span>shows how this is done.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-5. <span class="s33">Making a prediction</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">future = m.make_future_dataframe(periods=len(test)) forecast = m.predict(future)</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 126%;text-align: left;">To assess the quality of this forecast, let’s compute the R2 score on the test data using Listing <span style=" color: #00F;">19-6</span>. Once you make the forecast, you obtain a dataframe in which the column <span class="s19">‘yhat’ </span>contains the forecast.</p><p class="s28" style="padding-top: 11pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-6. <span class="s33">Computing the R2 score</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">from sklearn.metrics import r2_score print(r2_score(list(test[&#39;y&#39;]), list(forecast.loc[450:,&#39;yhat&#39;] )))</p><p style="padding-top: 8pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">This basic Prophet model obtains an R2 score of <span class="s19">0.808</span>. This is already quite good!</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Let’s see how the model is fitting, by plotting the forecast against the test data. The plot is created using Listing <span style=" color: #00F;">19-7</span>, and it is shown in Figure <span style=" color: #00F;">19-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark205">Listing 19-7. </a><span class="s33">Plotting the fit of the model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt plt.plot(list(test[&#39;y&#39;])) plt.plot(list(forecast.loc[450:,&#39;yhat&#39;] )) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="214" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_124.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 19-1. <span class="s29">Plot of the prediction against the test data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">There are two other great plots available when using the Prophet model. The first one is showing the forecast against the observed data points for the past and future data. It can be accessed using Listing <span style=" color: #00F;">19-8</span>. You can see the plot in Figure <span style=" color: #00F;">19-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 19-8. <span class="s33">Creating a Prophet forecast plot</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">fig1 = m.plot(forecast) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="327" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_125.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark206">Figure 19-2. </a><span class="s29">Prophet forecast plot</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The second plot that you can obtain from the Prophet model is a decomposition of the different impacts of the model. This means that the decomposition can show you the impact of the different seasonalities at each time step. This can be done using Listing <span style=" color: #00F;">19-9</span>. You can see the plot in Listing <span style=" color: #00F;">19-3</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-9. <span class="s33">Creating a Prophet decomposition plot</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">fig2 = m.plot_components(forecast) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="362" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_126.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark207">Figure 19-3. </a><span class="s29">Prophet decomposition plot</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In this plot, you observe the trends that have been fitted. You see that the current model is standard fitting a long-term trend and a weekday effect. The overall trend that the model has fitted seems to be overall increasing, but the growth is slowing down. The weekly plot shows that Saturday is the big day for restaurants and then there is a trend throughout the week.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In the coming parts of this chapter, we’ll be adding all possible effects to the model so that the model becomes more and more performant.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark208">Adding Monthly Seasonality to Prophet</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">As a next step, let’s see how to add an additional seasonality effect to Prophet. In this case, we add a monthly seasonality. This is shown in Listing <span style=" color: #00F;">19-10</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-10. <span class="s33">Add a monthly seasonality to the plot</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">m2 = Prophet()</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">m2.add_seasonality(name=&#39;monthly&#39;, period=30.5, fourier_order=5) m2.fit(train)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">future2 = m2.make_future_dataframe(periods=len(test)) forecast2 = m2.predict(future)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">print(r2_score(list(test[&#39;y&#39;]), list(forecast2.loc[450:,&#39;yhat&#39;] )))</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">fig2 = m2.plot_components(forecast2) plt.show()</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The R2 score of this model is <span class="s19">0.787</span>. This is slightly lower than the previous model, so we can conclude that adding months has no added value. As this model now has a new effect, you can see that there are now three graphs in the decomposition plot. It is shown in Figure <span style=" color: #00F;">19-4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><span><img width="511" height="509" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_127.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark209">Figure 19-4. </a><span class="s29">Prophet decomposition plot with months</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Adding Holiday Data to Basic Prophet</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">As a next step, let’s see how to add holiday data to Prophet. Holidays are a standard input in Prophet, which makes it very user-friendly to add holidays. The only thing to do is to make sure that your holiday data are in a dataframe of the right format. The format that you should obtain is a dataframe with</p><ul id="l63"><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">A column called ‘ds’ that contains the date of the holiday.</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">A column called ‘holiday’ that contains the name of the holiday.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l64"><li><p style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Optionally, you can add two columns ‘upper_window’ and ‘lower_ window’ that contain the number of dates around the holiday that may be impacted by the holiday. For example, Easter and Easter Monday could be modeled together by setting the date on Easter and adding an upper_window of 1.</p><p style="padding-top: 6pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">Listing <span style=" color: #00F;">19-11 </span>shows how this can be done.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-11. <span class="s33">Prepare holiday data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">holidays = pd.read_csv(&#39;date_info.csv.zip&#39;) holidays = holidays[holidays[&#39;holiday_flg&#39;] == 1]</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">holidays = holidays[[&#39;calendar_date&#39;, &#39;holiday_flg&#39;]] holidays = holidays.drop([&#39;holiday_flg&#39;], axis=1) holidays[&#39;holiday&#39;] = &#39;holiday&#39;</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">holidays.columns = [&#39;ds&#39;, &#39;holiday&#39;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;">Then you can fit the model while adding the holiday parameter using Listing <span style=" color: #00F;">19-12</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-12. <span class="s33">Add holidays to the model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">m3 = Prophet(holidays=holidays) m3.fit(train)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">future3 = m3.make_future_dataframe(periods=len(test)) forecast3 = m3.predict(future)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">print(r2_score(list(test[&#39;y&#39;]), list(forecast3.loc[450:,&#39;yhat&#39;] )))</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">fig2 = m3.plot_components(forecast3) plt.show()</p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;">With this model, you obtain an R2 score of <span class="s19">0.806</span>. It’s again not any better than the default model, meaning that the holidays may not help the model to predict better. The plot with the components is shown in Figure <span style=" color: #00F;">19-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="551" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_128.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark210">Figure 19-5. </a><span class="s29">Prophet decomposition plot with holidays</span></p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark211">Adding an Extra Regressor to Prophet</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Also available in the dataset is a file with reservations. Of course, we could expect reservations to be a great additional variable to forecast the number of restaurant visitors. You can add an additional regressor to a Prophet model quite easily. First, add the additional variable into the dataframe that contains the columns ‘ds’ and ‘y’. You can do this using Listing <span style=" color: #00F;">19-13</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-13. <span class="s33">Add reservations to the data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_reservations = pd.read_csv(&#39;air_reserve.csv.zip&#39;) X_reservations[&#39;visit_date&#39;] = pd.to_datetime(X_reservations[&#39;visit_ datetime&#39;]).dt.date</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_reservations = pd.DataFrame(X_reservations.groupby(&#39;visit_date&#39;) [&#39;reserve_visitors&#39;].sum())</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_reservations = X_reservations.reset_index(drop = False) train4 = train.copy()</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">train4[&#39;ds&#39;] = pd.to_datetime(train4[&#39;ds&#39;]).dt.date</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">train4 = train4.merge(X_reservations, left_on = &#39;ds&#39;, right_on = &#39;visit_date&#39;, how = &#39;left&#39;)[[&#39;ds&#39;, &#39;y&#39;, &#39;reserve_visitors&#39;]].fillna(0)</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Then in the model fitting, you can use the method “add_regressor” to add it into the model. You also need to add the additional variable into the future dataframe. You can use Listing <span style=" color: #00F;">19-14 </span>to do this.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-14. <span class="s33">Add reservations to the model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">m4 = Prophet() m4.add_regressor(&#39;reserve_visitors&#39;) m4.fit(train4)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">future4 = m4.make_future_dataframe(periods=len(test)) future4[&#39;ds&#39;] = pd.to_datetime(future4[&#39;ds&#39;]).dt.date</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">future4 = future4.merge(X_reservations, left_on = &#39;ds&#39;, right_on = &#39;visit_date&#39;, how = &#39;left&#39;)[[&#39;ds&#39;, &#39;reserve_visitors&#39;]].fillna(0)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">forecast4 = m4.predict(future4)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 24pt;text-align: left;">print(r2_score(list(test[&#39;y&#39;]), list(forecast4.loc[450:,&#39;yhat&#39;] ))) plt.plot(list(test[&#39;y&#39;]))</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">plt.plot(list(forecast4.loc[450:,&#39;yhat&#39;] ))</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">fig2 = m4.plot_components(forecast4) plt.show()</p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">This model obtains an R2 score of <span class="s19">0.849</span>. This is better than any of the previous configurations, and it means that the number of reservations is a good addition to the model. You can see the plot of the predictions in Figure <span style=" color: #00F;">19-6</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="214" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_129.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 19-6. <span class="s29">Predictions against the actual data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can also see the different components fitted by this model in Figure <span style=" color: #00F;">19-7</span>. In this specific model, you see the impact of the additional variable “reservations.”</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="551" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_130.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark212">Figure 19-7. </a><span class="s29">Decomposition of the model</span></p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark213">Tuning Hyperparameters Using Grid Search</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">To recap, let’s list the different variants of the Prophet model that we’ve seen until here in Table <span style=" color: #00F;">19-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 74pt;text-indent: 0pt;text-align: left;">Table 19-1. <span class="s29">Results of the Prophet models</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:74.75pt" cellspacing="0"><tr style="height:24pt"><td style="width:157pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Model Variant</p></td><td style="width:106pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">R2 Score</p></td></tr><tr style="height:22pt"><td style="width:157pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s30" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Basic Prophet</p></td><td style="width:106pt;border-top-style:solid;border-top-width:1pt;border-top-color:#2B2A29"><p class="s30" style="padding-top: 5pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">0.808</p></td></tr><tr style="height:20pt"><td style="width:157pt"><p class="s30" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">With monthly seasonality</p></td><td style="width:106pt"><p class="s30" style="padding-top: 3pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">0.787</p></td></tr><tr style="height:20pt"><td style="width:157pt"><p class="s30" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">With holidays</p></td><td style="width:106pt"><p class="s31" style="padding-top: 3pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">0.806</p></td></tr><tr style="height:22pt"><td style="width:157pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s30" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">With reservations as regressor</p></td><td style="width:106pt;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#2B2A29"><p class="s31" style="padding-top: 3pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">0.849</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now 0.849 is a great score. We have now covered all additions that we can add to the model: extra seasonality, holidays, and additional regressors. There is still room for improvement by tuning a number of hyperparameters. The hyperparameters of the Prophet model are as follows:</p></li></ul></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;line-height: 128%;text-align: left;">The <span class="s19">Fourier order </span>can be set for each seasonality. A higher Fourier order means higher frequency of changes, which means that seasonality curves with a higher Fourier order are less smooth.</p></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;line-height: 127%;text-align: left;">The <span class="s19">changepoint_prior_scale </span>allows the trend to be fit more or less flexible. The higher the value, the more flexible the trend.</p></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;line-height: 128%;text-align: left;">The <span class="s19">holidays_prior_scale </span>allows holidays’ effect to be less important. The default value is 10, and the lower it gets, the less important the holidays are.</p></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;line-height: 127%;text-align: left;">The <span class="s19">seasonalities </span>also have a <span class="s19">prior scale</span>. For this tuning example, we will leave them out; but know that you can tune them as well.</p><p style="padding-top: 6pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">We will do a grid search of those hyperparameters to see whether this can improve the model. When we are at it, we might as well add the previous tests into the equation again. After all, there is a possibility that when changing some of the new hyperparameters, this changes the optimum on the other choices.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">As covered before, a basic approach to grid search is to loop through any combination of values for hyperparameters and then select the best-performing combination. Listing <span style=" color: #00F;">19-15 </span>shows how to go about this.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 19-15. <span class="s33">Grid searching Prophet</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">def model_test(holidays, weekly_seasonality,</p><p class="s34" style="padding-top: 1pt;padding-left: 70pt;text-indent: 0pt;line-height: 113%;text-align: left;">yearly_seasonality, add_monthly, add_reserve, changepoint_prior_ scale, holidays_prior_scale, month_fourier):</p><p class="s34" style="padding-top: 7pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">m4 = Prophet(</p><p class="s34" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;">yearly_seasonality=yearly_seasonality, weekly_seasonality=weekly_seasonality, holidays=holidays, changepoint_prior_scale=changepoint_prior_scale, holidays_prior_scale=holidays_prior_scale)</p><p class="s34" style="padding-top: 7pt;padding-left: 70pt;text-indent: -22pt;line-height: 113%;text-align: left;">if add_monthly: m4.add_seasonality(</p><p class="s34" style="padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;">name=&#39;monthly&#39;, period=30.5,</p><p class="s34" style="padding-left: 92pt;text-indent: 0pt;line-height: 14pt;text-align: left;">fourier_order=month_fourier)</p><p class="s34" style="padding-top: 9pt;padding-left: 70pt;text-indent: -22pt;line-height: 113%;text-align: left;">if add_reserve: m4.add_regressor(&#39;reserve_visitors&#39;)</p><p class="s34" style="padding-top: 7pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">m4.fit(train4)</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 24pt;text-align: justify;">future4 = m4.make_future_dataframe(periods=len(test)) future4[&#39;ds&#39;] = pd.to_datetime(future4[&#39;ds&#39;]).dt.date if add_reserve:</p><p class="s34" style="padding-top: 1pt;padding-left: 92pt;text-indent: -22pt;line-height: 113%;text-align: left;">future4 = future4.merge( X_reservations, left_on = &#39;ds&#39;,</p><p class="s34" style="padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;">right_on = &#39;visit_date&#39;, how = &#39;left&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 52pt;text-indent: 0pt;line-height: 113%;text-align: left;">future4 = future4[[&#39;ds&#39;, &#39;reserve_visitors&#39;]] future4 = future4.fillna(0)</p><p class="s34" style="padding-top: 7pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">forecast4 = m4.predict(future4)</p><p class="s34" style="padding-top: 9pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">return r2_score(</p><p class="s34" style="padding-top: 1pt;padding-left: 96pt;text-indent: 0pt;line-height: 113%;text-align: left;">list(test[&#39;y&#39;]), list(forecast4.loc[450:,&#39;yhat&#39;] ))</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"># Setting the grid</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">holidays_opt = [holidays, None] weekly_seas = [ 5, 10, 30, 50]</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">yearly_seas = [ 5, 10, 30, 50] add_monthly = [True, False] add_reserve = [True, False]</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">changepoint_prior_scale = [0.1, 0.3, 0.5]</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">holidays_prior_scale = [0.1, 0.3, 0.5]</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">month_fourier = [5, 10, 30, 50]</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Looping through the grid grid_results = []</p><p class="s34" style="padding-left: 19pt;text-indent: -11pt;line-height: 113%;text-align: left;">for h in holidays_opt: for w in weekly_seas:</p><p class="s34" style="padding-left: 41pt;text-indent: -11pt;line-height: 113%;text-align: left;">for ys in yearly_seas: for m in add_monthly:</p><p class="s34" style="padding-left: 52pt;text-indent: 0pt;line-height: 14pt;text-align: left;">for r in add_reserve:</p><p class="s34" style="padding-top: 1pt;padding-left: 79pt;text-indent: -11pt;line-height: 113%;text-align: left;">for c in changepoint_prior_scale: for hp in holidays_prior_scale:</p><p class="s34" style="padding-left: 107pt;text-indent: -16pt;line-height: 113%;text-align: left;">for mf in month_fourier: r2=model_test(h,w,ys,m,r,c,hp,mf) print([w,ys,m,r,c,hp,mf,r2]) grid_results.append([h,w,ys,m,r,c,hp,mf,r2])</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># adding it all to a dataframe and extract the best model benchmark = pd.DataFrame(grid_results)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">benchmark = benchmark.sort_values(8, ascending=False)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">h, w,ys, m, r, c,hp,mf,r2 = list(benchmark.iloc[0,:])</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Fit the Prophet with those best hyperparameters m4 = Prophet(</p><p class="s34" style="padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;">yearly_seasonality=ys, weekly_seasonality=w, holidays=h, changepoint_prior_scale=c, holidays_prior_scale=hp)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">if m:</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">m4.add_seasonality(</p><p class="s34" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;">name=&#39;monthly&#39;, period=30.5, fourier_order=mf)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">if r:</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 22pt;line-height: 170%;text-align: left;">m4.add_regressor(&#39;reserve_visitors&#39;) m4.fit(train4)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 170%;text-align: left;">future4 = m4.make_future_dataframe(periods=len(test)) future4[&#39;ds&#39;] = pd.to_datetime(future4[&#39;ds&#39;]).dt.date</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">if r:</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">future4 = future4.merge(</p><p class="s34" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_reservations, left_on = &#39;ds&#39;,</p><p class="s34" style="padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;">right_on = &#39;visit_date&#39;, how = &#39;left&#39;)</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">future4 = future4[[&#39;ds&#39;, &#39;reserve_visitors&#39;]] future4 = future4.fillna(0)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">forecast4 = m4.predict(future4)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: left;">This tuned model gives an R2 score of <span class="s19">0.928</span>. The model uses the trend, the weekly and yearly seasonality, together with the number of reservations as an extra regressor. The weekly seasonality order is <span class="s19">50</span>, and the yearly seasonality Fourier order is <span class="s19">10</span>. The “changepoint_prior_scale” is <span class="s19">0.1</span>, and the “holidays_prior_scale” does not matter as holidays aren’t used in the final model. You can see the predictive performance in Figure <span style=" color: #00F;">19-8</span>, and the decomposition of this final model is shown in Figure <span style=" color: #00F;">19-9</span>.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">It may be possible that a better result lies outside of the grid. Feel free to change the grid parameters and rerun the grid search to see if you can obtain an even better score.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="214" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_131.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 19-8. <span class="s29">Predictive performances of the tuned model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="459" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_132.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark214">Figure 19-9. </a><span class="s29">Decomposition of the tuned model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4><ul id="l65"><li><p style="padding-top: 10pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">Facebook Prophet is an automated procedure for building forecasting models developed by Facebook.</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The model is easy to set up and tune, which is a great advantage.</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">Input possibilities are</p><ul id="l66"><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">Seasonality of any regular order</p></li><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">Holidays</p></li><li><p style="padding-top: 9pt;padding-left: 90pt;text-indent: -17pt;text-align: left;">Additional regressors</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li></ul></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">You can use Prophet for univariate models only. For multivariate models, you need to build multiple Prophet models.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">The model’s goal is to obtain good results without too much intervention, but there are a few hyperparameters that you can tune:</p><ul id="l67"><li><p class="s20" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;line-height: 126%;text-align: left;">The Fourier order of the seasonality<span class="p">: A higher order means more flexibility.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;line-height: 126%;text-align: left;">The <span class="p">changepoint_prior_scale </span>plays on the trend<span class="p">: The higher the value, the more flexible the trend.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;line-height: 126%;text-align: left;">The <span class="p">holidays_prior_scale: The lower it is, the less important the holidays are for the model.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">The prior scale for the seasonality</p></li></ul></li></ul><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark215">CHAPTER 20</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">The DeepAR Model</h1><p class="s19" style="padding-top: 21pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">DeepAR <span class="p">is a model developed by researchers at Amazon. DeepAR provides an interface to building time series models using a deep learning architecture based on RNNs. The advantage of using DeepAR is that it comes with an interface that is easier to use for model building when compared to Keras.</span></p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">DeepAR can be considered a competitor with Facebook’s Prophet that you have seen in the previous chapter. Both models try to deliver a simple-to-use model interface for models that are very complex under the hood. If you want to obtain good models with relatively little work, DeepAR and Prophet are definitely worth adding to your model benchmark.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">About DeepAR</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">DeepAR forecasts univariate or multivariate time series using RNNs. The theoretical added value of DeepAR is that it fits a single model on all the time series at the same time. It is designed to benefit from correlations between multiple time series, and therefore it is a great model for multivariate forecasting. Yet it can also be applied to a single time series.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">DeepAR models multiple types of seasonality variables. It also creates variables for the day of the month, day of the year, and other derived variables. As a user</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">of the DeepAR model, we do not have much control over those variables and the inner workings of the algorithm. The model is relatively a black box aside from a few hyperparameters that we will come back to later.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s24">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_20#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_20#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_20</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">273</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;"><a name="bookmark216">Model Training with DeepAR</a></h4><p class="s68" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 120%;text-align: justify;"><a href="http://www.kaggle.com/c/recruit-restaurant-visitor-forecasting" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">To discover the DeepAR model, we will use the dataset on restaurant visits that was used in the previous chapter. This way, we can do a benchmark of DeepAR against Facebook’s Prophet. The data are accessible on </a><a href="http://www.kaggle.com/c/recruit-restaurant-visitor-forecasting" class="s32" target="_blank">www.kaggle.com/c/recruit-restaurant-visitor- </a>forecasting<span class="p">. You can prepare the data by doing the sum of all restaurants using</span></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Listing <span style=" color: #00F;">20-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Listing 20-1. <span class="s33">Importing the data</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">import pandas as pd</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">y = pd.read_csv(&#39;air_visit_data.csv.zip&#39;)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">y = y.pivot(index=&#39;visit_date&#39;, columns=&#39;air_store_id&#39;)[&#39;visitors&#39;] y = y.fillna(0)</p><p class="s34" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">y = pd.DataFrame(y.sum(axis=1))</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">y = y.reset_index(drop=False) y.columns = [&#39;date&#39;, &#39;y&#39;]</p><p style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">As a recap, the tuned, final, Facebook Prophet model obtained an R2 Score of 0.928.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_133.png"/></span></p><p class="s68" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 120%;text-align: left;"><a href="https://ts.gluon.ai/install.html" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;" target="_blank">Let’s see whether DeepAR is able to match this performance. We will be using the DeepAR model that is presented by the gluonts library. I recommend using the following link for installation instructions and understanding the dependencies: </a><a href="https://ts.gluon.ai/install.html" class="s32" target="_blank">https:// </a>ts.gluon.ai/install.html<span class="p">.</span></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 125%;text-align: justify;">To use this library, we need to start by preparing the data to be in the format that this library understands. Gluonts uses a relatively unintuitive data format. It uses an object called <span class="s19">ListDataset</span>. It must contain <span class="s20">the target variable, a start timestamp, and a frequency</span>. The train data needs to contain the train data only, whereas the test data needs to contain the train and the test data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can build those objects using Listing <span style=" color: #00F;">20-2</span>. Note: Unlogically, it is necessary to specify the seasonality “H” for hourly rather than “D” for daily to obtain a reasonably accurate forecast. This is likely due to an unresolved bug in gluonts at the time of writing.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 20-2. <span class="s33">Preparing the data format required by the gluonts library</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">from gluonts.dataset.common import ListDataset start = pd.Timestamp(&quot;01-01-2016&quot;, freq=&quot;H&quot;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark217"># train dataset: cut the last window of length &quot;prediction_length&quot;, add &quot;target&quot; and &quot;start&quot; fields</a></p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">train_ds = ListDataset([{&#39;target&#39;: y.loc[:450,&#39;y&#39;], &#39;start&#39;: start}], freq=&#39;H&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># test dataset: use the whole dataset, add &quot;target&quot; and &quot;start&quot; fields test_ds = ListDataset([{&#39;target&#39;: y[&#39;y&#39;], &#39;start&#39;: start}],freq=&#39;H&#39;)</p><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 17pt;line-height: 122%;text-align: justify;">Now, let’s start building a simple DeepAR model using gluonts in Listing <span style=" color: #00F;">20-3</span>. To do this, you need to create an <span class="s19">estimator </span>called the <span class="s19">DeepAREstimator</span>, which in turn uses a <span class="s19">trainer. </span>The important arguments in this function are the following:</p><ul id="l68"><li><p style="padding-top: 5pt;padding-left: 73pt;text-indent: -12pt;line-height: 127%;text-align: left;">The <span class="s19">prediction_length</span>, which is the number of steps that you want to predict.</p></li><li><p style="padding-top: 5pt;padding-left: 73pt;text-indent: -12pt;line-height: 128%;text-align: left;">The <span class="s19">ctx </span>is where you can specify whether you want to run on CPU or on GPU. Running on GPU is much faster, but you can do this only if you have a GPU available on your hardware.</p></li><li><p style="padding-top: 5pt;padding-left: 73pt;text-indent: -12pt;line-height: 127%;text-align: left;">The number of <span class="s19">epochs </span>is the number of times that you want all the data to be passed through the underlying deep neural network.</p></li><li><p style="padding-top: 5pt;padding-left: 73pt;text-indent: -12pt;line-height: 129%;text-align: left;">The <span class="s19">learning rate </span>is the step size for the optimizer of the neural network: a larger learning rate allows you to make larger steps, but it can miss the optimum, whereas a smaller learning rate may make the optimization slow or let you get stuck in a local optimum.</p></li></ul><p class="s28" style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 20-3. <span class="s33">Fitting the default DeepAR model</span></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 106%;text-align: left;">from gluonts.model.deepar import DeepAREstimator from gluonts.trainer import Trainer</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 106%;text-align: left;">import mxnet as mx import numpy as np</p><p class="s34" style="padding-top: 8pt;padding-left: 26pt;text-indent: 0pt;line-height: 106%;text-align: left;">np.random.seed(7) mx.random.seed(7)</p><p class="s34" style="padding-top: 8pt;padding-left: 48pt;text-indent: -22pt;line-height: 106%;text-align: left;">estimator = DeepAREstimator( prediction_length=28, context_length=100, freq=&#39;H&#39;,</p><p class="s34" style="padding-top: 1pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">trainer=Trainer(ctx=&quot;gpu&quot;, # remove if running on windows</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 118pt;text-indent: 0pt;line-height: 113%;text-align: left;"><a name="bookmark218">epochs=5, learning_rate=1e-3, num_batches_per_epoch=100</a></p><p class="s34" style="padding-left: 112pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">predictor = estimator.train(train_ds)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 10pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Predictions with DeepAR</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Then to make predictions on the test set, you can use Listing <span style=" color: #00F;">20-4</span>. There is something complicated going on in this step. The model doesn’t simply make one prediction, but rather many predictions. It makes, by default, a hundred predictions per time step, each an alternative future trajectory.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This may sound weird, as you generally want to have just one forecast. This forecast should be the most accurate possible. Yet the idea is having forecasted a large number of potential trajectories, you can take the median of those 100 alternative trajectories as a forecast. The median is also called the 0.5th quantile.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 20-4. <span class="s33">Prediction</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">predictions = predictor.predict(test_ds) predictions = list(predictions)[0] predictions = predictions.quantile(0.5)</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Let’s compute the test R2 score and make a plot to see what those predictions look like using Listing <span style=" color: #00F;">20-5</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 20-5. <span class="s33">R2 score and prediction graph</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">from sklearn.metrics import r2_score</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">print(r2_score( list(test_ds)[0][&#39;target&#39;][-28:], predictions))</p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">import matplotlib.pyplot as plt plt.plot(predictions) plt.plot(list(test_ds)[0][&#39;target&#39;][-28:]) plt.legend([&#39;predictions&#39;, &#39;actuals&#39;]) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 26pt;text-indent: 17pt;line-height: 128%;text-align: left;"><a name="bookmark219">The R2 score that this model obtains is </a><span class="s19">0.83</span>. Be aware that DeepAR predictions are not the same every run, even though the random seed has been set. You may obtain a different outcome. You can see the prediction graph in Figure <span style=" color: #00F;">20-1</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="340" height="214" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_134.jpg"/></span></p><p class="s28" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Figure 20-1. <span class="s29">Predictions of the default DeepAR forecast</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Probability Predictions with DeepAR</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Since DeepAR builds many possible trajectories of one and the same forecast, this allows to make a similar graph, but that adds a confidence interval around the curve. You can use Listing <span style=" color: #00F;">20-6 </span>to obtain the graph, and it is shown in Figure <span style=" color: #00F;">20-2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Listing 20-6. <span class="s33">Probability forecast graph</span></p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 24pt;text-align: left;">from gluonts.evaluation.backtest import make_evaluation_predictions forecast_it, ts_it = make_evaluation_predictions(</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 113%;text-align: left;">dataset=test_ds, # test dataset predictor=predictor, # predictor</p><p class="s34" style="padding-left: 48pt;text-indent: 0pt;line-height: 14pt;text-align: left;">num_samples=100, # number of sample paths we want for evaluation</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">forecasts = list(forecast_it) tss = list(ts_it)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">ts_entry = tss[0]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark220">forecast_entry = forecasts[0]</a></p><p class="s34" style="padding-top: 8pt;padding-left: 30pt;text-indent: -22pt;line-height: 106%;text-align: left;">def plot_prob_forecasts(ts_entry, forecast_entry): plot_length = 150</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;text-align: left;">prediction_intervals = (50.0, 90.0)</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 106%;text-align: left;">legend = [&quot;observations&quot;, &quot;median prediction&quot;] + [f&quot;{k}% prediction interval&quot; for k in prediction_intervals][::-1]</p><p class="s34" style="padding-top: 8pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">fig, ax = plt.subplots(1, 1, figsize=(10, 7))</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_entry[-plot_length:].plot(ax=ax) # plot the time series forecast_entry.plot(prediction_intervals=prediction_intervals, color=&#39;g&#39;)</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 106%;text-align: left;">plt.grid(which=&quot;both&quot;) plt.legend(legend, loc=&quot;upper left&quot;) plt.show()</p><p class="s34" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">plot_prob_forecasts(ts_entry, forecast_entry) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 0pt;text-align: left;"><span><img width="529" height="371" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_135.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Figure 20-2. <span class="s29">Plot of the probabilities around the forecast</span></p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark221">Adding Extra Regressors to DeepAR</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">To go further with DeepAR, let’s discover how to add additional regressors to the model. In many cases of forecasting, you have information about the future that can improve the quality of your forecast. In the case of the restaurant visits forecast, we have two additional datasets: the holidays and the restaurant reservations.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can imagine that those will help the model to learn better. The advantage of using a model like DeepAR is that the only thing that you have to do is to update the ListDataset to include those additional variables and it will automatically model them. The model itself will decide which balance to make between seasonality and additional regressors. Let’s see how to specify the ListDataset with additional regressors in Listing <span style=" color: #00F;">20-7</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 26pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 20-7. <span class="s33">Preparing holiday and reservation data and adding them into the ListDataset</span></p><p class="s34" style="padding-top: 6pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_reservations = pd.read_csv(&#39;air_reserve.csv.zip&#39;) X_reservations[&#39;visit_date&#39;] = pd.to_datetime(X_reservations[&#39;visit_ datetime&#39;]).dt.date</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">X_reservations = pd.DataFrame(X_reservations.groupby(&#39;visit_date&#39;) [&#39;reserve_visitors&#39;].sum())</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">X_reservations = X_reservations.reset_index(drop = False)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Convert to datatime for merging correctly y.date = pd.to_datetime(y.date)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">X_reservations.visit_date = pd.to_datetime(X_reservations.visit_date)</p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Merging and filling missing dates with 0</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">y = y.merge(X_reservations, left_on = &#39;date&#39;, right_on = &#39;visit_date&#39;, how = &#39;left&#39;).fillna(0)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Preparing and merging holidays data holidays = pd.read_csv(&#39;date_info.csv.zip&#39;)</p><p class="s34" style="padding-left: 26pt;text-indent: 0pt;line-height: 14pt;text-align: left;">holidays.calendar_date = pd.to_datetime(holidays.calendar_date)</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">y = y.merge(holidays, left_on = &#39;date&#39;, right_on = &#39;calendar_date&#39;, how = &#39;left&#39;).fillna(0)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Preparing the ListDatasets</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 30pt;text-indent: -22pt;line-height: 113%;text-align: left;"><a name="bookmark222">train_ds = ListDataset([{ &#39;target&#39;: y.loc[:450,&#39;y&#39;], &#39;start&#39;: start,</a></p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;feat_dynamic_real&#39;: y.loc[:450,[&#39;reserve_visitors&#39;, &#39;holiday_flg&#39;]]. values</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 14pt;text-align: left;">}], freq=&#39;H&#39;)</p><p class="s34" style="padding-top: 9pt;padding-left: 30pt;text-indent: -22pt;line-height: 113%;text-align: left;">test_ds = ListDataset([{ &#39;target&#39;: y[&#39;y&#39;], &#39;start&#39;: start,</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">&#39;feat_dynamic_real&#39;: y.loc[:,[&#39;reserve_visitors&#39;, &#39;holiday_flg&#39;]]. values</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 14pt;text-align: left;">}],freq=&#39;H&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Now to use this model, you can simply use the exact same code as before: the change has to be done only in the ListDataset as a feat_entry_real. DeepAR will automatically understand this as an additional regressor, and it’ll know what to do with it. As proof, Listing <span style=" color: #00F;">20-8 </span>fits the model and prints the R2 score of the model with regressors holidays and reservations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;line-height: 123%;text-align: left;">Listing 20-8. <span class="s33">Same code for fitting a different model: this model contains the two additional regressors</span></p><p class="s34" style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">np.random.seed(7) mx.random.seed(7)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Build and fit model estimator = DeepAREstimator(</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 113%;text-align: left;">prediction_length=28, context_length=100, freq=&#39;H&#39;,</p><p class="s34" style="padding-left: 118pt;text-indent: -88pt;line-height: 113%;text-align: left;">trainer=Trainer(ctx=&quot;gpu&quot;, # remove if running on windows epochs=5,</p><p class="s34" style="padding-left: 118pt;text-indent: 0pt;line-height: 113%;text-align: left;">learning_rate=1e-3, num_batches_per_epoch=100</p><p class="s34" style="padding-left: 112pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s34" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark223">predictor = estimator.train(train_ds)</a></p><p class="s34" style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Make Predictions</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 113%;text-align: left;">predictions = predictor.predict(test_ds) predictions = list(predictions)[0] predictions = predictions.quantile(0.5)</p><p class="s34" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"># Compute and print R2 score</p><p class="s34" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">print(r2_score( list(test_ds)[0][&#39;target&#39;][-28:], predictions))</p><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The R2 score obtained by this model is <span class="s19">0.872</span>. Be aware that your result may vary due to uncontrolled randomness. This R2 score is better than the R2 score of the model without the additional regressors, so let’s keep them in our dataset while moving forward.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_136.png"/></span></p><p class="s38" style="padding-top: 7pt;padding-left: 33pt;text-indent: 0pt;line-height: 113%;text-align: left;">Note <span class="s39">often, when running models on CpU vs. GpU or when using distributed computation, it is very difficult to control randomness. this can be a disadvantage, but GpU and distributed computation make the execution so much faster that it is often worth it to lose the control over your randomness. after all, the randomness is, at least theoretically, not supposed to play a role in obtaining good results.</span></p><p class="s39" style="padding-left: 33pt;text-indent: 0pt;line-height: 113%;text-align: left;">When you have inconsistent results, the best thing is to test multiple runs and try to estimate how much variation you observe. If this is too much for having reliable forecasts, you can always switch to simpler models or try and tweak the model to become more stable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="555" height="1" alt="image" src="Joos Korstanje - Advanced Forecasting with Python_ With State-of-the-Art-Models Including LSTMs, Facebook’s Prophet, and Amazon’s DeepAR-Apress (2021)/Image_137.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Hyperparameters of the DeepAR</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Now it’s time to look at some hyperparameters and see how this model can be tuned. Three hyperparameters will be added into a small grid search loop to see whether they can improve the R2 score:</p><ul id="l69"><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">learning_rate</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">num_layers: The number of RNN layers (default: 2)</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">num_cells: The number of RNN cells for each layer (default: 40)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">You can use Listing <span style=" color: #00F;">20-9 </span>to do a loop through a selection of values for those three hyperparameters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Listing 20-9. <span class="s33">Tuning the hyperparameters</span></p><p class="s34" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">np.random.seed(7) mx.random.seed(7)</p><p class="s34" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">results = []</p><p class="s34" style="padding-top: 9pt;padding-left: 19pt;text-indent: -11pt;line-height: 113%;text-align: left;">for learning_rate in [1e-4, 1e-2]: for num_layers in [2, 5]:</p><p class="s34" style="padding-left: 30pt;text-indent: 0pt;line-height: 14pt;text-align: left;">for num_cells in [30, 100]:</p><p class="s34" style="padding-top: 9pt;padding-left: 63pt;text-indent: -22pt;line-height: 113%;text-align: left;">estimator = DeepAREstimator( prediction_length=28, freq=&#39;H&#39;,</p><p class="s34" style="padding-left: 63pt;text-indent: 0pt;line-height: 14pt;text-align: left;">trainer=Trainer(ctx=&quot;gpu&quot;, # remove if on Windows</p><p class="s34" style="padding-top: 1pt;padding-left: 151pt;text-indent: 0pt;line-height: 113%;text-align: left;">epochs=10, learning_rate=learning_rate, num_batches_per_epoch=100</p><p class="s34" style="padding-left: 140pt;text-indent: 0pt;line-height: 14pt;text-align: left;">),</p><p class="s34" style="padding-top: 1pt;padding-left: 63pt;text-indent: 0pt;line-height: 113%;text-align: left;">num_layers = num_layers, num_cells = num_cells,</p><p class="s34" style="padding-left: 41pt;text-indent: 0pt;line-height: 14pt;text-align: left;">)</p><p class="s34" style="padding-top: 9pt;padding-left: 41pt;text-indent: 0pt;line-height: 170%;text-align: left;">predictor = estimator.train(train_ds) predictions = predictor.predict(test_ds)</p><p class="s34" style="padding-left: 41pt;text-indent: 0pt;line-height: 113%;text-align: left;">r2 = r2_score(list(predictions)[0].quantile(0.5), list(test_ds)[0] [&#39;target&#39;][-28:])</p><p class="s34" style="padding-left: 41pt;text-indent: 0pt;line-height: 113%;text-align: left;">result = [learning_rate, num_layers, num_cells, r2] print(result)</p><p class="s34" style="padding-left: 41pt;text-indent: 0pt;line-height: 14pt;text-align: left;">results.append(result)</p><p style="padding-top: 10pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">The best model coming out of this grid search has a learning rate of <span class="s19">0.01</span>, with <span class="s19">two</span></p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">RNN layers and <span class="s19">100 </span>cells per layer. This model obtains an R2 score of <span class="s19">0.874</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark224">This hyperparameter search is relatively short on purpose, as the model takes quite some time to run. As a more general rule, here is an overview of all the hyperparameters that you could try to tune according to the DeepAR literature:</a></p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">learning_rate between 1e-5 and 1e-1</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">num_layers between 1 and 8 (default: 2)</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">num_cells between 30 and 200 (default: 40)</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">epochs between 1 and 1000</p></li><li><p style="padding-top: 9pt;padding-left: 72pt;text-indent: -17pt;line-height: 129%;text-align: left;">context_length: The number of steps to unroll the RNN before computing predictions between 1 and 200 (default: none)</p></li><li><p style="padding-top: 6pt;padding-left: 72pt;text-indent: -17pt;text-align: left;">dropout_rate: The dropout regularization parameter between 0 and</p></li></ul><p style="padding-top: 3pt;padding-left: 72pt;text-indent: 0pt;text-align: left;">0.2 (default: 0.1)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Benchmark and Conclusion</h4><p style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;line-height: 121%;text-align: left;">Now using DeepAR, you were able to obtain an R2 score of <span class="s19">0.874</span>. This is great, but it is not beating the best-tuned version of Facebook’s Prophet, which obtained <span class="s19">0.928</span>.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">We’ve observed something difficult in DeepAR: it is hard to control randomness in this model. When making the forecast, DeepAR makes a large number of potential</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">trajectories, and the median of those can be used as final forecast. Yet when running the code a second time, those trajectories are not the same, and therefore their median will differ a bit as well. This could be considered a disadvantage of DeepAR.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">If you want to use this model for accurate forecasting, you will need to analyze the variability of the model thoroughly. DeepAR is a powerful model and can also be</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">used as a great analysis tool. Obtaining a large number of trajectories can also give you very useful boundaries of probability. In some cases, it may be much more useful to understand the different scenarios that could occur rather than have one, most accurate forecast.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">For example, imagine that you’re forecasting the number of restaurant visitors to estimate whether you’ll make enough money to open a second restaurant. In this case, it may be very valuable to have a lower bound of the number of visitors. If you would go bankrupt with this low scenario, you know that you take a serious risk if you’d choose to open the restaurant.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">For forecasting accurately, there are still other elements to consider: DeepAR is much more suitable than Prophet for doing multivariate forecasting. This was not included in the benchmark as it is simply impossible with Prophet. With Prophet you’d have to develop and tune a model for each time series, whereas one tuned DeepAR model can work on all the time series at once.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">In the next chapter, we’ll do a final overview of model benchmarking and model selections to understand which data and other decision rules to use for those decisions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4><ul id="l70"><li><p style="padding-top: 10pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">DeepAR is a model built by Amazon that builds deep forecasts based on RNNs.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">The gluonts package implements DeepAR in the DeepAREstimator.</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">The DeepAR model can learn trends from many time series at the same time and make forecasts of multiple time series.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">When predicting, it predicts a large number of potential trajectories. You can use the median of those trajectories as forecast.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">DeepAR has a large component of randomness, and it is difficult to obtain reproducible results. This can be a no-go depending on the exact use case that you are working on.</p></li></ul><p class="s22" style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark225">CHAPTER 21</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Model Selection</h1><p style="padding-top: 22pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">In the last 18 chapters, you’ve seen a long list of machine learning models that can be used for forecasting. In this chapter, you’ll find some final reflections on how to decide which of the models to use for your practical use cases.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Model Selection Based on Metrics</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">The book started with an overview of model metrics. Throughout the different chapters, a number of model benchmarks have been done based on R2 scores. This was done firstly in Chapter <span style=" color: #00F;">15</span>, where XGBoost performances were benchmarked against LightGBM performances. Another benchmark was presented in Chapter <span style=" color: #00F;">17</span>, between the SimpleRNN, GRU, and LSTM.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The benchmarks presented in this book were based on metrics. When using this approach, there are two general approaches. A first approach is to build a train and a test dataset. You then tune and optimize a number of models on the train data, after which you compute the R2 score on the test data. The model that performs best on the test data is selected.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The second common way to do metrics-based model selection is to use cross- validation. When you have little data, it can be costly to keep out data for a test set, and cross-validation can be a solution to have metrics-based model selection. Another advantage of using cross-validation is that each model is trained and tested multiple times, so randomness is less influential. After all, the test could be favorable to a certain model, purely due to the random selection process and bad luck.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Yet in practice, there are other things than metrics to consider when choosing the right model for your use case. Let’s see a few of them.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 9pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s174">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6_21#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6_21#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6_21</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;">285</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark226">Model Structure and Inputs</a></h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">Another thing to think about when selecting a model is that it needs to fit with the underlying variation of your dataset. For example, it does not make sense to use a model with autoregressive components if you know empirically that there is no autoregressive effect in your use case. Let’s do a quick recap of the types of components that can be modeled in the different chapters of this book.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 124%;text-align: left;">We started with <span class="s167">univariate time series models</span>, which allow forecasting one variable based on past variation in itself. Those models are useful when you either don’t have any other data to use in the modeling process or, more importantly, when there really is <span class="s167">a time series component </span>(like autoregression, moving average, etc.) present in the data.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">After that, we covered <span class="s167">multivariate time series models</span>, which do the same thing but applied to multiple time series at the same time. They have an advantage when you need to forecast multiple time series at the same time, as you have to build only one model for multiple time series rather than building a model for each. Also, it can benefit from shared variation between the time series and have an advantage in performance.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 128%;text-align: left;">Then you’ve seen how to use a number of classical <span class="s167">supervised machine learning models </span>for forecasting. From a high level, those models all work in the same way. They convert a number of input variables into a target variable. To add seasonality to those models, you can convert the seasonality information into input variables. This type of model is great if you have not just a target variable, but you also have other information about the future that you can use for forecasting, for example, using the number of restaurant reservations when trying to forecast restaurant visits.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 126%;text-align: left;">We then moved on to using multiple types of <span class="s167">Neural Networks</span>. The first type of Neural Network that we saw was a <span class="s167">Dense Feedforward Neural Network</span>. This neural network is really similar to the way that classical supervised machine learning models work: there are input variables and one (or multiple) target variable(s).</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">The other Neural Networks that we have seen were <span class="s167">Recurrent Neural Networks</span>. They are quite different as they are made to learn and predict sequences of data. The input data for RNNs have therefore to be prepared as sequences. You can also add external variables into RNN models.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Neural Networks can be very performant when parameterized well. Yet tuning and parameterizing a neural network can be hard: it takes a lot of time from the developer, but also computation time to get to a good result.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark227">Lastly, you’ve seen two modeling techniques from </a><span class="s167">large technology companies</span>. You have seen Facebook’s Prophet and Amazon’s DeepAR. Those models are generally using neural networks under the hood. Those approaches try to automate complex technology with easy-to-use interfaces.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">In some ways this works: it is easier to tune Prophet or DeepAR than tuning a neural network. In other ways, there is still work to do: we have seen that tuning is still necessary to obtain a great result. It is therefore not totally automated.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Prophet and DeepAR are also able to learn both seasonal components and additional data. Prophet is not able to model multiple time series at once, whereas that is possible with DeepAR.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">One-Step Forecasts vs. Multistep Forecasts</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Related to the question of models being able to use additional regressors is the question of models being able to do multistep forecasts. As we’ve seen, there are basically three categories of models.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">The first category of models can simply not do forecasts of multiple steps. This is the case for models that need the latest data point each time that you want to predict a next data point.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The second category is when the model can do multistep forecast by iterating over the forecasted values and redoing one-step forecasts every time. In this case, it is taking the forecasted value as a past data point at the risk of adding up more and more errors.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: justify;">The third category can easily learn multistep forecasts. In some cases, this is by doing one multivariate forecast in which you treat each time step as a new dependent variable. An alternative is to learn relations between input sequences and output sequences.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Model Complexity vs. Gain</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Besides metrics and the types of variables that can be used by a model, I want to come back to model complexity. For some models, it takes a long time to obtain a slight improvement in R2. You should wonder if, for your use case, there is a way to say that a certain improvement of accuracy is still worth it. For example, spending a year for 0.001 points of R2 improvement is probably not worth it when doing sales forecasting, yet it may be totally worth it when doing medical work or aircraft safety or things for the army.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;"><a name="bookmark228">You must consider that model tuning costs time for a modeler, but it also costs computing time, which can become expensive when running, for example, cloud computing setups for very long periods.</a></p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">This type of consideration can be seen when we moved from classical supervised models to Neural Networks. Neural Networks are very performant and can obtain great results. It is much more complex to parameterize and optimize a neural network</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">architecture than to do a hyperparameter tuning of, for example, a gradient boosting. In short, you would need to have an idea of the impact of your R2 score to be sure how your forecast still has a practical impact on your use case.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Model Complexity vs. Interpretability</h4><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 129%;text-align: left;">A second problem with complex models is that it often becomes very difficult to understand what the models have learned. For univariate time series or for classical supervised models, it is relatively easy to extract the model coefficients or to use other tools for model interpretation like printing a decision tree or extracting variable importance.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">For more complex models, including multivariate time series, but mainly neural networks, it is much harder to go through huge coefficient matrices and understand in humanly understandable terms what it is that the model has learned.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">For Prophet and DeepAR, model interpretability is even worse, as the black box approach of those models has put the model builder even further away from the actual technicalities of what has been learned.</p><p style="padding-left: 8pt;text-indent: 17pt;line-height: 129%;text-align: left;">Model interpretability is important. As a forecaster, you need to be confident in your model. To be confident, you should at least be totally sure of what it does. This is often not the case with complex models and can lead to very difficult situations.</p><h4 style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark229">Model Stability and Variation</a></h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">As a last reflection for choosing a model for your forecasts, I want to come back to the question of model stability. In Chapter <span style=" color: #00F;">20</span>, we have observed that DeepAR is great at giving a confidence region in which we may expect our model to be, but that when repeating the model training, it is generally not capable of reproducing the same result. This problem of model stability may be a serious problem when you are trying to make a reliable forecast.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Model variation is also important when talking about overfitting models. We have seen that when models overfit, they can get to learn the training data by heart, and therefore they are not learning a general truth anymore, but rather they are learning noise of the training data.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">This is also a type of variation: when you expect a great result based on your modeling process, but you obtain bad results when forecasting in practice, this may be a variation in metrics that is due to overfitting. In a way, it is also a model metric. It may</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">not be the one we generally talk about first, yet it is still a very important concept to keep in mind when moving from the development phase to the actual forecasting on the field.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 9pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">Conclusion</h4><p style="padding-top: 10pt;padding-left: 26pt;text-indent: 0pt;line-height: 129%;text-align: left;">Throughout the book, you have seen how to manage model benchmarks, and you have seen different techniques for model evaluation, including the train-test set, cross- validation, and more. Until here, model performance has been generally defined by a metric, like the R2 score. Those metrics show how good a model performs on average.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">The average performance is generally the first thing to consider, but in this chapter, you have seen a number of additional reflections on model selection that are also important to have during the model development phase.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">Having gone through the first 20 chapters of this book, you should have all necessary input for making performant forecasts using Python. The general reflections posed in this chapter should help you to avoid some easily made yet important mistakes.</p><p style="padding-left: 26pt;text-indent: 17pt;line-height: 129%;text-align: left;">After all, when you make a forecasting model, the most important thing is not whether it works only on your training data and not even whether it works on your test data. When applying in practice, all that matters is whether your prediction for the future comes true.</p><h4 style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Key Takeaways</h4><ul id="l71"><li><p style="padding-top: 10pt;padding-left: 54pt;text-indent: -17pt;text-align: left;">Model metrics generally tell us whether a model is good on average.</p></li><li><p style="padding-top: 9pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">Model variation tells us whether there is a lot of variation in a model’s quality metric, for example, if we retrain it on a slightly different dataset.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">More complex models can sometimes give better model metrics, but they often come at a cost of time spent on modeling, computing costs, and a cost of lower model interpretability.</p></li><li><p style="padding-top: 6pt;padding-left: 54pt;text-indent: -17pt;line-height: 129%;text-align: left;">When selecting a model type, you should take into account whether the model you use is capable of fitting the types of effects that you would theoretically expect to be present in your dataset, including the possibility for adding extra regressors and the possibility to do multistep forecasts.</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 12pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a name="bookmark230">Index</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">A</p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a href="#bookmark170" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Activation function, </a><a href="#bookmark171" class="a">210, </a><a href="#bookmark196" class="a">211, </a><a href="#bookmark196">242</a></p><p style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a href="#bookmark145" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Aggregation, </a><a href="#bookmark146" class="a">180, </a><a href="#bookmark146">181</a></p><p style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a href="#bookmark76" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ARMA model, </a><a href="#bookmark76">89</a></p><p class="s27" style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;line-height: 117%;text-align: left;"><a href="#bookmark80" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ACF and PACF plots, </a><a href="#bookmark81" class="a">93, </a>94 <span style=" color: #2B2A29;">actual </span><span class="s80">vs</span><a href="#bookmark83" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. fitted values, </a>96 <a href="#bookmark86" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">cross-validation, </a><a href="#bookmark87" class="a">100, </a><a href="#bookmark87">101</a></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark76" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">development processes, </a><a href="#bookmark76">89</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark88" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">grid search, </a><a href="#bookmark88">100–103</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark86" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameters, </a><a href="#bookmark86">100</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark85" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameter tuning, </a><a href="#bookmark85">99</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark85" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">KPI, </a><a href="#bookmark85">96–99</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark77" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">mathematical definition, </a><a href="#bookmark77">90</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark84" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">metrics, </a><a href="#bookmark84">95–97</a></p><p class="s27" style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark76" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">past predictions, </a>89 <a href="#bookmark77" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Python source code, </a>90 <a href="#bookmark78" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">sunspot data, </a><a href="#bookmark79" class="a">91, </a><a href="#bookmark79">92</a></p><p class="s27" style="padding-left: 26pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark8" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Artificial Intelligence (AI), </a>3 <span style=" color: #2B2A29;">Augmented Dickey Fuller (ADF) test,</span></p><p style="padding-left: 60pt;text-indent: 0pt;text-align: left;"><a href="#bookmark45" class="a">51, </a><a href="#bookmark66" class="a">52, </a><a href="#bookmark79" class="a">77, </a><a href="#bookmark83" class="a">92, </a><a href="#bookmark111" class="a">96, </a><a href="#bookmark111">134</a></p><p class="s27" style="padding-top: 2pt;padding-left: 41pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark49" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Autocorrelation differenced data, </a>54–56 <span style=" color: #2B2A29;">Earthquake dataset</span></p><p style="padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark42" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">computing correlation, </a><a href="#bookmark42">49</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark43" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">correlation matrix, </a><a href="#bookmark43">50</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark39" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">dataset, </a><a href="#bookmark39">46</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark40" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">describe method, </a><a href="#bookmark40">47</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark41" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">evaluation, </a><a href="#bookmark41">48</a></p><p style="padding-top: 2pt;padding-left: 63pt;text-indent: -7pt;line-height: 121%;text-align: left;"><a href="#bookmark43" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">nan value (missing value), </a><a href="#bookmark43">50</a></p><p style="padding-top: 6pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark40" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">profile report, </a><a href="#bookmark40">47</a></p><p class="s27" style="padding-top: 2pt;padding-left: 41pt;text-indent: 15pt;line-height: 121%;text-align: left;"><a href="#bookmark42" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">shifted data, </a>49 <span style=" color: #2B2A29;">lags</span></p><p style="padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark48" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">autocorrelation function, </a><a href="#bookmark48">55</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark48" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">correlation coefficient, </a><a href="#bookmark49" class="a">55, </a><a href="#bookmark49">56</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark48" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">definition, </a><a href="#bookmark48">55</a></p><p class="s27" style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark48" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">differenced data, </a>55 <span style=" color: #2B2A29;">model evaluation and</span></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: center;"><a href="#bookmark52" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">benchmarking, </a><a href="#bookmark52">59</a></p><p style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: center;"><a href="#bookmark50" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">partial model, </a><a href="#bookmark51" class="a">57, </a><a href="#bookmark51">58</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark52" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">predictive performance, </a><a href="#bookmark52">59</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark48" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">statsmodels package, </a><a href="#bookmark48">55</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark51" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">time series, </a><a href="#bookmark52" class="a">58, </a><a href="#bookmark52">59</a></p><p class="s27" style="padding-top: 2pt;padding-left: 26pt;text-indent: 15pt;line-height: 121%;text-align: left;"><a href="#bookmark43" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">positive/negative, </a>50 <span style=" color: #2B2A29;">Autocorrelation function (ACF)</span></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark92" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ARIMA model, </a><a href="#bookmark92">108</a></p><p class="s27" style="padding-top: 2pt;padding-left: 26pt;text-indent: 15pt;line-height: 121%;text-align: left;"><a href="#bookmark80" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ARMA model, </a>93 <a href="#bookmark38" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Autoregressive (AR) model, </a><a href="#bookmark38">45</a></p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Autocorrelation (<span class="s80">see </span>Autocorrelation)</p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark52" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">definition, </a><a href="#bookmark52">59</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark48" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">differenced data, </a><a href="#bookmark48">53–55</a></p><p class="s27" style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark44" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">stationarity/ADF test, </a><a href="#bookmark45" class="a">51, </a>52 <a href="#bookmark38" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">univariate time series models, </a>45 <a href="#bookmark59" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Yule-Walker method, </a><a href="#bookmark59">60–68</a></p><p style="padding-left: 60pt;text-indent: -34pt;line-height: 121%;text-align: left;">Autoregressive, Integrated Moving Average (ARIMA) model</p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;">CO2 data</p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark92" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ACF and PACF plots, </a><a href="#bookmark94" class="a">108, </a><a href="#bookmark94">110</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark95" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameter tuning, </a><a href="#bookmark95">110–113</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark91" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">importing data, </a><a href="#bookmark91">107</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 9pt;text-align: left;">© Joos Korstanje 2021</p><p class="s23" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">J. Korstanje, <span class="s174">Advanced Forecasting with Python</span><a href="https://doi.org/10.1007/978-1-4842-7150-6#DOI" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt;" target="_blank">, </a><a href="https://doi.org/10.1007/978-1-4842-7150-6#DOI" class="s25" target="_blank">https://doi.org/10.1007/978-1-4842-7150-6</a></p><p style="padding-top: 5pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">291</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: -30pt;line-height: 117%;text-align: left;">Autoregressive, Integrated Moving Average (ARIMA) model (<span class="s80">cont.</span><a href="#bookmark91" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">) plotting data, </a><a href="#bookmark91">107</a></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><a href="#bookmark90" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">definition, </a><a href="#bookmark90">106</a></p><p style="padding-top: 2pt;padding-left: 22pt;text-indent: 0pt;text-align: left;"><a href="#bookmark90" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">equation, </a><a href="#bookmark90">106</a></p><p style="padding-top: 2pt;padding-left: 22pt;text-indent: 0pt;text-align: left;"><a href="#bookmark90" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">integration, </a><a href="#bookmark90">106</a></p><p class="s27" style="padding-top: 2pt;padding-left: 22pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark90" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">linear trend, </a>106 <a href="#bookmark89" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">univariate time series, </a><a href="#bookmark89">105</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">B</p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark173" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Backpropagation algorithm, </a><a href="#bookmark173">211–213</a></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark33" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Backtesting model, </a><a href="#bookmark33">39</a></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark145" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Bagging, </a><a href="#bookmark145">180</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark167" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Bayesian optimization compare performances, </a><a href="#bookmark167">204</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark163" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameter tuning, </a><a href="#bookmark164" class="a">200, </a><a href="#bookmark164">201</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark165" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">LightGBM model, </a><a href="#bookmark166" class="a">202, </a><a href="#bookmark166">203</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark165" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">scikit-optimize package, </a><a href="#bookmark165">202</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark166" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">XGBoost, </a><a href="#bookmark166">203</a></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark145" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Bootstrap aggregation, </a><a href="#bookmark145">180</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">C</p><p style="padding-top: 5pt;padding-left: 23pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark31" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Cross-validation K-fold, </a><a href="#bookmark31">34–36</a></p><p class="s27" style="padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark32" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">rolling time series, </a><a href="#bookmark33" class="a">38, </a>39 <a href="#bookmark32" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">time series models, </a><a href="#bookmark32">36–38</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">D</p><p style="padding-top: 5pt;padding-left: 23pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark136" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Decision Tree model bike-sharing, </a><a href="#bookmark136">166</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark131" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">bike-sharing users, </a><a href="#bookmark131">161</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark135" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">DecisionTreeRegressor, </a><a href="#bookmark135">165</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark137" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">dendrogram, </a><a href="#bookmark137">167</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark132" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">explanatory values, </a><a href="#bookmark132">162</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark134" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">grid search, </a><a href="#bookmark135" class="a">164, </a><a href="#bookmark135">165</a></p><p style="padding-top: 5pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark134" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameter tuning, </a><a href="#bookmark134">164</a></p><p class="s27" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark129" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">if-else decisions, </a>159 <a href="#bookmark132" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">influential outlier value, </a>162 <a href="#bookmark129" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">intuitive, </a>159 <span style=" color: #2B2A29;">mathematical/algorithmic</span></p><p style="padding-left: 45pt;text-indent: 0pt;text-align: left;"><a href="#bookmark130" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">components, </a><a href="#bookmark130">160</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark130" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">pruning/reducing complexity, </a><a href="#bookmark130">160</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark130" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">splitting, </a><a href="#bookmark130">160</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark137" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">max_depth, </a><a href="#bookmark137">167</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark135" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">parameters, </a><a href="#bookmark135">165</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">training dataset, </a><a href="#bookmark133">163</a></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark215" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">DeepAR model, </a><a href="#bookmark216" class="a">273, </a><a href="#bookmark216">274</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">E</p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark159" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Exclusive Feature Bundling (EFB), </a><a href="#bookmark159">196</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">F</p><p class="s27" style="padding-top: 5pt;padding-left: 8pt;text-indent: -1pt;line-height: 121%;text-align: center;"><a href="#bookmark8" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Forecasting, computing power, </a>3 <a href="#bookmark171" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Fully connected neural networks schema, </a><a href="#bookmark171">209–211</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">G, H, I, J</p><p class="s27" style="padding-top: 5pt;padding-left: 23pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark184" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Gated Recurrent Unit (GRU), </a>227 <a href="#bookmark196" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hidden layers, </a><a href="#bookmark196">240–242</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark193" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">history graph, </a><a href="#bookmark195" class="a">239, </a><a href="#bookmark195">241</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark192" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">model architecture, </a><a href="#bookmark193" class="a">238, </a><a href="#bookmark193">239</a></p><p class="s27" style="padding-top: 2pt;padding-left: 8pt;text-indent: 15pt;line-height: 121%;text-align: left;"><a href="#bookmark191" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">technical issues, </a>237 <span style=" color: #2B2A29;">Gradient-Based One-Side</span></p><p style="padding-left: 42pt;text-indent: 0pt;text-align: left;"><a href="#bookmark159" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Sample (GOSS), </a><a href="#bookmark159">196</a></p><p style="padding-top: 2pt;padding-left: 42pt;text-indent: -34pt;line-height: 121%;text-align: left;"><a href="#bookmark156" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Gradient boosting (XGBoost/LightGBM) model, </a><a href="#bookmark156">193</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark158" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">AdaBoost algorithm, </a><a href="#bookmark158">195</a></p><p style="padding-top: 1pt;padding-left: 45pt;text-indent: -22pt;line-height: 120%;text-align: left;">Bayesian optimization (<span class="s80">see </span>Bayesian optimization)</p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;">boosting process</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark156" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ensemble learning, </a><a href="#bookmark156">193</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark158" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">histogram-based splitting, </a><a href="#bookmark158">195</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark156" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">iterative process, </a><a href="#bookmark156">193</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark157" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">partial derivatives, </a><a href="#bookmark157">194</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark160" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">differences, </a><a href="#bookmark160">195–197</a></p><p class="s27" style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark160" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">forecasting traffic data, </a><a href="#bookmark161" class="a">197, </a>198 <span style=" color: #2B2A29;">LightGBM forecasting traffic</span></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><a href="#bookmark162" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">data, </a><a href="#bookmark163" class="a">199, </a><a href="#bookmark163">200</a></p><p style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark148" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">grid search, </a><a href="#bookmark149" class="a">184, </a><a href="#bookmark149">185</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">K</p><p style="padding-top: 5pt;padding-left: 41pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark179" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Keras library compilation, </a><a href="#bookmark179">220</a></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark179" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">development process, </a><a href="#bookmark179">220</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark183" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">history plot, </a><a href="#bookmark183">225</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark181" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">history plots, </a><a href="#bookmark181">223</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark178" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameters, </a><a href="#bookmark178">219</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark179" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">model architecture, </a><a href="#bookmark179">220</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark181" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">source code, </a><a href="#bookmark182" class="a">223, </a><a href="#bookmark182">224</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark180" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">training history, </a><a href="#bookmark180">221</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark178" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">train-test split, </a><a href="#bookmark178">219</a></p><p class="s27" style="padding-top: 2pt;padding-left: 26pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark85" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Key performance indicator (KPI), </a>96–99 <a href="#bookmark31" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">K-fold cross-validation, </a><a href="#bookmark31">34–36</a></p><p style="padding-left: 41pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark140" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">k-nearest neighbors (kNN) model alternative predictions, </a><a href="#bookmark141" class="a">172, </a><a href="#bookmark141">173</a></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark142" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">grid search, </a><a href="#bookmark142">175</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark138" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">intuitive explanation, </a><a href="#bookmark138">169</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark139" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">mathematical definition, </a><a href="#bookmark139">169–171</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark139" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">prevalent methods, </a><a href="#bookmark139">171</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark143" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">random search, </a><a href="#bookmark143">176</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark142" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">traffic data, </a><a href="#bookmark142">172–175</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark139" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">weighted heavier, </a><a href="#bookmark139">171</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">L</p><p style="padding-top: 5pt;padding-left: 60pt;text-indent: -34pt;line-height: 121%;text-align: left;"><a href="#bookmark158" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">LightGBM (gradient boosting algorithm), </a><a href="#bookmark158">195</a></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;">Linear regression</p><p style="padding-top: 5pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">CO<span class="s158">2 </span>dataset</p><p class="s27" style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark127" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">drop missing values, </a>155 <a href="#bookmark124" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">graphical data, </a><a href="#bookmark124">152</a></p><p style="padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark127" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">lagged variables, </a><a href="#bookmark127">155</a></p><p class="s27" style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark126" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">predictive performance, </a><a href="#bookmark128" class="a">154, </a>156 <a href="#bookmark123" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Python source code, </a>151 <a href="#bookmark124" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">variables, </a><a href="#bookmark125" class="a">152, </a><a href="#bookmark125">153</a></p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark123" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">matrix notation, </a><a href="#bookmark123">151</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark122" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">model definition, </a><a href="#bookmark122">150</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark123" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">OLS model, </a><a href="#bookmark123">151</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark121" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">variables, </a><a href="#bookmark121">149</a></p><p class="s27" style="padding-top: 2pt;padding-left: 25pt;text-indent: 15pt;line-height: 121%;text-align: left;"><a href="#bookmark122" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">visual interpretation, </a>150 <span style=" color: #2B2A29;">Long Short-Term Memory</span></p><p style="padding-left: 60pt;text-indent: 0pt;text-align: left;"><a href="#bookmark184" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">(LSTM), </a><a href="#bookmark197" class="a">227, </a><a href="#bookmark197">243</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark197" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">definition, </a><a href="#bookmark197">243</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark200" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">one-layer network, </a><a href="#bookmark200">246–248</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark199" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">performances, </a><a href="#bookmark199">244–246</a></p><p class="s27" style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark197" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">sigmoids/tanh operations, </a><a href="#bookmark198" class="a">243, </a>244 <a href="#bookmark201" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">three layers/64 cells, </a>248–250 <a href="#bookmark200" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">training history, </a><a href="#bookmark200">248</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">M</p><p style="padding-top: 5pt;padding-left: 41pt;text-indent: -15pt;line-height: 117%;text-align: left;">Machine learning techniques classification <span class="s80">vs</span><a href="#bookmark18" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. regression, </a><span style=" color: #00F;">17 </span><a href="#bookmark9" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">description, </a><a href="#bookmark9">4</a></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark9" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">landscape, </a><a href="#bookmark9">4</a></p><p class="s27" style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;line-height: 118%;text-align: left;"><a href="#bookmark18" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">supervised model, </a>9–17 <a href="#bookmark14" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">time series models, </a>4–9 <span style=" color: #2B2A29;">univariate </span><span class="s80">vs</span><span style=" color: #2B2A29;">. multivariate</span></p><p style="padding-left: 41pt;text-indent: 22pt;line-height: 114%;text-align: left;"><a href="#bookmark19" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">models, </a><span style=" color: #00F;">18 </span>unsupervised <span class="s80">vs</span>. supervised</p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><a href="#bookmark18" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">models, </a><a href="#bookmark18">17</a></p><p class="s27" style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark24" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Mean Absolute Error (MAE), </a><a href="#bookmark179" class="a">27, </a>220 <span style=" color: #2B2A29;">Mean Absolute Percent</span></p><p style="padding-left: 60pt;text-indent: 0pt;text-align: left;"><a href="#bookmark25" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Error (MAPE), </a><a href="#bookmark25">28</a></p><p style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark22" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Mean Squared Error (MSE), </a><a href="#bookmark23" class="a">25, </a><a href="#bookmark23">26</a></p><p style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark21" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Metrics, </a><a href="#bookmark22" class="a">24, </a><a href="#bookmark22">25</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark20" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Model evaluation, </a><a href="#bookmark20">21</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark33" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">backtesting model, </a><a href="#bookmark33">39</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">combination, </a><a href="#bookmark36">40–42</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark35" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">explanatory variables, </a><a href="#bookmark35">41</a></p><p class="s27" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark21" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hypothetical data, </a>21–24 <span style=" color: #2B2A29;">strategies</span></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark31" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">cross-validation, </a><a href="#bookmark31">34–36</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark26" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">errors, </a><a href="#bookmark26">29</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark27" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">overfitting, </a><a href="#bookmark27">30</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark27" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">train-test split, </a><a href="#bookmark28" class="a">30, </a><a href="#bookmark28">31</a></p><p class="s27" style="padding-top: 2pt;padding-left: 8pt;text-indent: 29pt;line-height: 121%;text-align: left;"><a href="#bookmark30" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">validation split, </a>32–34 <span style=" color: #2B2A29;">Model selection</span></p><p style="padding-left: 23pt;text-indent: 0pt;line-height: 12pt;text-align: left;">complexity <span class="s80">vs</span><a href="#bookmark227" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. gain, </a><a href="#bookmark227">287</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark228" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">interpretability, </a><a href="#bookmark228">288</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark225" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">metrics, </a><a href="#bookmark225">285</a></p><p style="padding-top: 1pt;padding-left: 23pt;text-indent: 0pt;line-height: 117%;text-align: left;">one-step <span class="s80">vs</span><a href="#bookmark227" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. multistep forecasts, </a><span style=" color: #00F;">287 </span>stability <span class="s80">vs</span><a href="#bookmark229" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. variation, </a><span style=" color: #00F;">289 </span><a href="#bookmark226" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">structure/inputs, </a><a href="#bookmark226">286</a></p><p style="padding-left: 23pt;text-indent: -15pt;line-height: 114%;text-align: left;">Moving Average (MA) model actuals <span class="s80">vs</span><a href="#bookmark68" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. forecast, </a><a href="#bookmark69" class="a">79, </a><a href="#bookmark69">80</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark63" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">AR model, </a><a href="#bookmark64" class="a">74, </a><a href="#bookmark64">75</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark67" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">autocorrelation function, </a><a href="#bookmark67">78</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark66" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">differenced data, </a><a href="#bookmark66">77</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark62" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">fitting model, </a><a href="#bookmark62" class="a">73,</a><a href="#bookmark63" class="a"> </a><a href="#bookmark63">74</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark65" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">graphical data, </a><a href="#bookmark65">76</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">grid search, </a><a href="#bookmark75">85–87</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark74" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">impulses, </a><a href="#bookmark74">86</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark61" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">model definition, </a><a href="#bookmark61">72</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark72" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">multistep forecasting, </a><a href="#bookmark72">82–84</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark62" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">optimization methods, </a><a href="#bookmark62">73</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark70" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">out-of-sample forecast, </a><a href="#bookmark70">81</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark63" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">parameters, </a><a href="#bookmark63">74</a></p><p class="s27" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark68" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">partial autocorrelation function, </a>79 <a href="#bookmark63" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">stationarity, </a><a href="#bookmark63">74</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark65" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">stock price data, </a><a href="#bookmark65">76</a></p><p class="s27" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark70" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">train data and evaluation, </a>81 <a href="#bookmark60" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">univariate time series, </a>71 <a href="#bookmark65" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Yahoo Finance package, </a><a href="#bookmark65">76</a></p><p class="s26" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">N</p><p class="s27" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark44" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Negative autocorrelation, </a>51 <span style=" color: #2B2A29;">Neural Networks (NNs)</span></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark171" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">activation layers, </a><a href="#bookmark171">211</a></p><p class="s27" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark171" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">backpropagation, </a>211 <span style=" color: #2B2A29;">data preparation</span></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark176" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">MinMaxScaler code, </a><a href="#bookmark176">216</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark178" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">PCA model, </a><a href="#bookmark178">216–219</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark175" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">scaling/standardization, </a><a href="#bookmark175">215</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark177" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">scree plot, </a><a href="#bookmark177">218</a></p><p class="s27" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark174" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">data preparation methods, </a><a href="#bookmark175" class="a">214, </a>215 <span style=" color: #2B2A29;">fully connected neural networks</span></p><p style="padding-left: 45pt;text-indent: 0pt;text-align: left;"><a href="#bookmark170" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">schema, </a><a href="#bookmark171" class="a">210, </a><a href="#bookmark171">211</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark173" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameter tuning, </a><a href="#bookmark174" class="a">213, </a><a href="#bookmark174">214</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark183" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">keras, </a><a href="#bookmark183">219–225</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark172" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">learning rates, </a><a href="#bookmark172">212</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark172" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">optimizers, </a><a href="#bookmark172">212</a></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark44" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Non-stationary, </a><a href="#bookmark45" class="a">51, </a><a href="#bookmark45">52</a></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark84" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Normal distribution, </a><a href="#bookmark84">97</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">O</p><p style="padding-top: 5pt;padding-left: 42pt;text-indent: -34pt;line-height: 121%;text-align: left;"><a href="#bookmark54" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Ordinary Least Squares (OLS) method, </a><a href="#bookmark123" class="a">61, </a><a href="#bookmark123">151</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark27" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Overfitting models, </a><a href="#bookmark36" class="a">30, </a><a href="#bookmark36">42</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">P, Q</p><p style="padding-top: 5pt;padding-left: 42pt;text-indent: -34pt;line-height: 121%;text-align: left;"><a href="#bookmark50" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Partial autocorrelation function (PACF), </a><a href="#bookmark63" class="a">57, </a><a href="#bookmark81" class="a">74, </a><a href="#bookmark81">94</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark93" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ARIMA model, </a><a href="#bookmark93">109</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark80" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ARMA model, </a><a href="#bookmark80">93</a></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark43" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Positive autocorrelation, </a><a href="#bookmark43">50</a></p><p style="padding-top: 2pt;padding-left: 42pt;text-indent: -34pt;line-height: 121%;text-align: left;"><a href="#bookmark178" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Prinicipal component analysis (PCA), </a><a href="#bookmark178">216–219</a></p><p style="padding-left: 23pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark202" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Prophet model advantage/disadvantage, </a><a href="#bookmark202">253</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark203" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">data format, </a><a href="#bookmark204" class="a">254, </a><a href="#bookmark204">255</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark206" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">decomposition plot, </a><a href="#bookmark207" class="a">257, </a><a href="#bookmark207">258</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark217" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">DeepAREstimator, </a><a href="#bookmark217">275</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark203" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">dependent variable, </a><a href="#bookmark203">254</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Facebook’s Prophet, </a><a href="#bookmark202">253</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark205" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">forecast plot, </a><a href="#bookmark206" class="a">256, </a><a href="#bookmark206">257</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark216" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">gluonts library, </a><a href="#bookmark216">274</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark214" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">grid search, </a><a href="#bookmark214">266–271</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark210" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">holiday data, </a><a href="#bookmark210">260–262</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark213" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameters, </a><a href="#bookmark224" class="a">266, </a><a href="#bookmark224">281–283</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark203" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Kaggle page, </a><a href="#bookmark203">254</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark221" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ListDataset, </a><a href="#bookmark222" class="a">279, </a><a href="#bookmark222">280</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark204" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">prediction, </a><a href="#bookmark205" class="a">255, </a><a href="#bookmark205">256</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark219" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">predictions, </a><a href="#bookmark220" class="a">277, </a><a href="#bookmark220">278</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark219" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">probability forecast graph, </a><a href="#bookmark220" class="a">277, </a><a href="#bookmark220">278</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark204" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">R2 score, </a><a href="#bookmark204">255</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark212" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">regressors, </a><a href="#bookmark223" class="a">26–265, </a><a href="#bookmark223">279–281</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark211" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">reservations, </a><a href="#bookmark211">263</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark208" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">seasonality, </a><a href="#bookmark209" class="a">259, </a><a href="#bookmark209">260</a></p><p style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;text-align: left;"><a href="#bookmark218" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">training model, </a><a href="#bookmark218">274–276</a></p><p class="s27" style="padding-top: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark203" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">univariate forecasts, </a>254 <a href="#bookmark215" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">univariate/multivariate time series, </a><a href="#bookmark215">273</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">R</p><p class="s27" style="padding-top: 5pt;padding-left: 41pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark144" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Random Forest/XGBoost model, </a>179 <span style=" color: #2B2A29;">distributions</span></p><p style="padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark150" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">max_features, </a><a href="#bookmark150" class="a">186,</a><a href="#bookmark151" class="a"> </a><a href="#bookmark151">187</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark151" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">n_estimators, </a><a href="#bookmark151" class="a">187,</a><a href="#bookmark152" class="a"> </a><a href="#bookmark152">188</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark152" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">RandomizedSearchCV, </a><a href="#bookmark153" class="a">188, </a><a href="#bookmark153">189</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark149" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">search option, </a><a href="#bookmark149">185</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark151" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">uniform distribution, </a><a href="#bookmark151">187</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark145" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ensemble learning, </a><a href="#bookmark146" class="a">180, </a><a href="#bookmark146">181</a></p><p style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><a href="#bookmark145" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">ensemble learning, </a><a href="#bookmark145">180</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark154" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">feature importance, </a><a href="#bookmark154">190</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark148" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameter tuning, </a><a href="#bookmark148">184</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark155" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">interpretation, </a><a href="#bookmark155">189–191</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark144" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">intuitive data, </a><a href="#bookmark144">179</a></p><p style="padding-top: 2pt;padding-left: 41pt;text-indent: 0pt;text-align: left;"><a href="#bookmark148" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">sunspots, </a><a href="#bookmark148">182–184</a></p><p style="padding-top: 5pt;padding-left: 34pt;text-indent: 0pt;text-align: left;"><a href="#bookmark147" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">variable subsets, </a><a href="#bookmark147">182</a></p><p class="s27" style="padding-top: 2pt;padding-left: 34pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark184" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Recurrent Neural Networks (RNNs), </a>227 <a href="#bookmark184" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">architecture, </a><a href="#bookmark185" class="a">227, </a><a href="#bookmark185">228</a></p><p class="s27" style="padding-left: 34pt;text-indent: 0pt;line-height: 117%;text-align: left;"><a href="#bookmark189" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">data preparation, </a>230–233 <span class="s80">vs</span><a href="#bookmark185" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. dense network, </a>228 <a href="#bookmark195" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">GRU model, </a><a href="#bookmark195">237–241</a></p><p style="padding-left: 34pt;text-indent: 0pt;text-align: left;"><a href="#bookmark186" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">lagged variables, </a><a href="#bookmark186">229</a></p><p style="padding-top: 2pt;padding-left: 34pt;text-indent: 0pt;text-align: left;"><a href="#bookmark188" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">MinMaxScaler, </a><a href="#bookmark188">231</a></p><p style="padding-top: 2pt;padding-left: 34pt;text-indent: 0pt;text-align: left;"><a href="#bookmark187" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">multivariable, </a><a href="#bookmark187">230</a></p><p style="padding-top: 2pt;padding-left: 34pt;text-indent: 0pt;text-align: left;"><a href="#bookmark187" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">prediction, </a><a href="#bookmark187">230</a></p><p style="padding-top: 1pt;padding-left: 19pt;text-indent: 15pt;line-height: 120%;text-align: left;">SimpleRNN (<span class="s80">see </span><a href="#bookmark32" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">SimpleRNN layer) Rolling time series model, </a><a href="#bookmark33" class="a">38, </a><a href="#bookmark33">39</a></p><p class="s27" style="padding-left: 19pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark23" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Root Mean Squared Error (RMSE), </a>26 <a href="#bookmark25" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">R2 (R squared) metrics, </a><a href="#bookmark26" class="a">28, </a><a href="#bookmark26">29</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">S</p><p style="padding-top: 5pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">SARIMAX model</p><p style="padding-top: 2pt;padding-left: 34pt;text-indent: 0pt;text-align: left;"><a href="#bookmark105" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">mathematical definition, </a><a href="#bookmark105">126</a></p><p style="padding-top: 2pt;padding-left: 34pt;text-indent: 0pt;text-align: left;"><a href="#bookmark104" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">time series, </a><a href="#bookmark104">125</a></p><p class="s80" style="padding-top: 1pt;padding-left: 34pt;text-indent: 0pt;line-height: 120%;text-align: left;">vs<a href="#bookmark105" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. supervised models, </a><span class="s27">126 </span><a href="#bookmark106" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Walmart dataset, </a><a href="#bookmark106">127</a></p><p class="s27" style="padding-left: 49pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark107" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">correlation matrix, </a>128 <a href="#bookmark108" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">endog and exog, </a>129 <a href="#bookmark106" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">graphical data, </a><a href="#bookmark106">127</a></p><p style="padding-left: 49pt;text-indent: 0pt;text-align: left;"><a href="#bookmark108" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameter search, </a><a href="#bookmark108">129</a></p><p style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;"><a href="#bookmark109" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">predictive performance, </a><a href="#bookmark109">130</a></p><p style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;"><a href="#bookmark106" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">source code, </a><a href="#bookmark106">127</a></p><p style="padding-top: 2pt;padding-left: 34pt;text-indent: 0pt;text-align: left;"><a href="#bookmark104" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">X component, </a><a href="#bookmark104">125</a></p><p style="padding-top: 2pt;padding-left: 54pt;text-indent: -34pt;line-height: 121%;text-align: left;"><a href="#bookmark96" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Seasonal Autoregressive Integrated Moving Average (SARIMA) model, </a><a href="#bookmark96">115</a></p><p style="padding-left: 34pt;text-indent: 0pt;text-align: left;"><a href="#bookmark96" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">components, </a><a href="#bookmark96">115</a></p><p class="s27" style="padding-top: 2pt;padding-left: 34pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark97" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">model definition, </a><a href="#bookmark98" class="a">116, </a>117 <span style=" color: #2B2A29;">Walmart sales</span></p><p style="padding-left: 49pt;text-indent: 0pt;text-align: left;"><a href="#bookmark102" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">fitting model, </a><a href="#bookmark102">120–122</a></p><p style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;"><a href="#bookmark101" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">grid search, </a><a href="#bookmark101">121</a></p><p style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;"><a href="#bookmark98" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">importing data, </a><a href="#bookmark98">117</a></p><p style="padding-top: 2pt;padding-left: 49pt;text-indent: 0pt;text-align: left;"><a href="#bookmark101" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">predictive performance, </a><a href="#bookmark101">119–121</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 23pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark185" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">SimpleRNN layer architecture, </a><a href="#bookmark186" class="a">228, </a><a href="#bookmark186">229</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark191" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hidden layers, </a><a href="#bookmark191">235–237</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark190" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">parameterize network, </a><a href="#bookmark190">233–235</a></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark44" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Stationarity, </a><a href="#bookmark45" class="a">51, </a><a href="#bookmark45">52</a></p><p class="s27" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark172" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Stochastic Gradient Descent (SGD), </a>212 <span style=" color: #2B2A29;">Supervised machine learning</span></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark17" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">correlation coefficient, </a><a href="#bookmark18" class="a">16, </a><a href="#bookmark18">17</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark18" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">explanatory variables, </a><a href="#bookmark18">9–17</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark16" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">historical data, </a><a href="#bookmark16">9–15</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark15" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">zooming view, </a><a href="#bookmark16" class="a">14, </a><a href="#bookmark16">15</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">T</p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Time series models</p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark32" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">cross-validation, </a><a href="#bookmark32">36–38</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark12" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">explanatory variables, </a><a href="#bookmark14" class="a">7, </a><a href="#bookmark14">9</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark48" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">differenced data, </a><a href="#bookmark48">53–55</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark11" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">graphical representation, </a><a href="#bookmark11">6</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark9" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">historical developments, </a><a href="#bookmark9">4</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark10" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hypothetical approach, </a><a href="#bookmark10">5</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark10" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">linear data, </a><a href="#bookmark10">5</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark11" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Python source code, </a><a href="#bookmark13" class="a">6, </a><a href="#bookmark13">8</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark12" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">seasonality, </a><a href="#bookmark12">7</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">U</p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Underfitting models, </a><a href="#bookmark36">42</a></p><p style="padding-top: 1pt;padding-left: 42pt;text-indent: -34pt;line-height: 120%;text-align: left;">Univariate time series, (<span class="s80">see </span>Time series models)</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Univariate <span class="s80">vs</span><a href="#bookmark19" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. multivariate models, </a><a href="#bookmark19">18</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">V, W</p><p style="padding-top: 5pt;padding-left: 42pt;text-indent: -34pt;line-height: 121%;text-align: left;">Vector autoregression moving average exogenous variables (VARMAX) model</p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark116" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">components, </a><a href="#bookmark116">141</a></p><p style="padding-top: 5pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark118" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameters, </a><a href="#bookmark118">143</a></p><p class="s27" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark117" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">mathematical model, </a>142 <span style=" color: #2B2A29;">multivariate time series</span></p><p style="padding-left: 45pt;text-indent: 0pt;text-align: left;"><a href="#bookmark119" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">modeling, </a><a href="#bookmark119">142–144</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: -15pt;line-height: 121%;text-align: left;"><a href="#bookmark112" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Vector autoregression (VAR) models coefficients, </a><a href="#bookmark112">135</a></p><p class="s27" style="padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark111" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">differencing/integration, </a>134 <span style=" color: #2B2A29;">forecasting Walmart sales</span></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark114" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">fitting model, </a><a href="#bookmark114">137</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark115" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">maxlags, </a><a href="#bookmark115">138</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark113" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">time series, </a><a href="#bookmark113">136</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark111" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameters, </a><a href="#bookmark111">134</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark110" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">model definition, </a><a href="#bookmark110">133</a></p><p style="padding-top: 1pt;padding-left: 45pt;text-indent: -22pt;line-height: 120%;text-align: left;">multiple univariate <span class="s80">vs</span><a href="#bookmark112" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">. multivariate model, </a><a href="#bookmark112">135</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark110" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">multivariate model, </a><a href="#bookmark110">133</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark111" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">stationarity, </a><a href="#bookmark111">134</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">X</p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark158" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">XGBoost, </a><a href="#bookmark158">195</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s26" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Y, Z</p><p style="padding-top: 5pt;padding-left: 23pt;text-indent: -15pt;line-height: 121%;text-align: left;">Yule-Walker method autocovariance</p><p style="padding-left: 45pt;text-indent: 0pt;text-align: left;"><a href="#bookmark53" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">function, </a><a href="#bookmark53">60</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark53" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">coefficients, </a><a href="#bookmark55" class="a">60, </a><a href="#bookmark55">62</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark53" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">equations, </a><a href="#bookmark53">60</a></p><p class="s27" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark58" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">hyperparameters, </a>66 <a href="#bookmark53" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">Kronecker delta function, </a>60 <a href="#bookmark53" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">matrix format, </a><a href="#bookmark54" class="a">60, </a><a href="#bookmark54">61</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark54" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">OLS method, </a><a href="#bookmark54">61</a></p><p style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark55" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">source code, </a><a href="#bookmark56" class="a">62, </a><a href="#bookmark56">63</a></p><p class="s27" style="padding-top: 2pt;padding-left: 23pt;text-indent: 0pt;line-height: 121%;text-align: left;"><a href="#bookmark59" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">stand-alone model, </a>68 <a href="#bookmark56" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">strategies and metrics, </a>63 <a href="#bookmark59" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">train-test evaluation, </a><a href="#bookmark59">64–68</a></p><p style="padding-left: 23pt;text-indent: 0pt;text-align: left;"><a href="#bookmark58" style=" color: #2B2A29; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt;">underfitting, </a><a href="#bookmark58">66</a></p></body></html>
