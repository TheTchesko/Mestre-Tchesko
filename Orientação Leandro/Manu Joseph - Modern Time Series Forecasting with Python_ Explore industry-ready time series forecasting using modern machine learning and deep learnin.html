<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h4 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 25pt; }
 .s1 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 15pt; }
 .s2 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 .p, p { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .s3 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14.5pt; }
 .s4 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .a, a { color: #1D1D1B; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 h3 { color: #231F20; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 30pt; }
 .s6 { color: #231F20; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 13pt; }
 .s7 { color: #231F20; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 18pt; }
 .s8 { color: #F97141; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 20pt; }
 .s9 { color: #231F20; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s10 { color: #58595B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s13 { color: #58595B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s14 { color: #231F20; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s20 { color: black; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s21 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s22 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s23 { color: black; font-family:"Courier New", monospace; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s24 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 10.5pt; }
 .s25 { color: #13110D; font-family:"Courier New", monospace; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s26 { color: #13110D; font-family:"Courier New", monospace; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s27 { color: #1D1D1B; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s28 { color: #13110D; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s29 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s30 { color: black; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s31 { color: #FFF; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 25pt; }
 .s32 { color: #FFF; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s33 { color: #FFF; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s34 { color: #FFF; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 h2 { color: #646363; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 33pt; }
 .s35 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s36 { color: #1D1D1B; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s37 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s38 { color: black; font-family:"Courier New", monospace; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s39 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8pt; }
 .s40 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8pt; }
 .s41 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; }
 .s42 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -2pt; }
 .s43 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s44 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 1pt; }
 .s45 { color: #1D1D1B; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .s46 { color: #1D1D1B; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .s47 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s48 { color: black; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .s49 { color: #13110D; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s50 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s51 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; }
 .s52 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; }
 .s53 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6.5pt; vertical-align: -2pt; }
 .s54 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s55 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -4pt; }
 .s56 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; }
 .s57 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; }
 .s58 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s59 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 1pt; }
 .s60 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 3pt; }
 .s61 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 2pt; }
 .s62 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s63 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s64 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s65 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s66 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -6pt; }
 .s67 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 3pt; }
 .s68 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s69 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 3pt; }
 .s70 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s71 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -6pt; }
 .s72 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; vertical-align: 6pt; }
 .s73 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s74 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s75 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s76 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s77 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -6pt; }
 .s78 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -5pt; }
 .s79 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s80 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; }
 .s81 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 5pt; }
 .s82 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -2pt; }
 .s83 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: 3pt; }
 .s84 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 7pt; }
 .s85 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: 2pt; }
 .s86 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -1pt; }
 .s87 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -4pt; }
 .s88 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -2pt; }
 .s89 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -3pt; }
 .s90 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .s91 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 2pt; }
 .s92 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 3pt; }
 .s93 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 2pt; }
 .s94 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s95 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s96 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s97 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s98 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s99 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s100 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 3pt; }
 .s101 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -2pt; }
 .s102 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; vertical-align: 1pt; }
 .s103 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s104 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s105 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s106 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s107 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; }
 .s108 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 1pt; }
 .s109 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 1pt; }
 .s110 { color: black; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s111 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s112 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s113 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 2pt; }
 .s114 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 3pt; }
 .s115 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 1pt; }
 .s116 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s117 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 1pt; }
 .s118 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s119 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; vertical-align: 3pt; }
 .s120 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s121 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -1pt; }
 .s122 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 3pt; }
 .s123 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 4pt; }
 .s124 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 1pt; }
 .s125 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6.5pt; }
 .s126 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -2pt; }
 .s127 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s128 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -2pt; }
 .s129 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 2pt; }
 .s130 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 2pt; }
 .s131 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .s132 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14.5pt; }
 .s133 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 1pt; }
 .s134 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s135 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s136 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 1pt; }
 .s137 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s138 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s139 { color: #1D1D1B; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s140 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s141 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s142 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -2pt; }
 .s143 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 2pt; }
 .s144 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 1pt; }
 .s145 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -1pt; }
 .s146 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s147 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -8pt; }
 .s148 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -5pt; }
 .s149 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 3pt; }
 .s150 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; vertical-align: 1pt; }
 .s151 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s152 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 6pt; }
 .s153 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -1pt; }
 .s154 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -6pt; }
 .s155 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: -5pt; }
 .s156 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -7pt; }
 .s157 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s158 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -4pt; }
 .s159 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8.5pt; }
 .s160 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8.5pt; }
 .s161 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 4pt; }
 .s162 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: -1pt; }
 .s163 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 2pt; }
 .s164 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: -8pt; }
 .s165 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10.5pt; }
 .s166 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s167 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s168 { color: black; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .s169 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -2pt; }
 .s170 { color: #1D1D1B; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s171 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: 1pt; }
 .s172 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: 2pt; }
 .s173 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 2pt; }
 .s174 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 2pt; }
 .s175 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -2pt; }
 .s176 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s177 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s178 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s179 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 3pt; }
 .s180 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 1pt; }
 .s181 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -1pt; }
 .s182 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 1pt; }
 .s183 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 8pt; }
 .s184 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 4pt; }
 .s185 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 1pt; }
 .s186 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 4pt; }
 .s187 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 3pt; }
 .s188 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 6pt; }
 .s189 { color: #1D1D1B; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s190 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 4pt; }
 .s191 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s192 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s193 { color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 .s194 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -1pt; }
 .s195 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 7pt; }
 .s196 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s197 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s198 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s199 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s200 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; vertical-align: 1pt; }
 .s201 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 1pt; }
 .s202 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s203 { color: #1D1D1B; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -2pt; }
 .s204 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s205 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10.5pt; }
 .s207 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; vertical-align: -4pt; }
 .s209 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: -5pt; }
 .s210 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: 1pt; }
 .s211 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 3.5pt; }
 .s212 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4.5pt; vertical-align: 1pt; }
 .s213 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4.5pt; }
 .s214 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 1pt; }
 .s215 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: -6pt; }
 .s216 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: -7pt; }
 .s217 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 8pt; }
 .s218 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 6pt; }
 .s219 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: 4pt; }
 .s220 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s221 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 5pt; }
 .s222 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 7pt; }
 .s223 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 1pt; }
 .s224 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s225 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: 6pt; }
 .s226 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; vertical-align: 4pt; }
 .s227 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; vertical-align: 1pt; }
 .s228 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 3pt; }
 .s229 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6pt; vertical-align: -1pt; }
 .s230 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 1pt; }
 .s231 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s232 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 3.5pt; vertical-align: 6pt; }
 .s233 { color: #1D1D1B; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10.5pt; }
 .s234 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; }
 .s235 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: 1pt; }
 .s236 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -3pt; }
 .s237 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -7pt; }
 .s238 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -4pt; }
 .s239 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s240 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s241 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -3pt; }
 .s242 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13.5pt; }
 .s243 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; vertical-align: 1pt; }
 .s244 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 4pt; }
 .s245 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 6pt; }
 .s246 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -2pt; }
 .s247 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -3pt; }
 .s248 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; }
 .s249 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -1pt; }
 .s250 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -1pt; }
 .s251 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -3pt; }
 .s252 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 1pt; }
 .s253 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -1pt; }
 .s254 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 2pt; }
 .s255 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s256 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; vertical-align: 2pt; }
 .s257 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s258 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 6pt; }
 .s259 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 1pt; }
 .s260 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; vertical-align: 2pt; }
 .s261 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -2pt; }
 .s262 { color: #1D1D1B; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s263 { color: #1D1D1B; font-family:Wingdings; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s264 { color: #13110D; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s265 { color: black; font-family:"Courier New", monospace; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s266 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -1pt; }
 .s268 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: 1pt; }
 .s269 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 5pt; }
 .s270 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 4pt; }
 .s271 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -1pt; }
 .s272 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -5pt; }
 .s273 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s274 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s275 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 5pt; }
 .s276 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13.5pt; vertical-align: 2pt; }
 .s277 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s278 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 1pt; }
 .s279 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 2pt; }
 .s280 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 5pt; }
 .s281 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: -1pt; }
 .s282 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: -1pt; }
 .s283 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s284 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -6pt; }
 .s285 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -1pt; }
 .s286 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -1pt; }
 .s287 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: 3pt; }
 .s288 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -4pt; }
 .s289 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 1pt; }
 .s290 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 4pt; }
 .s291 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13pt; vertical-align: -7pt; }
 .s292 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 2pt; }
 .s293 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13.5pt; vertical-align: 1pt; }
 .s294 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -5pt; }
 .s295 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: -2pt; }
 .s296 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; vertical-align: -1pt; }
 .s297 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7.5pt; }
 .s299 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -4pt; }
 .s300 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -4pt; }
 .s301 { color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s302 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -3pt; }
 .s303 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 3pt; }
 .s304 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 3pt; }
 .s305 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -2pt; }
 .s306 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 4pt; }
 .s307 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 3pt; }
 .s308 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 2pt; }
 .s309 { color: black; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s310 { color: black; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s311 { color: black; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11.5pt; }
 .s312 { color: black; font-family:"Cambria Math", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s313 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; vertical-align: 3pt; }
 .s314 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -4pt; }
 .s315 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -4pt; }
 .s316 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s317 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: -4pt; }
 .s318 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7pt; }
 .s319 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: 5pt; }
 .s320 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9.5pt; }
 .s321 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9.5pt; }
 .s322 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 5pt; }
 .s323 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6.5pt; vertical-align: 5pt; }
 .s324 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; vertical-align: 7pt; }
 .s325 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 6pt; }
 .s327 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 9pt; vertical-align: 7pt; }
 .s328 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 7pt; }
 .s329 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: -3pt; }
 .s330 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s331 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -1pt; }
 .s332 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; vertical-align: -5pt; }
 .s333 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -2pt; }
 .s334 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 9pt; }
 .s335 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; vertical-align: 8pt; }
 .s336 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7.5pt; vertical-align: -2pt; }
 .s337 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7.5pt; }
 .s338 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s339 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; vertical-align: -3pt; }
 .s340 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12.5pt; vertical-align: -8pt; }
 .s341 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: -5pt; }
 .s343 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -2pt; }
 .s344 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 3pt; }
 .s345 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7pt; }
 .s346 { color: black; font-family:"Cambria Math", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7pt; vertical-align: 7pt; }
 h1 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 38pt; }
 .s347 { color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 18pt; }
 .s348 { color: #1D1D1B; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s349 { color: #231F20; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s350 { color: #231F20; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s351 { color: #231F20; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14.5pt; }
 .s352 { color: #231F20; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 25pt; }
 .s353 { color: #231F20; font-family:Calibri, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s355 { color: #231F20; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt; }
 #l2> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l3 {padding-left: 0pt;counter-reset: d1 1; }
 #l3> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 li {display: block; }
 #l4 {padding-left: 0pt;counter-reset: e1 1; }
 #l4> li>*:first-child:before {counter-increment: e1; content: counter(e1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 li {display: block; }
 #l5 {padding-left: 0pt;counter-reset: f1 1; }
 #l5> li>*:first-child:before {counter-increment: f1; content: counter(f1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 li {display: block; }
 #l6 {padding-left: 0pt; }
 #l6> li>*:first-child:before {content: "• "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l7 {padding-left: 0pt; }
 #l7> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l8 {padding-left: 0pt; }
 #l8> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l9 {padding-left: 0pt; }
 #l9> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l10 {padding-left: 0pt; }
 #l10> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l11 {padding-left: 0pt; }
 #l11> li>*:first-child:before {content: "– "; color: #1D1D1B; font-family:Calibri, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14.5pt; }
 #l12 {padding-left: 0pt; }
 #l12> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l13 {padding-left: 0pt; }
 #l13> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l14 {padding-left: 0pt; }
 #l14> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l15 {padding-left: 0pt;counter-reset: j1 1; }
 #l15> li>*:first-child:before {counter-increment: j1; content: counter(j1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l15> li:first-child>*:first-child:before {counter-increment: j1 0;  }
 #l16 {padding-left: 0pt;counter-reset: k1 1; }
 #l16> li>*:first-child:before {counter-increment: k1; content: counter(k1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l16> li:first-child>*:first-child:before {counter-increment: k1 0;  }
 #l17 {padding-left: 0pt;counter-reset: k2 1; }
 #l17> li>*:first-child:before {counter-increment: k2; content: counter(k2, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l17> li:first-child>*:first-child:before {counter-increment: k2 0;  }
 #l18 {padding-left: 0pt;counter-reset: l1 1; }
 #l18> li>*:first-child:before {counter-increment: l1; content: counter(l1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l18> li:first-child>*:first-child:before {counter-increment: l1 0;  }
 #l19 {padding-left: 0pt;counter-reset: m1 1; }
 #l19> li>*:first-child:before {counter-increment: m1; content: counter(m1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l19> li:first-child>*:first-child:before {counter-increment: m1 0;  }
 #l20 {padding-left: 0pt;counter-reset: n1 1; }
 #l20> li>*:first-child:before {counter-increment: n1; content: counter(n1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l20> li:first-child>*:first-child:before {counter-increment: n1 0;  }
 #l21 {padding-left: 0pt; }
 #l21> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l22 {padding-left: 0pt;counter-reset: o1 1; }
 #l22> li>*:first-child:before {counter-increment: o1; content: counter(o1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l22> li:first-child>*:first-child:before {counter-increment: o1 0;  }
 #l23 {padding-left: 0pt; }
 #l23> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l24 {padding-left: 0pt; }
 #l24> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l25 {padding-left: 0pt;counter-reset: p1 1; }
 #l25> li>*:first-child:before {counter-increment: p1; content: counter(p1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l25> li:first-child>*:first-child:before {counter-increment: p1 0;  }
 li {display: block; }
 #l26 {padding-left: 0pt; }
 #l26> li>*:first-child:before {content: "· "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l27 {padding-left: 0pt; }
 #l27> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l28 {padding-left: 0pt; }
 #l28> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l29 {padding-left: 0pt; }
 #l29> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l30 {padding-left: 0pt; }
 #l30> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l31 {padding-left: 0pt;counter-reset: s1 1; }
 #l31> li>*:first-child:before {counter-increment: s1; content: counter(s1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l31> li:first-child>*:first-child:before {counter-increment: s1 0;  }
 #l32 {padding-left: 0pt; }
 #l32> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l33 {padding-left: 0pt; }
 #l33> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l34 {padding-left: 0pt;counter-reset: t1 10; }
 #l34> li>*:first-child:before {counter-increment: t1; content: counter(t1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l34> li:first-child>*:first-child:before {counter-increment: t1 0;  }
 #l35 {padding-left: 0pt;counter-reset: t2 1; }
 #l35> li>*:first-child:before {counter-increment: t2; content: counter(t2, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l35> li:first-child>*:first-child:before {counter-increment: t2 0;  }
 #l36 {padding-left: 0pt;counter-reset: u1 5; }
 #l36> li>*:first-child:before {counter-increment: u1; content: counter(u1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l36> li:first-child>*:first-child:before {counter-increment: u1 0;  }
 #l37 {padding-left: 0pt; }
 #l37> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l38 {padding-left: 0pt; }
 #l38> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l39 {padding-left: 0pt;counter-reset: w1 1; }
 #l39> li>*:first-child:before {counter-increment: w1; content: counter(w1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l39> li:first-child>*:first-child:before {counter-increment: w1 0;  }
 #l40 {padding-left: 0pt; }
 #l40> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l41 {padding-left: 0pt;counter-reset: x1 1; }
 #l41> li>*:first-child:before {counter-increment: x1; content: counter(x1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l41> li:first-child>*:first-child:before {counter-increment: x1 0;  }
 #l42 {padding-left: 0pt; }
 #l42> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l43 {padding-left: 0pt; }
 #l43> li>*:first-child:before {content: "• "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l44 {padding-left: 0pt;counter-reset: z1 1; }
 #l44> li>*:first-child:before {counter-increment: z1; content: counter(z1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l44> li:first-child>*:first-child:before {counter-increment: z1 0;  }
 #l45 {padding-left: 0pt; }
 #l45> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l46 {padding-left: 0pt; }
 #l46> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l47 {padding-left: 0pt; }
 #l47> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l48 {padding-left: 0pt; }
 #l48> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l49 {padding-left: 0pt; }
 #l49> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l50 {padding-left: 0pt; }
 #l50> li>*:first-child:before {content: "– "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l51 {padding-left: 0pt; }
 #l51> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l52 {padding-left: 0pt; }
 #l52> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l53 {padding-left: 0pt; }
 #l53> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l54 {padding-left: 0pt; }
 #l54> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l55 {padding-left: 0pt; }
 #l55> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l56 {padding-left: 0pt; }
 #l56> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l57 {padding-left: 0pt; }
 #l57> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l58 {padding-left: 0pt;counter-reset: f1 1; }
 #l58> li>*:first-child:before {counter-increment: f1; content: counter(f1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l58> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l59 {padding-left: 0pt; }
 #l59> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l60 {padding-left: 0pt; }
 #l60> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l61 {padding-left: 0pt;counter-reset: f1 1; }
 #l61> li>*:first-child:before {counter-increment: f1; content: counter(f1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l61> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l62 {padding-left: 0pt;counter-reset: f2 1; }
 #l62> li>*:first-child:before {counter-increment: f2; content: counter(f2, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l62> li:first-child>*:first-child:before {counter-increment: f2 0;  }
 #l63 {padding-left: 0pt; }
 #l63> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l64 {padding-left: 0pt; }
 #l64> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l65 {padding-left: 0pt; }
 #l65> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l66 {padding-left: 0pt;counter-reset: g1 1; }
 #l66> li>*:first-child:before {counter-increment: g1; content: counter(g1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l66> li:first-child>*:first-child:before {counter-increment: g1 0;  }
 #l67 {padding-left: 0pt;counter-reset: h1 1; }
 #l67> li>*:first-child:before {counter-increment: h1; content: counter(h1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l67> li:first-child>*:first-child:before {counter-increment: h1 0;  }
 #l68 {padding-left: 0pt; }
 #l68> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l69 {padding-left: 0pt;counter-reset: i1 1; }
 #l69> li>*:first-child:before {counter-increment: i1; content: counter(i1, upper-roman)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l69> li:first-child>*:first-child:before {counter-increment: i1 0;  }
 #l70 {padding-left: 0pt; }
 #l70> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l71 {padding-left: 0pt; }
 #l71> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l72 {padding-left: 0pt; }
 #l72> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l73 {padding-left: 0pt;counter-reset: k1 8; }
 #l73> li>*:first-child:before {counter-increment: k1; content: counter(k1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l73> li:first-child>*:first-child:before {counter-increment: k1 0;  }
 #l74 {padding-left: 0pt;counter-reset: k2 17; }
 #l74> li>*:first-child:before {counter-increment: k2; content: counter(k1, decimal)"."counter(k2, decimal)" "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l74> li:first-child>*:first-child:before {counter-increment: k2 0;  }
 #l75 {padding-left: 0pt; }
 #l75> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l76 {padding-left: 0pt; }
 #l76> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l77 {padding-left: 0pt;counter-reset: m1 1; }
 #l77> li>*:first-child:before {counter-increment: m1; content: counter(m1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l77> li:first-child>*:first-child:before {counter-increment: m1 0;  }
 #l78 {padding-left: 0pt; }
 #l78> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l79 {padding-left: 0pt;counter-reset: n1 1; }
 #l79> li>*:first-child:before {counter-increment: n1; content: counter(n1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l79> li:first-child>*:first-child:before {counter-increment: n1 0;  }
 #l80 {padding-left: 0pt; }
 #l80> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l81 {padding-left: 0pt;counter-reset: o1 1; }
 #l81> li>*:first-child:before {counter-increment: o1; content: counter(o1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l81> li:first-child>*:first-child:before {counter-increment: o1 0;  }
 #l82 {padding-left: 0pt;counter-reset: o2 1; }
 #l82> li>*:first-child:before {counter-increment: o2; content: counter(o2, upper-roman)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l82> li:first-child>*:first-child:before {counter-increment: o2 0;  }
 #l83 {padding-left: 0pt;counter-reset: p1 1; }
 #l83> li>*:first-child:before {counter-increment: p1; content: counter(p1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l83> li:first-child>*:first-child:before {counter-increment: p1 0;  }
 #l84 {padding-left: 0pt; }
 #l84> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l85 {padding-left: 0pt;counter-reset: q1 1; }
 #l85> li>*:first-child:before {counter-increment: q1; content: counter(q1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l85> li:first-child>*:first-child:before {counter-increment: q1 0;  }
 #l86 {padding-left: 0pt;counter-reset: q2 1; }
 #l86> li>*:first-child:before {counter-increment: q2; content: counter(q2, upper-roman)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l86> li:first-child>*:first-child:before {counter-increment: q2 0;  }
 #l87 {padding-left: 0pt; }
 #l87> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l88 {padding-left: 0pt;counter-reset: s1 1; }
 #l88> li>*:first-child:before {counter-increment: s1; content: counter(s1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l88> li:first-child>*:first-child:before {counter-increment: s1 0;  }
 #l89 {padding-left: 0pt;counter-reset: s2 1; }
 #l89> li>*:first-child:before {counter-increment: s2; content: counter(s2, upper-roman)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l89> li:first-child>*:first-child:before {counter-increment: s2 0;  }
 #l90 {padding-left: 0pt;counter-reset: s3 1; }
 #l90> li>*:first-child:before {counter-increment: s3; content: counter(s3, lower-roman)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l90> li:first-child>*:first-child:before {counter-increment: s3 0;  }
 #l91 {padding-left: 0pt; }
 #l91> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l92 {padding-left: 0pt;counter-reset: u1 1; }
 #l92> li>*:first-child:before {counter-increment: u1; content: counter(u1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l92> li:first-child>*:first-child:before {counter-increment: u1 0;  }
 #l93 {padding-left: 0pt;counter-reset: v1 1; }
 #l93> li>*:first-child:before {counter-increment: v1; content: counter(v1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l93> li:first-child>*:first-child:before {counter-increment: v1 0;  }
 #l94 {padding-left: 0pt;counter-reset: w1 1; }
 #l94> li>*:first-child:before {counter-increment: w1; content: counter(w1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l94> li:first-child>*:first-child:before {counter-increment: w1 0;  }
 #l95 {padding-left: 0pt; }
 #l95> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l96 {padding-left: 0pt; }
 #l96> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l97 {padding-left: 0pt; }
 #l97> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l98 {padding-left: 0pt; }
 #l98> li>*:first-child:before {content: "– "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l99 {padding-left: 0pt; }
 #l99> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l100 {padding-left: 0pt; }
 #l100> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l101 {padding-left: 0pt; }
 #l101> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l102 {padding-left: 0pt;counter-reset: z1 20; }
 #l102> li>*:first-child:before {counter-increment: z1; content: counter(z1, lower-latin)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l102> li:first-child>*:first-child:before {counter-increment: z1 0;  }
 #l103 {padding-left: 0pt;counter-reset: z2 4; }
 #l103> li>*:first-child:before {counter-increment: z2; content: counter(z1, lower-latin)"-"counter(z2, lower-latin)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 #l103> li:first-child>*:first-child:before {counter-increment: z2 0;  }
 #l104 {padding-left: 0pt;counter-reset: z3 1; }
 #l104> li>*:first-child:before {counter-increment: z3; content: counter(z3, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l104> li:first-child>*:first-child:before {counter-increment: z3 0;  }
 #l105 {padding-left: 0pt; }
 #l105> li>*:first-child:before {content: "• "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l106 {padding-left: 0pt; }
 #l106> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l107 {padding-left: 0pt; }
 #l107> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l108 {padding-left: 0pt; }
 #l108> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l109 {padding-left: 0pt; }
 #l109> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l110 {padding-left: 0pt; }
 #l110> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l111 {padding-left: 0pt; }
 #l111> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l112 {padding-left: 0pt;counter-reset: f1 1; }
 #l112> li>*:first-child:before {counter-increment: f1; content: counter(f1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l112> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l113 {padding-left: 0pt;counter-reset: f2 1; }
 #l113> li>*:first-child:before {counter-increment: f2; content: counter(f2, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l113> li:first-child>*:first-child:before {counter-increment: f2 0;  }
 #l114 {padding-left: 0pt; }
 #l114> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l115 {padding-left: 0pt;counter-reset: h1 1; }
 #l115> li>*:first-child:before {counter-increment: h1; content: counter(h1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l115> li:first-child>*:first-child:before {counter-increment: h1 0;  }
 #l116 {padding-left: 0pt; }
 #l116> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l117 {padding-left: 0pt; }
 #l117> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l118 {padding-left: 0pt; }
 #l118> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l119 {padding-left: 0pt; }
 #l119> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l120 {padding-left: 0pt; }
 #l120> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l121 {padding-left: 0pt;counter-reset: i1 1; }
 #l121> li>*:first-child:before {counter-increment: i1; content: counter(i1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l121> li:first-child>*:first-child:before {counter-increment: i1 0;  }
 #l122 {padding-left: 0pt; }
 #l122> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l123 {padding-left: 0pt; }
 #l123> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l124 {padding-left: 0pt; }
 #l124> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l125 {padding-left: 0pt; }
 #l125> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l126 {padding-left: 0pt; }
 #l126> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l127 {padding-left: 0pt; }
 #l127> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l128 {padding-left: 0pt; }
 #l128> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l129 {padding-left: 0pt; }
 #l129> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l130 {padding-left: 0pt;counter-reset: l1 1; }
 #l130> li>*:first-child:before {counter-increment: l1; content: counter(l1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l130> li:first-child>*:first-child:before {counter-increment: l1 0;  }
 #l131 {padding-left: 0pt; }
 #l131> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l132 {padding-left: 0pt; }
 #l132> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l133 {padding-left: 0pt;counter-reset: m1 1; }
 #l133> li>*:first-child:before {counter-increment: m1; content: counter(m1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l133> li:first-child>*:first-child:before {counter-increment: m1 0;  }
 #l134 {padding-left: 0pt; }
 #l134> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l135 {padding-left: 0pt; }
 #l135> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l136 {padding-left: 0pt; }
 #l136> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l137 {padding-left: 0pt; }
 #l137> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l138 {padding-left: 0pt; }
 #l138> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l139 {padding-left: 0pt; }
 #l139> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l140 {padding-left: 0pt; }
 #l140> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l141 {padding-left: 0pt; }
 #l141> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l142 {padding-left: 0pt; }
 #l142> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l143 {padding-left: 0pt;counter-reset: p1 1; }
 #l143> li>*:first-child:before {counter-increment: p1; content: counter(p1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l143> li:first-child>*:first-child:before {counter-increment: p1 0;  }
 #l144 {padding-left: 0pt; }
 #l144> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l145 {padding-left: 0pt;counter-reset: q1 1; }
 #l145> li>*:first-child:before {counter-increment: q1; content: counter(q1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l145> li:first-child>*:first-child:before {counter-increment: q1 0;  }
 #l146 {padding-left: 0pt;counter-reset: r1 1; }
 #l146> li>*:first-child:before {counter-increment: r1; content: counter(r1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l146> li:first-child>*:first-child:before {counter-increment: r1 0;  }
 #l147 {padding-left: 0pt; }
 #l147> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l148 {padding-left: 0pt; }
 #l148> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l149 {padding-left: 0pt; }
 #l149> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l150 {padding-left: 0pt;counter-reset: t1 14; }
 #l150> li>*:first-child:before {counter-increment: t1; content: counter(t1, upper-latin)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l150> li:first-child>*:first-child:before {counter-increment: t1 0;  }
 #l151 {padding-left: 0pt;counter-reset: t2 2; }
 #l151> li>*:first-child:before {counter-increment: t2; content: counter(t1, upper-latin)"-"counter(t2, upper-latin)" "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l151> li:first-child>*:first-child:before {counter-increment: t2 0;  }
 #l152 {padding-left: 0pt; }
 #l152> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l153 {padding-left: 0pt; }
 #l153> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l154 {padding-left: 0pt;counter-reset: u1 14; }
 #l154> li>*:first-child:before {counter-increment: u1; content: counter(u1, upper-latin)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l154> li:first-child>*:first-child:before {counter-increment: u1 0;  }
 #l155 {padding-left: 0pt;counter-reset: u2 2; }
 #l155> li>*:first-child:before {counter-increment: u2; content: counter(u1, upper-latin)"-"counter(u2, upper-latin)" "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l155> li:first-child>*:first-child:before {counter-increment: u2 0;  }
 #l156 {padding-left: 0pt; }
 #l156> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l157 {padding-left: 0pt;counter-reset: v1 14; }
 #l157> li>*:first-child:before {counter-increment: v1; content: counter(v1, upper-latin)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l157> li:first-child>*:first-child:before {counter-increment: v1 0;  }
 #l158 {padding-left: 0pt;counter-reset: v2 2; }
 #l158> li>*:first-child:before {counter-increment: v2; content: counter(v1, upper-latin)"-"counter(v2, upper-latin)" "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l158> li:first-child>*:first-child:before {counter-increment: v2 0;  }
 #l159 {padding-left: 0pt; }
 #l159> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l160 {padding-left: 0pt;counter-reset: w1 14; }
 #l160> li>*:first-child:before {counter-increment: w1; content: counter(w1, upper-latin)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l160> li:first-child>*:first-child:before {counter-increment: w1 0;  }
 #l161 {padding-left: 0pt;counter-reset: w2 8; }
 #l161> li>*:first-child:before {counter-increment: w2; content: counter(w1, upper-latin)"-"counter(w2, upper-latin)" "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l161> li:first-child>*:first-child:before {counter-increment: w2 0;  }
 #l162 {padding-left: 0pt; }
 #l162> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l163 {padding-left: 0pt; }
 #l163> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l164 {padding-left: 0pt;counter-reset: y1 3; }
 #l164> li>*:first-child:before {counter-increment: y1; content: counter(y1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l164> li:first-child>*:first-child:before {counter-increment: y1 0;  }
 #l165 {padding-left: 0pt;counter-reset: z1 6; }
 #l165> li>*:first-child:before {counter-increment: z1; content: counter(z1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l165> li:first-child>*:first-child:before {counter-increment: z1 0;  }
 #l166 {padding-left: 0pt;counter-reset: c1 1; }
 #l166> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l166> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l167 {padding-left: 0pt;counter-reset: d1 1; }
 #l167> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l167> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l168 {padding-left: 0pt;counter-reset: d2 3; }
 #l168> li>*:first-child:before {counter-increment: d2; content: counter(d2, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l168> li:first-child>*:first-child:before {counter-increment: d2 0;  }
 #l169 {padding-left: 0pt;counter-reset: e1 9; }
 #l169> li>*:first-child:before {counter-increment: e1; content: counter(e1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l169> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 #l170 {padding-left: 0pt; }
 #l170> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l171 {padding-left: 0pt; }
 #l171> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l172 {padding-left: 0pt; }
 #l172> li>*:first-child:before {content: "• "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l173 {padding-left: 0pt; }
 #l173> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l174 {padding-left: 0pt;counter-reset: h1 1; }
 #l174> li>*:first-child:before {counter-increment: h1; content: counter(h1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l174> li:first-child>*:first-child:before {counter-increment: h1 0;  }
 #l175 {padding-left: 0pt; }
 #l175> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l176 {padding-left: 0pt; }
 #l176> li>*:first-child:before {content: " "; color: #1D1D1B; font-family:"Wingdings 2", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l177 {padding-left: 0pt;counter-reset: j1 1; }
 #l177> li>*:first-child:before {counter-increment: j1; content: counter(j1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l177> li:first-child>*:first-child:before {counter-increment: j1 0;  }
 #l178 {padding-left: 0pt; }
 #l178> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l179 {padding-left: 0pt; }
 #l179> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l180 {padding-left: 0pt; }
 #l180> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l181 {padding-left: 0pt; }
 #l181> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l182 {padding-left: 0pt; }
 #l182> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l183 {padding-left: 0pt;counter-reset: l1 1; }
 #l183> li>*:first-child:before {counter-increment: l1; content: counter(l1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l183> li:first-child>*:first-child:before {counter-increment: l1 0;  }
 #l184 {padding-left: 0pt; }
 #l184> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l185 {padding-left: 0pt; }
 #l185> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l186 {padding-left: 0pt; }
 #l186> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l187 {padding-left: 0pt;counter-reset: m1 1; }
 #l187> li>*:first-child:before {counter-increment: m1; content: counter(m1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l187> li:first-child>*:first-child:before {counter-increment: m1 0;  }
 #l188 {padding-left: 0pt; }
 #l188> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l189 {padding-left: 0pt; }
 #l189> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l190 {padding-left: 0pt; }
 #l190> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l191 {padding-left: 0pt; }
 #l191> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l192 {padding-left: 0pt; }
 #l192> li>*:first-child:before {content: "• "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l193 {padding-left: 0pt;counter-reset: n1 1; }
 #l193> li>*:first-child:before {counter-increment: n1; content: counter(n1, decimal)". "; color: #1D1D1B; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l193> li:first-child>*:first-child:before {counter-increment: n1 0;  }
 li {display: block; }
 #l194 {padding-left: 0pt;counter-reset: o1 20; }
 #l194> li>*:first-child:before {counter-increment: o1; content: counter(o1, lower-latin)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l194> li:first-child>*:first-child:before {counter-increment: o1 0;  }
 #l195 {padding-left: 0pt;counter-reset: o2 4; }
 #l195> li>*:first-child:before {counter-increment: o2; content: counter(o1, lower-latin)"-"counter(o2, lower-latin)" "; color: #1D1D1B; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 #l195> li:first-child>*:first-child:before {counter-increment: o2 0;  }
 #l196 {padding-left: 0pt; }
 #l196> li>*:first-child:before {content: "• "; color: #231F20; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l197 {padding-left: 0pt; }
 #l197> li>*:first-child:before {content: "• "; color: #231F20; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l198 {padding-left: 0pt;counter-reset: p1 1; }
 #l198> li>*:first-child:before {counter-increment: p1; content: counter(p1, decimal)". "; color: #231F20; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l198> li:first-child>*:first-child:before {counter-increment: p1 0;  }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><span><img width="733" height="913" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_001.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark1">Modern Time Series Forecasting with Python</a></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 28pt;text-indent: 0pt;line-height: 114%;text-align: left;">Explore industry-ready time series forecasting using modern machine learning and deep learning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Manu Joseph</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><img width="154" height="39" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_002.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">BIRMINGHAM—MUMBAI</p><p class="s3" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark2">Modern Time Series Forecasting with Python</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Copyright © 2022 Packt Publishing</p><p class="s4" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 89%;text-align: justify;">All rights reserved<span class="p">. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.</p><p class="s5" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;line-height: 127%;text-align: left;">Publishing Product Manager<span class="p">: Dhruv Kataria </span>Senior Editors<span class="p">: Roshan Ravikumar, Tazeen Shaikh </span>Content Development Editor<span class="p">: Shreya Moharir </span>Technical Editor<span class="p">: Devanshi Ayare</span></p><p class="s5" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Copy Editor<span class="p">: Safis Editing</span></p><p class="s5" style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Project Coordinator<span class="p">: Farheen Fathima</span></p><p class="s5" style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Proofreader<span class="p">: Safis Editing</span></p><p class="s5" style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 127%;text-align: left;">Indexer<span class="p">: Subalakshmi Govindhan </span>Production Designer<span class="p">: Alishon Mendonca </span>Marketing Coordinator<span class="p">: Shifa Ansari</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 127%;text-align: left;">First published: November 2022 Production reference: 1181122</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 127%;text-align: left;">Published by Packt Publishing Ltd. Livery Place</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 127%;text-align: left;">35 Livery Street Birmingham B3 2PB, UK.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">ISBN 978-1-80324-680-2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="http://www.packt.com/">www.packt.com</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 11pt;padding-left: 173pt;text-indent: 0pt;text-align: center;"><a name="bookmark3">For my son, Zane,</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 10pt;padding-left: 173pt;text-indent: 0pt;line-height: 156%;text-align: center;">For his boundless curiosity, For his endless questions,</p><p class="s4" style="padding-left: 12pt;text-indent: 0pt;line-height: 13pt;text-align: center;">And for his innocent love of learning.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">(All great qualities for adults who read this book as well.)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 5pt;padding-left: 265pt;text-indent: 0pt;text-align: left;"><a name="bookmark4">Contributors</a></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">About the author</p><p class="s5" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Manu Joseph <span class="p">is a self-made data scientist with more than a decade of experience working with many Fortune 500 companies, enabling digital and AI transformations, specifically in machine learning- based demand forecasting. He is considered an expert, thought leader, and strong voice in the world of time series forecasting. Currently, Manu leads applied research at Thoucentric, where he advances research by bringing cutting-edge AI technologies to the industry. He is also an active open source contributor and has developed an open source library—PyTorch Tabular—which makes deep learning for tabular data easy and accessible. Originally from Thiruvananthapuram, India, Manu currently resides in Bengaluru, India, with his wife and son.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">About the reviewers</p><p class="s5" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Dr. Julien Siebert <span class="p">is currently working as a researcher at the Fraunhofer </span>Institute for Experimental Software Engineering <span class="p">(</span>IESE<span class="p">), in Kaiserslautern, Germany. He studied engineering sciences and AI and obtained a PhD in computer science on the topic of modeling and simulation of complex systems. After several years of research both in computer science and theoretical physics, Dr. Julien Siebert worked as a data scientist for an e-commerce fashion company. Since 2018, he has been working at the intersection between software engineering and data science.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s5">Gerzson David Boros </span>is the owner and CEO of Data Science Europe and a senior data scientist who has been involved in data science for more than 10 years. He has an MSc and is a candidate for an MBA. In the last 5 years, he and his team have made business proposals for 100 different executives and worked on more than 30 different projects on the topic of data science and artificial intelligence. His motto is “<i>Social responsibility is also achievable with the help of data</i>.”</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 13pt;padding-left: 12pt;text-indent: 0pt;text-align: center;"><a name="bookmark5">Table of Contents</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_003.png"/></span></p><p style="padding-top: 12pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark6" class="s6">Preface xvii</a></p><p style="padding-top: 15pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark8" class="s7">Part 1 – Getting Familiar with Time Series 1</a></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark11" class="s8">1</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_004.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark11" class="s6">Introducing Time Series 3</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark11" class="s9">Technical requirements 3</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark24" class="s9">What is a time series? 4</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark24" class="s10">Types of time series 4</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark24" class="s10">Main areas of application for time series analysis 4</a></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark25" class="s9">Data-generating process (DGP) 5</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark27" class="s10">Generating synthetic time series 7</a></p><p style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark39" class="s8">2</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark32" class="s10">Stationary and non-stationary time series 13</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark35" class="s9">What can we forecast? 16</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s9">Forecasting terminology 17</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark37" class="s9">Summary 18</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark37" class="s9">Further reading 18</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_005.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark39" class="s6">Acquiring and Processing Time Series Data 19</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark39" class="s9">Technical requirements 19</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark62" class="s9">Understanding the time series dataset 20</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark64" class="s10">Preparing a data model 22</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark65" class="s9">pandas datetime operations, indexing, and slicing</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark65" class="s9">– a refresher 23</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark66" class="s10">Converting the date columns into pd.Timestamp/DatetimeIndex 24</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark67" class="s10">Using the .dt accessor and datetime properties 25</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark68" class="s10">Slicing and indexing 26</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark69" class="s10">Creating date sequences and managing date</a></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark69" class="s10">offsets 27</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark70" class="s9">Handling missing data 28</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" class="s10">Converting the half-hourly block-level data (hhblock) into time series data 33</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" class="s10">Compact, expanded, and wide forms of data 33</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark76" class="s10">Enforcing regular intervals in time series 34</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark77" class="s10">Converting the London Smart Meters dataset</a></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark77" class="s10">into a time series format 35</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_006.png"/></span></p><p style="padding-top: 18pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark78" class="s9">Mapping additional information 36</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark80" class="s9">Saving and loading files to disk 38</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark80" class="s9">Handling longer periods of missing</a></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark80" class="s9">data 38</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark81" class="s10">Imputing with the previous day 42</a></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark88" class="s8">3</a></p><p style="padding-top: 20pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark82" class="s10">Hourly average profile 43</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark83" class="s10">The hourly average for each weekday 45</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark84" class="s10">Seasonal interpolation 46</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark86" class="s9">Summary 48</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_007.png"/></span></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark88" class="s6">Analyzing and Visualizing Time Series Data 49</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark88" class="s9">Technical requirements 49</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark115" class="s9">Components of a time series 50</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark115" class="s10">The trend component 50</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark116" class="s10">The seasonal component 51</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark116" class="s10">The cyclical component 51</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark116" class="s10">The irregular component 51</a></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark117" class="s9">Visualizing time series data 52</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark117" class="s10">Line charts 52</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark118" class="s10">Seasonal plots 55</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark119" class="s10">Seasonal box plots 56</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark121" class="s10">Calendar heatmaps 58</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark121" class="s10">Autocorrelation plot 58</a></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark123" class="s9">Decomposing a time series 60</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark138" class="s8">4</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark123" class="s10">Detrending 60</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark124" class="s10">Deseasonalizing 61</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark125" class="s10">Implementations 63</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s9">Detecting and treating outliers 72</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s10">Standard deviation 72</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark134" class="s10">Interquartile range (IQR) 73</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark134" class="s10">Isolation Forest 73</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark134" class="s10">Extreme studentized deviate (ESD) and</a></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark134" class="s10">seasonal ESD (S-ESD) 73</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark135" class="s10">Treating outliers 74</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark136" class="s9">Summary 75</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark136" class="s9">References 75</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark136" class="s9">Further reading 75</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_008.png"/></span></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark138" class="s6">Setting a Strong Baseline Forecast 77</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark161" class="s9">Technical requirements 78</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark161" class="s9">Setting up a test harness 78</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark161" class="s10">Creating holdout (test) and validation datasets 78</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark162" class="s10">Choosing an evaluation metric 79</a></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark163" class="s9">Generating strong baseline forecasts 80</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark164" class="s10">Naïve forecast 82</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark165" class="s10">Moving average forecast 83</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark165" class="s10">Seasonal naive forecast 83</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark166" class="s10">Exponential smoothing (ETS) 84</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark169" class="s10">ARIMA 87</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark171" class="s10">Theta Forecast 89</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark173" class="s10">Fast Fourier Transform forecast 91</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark175" class="s10">Evaluating the baseline forecasts 94</a></p><p style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark176" class="s9">Assessing the forecastability of a time series 96</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_009.png"/></span></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:16pt"><td style="width:149pt"><p style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark176" class="s13">Coefficient of Variation (CoV)</a></p></td><td style="width:52pt"><p style="padding-top: 1pt;padding-right: 8pt;text-indent: 0pt;text-align: right;"><a href="#bookmark176" class="s13">96</a></p></td><td style="width:119pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark182" class="s14">Summary</a></p></td><td style="width:81pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark182" class="s14">103</a></p></td></tr><tr style="height:16pt"><td style="width:149pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark177" class="s13">Residual variability (RV)</a></p></td><td style="width:52pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark177" class="s13">97</a></p></td><td style="width:119pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark182" class="s14">References</a></p></td><td style="width:81pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark182" class="s14">103</a></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 47pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark178" class="s10">Entropy-based measures 98</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark180" class="s10">Kaboudan metric 101</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark183" class="s9">Further reading 104</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark184" class="s7">Part 2 – Machine Learning for Time Series 105</a></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark187" class="s8">5</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_010.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark187" class="s6">Time Series Forecasting as Regression 107</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark187" class="s9">Understanding the basics of machine learning 107</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s10">Supervised machine learning tasks 110</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s10">Overfitting and underfitting 110</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark205" class="s10">Hyperparameters and validation sets 113</a></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark206" class="s9">Time series forecasting as regression 114</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark206" class="s10">Time delay embedding 114</a></p><p style="padding-top: 13pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark212" class="s8">6</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark208" class="s10">Temporal embedding 116</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark208" class="s9">Global forecasting models – a</a></p><table style="border-collapse:collapse;margin-left:12.5069pt" cellspacing="0"><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark208" class="s14">paradigm shift</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;"><a href="#bookmark208" class="s14">116</a></p></td></tr><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark209" class="s14">Summary</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;"><a href="#bookmark209" class="s14">118</a></p></td></tr><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark210" class="s14">References</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;"><a href="#bookmark210" class="s14">119</a></p></td></tr><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark210" class="s14">Further reading</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;"><a href="#bookmark210" class="s14">119</a></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_011.png"/></span></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark212" class="s6">Feature Engineering for Time Series Forecasting 121</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark212" class="s9">Technical requirements 121</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark235" class="s10">Exponentially weighted moving averages</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:25.5pt" cellspacing="0"><tr style="height:15pt"><td style="width:159pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark228" class="s14">Feature engineering</a></p></td><td style="width:42pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark228" class="s14">122</a></p></td><td style="width:143pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a href="#bookmark235" class="s13">(EWMA)</a></p></td><td style="width:57pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 9pt;text-align: right;"><a href="#bookmark235" class="s13">131</a></p></td></tr><tr style="height:17pt"><td style="width:159pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark229" class="s14">Avoiding data leakage</a></p></td><td style="width:42pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark229" class="s14">123</a></p></td><td style="width:143pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark237" class="s14">Temporal embedding</a></p></td><td style="width:57pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark237" class="s14">134</a></p></td></tr><tr style="height:16pt"><td style="width:159pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark230" class="s14">Setting a forecast horizon</a></p></td><td style="width:42pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark230" class="s14">124</a></p></td><td style="width:143pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark237" class="s13">Calendar features</a></p></td><td style="width:57pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark237" class="s13">134</a></p></td></tr><tr style="height:16pt"><td style="width:159pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark231" class="s14">Time delay embedding</a></p></td><td style="width:42pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark231" class="s14">125</a></p></td><td style="width:143pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark238" class="s13">Time elapsed</a></p></td><td style="width:57pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 10pt;text-align: right;"><a href="#bookmark238" class="s13">135</a></p></td></tr><tr style="height:14pt"><td style="width:159pt"><p style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark231" class="s13">Lags or backshift</a></p></td><td style="width:42pt"><p style="padding-top: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: right;"><a href="#bookmark231" class="s13">125</a></p></td><td style="width:143pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="#bookmark239" class="s13">Fourier terms</a></p></td><td style="width:57pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 8pt;text-align: right;"><a href="#bookmark239" class="s13">136</a></p></td></tr><tr style="height:15pt"><td style="width:159pt"><p style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark233" class="s13">Rolling window aggregations</a></p></td><td style="width:42pt"><p style="padding-top: 1pt;padding-right: 8pt;text-indent: 0pt;text-align: right;"><a href="#bookmark233" class="s13">127</a></p></td><td style="width:143pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark240" class="s14">Summary</a></p></td><td style="width:57pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark240" class="s14">138</a></p></td></tr><tr style="height:13pt"><td style="width:159pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark234" class="s13">Seasonal rolling window aggregations</a></p></td><td style="width:42pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark234" class="s13">129</a></p></td><td style="width:143pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:57pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_012.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark242" class="s8">7</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_013.png"/></span></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark242" class="s6">Target Transformations for Time Series Forecasting 139</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark242" class="s9">Technical requirements 139</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark266" class="s9">Handling non-stationarity in time</a></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark266" class="s9">series 140</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark268" class="s9">Detecting and correcting for unit</a></p><p style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark277" class="s9">Detecting and correcting for</a></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark277" class="s9">seasonality 151</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark277" class="s10">Detecting seasonality 151</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark279" class="s10">Deseasonalizing transform 153</a></p><table style="border-collapse:collapse;margin-left:34.5pt" cellspacing="0"><tr style="height:31pt"><td style="width:167pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark268" class="s14">roots</a></p><p style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark268" class="s13">Unit roots</a></p></td><td style="width:33pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark268" class="s14">142</a></p><p style="padding-top: 2pt;padding-left: 11pt;text-indent: 0pt;text-align: left;"><a href="#bookmark268" class="s13">142</a></p></td><td style="width:201pt" colspan="2"><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark280" class="s14">Detecting and correcting for heteroscedasticity 155</a></p></td></tr><tr style="height:14pt"><td style="width:167pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark269" class="s13">The Augmented Dickey-Fuller (ADF) test</a></p></td><td style="width:33pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 10pt;text-align: right;"><a href="#bookmark269" class="s13">143</a></p></td><td style="width:135pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark280" class="s13">Detecting heteroscedasticity</a></p></td><td style="width:66pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark280" class="s13">155</a></p></td></tr><tr style="height:15pt"><td style="width:167pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark270" class="s13">Differencing transform</a></p></td><td style="width:33pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 10pt;text-align: right;"><a href="#bookmark270" class="s13">144</a></p></td><td style="width:135pt"><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark281" class="s13">Log transform</a></p></td><td style="width:66pt"><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark281" class="s13">156</a></p></td></tr><tr style="height:18pt"><td style="width:167pt"><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark271" class="s14">Detecting and correcting for trends</a></p></td><td style="width:33pt"><p style="padding-right: 8pt;text-indent: 0pt;text-align: right;"><a href="#bookmark271" class="s14">145</a></p></td><td style="width:135pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark282" class="s13">Box-Cox transform</a></p></td><td style="width:66pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark282" class="s13">157</a></p></td></tr><tr style="height:15pt"><td style="width:167pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark272" class="s13">Deterministic and stochastic trends</a></p></td><td style="width:33pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: right;"><a href="#bookmark272" class="s13">146</a></p></td><td style="width:201pt" colspan="2"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark283" class="s14">AutoML approach to target</a></p></td></tr><tr style="height:14pt"><td style="width:167pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark273" class="s13">Kendall’s Tau</a></p></td><td style="width:33pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: right;"><a href="#bookmark273" class="s13">147</a></p></td><td style="width:201pt" colspan="2"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark283" class="s14">transformation 159</a></p></td></tr></table><table style="border-collapse:collapse" cellspacing="0"><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark285" class="s14">Summary</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark285" class="s14">162</a></p></td></tr><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark285" class="s14">References</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark285" class="s14">162</a></p></td></tr><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark285" class="s14">Further reading</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark285" class="s14">162</a></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark275" class="s10">Mann-Kendall test (M-K test) 149</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark276" class="s10">Detrending transform 150</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark287" class="s8">8</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_014.png"/></span></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark287" class="s6">Forecasting Time Series with Machine Learning Models 165</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark287" class="s9">Technical requirements 165</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark308" class="s9">Training and predicting with</a></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark308" class="s9">machine learning models 166</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark309" class="s9">Generating single-step forecast</a></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark309" class="s9">baselines 167</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark309" class="s9">Standardized code to train and</a></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark309" class="s9">evaluate machine learning models 167</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark310" class="s10">FeatureConfig 168</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark311" class="s10">MissingValueConfig 169</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark312" class="s10">ModelConfig 170</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark313" class="s10">MLForecast 171</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark314" class="s10">Helper functions for evaluating models 172</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark315" class="s10">Linear regression 173</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark317" class="s10">Regularized linear regression 176</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark321" class="s10">Decision trees 183</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark325" class="s10">Random forest 188</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark328" class="s10">Gradient boosting decision trees 192</a></p><p style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark333" class="s9">Training and predicting for multiple households 198</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark334" class="s10">Using AutoStationaryTransformer 199</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark335" class="s9">Summary 200</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark336" class="s9">References 201</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark336" class="s9">Further reading 201</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark338" class="s8">9</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_015.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark338" class="s6">Ensembling and Stacking 203</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark338" class="s9">Technical requirements 203</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark352" class="s9">Combining forecasts 204</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark353" class="s10">Best fit 205</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark354" class="s10">Measures of central tendency 206</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark356" class="s10">Simple hill climbing 208</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark357" class="s10">Stochastic hill climbing 210</a></p><p style="padding-top: 17pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark368" class="s8">10</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark358" class="s10">Simulated annealing 212</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark360" class="s10">Optimal weighted ensemble 216</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark362" class="s9">Stacking or blending 219</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark366" class="s9">Summary 223</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark366" class="s9">References 223</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark366" class="s9">Further reading 223</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_016.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark368" class="s6">Global Forecasting Models 225</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark368" class="s9">Technical requirements 225</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark386" class="s9">Why Global Forecasting Models</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark386" class="s9">(GFMs)? 226</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark387" class="s10">Sample size 227</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark387" class="s10">Cross-learning 227</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark388" class="s10">Multi-task learning 228</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark388" class="s10">Engineering complexity 228</a></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark389" class="s9">Creating GFMs 229</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark390" class="s9">Strategies to improve GFMs 231</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark392" class="s10">Increasing memory 233</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark393" class="s10">Using time series meta-features 234</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark398" class="s10">Tuning hyperparameters 242</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark405" class="s10">Partitioning 249</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark410" class="s9">Bonus – interpretability 256</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark410" class="s9">Summary 256</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark410" class="s9">References 256</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark411" class="s9">Further reading 257</a></p><p style="padding-top: 12pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark412" class="s7">Part 3 – Deep Learning for Time Series 259</a></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark415" class="s8">11</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_017.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark415" class="s6">Introduction to Deep Learning 261</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark415" class="s9">Technical requirements 261</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark415" class="s9">What is deep learning and why now? 261</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark432" class="s10">Why now? 262</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark435" class="s10">What is deep learning? 265</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark436" class="s10">Perceptron – the first neural network 266</a></p><p style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark438" class="s9">Components of a deep learning</a></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark438" class="s9">system 270</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark440" class="s10">Representation learning 272</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark441" class="s10">Linear transformation 273</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark441" class="s10">Activation functions 273</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_018.png"/></span></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:16pt"><td style="width:142pt"><p style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark446" class="s13">Output activation functions</a></p></td><td style="width:59pt"><p style="padding-top: 1pt;padding-right: 8pt;text-indent: 0pt;text-align: right;"><a href="#bookmark446" class="s13">278</a></p></td><td style="width:119pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark452" class="s14">Summary</a></p></td><td style="width:81pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark452" class="s14">286</a></p></td></tr><tr style="height:16pt"><td style="width:142pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark447" class="s13">Loss function</a></p></td><td style="width:59pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark447" class="s13">279</a></p></td><td style="width:119pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark452" class="s14">References</a></p></td><td style="width:81pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark452" class="s14">286</a></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 47pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark448" class="s10">Forward and backward propagation 280</a></p><p style="padding-top: 16pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark455" class="s8">12</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark453" class="s9">Further reading 287</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_019.png"/></span></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark455" class="s6">Building Blocks of Deep Learning for Time Series 289</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark455" class="s9">Technical requirements 289</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark473" class="s9">Understanding the encoder-decoder paradigm 290</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark475" class="s9">Feed-forward networks 292</a></p><p style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark486" class="s9">Gated recurrent unit (GRU) 304</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark488" class="s10">The GRU layer in PyTorch 306</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark489" class="s9">Convolution networks 307</a></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:17pt"><td style="width:150pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark479" class="s14">Recurrent neural networks</a></p></td><td style="width:51pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark479" class="s14">296</a></p></td><td style="width:157pt"><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark490" class="s13">Padding, stride, and dilations</a></p></td><td style="width:43pt"><p style="padding-top: 3pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark490" class="s13">308</a></p></td></tr><tr style="height:14pt"><td style="width:150pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark481" class="s13">The RNN layer in PyTorch</a></p></td><td style="width:51pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark481" class="s13">299</a></p></td><td style="width:157pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark491" class="s13">The convolution layer in PyTorch</a></p></td><td style="width:43pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark491" class="s13">311</a></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark489" class="s10">Convolution 307</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark493" class="s14">Summary</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark493" class="s14">313</a></p></td></tr><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark493" class="s14">References</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark493" class="s14">313</a></p></td></tr><tr style="height:16pt"><td style="width:125pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark494" class="s14">Further reading</a></p></td><td style="width:70pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark494" class="s14">314</a></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 12pt;padding-left: 37pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark484" class="s9">Long short-term memory (LSTM) networks 302</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark485" class="s10">The LSTM layer in PyTorch 303</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark496" class="s8">13</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_020.png"/></span></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark496" class="s6">Common Modeling Patterns for Time Series 317</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark496" class="s9">Technical requirements 317</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark507" class="s9">Tabular regression 318</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark510" class="s9">Single-step-ahead recurrent neural networks 322</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark517" class="s9">Sequence-to-sequence (Seq2Seq)</a></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark517" class="s9">models 334</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark523" class="s8">14</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark517" class="s10">RNN-to-fully connected network 334</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark519" class="s10">RNN-to-RNN 337</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark521" class="s9">Summary 345</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark521" class="s9">Reference 345</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark521" class="s9">Further reading 345</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_021.png"/></span></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark523" class="s6">Attention and Transformers for Time Series 347</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark523" class="s9">Technical requirements 347</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark538" class="s9">What is attention? 348</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark539" class="s9">The generalized attention model 350</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark540" class="s10">Alignment functions 352</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark543" class="s10">The distribution function 355</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark544" class="s9">Forecasting with sequence-to-</a></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark544" class="s9">sequence models and attention 356</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark546" class="s9">Transformers – Attention is all you</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark546" class="s9">need 360</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark546" class="s10">Attention is all you need 360</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark556" class="s10">Transformers in time series 372</a></p><p style="padding-top: 15pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark562" class="s8">15</a></p><p style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark557" class="s9">Forecasting with Transformers 373</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark558" class="s9">Summary 378</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark559" class="s9">References 379</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark560" class="s9">Further reading 380</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_022.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark562" class="s6">Strategies for Global Deep Learning Forecasting Models 381</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark562" class="s9">Technical requirements 381</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark581" class="s9">Creating global deep learning</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark581" class="s9">forecasting models 382</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark582" class="s10">Preprocessing the data 383</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark584" class="s10">Understanding TimeSeriesDataset from</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark584" class="s10">PyTorch Forecasting 386</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark589" class="s10">Building the first global deep learning</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark589" class="s10">forecasting model 391</a></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark592" class="s9">Using time-varying information 394</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark593" class="s9">Using static/meta information 397</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark593" class="s10">One-hot encoding and why it is not ideal 397</a></p><p style="padding-top: 13pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark604" class="s8">16</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;line-height: 111%;text-align: left;"><a href="#bookmark594" class="s10">Embedding vectors and dense representations 398 Defining a model with categorical features 398</a></p><p style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark596" class="s9">Using the scale of the time series 401</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark597" class="s9">Balancing the sampling procedure 402</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark598" class="s10">Visualizing the data distribution 403</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark599" class="s10">Tweaking the sampling procedure 404</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark601" class="s10">Using and visualizing the dataloader with WeightedRandomSampler 406</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark602" class="s9">Summary 407</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark602" class="s9">Further reading 407</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_023.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark604" class="s6">Specialized Deep Learning Architectures for Forecasting 409</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark636" class="s9">Technical requirements 410</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark637" class="s9">The need for specialized architectures 410 Neural Basis Expansion Analysis for Interpretable Time Series Forecasting</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark637" class="s9">(N-BEATS) 411</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark637" class="s10">The architecture of N-BEATS 411</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark641" class="s10">Forecasting with N-BEATS 415</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark642" class="s10">Interpreting N-BEATS forecasting 416</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark643" class="s9">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with Exogenous Variables</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark643" class="s9">(N-BEATSx) 417</a></p><p style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark644" class="s10">Handling exogenous variables 418</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark644" class="s10">Exogenous blocks 418</a></p><p style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark645" class="s9">Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS) 419</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark645" class="s10">The Architecture of N-HiTS 419</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark648" class="s10">Forecasting with N-HiTS 422</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark649" class="s9">Informer 423</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark650" class="s10">The architecture of the Informer model 424</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark653" class="s10">Forecasting with the Informer model 428</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark654" class="s9">Autoformer 429</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark655" class="s10">The architecture of the Autoformer model 430</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_024.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:15pt"><td style="width:172pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark659" class="s13">Forecasting with Autoformer</a></p></td><td style="width:23pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark659" class="s13">434</a></p></td></tr><tr style="height:19pt"><td style="width:172pt"><p style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark661" class="s14">Temporal Fusion Transformer (TFT)</a></p></td><td style="width:23pt"><p style="padding-top: 2pt;padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark661" class="s14">436</a></p></td></tr><tr style="height:14pt"><td style="width:172pt"><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark661" class="s13">The Architecture of TFT</a></p></td><td style="width:23pt"><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark661" class="s13">436</a></p></td></tr><tr style="height:14pt"><td style="width:172pt"><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark667" class="s13">Forecasting with TFT</a></p></td><td style="width:23pt"><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark667" class="s13">442</a></p></td></tr><tr style="height:16pt"><td style="width:172pt"><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark668" class="s13">Interpreting TFT</a></p></td><td style="width:23pt"><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark668" class="s13">443</a></p></td></tr><tr style="height:18pt"><td style="width:172pt"><p style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark670" class="s14">Interpretability</a></p></td><td style="width:23pt"><p style="padding-top: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark670" class="s14">445</a></p></td></tr></table><table style="border-collapse:collapse" cellspacing="0"><tr style="height:16pt"><td style="width:155pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark670" class="s14">Probabilistic forecasting</a></p></td><td style="width:39pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark670" class="s14">445</a></p></td></tr><tr style="height:14pt"><td style="width:155pt"><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark671" class="s13">Probability Density Function (PDF)</a></p></td><td style="width:39pt"><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark671" class="s13">446</a></p></td></tr><tr style="height:14pt"><td style="width:155pt"><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark672" class="s13">Quantile functions</a></p></td><td style="width:39pt"><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark672" class="s13">447</a></p></td></tr><tr style="height:16pt"><td style="width:155pt"><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark673" class="s13">Other approaches</a></p></td><td style="width:39pt"><p style="padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark673" class="s13">448</a></p></td></tr><tr style="height:18pt"><td style="width:155pt"><p style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark674" class="s14">Summary</a></p></td><td style="width:39pt"><p style="padding-top: 2pt;padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark674" class="s14">449</a></p></td></tr><tr style="height:16pt"><td style="width:155pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark674" class="s14">References</a></p></td><td style="width:39pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark674" class="s14">449</a></p></td></tr><tr style="height:16pt"><td style="width:155pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark675" class="s14">Further reading</a></p></td><td style="width:39pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark675" class="s14">451</a></p></td></tr></table><p class="s35" style="padding-left: 34pt;text-indent: 0pt;text-align: left;">	</p><p style="padding-top: 13pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark676" class="s7">Part 4 – Mechanics of Forecasting 453</a></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark679" class="s8">17</a></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:20pt"><td style="width:150pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20"><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><a href="#bookmark699" class="s14">Why multi-step forecasting?</a></p></td><td style="width:48pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20"><p style="padding-top: 4pt;padding-right: 8pt;text-indent: 0pt;text-align: right;"><a href="#bookmark699" class="s14">456</a></p></td><td style="width:159pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20"><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark703" class="s13">Forecasting regime</a></p></td><td style="width:39pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20"><p style="padding-top: 5pt;text-indent: 0pt;text-align: right;"><a href="#bookmark703" class="s13">460</a></p></td></tr><tr style="height:18pt"><td style="width:150pt"><p style="text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark700" class="s14">Recursive strategy</a></p></td><td style="width:48pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark700" class="s14">457</a></p></td><td style="width:159pt"><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark704" class="s14">Hybrid strategies</a></p></td><td style="width:39pt"><p style="padding-top: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark704" class="s14">461</a></p></td></tr><tr style="height:14pt"><td style="width:150pt"><p style="text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark701" class="s13">Training regime</a></p></td><td style="width:48pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: right;"><a href="#bookmark701" class="s13">458</a></p></td><td style="width:159pt"><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark704" class="s13">DirRec Strategy</a></p></td><td style="width:39pt"><p style="padding-top: 1pt;text-indent: 0pt;line-height: 11pt;text-align: right;"><a href="#bookmark704" class="s13">461</a></p></td></tr><tr style="height:15pt"><td style="width:150pt"><p style="text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark701" class="s13">Forecasting regime</a></p></td><td style="width:48pt"><p style="padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: right;"><a href="#bookmark701" class="s13">458</a></p></td><td style="width:159pt"><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark705" class="s13">Iterative block-wise direct strategy</a></p></td><td style="width:39pt"><p style="padding-top: 1pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark705" class="s13">462</a></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark679" class="s6">Multi-Step Forecasting 455</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark701" class="s9">Direct strategy 458</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark702" class="s10">Training regime 459</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark702" class="s10">Forecasting regime 459</a></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark702" class="s9">Joint strategy 459</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark703" class="s10">Training regime 460</a></p><p style="padding-top: 25pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark712" class="s8">18</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 9pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark706" class="s10">Rectify strategy 463</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark707" class="s10">RecJoint 464</a></p><p style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark708" class="s9">How to choose a multi-step</a></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark708" class="s9">forecasting strategy? 465</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark710" class="s9">Summary 468</a></p><p style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark710" class="s9">References 468</a></p><p style="padding-top: 11pt;padding-bottom: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark712" class="s6">Evaluating Forecasts – Forecast Metrics 469</a></p><table style="border-collapse:collapse;margin-left:34.5pt" cellspacing="0"><tr style="height:20pt"><td style="width:173pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20"><p style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark712" class="s14">Technical requirements</a></p></td><td style="width:21pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20"><p style="padding-top: 4pt;padding-right: 2pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark712" class="s14">469</a></p></td><td style="width:207pt;border-top-style:solid;border-top-width:1pt;border-top-color:#231F20" colspan="2"><p style="padding-top: 4pt;padding-left: 14pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark740" class="s14">Experimental study of the error</a></p></td></tr><tr style="height:17pt"><td style="width:173pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark727" class="s14">Taxonomy of forecast error measures</a></p></td><td style="width:228pt" colspan="3"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 71%;text-align: left;"><span style=" color: #231F20; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; vertical-align: -3pt;">470</span><a href="#bookmark740" class="s14"> measures 484</a></p></td></tr><tr style="height:14pt"><td style="width:173pt"><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark728" class="s13">Intrinsic metrics</a></p></td><td style="width:228pt" colspan="3"><p style="padding-left: 5pt;text-indent: 0pt;line-height: 75%;text-align: left;"><span style=" color: #58595B; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; vertical-align: -3pt;">471</span><a href="#bookmark741" class="s13"> Using Spearman’s rank correlation 485</a></p></td></tr><tr style="height:13pt"><td style="width:173pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark732" class="s13">Extrinsic metrics</a></p></td><td style="width:228pt" colspan="3"><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span style=" color: #58595B; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; vertical-align: 3pt;">475</span><a href="#bookmark742" class="s13"> </a><a href="#bookmark742" class="s14">Guidelines for choosing a metric 487</a></p></td></tr><tr style="height:21pt"><td style="width:173pt"><p style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark734" class="s14">Investigating the error measures</a></p></td><td style="width:21pt"><p style="padding-top: 4pt;padding-right: 2pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><a href="#bookmark734" class="s14">477</a></p></td><td style="width:126pt"><p style="padding-top: 4pt;padding-left: 14pt;text-indent: 0pt;text-align: left;"><a href="#bookmark743" class="s14">Summary</a></p></td><td style="width:81pt"><p style="padding-top: 4pt;padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark743" class="s14">489</a></p></td></tr><tr style="height:16pt"><td style="width:173pt"><p style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark734" class="s13">Loss curves and complementarity</a></p></td><td style="width:21pt"><p style="padding-top: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: right;"><a href="#bookmark734" class="s13">477</a></p></td><td style="width:126pt"><p style="padding-left: 14pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark743" class="s14">References</a></p></td><td style="width:81pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark743" class="s14">489</a></p></td></tr><tr style="height:13pt"><td style="width:173pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark739" class="s13">Bias towards over- or under-forecasting</a></p></td><td style="width:228pt" colspan="3"><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span style=" color: #58595B; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; vertical-align: 3pt;">482</span><a href="#bookmark744" class="s13"> </a><a href="#bookmark744" class="s14">Further reading 490</a></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark746" class="s8">19</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_025.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark746" class="s6">Evaluating Forecasts – Validation Strategies 491</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark746" class="s9">Technical requirements 491</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark760" class="s9">Model validation 492</a></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:16pt"><td style="width:128pt"><p style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark761" class="s13">Window strategy</a></p></td><td style="width:73pt"><p style="padding-top: 3pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;"><a href="#bookmark761" class="s13">493</a></p></td><td style="width:119pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark770" class="s14">Summary</a></p></td><td style="width:81pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark770" class="s14">502</a></p></td></tr><tr style="height:15pt"><td style="width:128pt"><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark763" class="s13">Calibration strategy</a></p></td><td style="width:73pt"><p style="padding-right: 8pt;text-indent: 0pt;text-align: right;"><a href="#bookmark763" class="s13">495</a></p></td><td style="width:119pt"><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark771" class="s14">References</a></p></td><td style="width:81pt"><p style="padding-right: 2pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark771" class="s14">503</a></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark761" class="s9">Holdout strategies 493</a></p><p style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark769" class="s9">Choosing a validation strategy 501</a></p><p style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 88%;text-align: left;"><a href="#bookmark770" class="s9">Validation strategies for datasets with multiple time series 502</a></p><p style="padding-top: 28pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark763" class="s10">Sampling strategy 495</a></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark766" class="s9">Cross-validation strategies 498</a></p><p style="padding-top: 29pt;padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark771" class="s9">Further reading 503</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_026.png"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark772" class="s6">Index 505</a></p><p style="padding-top: 15pt;padding-bottom: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark775" class="s6">Other Books You May Enjoy 522</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="523" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_027.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 5pt;text-indent: 0pt;text-align: right;"><a name="bookmark6">Preface</a><a name="bookmark7">&zwnj;</a></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Mankind has always sought the ability to predict the future. Since the earliest civilizations, people have tried to predict the future. Shamans, oracles, and prophets used anything ranging from astrology and palmistry to numerology to satisfy the human need to see into the future. In the last century, with the developments in IT, the mantle of predicting the future landed on data analysts and data scientists. And how do we predict the future? It’s not by examining the lines and creases on our hands or the positions of the stars anymore but by using data that has been generated in the past. And instead of prophecies, we now have forecasts.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Time, being the fourth dimension in our world, makes all the data generated in the world time series data. All the data that is generated in the real world has an element of time associated with it. Whether the temporal aspect is relevant to the problem or not is another question altogether. However, to be more concrete and immediate, we can find time series forecasting use cases in many industries, such as retail, energy, healthcare, and finance. We might want to know how many units of a particular product are to be dispatched to a particular store, or we might want to know how much electricity is to be produced to meet demand.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this book, using a real-world dataset, you will learn how to handle and visualize time series data using <span class="s20">pandas </span>and <span class="s20">plotly</span>, generate baseline forecasts using <span class="s20">darts</span>, and use machine learning and deep learning for forecasting, using popular Python libraries such as <span class="s20">scikit-learn </span>and <span class="s20">PyTorch</span>. We conclude the book with a few chapters that cover seldom-touched aspects, such as multi-step forecasting, forecast metrics and cross validation for time series.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The book will enable you to build real-world time series forecasting systems that scale to millions of time series by mastering and applying modern concepts in machine learning and deep learning.</p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Who this book is for</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The book is ideal for data scientists, data analysts, machine learning engineers and Python developers who want to build industry-ready time series models. Since the book explains most concepts from the ground up, basic proficiency in Python is all you need. A prior understanding of machine learning or forecasting would help speed up the learning. For seasoned practitioners in machine learning and forecasting, the book has a lot to offer in terms of advanced techniques and traversing the latest research frontiers in time series forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">What this book covers</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><i>Chapter 1</i>, <i>Introducing Time Series</i>, is all about introducing you to the world of time series. We lay down a definition of time series and talk about how it is related to a <span class="s5">Data Generating Process </span>(<span class="s5">DGP</span>). We will also talk about the limits of forecasting and talk about what we cannot forecast, and then we finish off the chapter by laying down some terminology that will help you understand the rest of the book.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><i>Chapter 2</i>, <i>Acquiring and Processing Time Series Data</i>, covers how you can process time series data. You will understand how different forms of time series data can be represented in a tabular form. You will learn different date-time-related functionalities in <span class="s20">pandas </span>and learn how to fill in missing data using techniques suited for time series. Finally, using a real-world dataset, you will go through a step-by-step journey in processing time series data using <span class="s20">pandas</span>.</p><p class="s4" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 3<span class="p">, </span>Analyzing and Visualizing Time Series Data<span class="p">, furthers your introduction to time series by learning how to visualize and analyze time series. You will learn different visualizations that are commonly used for time series data and then learn how to go one level deeper by decomposing time series into its components. To wrap it up, you will also look at ways to identify and treat outliers in time series data.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 4<span class="p">, </span>Setting a Strong Baseline Forecast<span class="p">, gets right to the topic of time series forecasting as we use tried and tested methods from econometrics, such as </span>ARIMA <span class="p">and </span>exponential smoothing<span class="p">, to generate strong baselines. These efficient forecasting methods will provide strong baselines so that we can go beyond these classical techniques and learn modern techniques, such as machine learning. You will also get an introduction to another key topic – assessing forecastability using techniques such as </span>spectral entropy <span class="p">and </span>coefficient of variation<span class="p">.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, starts our journey into using machine learning for forecasting. A short introduction to machine learning lays down the foundations of what is to come in the next chapters. You will also understand, conceptually, how we can cast a time series problem as a regression problem so that we can use machine learning for it. To close off the chapter, we tease you with the possibility of global forecasting models.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 6<span class="p">, </span>Feature Engineering for Time Series Forecasting<span class="p">, shifts gear into a more practical lesson. Using a real-world dataset, you will learn about different feature engineering techniques, such as </span>lag features<span class="p">, </span>rolling features<span class="p">, and </span>Fourier terms<span class="p">, which help us formulate a time series problem as a regression problem.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 7<span class="p">, </span>Target Transformations for Time Series Forecasting<span class="p">, continues the practice of exploring different target transformations to accommodate non-stationarity in time series. You will learn techniques such as the </span>augmented Dickey–Fuller test <span class="p">and </span>Mann–Kendall test <span class="p">to identify and treat non-stationarity.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 8<span class="p">, </span>Forecasting Time Series with Machine Learning Models<span class="p">, continues from where the last chapter left off to start training machine learning models on the dataset we have been working on. Using the standard code framework present in the book, you will train models such as </span>linear regression<span class="p">, </span>random forest<span class="p">, and </span>gradient-boosted decision trees <span class="p">on our dataset.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Chapter 9<span class="p">, </span>Ensembling and Stacking<span class="p">, takes a step back and explores how we can use multiple forecasts and combine them to create a better forecast. You will explore popular techniques such as </span>best fit<span class="p">, different versions of the </span>hill-climbing algorithm<span class="p">, </span>simulated annealing<span class="p">, and </span>stacking <span class="p">to combine the different forecasts we have generated to get a better one.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Chapter 10<span class="p">, </span>Global Forecasting Models<span class="p">, concludes your guided journey into machine learning-enabled forecasting to an exciting and new paradigm – global forecasting models. You will learn how to use global forecasting models and industry-proven techniques to improve their performance, which finally lets you develop scalable and efficient machine learning forecasting systems for thousands of time series.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Chapter 11<span class="p">, </span>Introduction to Deep Learning<span class="p">, we switch tracks and start with a specific type of machine learning – deep learning. In this chapter, we lay the foundations of deep learning by looking at different topics such as </span>representation learning<span class="p">, </span>linear transformations<span class="p">, </span>activation functions<span class="p">, and </span>gradient descent<span class="p">.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Chapter 12<span class="p">, </span>Building Blocks of Deep Learning for Time Series<span class="p">, continues the journey into deep learning by making it specific to time series. Keeping in mind the compositionality of deep learning systems, you will learn about different building blocks with which you can construct a deep learning architecture. The chapter starts off by establishing the </span>encoder-decoder architecture <span class="p">and then talks about different blocks such as </span>feed forward networks<span class="p">, </span>recurrent neural networks<span class="p">, and </span>convolutional neural networks<span class="p">.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Chapter 13<span class="p">, </span>Common Modeling Patterns for Time Series<span class="p">, strengthens the encoder-decoder architecture that you saw in the previous chapter by showing you a few concrete and common patterns in which you can arrange building blocks to generate forecasts. This is a hands-on chapter where you will be creating forecasts using deep learning-based </span>tabular regression <span class="p">and different </span>sequence-to-sequence models<span class="p">.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Chapter 14<span class="p">, </span>Attention and Transformers for Time Series<span class="p">, covers the contemporary topic of using attention to improve deep learning models. The chapter starts off by talking about a generalized attention model with which you will learn different types of attention schemes, such as </span>scaled dot product <span class="p">and </span>additive<span class="p">. You will also tweak the sequence-to-sequence models from the previous chapter to include attention and then train those models to generate a forecast. The chapter then talks about </span>transformer <span class="p">models, which is a deep learning architecture that relies solely on attention, and then you will use that to generate forecasts as well.</span></p><p class="s4" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Chapter 15<span class="p">, </span>Strategies for Global Deep Learning Forecasting Models<span class="p">, tackles yet another important aspect of deep learning-based forecasting. Although the book talked about global forecasting models earlier, there are some differences in how it is implemented for deep learning models. In this chapter, you will learn how to implement global deep learning models and techniques on how to make those models better. You will also see them working in the hands-on section, where we will be generating forecasts using the real-world dataset we have been working with.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 16<span class="p">, </span>Specialized Deep Learning Architectures for Forecasting<span class="p">, concludes your journey into deep learning-based time series forecasting by talking about a few popular, specialized deep learning architectures for time series forecasting. Using the concepts and building blocks you have learned through the previous chapters, this chapter takes you to the cutting edge of research and exposes the leading state-of-the-art models in time series forecasting such as </span>N-BEATS<span class="p">, </span>N-HiTS<span class="p">, </span>Informer<span class="p">, </span>Autoformer<span class="p">, and </span>Temporal Fusion Transformer<span class="p">. In addition to understanding them, you will also learn how to use these models to generate forecasts using a real-world dataset.</span></p><p class="s4" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 17<span class="p">, </span>Multi-Step Forecasting<span class="p">, tackles the rarely talked about but highly relevant topic of multi-step forecasting. You will learn about different strategies for generating forecasts for more than one time step into the future, such as </span>Recursive<span class="p">, </span>Direct<span class="p">, </span>DirRec<span class="p">, </span>RecJoint<span class="p">, and </span>Rectify<span class="p">. The book also talks about the merits and demerits of each of them and helps you choose the right strategy for your problem.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 18<span class="p">, </span>Evaluating Forecasts – Forecast Metrics<span class="p">, traverses yet another topic that is rarely talked about and rife with controversy, with many opinions from different quarters. You will learn about different ways to measure the goodness of a forecast and through experiments, which you can run, expose the strengths and weaknesses of different metrics. The chapter concludes by laying down some guidelines that can help you choose the correct metric for your problem.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Chapter 19<span class="p">, </span>Evaluating Forecasts – Validation Strategies<span class="p">, concludes the evaluation of forecasts and the book by talking about different validation strategies we can use for time series. You will learn different validation strategies such as hold-out, cross-validation, and their variations. The chapter also touches upon aspects to keep in mind while designing validation strategies for global settings as well. At the conclusion of the chapter, you will come across a few guidelines for choosing your validation strategies and answers to questions such as </span>can we use cross-validation for time series?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">To get the most out of this book</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">You should have basic familiarity with Python programming, as the entire code that we use for the practical sections is in <span class="s20">Python</span>. Familiarity with major libraries in Python, such as <span class="s20">pandas </span>and <span class="s20">scikit-learn</span>, are not essential (because the book covers some basics) but will help you get through the book much faster. Familiarity with <span class="s20">PyTorch</span>, the framework the book uses for deep learning, is also not essential but would accelerate your learning by many folds. Any of the software requirements shouldn’t stop you because, in today’s internet-enabled world, the only thing that is standing between you and a world of knowledge is the search bar in your favorite search engine.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another key aspect to get the most out of this book is to run the associated notebooks as you go along the lessons. Also, feel free to experiment with different variations that the book doesn’t go into. That is a surefire way to internalize what’s being talked about in the book. And for that, we need to set up an environment, as you’ll see in the following section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Setting up an environment</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The easiest way to set up an environment is by using Anaconda, a distribution of Python for scientific computing. You can use Miniconda, a minimal installer for Conda, as well if you do not want the pre-installed packages that come with Anaconda:</p><ol id="l1"><li><p class="s27" style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;"><span class="s5">Install Anaconda/Miniconda</span><a href="https://www.anaconda.com/products/distribution" class="s140" target="_blank">: Anaconda can be installed from </a><a href="https://www.anaconda.com/products/distribution" class="a" target="_blank">https://www.anaconda. </a>com/products/distribution<a href="https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links" class="s140" target="_blank">. Depending on your operating system, choose the corresponding file and follow the instructions. Alternatively, you can install Miniconda from here: </a><a href="https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links" class="a" target="_blank">https://docs.conda.io/en/latest/miniconda.html#latest- </a>miniconda-installer-links<span class="p">.</span></p></li><li><p class="s5" style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Open conda prompt<span class="p">: To open Anaconda Prompt (or Terminal on Linux or macOS), do the following:</span></p><ul id="l2"><li><p class="s5" style="padding-top: 8pt;padding-left: 62pt;text-indent: -11pt;text-align: left;">Windows<span class="p">: Open the Anaconda Prompt (</span>Start <span class="p">| </span>Anaconda Prompt<span class="p">)</span></p></li><li><p style="padding-top: 5pt;padding-left: 62pt;text-indent: -11pt;text-align: left;"><span class="s5">macOS</span>: Open Launchpad and then open Terminal. Type <span class="s20">conda activate</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 62pt;text-indent: -11pt;text-align: left;"><span class="s5">Linux</span>: Open Terminal. Type <span class="s20">conda activate</span>.</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;"><span class="s5">Navigate to the downloaded code</span>: Use operating system-specific commands to navigate to the folder where you have downloaded the code. For instance, in Windows, use <span class="s20">cd</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;"><span class="s5">Install the environment</span>: Using the <span class="s20">anaconda_env.yml </span>file that is included, install the environment:</p><p style="padding-top: 9pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><span class="s23" style=" background-color: #F3F2F1;">  conda env create -f anaconda_env.yml                       </span></p><p style="padding-top: 6pt;padding-left: 55pt;text-indent: 0pt;text-align: left;">This creates a new environment under the name <span class="s20">modern_ts </span>and will install all the required libraries in the environment. This can take a while.</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Checking the installation<span class="p">: We can check whether all the libraries required for the book are installed properly by executing a script in the downloaded code folder:</span></p><p style="padding-top: 9pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><span class="s23" style=" background-color: #F3F2F1;">  python test_installation.py                                </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 55pt;text-indent: -18pt;text-align: justify;"><span class="s5">Activating the environment and running notebooks</span>: Every time you want to run the notebooks, first activate the environment using the <span class="s20">conda activate modern_ts </span>command and then use the Jupyter Notebook (<span class="s20">jupyter notebook</span>) or JupyterLab (<span class="s20">jupyter lab</span>), according to your preference.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Download the data</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You are going to be using a single dataset throughout the book. The book uses the <i>London Smart Meters dataset </i><a href="https://www.kaggle.com/account/login?phase=startRegisterTab" class="s140" target="_blank">from Kaggle for this purpose. Therefore, if you don’t have an account with Kaggle, please go ahead and create one: </a><span class="s27">https://www.kaggle.com/account/login?phase=startRegisterTab</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are two ways you can download the data-automated and manual.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">For the automated way, we need to download a key from Kaggle. Let’s do that first (if you are going to choose the manual way, you can skip this):</p><ol id="l3"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Click on your profile picture in the top-right corner of Kaggle.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Select <span class="s5">Account</span>, and find the section for <span class="s5">API</span>.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Click the <span class="s5">Create New API Token </span>button. A file with the name <span class="s20">kaggle.json </span>will be downloaded.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Copy the file and place it in the <span class="s20">api_keys </span>folder in the downloaded code folder.</p></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have <span class="s20">kaggle.json </span>downloaded and placed in the right folder, let’s look at the two methods to download data:</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Method one – automated download</p><ol id="l4"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Activate the environment using <span class="s20">conda activate modern_ts</span>.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Run the provided script from the <span class="s20">root </span>directory of the downloaded code:</p><p style="padding-top: 9pt;padding-left: 64pt;text-indent: 0pt;text-align: left;"><span class="s23" style=" background-color: #F3F2F1;">  python scripts/download_data.py                            </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">That’s it. Now, just wait for the script to finish downloading, unzip it, and organize the files in the expected format.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Method two – manual download</p></li><li><p class="s27" style="padding-top: 9pt;padding-left: 64pt;text-indent: -18pt;text-align: left;"><a href="https://www.kaggle.com/jeanmidev/smart-meters-in-london" class="s140" target="_blank">Go to </a><a href="https://www.kaggle.com/jeanmidev/smart-meters-in-london" class="a" target="_blank">https://www.kaggle.com/jeanmidev/smart-meters-in-london</a> <span class="p">and download the dataset.</span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Unzip the contents to <span class="s20">data/london_smart_meters</span>.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Unzip <span class="s20">hhblock_dataset </span>to get the raw files we want to work with.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Make sure the unzipped files are in the expected folder structure (see the next section).</p></li></ol><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="146" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_028.png"/></span></p><p class="s25" style="text-indent: 0pt;text-align: right;">├── hhblock_dataset</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">├── block_0.csv</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">├── block_1.csv</p><p class="s25" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;line-height: 11pt;text-align: left;">├── ...</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;text-align: left;">│</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">│</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">│</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">│</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 11pt;text-align: left;">├── hhblock_dataset</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;text-align: left;">│</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">│</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">│</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">│</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">│</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;text-align: left;">data</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">├── london_smart_meters</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that you have downloaded the data, we need to make sure it is arranged in the following folder structure. The automated download does it automatically, but with the manual download, this structure needs to be created. To avoid ambiguity, the expected folder structure can be found as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="126" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_029.png"/></span></p><p class="s25" style="text-indent: 0pt;text-align: left;">│── acorn_details.csv</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">├── informations_households.csv</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">├── uk_bank_holidays.csv</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">├── weather_daily_darksky.csv</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">├── weather_hourly_darksky.csv</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 11pt;text-align: left;">├── block_109.csv</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 11pt;text-align: left;">│ │</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There can be additional files as part of the extraction process. You can remove them without impacting anything. There is a helpful script that checks this structure.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s26" style=" background-color: #F3F2F1;">  python test_data_download.py                                    </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 28pt;text-indent: 0pt;line-height: 111%;text-align: justify;">If you are using the digital version of this book, we advise you to type the code yourself or access the code from the book’s GitHub repository. Doing so will help you avoid any potential errors related to the copying and pasting of code.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The code that is provided along with the book is in no way a library but more of a guide for you to start experimenting on. The amount of learning you can derive from the book and code is directly proportional to how much you experiment with the code and stray outside your comfort zone. So, go ahead and start experimenting and putting the skills you pick up in the book to good use.</p><p class="s3" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Download the example code files</p><p class="s27" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python" class="s140" target="_blank">You can download the example code files for this book from GitHub at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python" class="a" target="_blank">https://github.com/ </a>PacktPublishing/Modern-Time-Series-Forecasting-with-Python<span class="p">. If there’s an update to the code, it will be updated in the GitHub repository.</span></p><p class="s27" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/" class="s140" target="_blank">We also have other code bundles from our rich catalog of books and videos available at </a><a href="https://github.com/PacktPublishing/" class="a" target="_blank">https:// </a>github.com/PacktPublishing/<span class="p">. Check them out!</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Download the color images</p><p class="s27" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://packt.link/5NVrW" class="s140" target="_blank">We also provide a PDF file that has color images of the screenshots and diagrams used in this book. You can download it here: </a>https://packt.link/5NVrW<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Conventions used</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are a number of text conventions used throughout this book.</p><p class="s20" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Code in text<span class="p">: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: “</span>statsmodels.tsa.seasonal <span class="p">has a function called </span>seasonal_decompose<span class="p">.”</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">A block of code is set as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Does not support missing values, so using imputed ts instead</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">res = seasonal_decompose(ts, period=7*48, model=&quot;additive&quot;, extrapolate_trend=&quot;freq&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Any command-line input or output is written as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s25" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">conda env create -f anaconda_env.yml</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_030.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Tips or important notes</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Appear like this.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Bold<span class="p">: Indicates a new term, an important word, or words that you see onscreen. For instance, words in menus or dialog boxes appear in </span>bold<span class="p">. Here is an example: “But if you look at the </span>Time Elapsed <span class="p">column, it stands out.”</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Get in touch</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Feedback from our readers is always welcome.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s5">General feedback</span>: If you have questions about any aspect of this book, email us at <span class="s20">customercare@ packtpub.com </span>and mention the book title in the subject of your message.</p><p class="s5" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Errata<a href="http://www.packtpub.com/support/errata" class="s140" target="_blank">: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit </a><a href="http://www.packtpub.com/support/errata" class="a" target="_blank">www.packtpub.com/support/errata</a><span class="s27"> </span><span class="p">and fill in the form.</span></p><p class="s5" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Piracy<a href="mailto:copyright@packt.com" class="s140" target="_blank">: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at </a><a href="mailto:copyright@packt.com" class="s30" target="_blank">copyright@packt.com</a><span class="s20"> </span><span class="p">with a link to the material.</span></p><p class="s5" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">If you are interested in becoming an author<a href="http://authors.packtpub.com/" class="s140" target="_blank">: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit </a><span class="s27">authors.packtpub.com.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Share Your Thoughts</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Once you’ve read <i>Modern Time Series Forecasting with Python</i><a href="https://packt.link/r/1-803-24680-4" class="s140" target="_blank">, we’d love to hear your thoughts! Please </a><a href="https://packt.link/r/1-803-24680-4" class="a" target="_blank">click here to go straight to the Amazon review page</a><span class="s27"> </span>for this book and share your feedback.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Download a free PDF copy of this book</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Thanks for purchasing this book!</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 147%;text-align: left;">Do you like to read on the go but are unable to carry your print books everywhere? Is your eBook purchase not compatible with the device of your choice?</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Follow these simple steps to get the benefits:</p><ol id="l5"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Scan the QR code or visit the link below</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 183pt;text-indent: 0pt;text-align: left;"><span><img width="138" height="138" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_031.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s27" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">https://packt.link/free-ebook/9781803246802</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 64pt;text-indent: -18pt;text-align: left;">Submit your proof of purchase</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">That’s it! We’ll send your free PDF and other benefits to your email directly</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s31" style="padding-top: 13pt;padding-left: 159pt;text-indent: 171pt;line-height: 114%;text-align: right;"><a name="bookmark8">Part 1 – Getting Familiar with</a><a name="bookmark9">&zwnj;</a></p><p class="s31" style="text-indent: 0pt;line-height: 30pt;text-align: right;">Time Series</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s32" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We dip our toes into time series forecasting by understanding what a time series is, how to process and manipulate time series data, and how to analyze and visualize time series data. This part also covers classical time series forecasting methods, such as ARIMA, to serve as strong baselines.</p><p class="s32" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This part comprises the following chapters:</p><ul id="l6"><li><p class="s34" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 1<span class="s32">, </span>Introducing Time Series</p></li><li><p class="s34" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 2<span class="s32">, </span>Acquiring and Processing Time Series Data</p></li><li><p class="s34" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 3<span class="s32">, </span>Analyzing and Visualizing Time Series Data</p></li><li><p class="s34" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 4<span class="s32">, </span>Setting a Strong Baseline Forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark10">1</a><a name="bookmark11">&zwnj;</a><a name="bookmark13">&zwnj;</a><a name="bookmark12">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 128pt;text-indent: 0pt;text-align: left;">Introducing Time Series</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Welcome to <i>Advanced Time Series Analysis Using Python</i>! This book is intended for data scientists or <span class="s5">machine learning </span>(<span class="s5">ML</span>) engineers who want to level up their time series analysis skills by learning new and advanced techniques from the ML world. <span class="s5">Time series analysis </span>is something that is commonly overlooked in regular ML books, courses, and so on. They typically start with classification, touch upon regression, and then move on. But it is also something that is immensely valuable and ubiquitous in business. As long as time is one of the four dimensions in the world we live in, time series data is all-pervasive.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Analyzing time series data unlocks a lot of value for a business. Time series analysis isn&#39;t new—it&#39;s been around since the 1920s and 1930s. But in the current age of data, the time series that are collected by businesses are growing larger and wider by the minute. Combined with an explosion in the quantum of data collected and the renewed interest in ML, the landscape of time series analysis also changed considerably. This book attempts to take you beyond classical statistical methods such as <span class="s5">AutoRegressive Integrated Moving Average </span>(<span class="s5">ARIMA</span>) and introduce to you the latest techniques from the ML world in time series analysis.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We are going to start with the basics and quickly scale up to more complex topics. In this chapter, we&#39;re going to cover the following main topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">What is a time series?</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Data-generating process (DGP)</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">What can we forecast?</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Forecasting terminology and notation</p><p class="s3" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the <span class="s5">Anaconda </span>environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s27" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter01" class="s140" target="_blank" name="bookmark24">The associated code for the chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter01" class="a" target="_blank">https://github.com/PacktPublish- ing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter01<span class="p">.</span><a name="bookmark16">&zwnj;</a><a name="bookmark15">&zwnj;</a><a name="bookmark14">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">What is a time series?</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">To keep it simple, a <span class="s5">time series </span>is a set of observations taken sequentially in time. The focus is on the word <i>time</i>. If we keep taking the same observation at different points in time, we will get a time series. For example, if you keep recording the number of bars of chocolate you have in a month, you&#39;ll end up with a time series of your chocolate consumption. Suppose you are recording your weight at the beginning of every month. You get another time series of your weight. Is there any relation between the two time series? Most likely, yeah. But we can analyze that scientifically by the end of this book.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">A few other examples of time series are the weekly closing price of a stock that you follow, daily rainfall or snow in your city, or hourly readings of your heartbeat from your smartwatch.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Types of time series</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are two types of time series data, as outlined here:</p><ul id="l7"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Regular time series<span class="p">: This is the most common type of time series where we have observations coming in at regular intervals of time, such as every hour or every month.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="106" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_032.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">This book only focuses on regular time series, which are evenly spaced in time. Irregular time series are slightly more advanced and require specialized techniques to handle them. A couple of survey papers on the topic is a good way to get started on irregular time series and you can find them in the <i>Further reading </i>section of this chapter.</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Irregular time series<span class="p">: There are a few time series where we do not have observations at a regular interval of time. For example, consider we have a sequence of readings from lab tests of a patient. We see an observation in the time series only when the patient heads to the clinic and carries out the lab test, and this may not happen in regular intervals of time.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Main areas of application for time series analysis</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">There are broadly three important areas of application for time series analysis, outlined as follows:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Time series forecasting<span class="p">: Predicting the future values of a time series, given the past values—for example, predict the next day&#39;s temperature using the last 5 years of temperature data.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a name="bookmark25">Time series classification</a><span class="p">: Sometimes, instead of predicting the future value of the time series, we may also want to predict an action based on past values. For example, given a history of an</span><span class="s35"> </span>electroencephalogram <span class="p">(</span>EEG<span class="p">; tracking electrical activity in the brain) or an </span>electrocardiogram <span class="p">(</span>EKG<span class="p">; tracking electrical activity in the heart), we need to predict whether the result of an EEG or an EKG is normal or abnormal.</span><a name="bookmark17">&zwnj;</a></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Interpretation and causality<span class="p">: Understand the whats and whys of the time series based on the past values, understand the interrelationships among several related time series, or derive causal inference based on time series data.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="106" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_033.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The focus of this book is predominantly on <i>time series forecasting</i>, but the techniques that you learn will help you approach <i>time series classification </i>problems also, with minimal change in the approach. <i>Interpretation </i>is also addressed, although only briefly, but <i>causality </i>is an area that this book does not address because it warrants a whole different approach.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now that we have an overview of the time series landscape, let&#39;s build a mental model on how time series data is generated.</p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Data-generating process (DGP)</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We saw that time series data is a collection of observations made sequentially along the time dimension. Any time series is, in turn, generated by some kind of <i>mechanism</i>. For example, time series data of daily shipments of your favorite chocolate from the manufacturing plant is affected by a lot of factors such as the time of the year, the holiday season, the availability of cocoa, the uptime of the machines working on the plant, and so on. In statistics, this underlying process that generated the time series is referred to as the <span class="s5">DGP</span>. Often, time series data is generated by a stochastic process.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">If we had complete and perfect knowledge of reality, all we must do is put this DGP together in a mathematical form and you will get the most accurate forecast possible. But sadly, nobody has complete and perfect knowledge of reality. So, what we try to do is approximate the DGP, mathematically, as much as possible so that our imitation of the DGP gives us the best possible forecast (or any other output we want from the analysis). This imitation is called a <span class="s5">model </span>that provides a useful approximation to the DGP.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark26">But we must remember that the model is not the DGP, but a representation of some essential aspects of reality. For example, let&#39;s consider an aerial view of Bengaluru and a map of Bengaluru, as represented here:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><span><img width="531" height="156" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_034.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 1.1 – An aerial view of Bengaluru (left) and a map of Bengaluru (right)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The map of Bengaluru is certainly useful—we can use it to go from point A to point B. But a map of Bengaluru is not the same as a photo of Bengaluru. It doesn&#39;t showcase the bustling nightlife or the insufferable traffic. A map is just a model that represents some useful features of a location, such as roads and places. The following diagram might help us internalize the concept and remember it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 81pt;text-indent: 0pt;text-align: left;"><span><img width="412" height="246" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_035.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 1.2 – DGP, model, and time series</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Naturally, the next question would be this: <i>Do we have a useful model? </i>Every model is unrealistic. As we saw already, a map of Bengaluru does not perfectly represent Bengaluru. But if our purpose is to navigate Bengaluru, then a map is a very useful model. What if we want to understand the culture? A map doesn&#39;t give you a flavor of that. So, now, the same model that was useful is utterly useless in the new context.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark27">Different kinds of models are required in different situations and for different objectives. For example, the best model for forecasting may not be the same as the best model to make a causal inference.</a><a name="bookmark18">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can use the concept of DGPs to generate multiple synthetic time series, of varying degrees of complexity.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Generating synthetic time series</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let&#39;s take a look at a few practical examples where we can generate a few time series using a set of fundamental building blocks. You can get creative and mix and match any of these components, or even add them together to generate a time series of arbitrary complexity.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">White and red noise</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">An extreme case of a stochastic process that generates a time series is a <span class="s5">white noise </span>process. It has a sequence of random numbers with zero mean and constant standard deviation. This is also one of the most popular assumptions of noise in a time series.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let&#39;s see how we can generate such a time series and plot it, as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Generate the time axis with sequential numbers upto 200</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">time = np.arange(200)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Sample 200 hundred random values</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">values = np.random.randn(200)*100 plot_time_series(time, values, &quot;White Noise&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Here is the output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="497" height="248" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_036.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 1.3 – White noise process</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark28"><span class="s5">Red noise</span></a>, on the other hand, has zero mean and constant variance but is serially correlated in time. This serial correlation or redness is parameterized by a correlation coefficient <i>r</i>, such that:</p><p class="s39" style="padding-top: 5pt;padding-left: 101pt;text-indent: 0pt;line-height: 8pt;text-align: center;"> <span class="s40">1</span></p><p class="s41" style="padding-left: 12pt;text-indent: 0pt;line-height: 14pt;text-align: center;">𝑥𝑥<span class="s42">𝑗</span><span class="s43">𝑗+1  </span>= r ⋅ 𝑥𝑥<span class="s42">𝑗𝑗</span><span class="s43">  </span>+ (1 − 𝑟𝑟2)<span class="s44">2</span><span class="s43">  </span>⋅ 𝑤𝑤</p><p style="padding-top: 8pt;padding-bottom: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 147%;text-align: left;">where <i>w </i>is a random sample from a white noise distribution. Let&#39;s see how we can generate that, as follows:</p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Setting the correlation coefficient</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">r = 0.4</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Generate the time axis</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">time = np.arange(200)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Generate white noise</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">white_noise = np.random.randn(200)*100</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Create Red Noise by introducing correlation between subsequent values in the white noise</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">values = np.zeros(200)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">for i, v in enumerate(white_noise): if i==0:</p><p class="s28" style="padding-left: 33pt;text-indent: 24pt;line-height: 131%;text-align: left;">values[i] = v else:</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 11pt;text-align: left;">values[i] = r*values[i-1]+ np.sqrt((1-np.power(r,2)))</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">*v</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">plot_time_series(time, values, &quot;Red Noise Process&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here is the output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;text-align: left;"><a name="bookmark29"><span><img width="513" height="256" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_037.gif"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 1.4 – Red noise process</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Cyclical or seasonal signals</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Among the most common signals you see in time series are seasonal or cyclical signals. Therefore, you can introduce seasonality into your generated series in a few ways.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let&#39;s take the help of a very useful library to generate the rest of the time series—<span class="s20">TimeSynth</span><a href="https://github.com/TimeSynth/TimeSynth" class="s140" target="_blank">. For more information, refer to </a><span class="s27">https://github.com/TimeSynth/TimeSynth</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">This is a very useful library to generate time series. It has all kinds of DGPs that you can mix and match and create authentic synthetic time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_038.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Important note</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">For the exact code and usage, please refer to the associated Jupyter notebooks.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let&#39;s see how we can use a sinusoidal function to create cyclicity. There is a helpful function in <span class="s20">TimeSynth </span>called <span class="s20">generate_timeseries </span>that helps us combine signals and generate time series. Have a look at the following code snippet:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">#Sinusoidal Signal with Amplitude=1.5 &amp; Frequency=0.25 <span class="s28">signal_1 =ts.signals.Sinusoidal(amplitude=1.5, frequency=0.25) </span>#Sinusoidal Signal with Amplitude=1 &amp; Frequency=0. 5</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">signal_2 = ts.signals.Sinusoidal(amplitude=1, frequency=0.5)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Generating the time series</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">samples_1, regular_time_samples, signals_1, errors_1 = generate_timeseries(signal=signal_1)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">samples_2, regular_time_samples, signals_2, errors_2 = generate_timeseries(signal=signal_2)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">plot_time_series(regular_time_samples,</p><p class="s28" style="padding-top: 3pt;padding-left: 111pt;text-indent: 0pt;line-height: 131%;text-align: left;">[samples_1, samples_2], &quot;Sinusoidal Waves&quot;,</p><p class="s28" style="padding-left: 9pt;text-indent: 102pt;line-height: 106%;text-align: left;">legends=[&quot;Amplitude = 1.5 | Frequency = 0.25&quot;, &quot;Amplitude = 1 | Frequency = 0.5&quot;])</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here is the output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><span><img width="525" height="245" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_039.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 40pt;text-indent: 0pt;text-align: center;">Figure 1.5 – Sinusoidal waves</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Note the two sinusoidal waves are different with respect to the frequency (how fast the time series crosses zero) and amplitude (how far away from zero the time series travels).</p><p class="s20" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">TimeSynth <span class="p">also has another signal called </span>PseudoPeriodic<span class="p">. This is like the </span>Sinusoidal <span class="p">class, but the frequency and amplitude itself has some stochasticity. We can see in the following code snippet that this is more realistic than the vanilla sine and cosine waves from the </span>Sinusoidal <span class="p">class:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;"># PseudoPeriodic signal with Amplitude=1 &amp; Frequency=0.25 <span class="s28">signal = ts.signals.PseudoPeriodic(amplitude=1, frequency=0.25) </span>#Generating Timeseries</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">samples, regular_time_samples, signals, errors = generate_ timeseries(signal=signal)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">plot_time_series(regular_time_samples,</p><p class="s28" style="padding-top: 3pt;padding-left: 111pt;text-indent: 0pt;text-align: left;">samples,</p><p class="s28" style="padding-top: 3pt;padding-left: 111pt;text-indent: 0pt;text-align: left;">&quot;Pseudo Periodic&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark30"/></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here is the output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><span><img width="515" height="256" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_040.gif"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 1.6 – Pseudo-periodic signal</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Autoregressive signals</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Another very popular signal in the real world is an <span class="s5">autoregressive (AR) signal</span><a href="#bookmark137" class="s140">. We will go into this in more detail in </a><i>Chapter 4</i>, <i>Setting a Strong Baseline Forecast</i>, but for now, an AR signal refers to when the value of a time series for the current timestep is dependent on the values of the time series in the previous timesteps. This serial correlation is a key property of the AR signal, and it is parametrized by a few parameters, outlined as follows:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Order of serial correlation—or, in other words, the number of previous timesteps the signal is dependent on</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Coefficients to combine the previous timesteps</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let&#39;s see how we can generate an AR signal and see what it looks like, as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;"># Autoregressive signal with parameters 1.5 and -0.75 # y(t) = 1.5*y(t-1) - 0.75*y(t-2)</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">signal=ts.signals.AutoRegressive(ar_param=[1.5, -0.75])</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Generate Timeseries</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">samples, regular_time_samples, signals, errors = generate_ timeseries(signal=signal)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">plot_time_series(regular_time_samples,</p><p class="s28" style="padding-top: 3pt;padding-left: 111pt;text-indent: 0pt;text-align: left;">samples,</p><p class="s28" style="padding-top: 3pt;padding-left: 111pt;text-indent: 0pt;text-align: left;">&quot;Auto Regressive&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark31"/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here is the output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="509" height="256" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_041.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 173pt;text-indent: 0pt;text-align: center;">Figure 1.7 – AR signal</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Mix and match</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are many more components you can use to create your DGP and thereby generate a time series, but let&#39;s quickly look at how we can combine the components we have already seen to generate a realistic time series.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let&#39;s use a pseudo-periodic signal with white noise and combine it with an AR signal, as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Generating Pseudo Periodic Signal</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">pseudo_samples, regular_time_samples, _, _ = generate_ timeseries(signal=ts.signals.PseudoPeriodic(amplitude=1, frequency=0.25), noise=ts.noise.GaussianNoise(std=0.3))</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Generating an Autoregressive Signal</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ar_samples, regular_time_samples, _, _ = generate_ timeseries(signal=ts.signals.AutoRegressive(ar_param=[1.5,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-0.75]))</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;"># Combining the two signals using a mathematical equation <span class="s28">ts = pseudo_samples*2+ar_samples plot_time_series(regular_time_samples,</span></p><p class="s28" style="padding-left: 111pt;text-indent: 0pt;line-height: 11pt;text-align: left;">ts,</p><p class="s28" style="padding-top: 3pt;padding-left: 111pt;text-indent: 0pt;text-align: left;">&quot;Pseudo Periodic with AutoRegression and White</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">Noise&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark32"/><a name="bookmark19">&zwnj;</a></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here is the output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="526" height="265" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_042.gif"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 1.8 – Pseudo-periodic signal with AR and white noise</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Stationary and non-stationary time series</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In time series, <span class="s5">stationarity </span>is of great significance and is a key assumption in many modeling approaches. Ironically, many (if not most) real-world time series are non-stationary. So, let&#39;s understand what a stationary time series is from a layman&#39;s point of view.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are multiple ways to look at stationarity, but one of the clearest and most intuitive ways is to think of the probability distribution or the data distribution of a time series. We call a time series stationary when the probability distribution remains the same at every point in time. In other words, if you pick different windows in time, the data distribution across all those windows should be the same.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark33">A standard Gaussian distribution is defined by two parameters—the mean and the standard deviation. So, there are two ways the stationarity assumption can be broken, as outlined here:</a></p><ul id="l8"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Change in mean over time</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Change in variance over time</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let&#39;s look at these assumptions in detail and understand them better.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Change in mean over time</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is the most popular way a non-stationary time series presents itself. If there is an upward/downward trend in the time series, the mean across two windows of time would not be the same.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another way non-stationarity manifests itself is in the form of seasonality. Suppose we are looking at the time series of average temperature measurements in a month for the last 5 years. From our experience, we know that temperature peaks during summer and falls in winter. So, when we take the mean temperature of winter and mean temperature of summer, they will be different.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let&#39;s generate a time series with trend and seasonality and see how it manifests, as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;"># Sinusoidal Signal with Amplitude=1 &amp; Frequency=0.25 <span class="s28">signal=ts.signals.Sinusoidal(amplitude=1, frequency=0.25) </span># White Noise with standard deviation = 0.3 <span class="s28">noise=ts.noise.GaussianNoise(std=0.3)</span></p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># Generate the time series</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">sinusoidal_samples, regular_time_samples, _, _ = generate_ timeseries(signal=signal, noise=noise)</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Regular_time_samples is a linear increasing time axis and can be used as a trend</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: justify;">trend = regular_time_samples*0.4 <span class="s38"># Combining the signal and trend </span>ts = sinusoidal_samples+trend</p><p class="s28" style="padding-left: 111pt;text-indent: -102pt;line-height: 131%;text-align: left;">plot_time_series(regular_time_samples, ts,</p><p class="s28" style="padding-left: 111pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&quot;Sinusoidal with Trend and White Noise&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark34">Here is the output:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="265" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_043.gif"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 1.9 – Sinusoidal signal with trend and white noise</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">If you examine the time series in <i>Figure 1.9</i>, you will be able to see a definite trend and the seasonality, which together make the mean of the data distribution change wildly across different windows of time.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Change in variance over time</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Non-stationarity can also present itself in the fluctuating variance of a time series. If the time series starts off with low variance and as time progresses, the variance keeps getting bigger and bigger, we have a non-stationary time series. In statistics, there is a scary name for this phenomenon—<span class="s5">heteroscedasticity</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This book just tries to give you intuition about stationary versus non-stationary time series. There is a lot of statistical theory and depth in this discussion that we are skipping over to keep our focus on the practical aspects of time series.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Armed with the mental model of the DGP, we are at the right place to think about another important question: <i>What can we forecast?</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark35">What can we forecast?</a><a name="bookmark20">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Before we move ahead, there is another aspect of time series forecasting that we have to understand— <i>the predictability of a time series</i>. The most basic assumption when we forecast a time series is that the future depends on the past. But not all time series are equally predictable.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let&#39;s take a look at a few examples and try to rank these in order of predictability (from easiest to hardest), as follows:</p></li><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">High tide next Monday</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Lottery numbers next Sunday</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">The stock price of Tesla next Friday</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Intuitively, it is very easy for us to rank them. High tide next Monday is going to be the easiest to predict because it is so predictable, the lottery numbers are going to be very hard to predict because these are pretty much random, and the stock price of Tesla next Friday is going to be difficult to predict, but not impossible.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="224" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_044.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">However, for people thinking that they can forecast stock prices with the awesome techniques covered in the book and get rich, that won&#39;t happen. Although it is worthy of a lengthy discussion, we can summarize the key points in a short paragraph.</p><p style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Share prices are not a function of their past values but an anticipation of their future values, and this thereby violates our first assumption while forecasting. And if that is not bad enough, financial stock prices typically have a very low signal-to-noise ratio. The final wrench in the process is the <span class="s5">efficient-market hypothesis </span>(<span class="s5">EMH</span>). This seemingly innocent hypothesis proclaims that all known information about a stock price is already factored into the price of the stock. The implication of the hypothesis is that if you can forecast accurately, many others will also be able to do that, and thereby the market price of the stock already reflects the change in price that this forecast brought about.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Coming back to the topic at hand—predictability—three main factors form a mental model for this, as follows:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Understanding the DGP<span class="p">: The better you understand the DGP, the higher the predictability of a time series.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Amount of data<span class="p">: The more data you have, the better your predictability is.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Adequately repeating pattern<span class="p">: For any mathematical model to work well, there should be an adequately repeating pattern in your time series. The more repeatable the pattern is, the better your predictability is.</span></p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark36">Forecasting terminology 17</a><a name="bookmark21">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_045.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark87" class="s140">Even though you have a mental model of how to think about predictability, we will look at more concrete ways of assessing the predictability of time series in </a>Chapter 3<span class="p">, </span>Analyzing and Visualizing Time Series Data<span class="p">, but the key takeaway is that not all time series are equally predictable.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In order to fully follow the discussion in the coming chapters, we need to establish a standard notation and get updated on terminology that is specific to time series analysis.</p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Forecasting terminology</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are a few terminologies that will help you follow the book as well as other literature on time series. These are described in more detail here:</p></li></ul></li><li><p class="s5" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Forecasting</p><p style="padding-top: 5pt;padding-left: 55pt;text-indent: 0pt;text-align: justify;">Forecasting is the prediction of future values of a time series using the known past values of the time series and/or some other related variables. This is very similar to prediction in ML where we use a model to predict unseen data.</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Multivariate forecasting</p><p style="padding-top: 5pt;padding-left: 55pt;text-indent: 0pt;text-align: justify;">Multivariate time series consist of more than one time series variable that is not only dependent on its past values but also has some dependency on the other variables. For example, a set of macroeconomic indicators such as <span class="s5">gross domestic product </span>(<span class="s5">GDP</span>), inflation, and so on of a particular country can be considered as a multivariate time series. The aim of multivariate forecasting is to come up with a model that captures the interrelationship between the different variables along with its relationship with its past and forecast all the time series together in the future.</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Explanatory forecasting</p><p style="padding-top: 5pt;padding-left: 55pt;text-indent: 0pt;text-align: justify;">In addition to the past values of a time series, we might use some other information to predict the future values of a time series. For example, for predicting retail store sales, information regarding promotional offers (both historical and future ones) is usually helpful. This type of forecasting, which uses information other than its own history, is called explanatory forecasting.</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Backtesting</p><p style="padding-top: 5pt;padding-left: 55pt;text-indent: 0pt;text-align: justify;">Setting aside a validation set from your training data to evaluate your models is a practice that is common in the ML world. Backtesting is the time series equivalent of validation, whereby you use the history to evaluate a trained model. We will cover the different ways of doing validation and cross-validation for time series data later.</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">In-sample and out-sample</p><p style="padding-top: 5pt;padding-left: 55pt;text-indent: 0pt;text-align: justify;">Again, drawing parallels with ML, in-sample refers to training data and out-sample refers to unseen or testing data. When you hear in-sample metrics, this is referring to metrics calculated on training data, and out-sample metrics is referring to metrics calculated on testing data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l9"><li><p class="s5" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;"><a name="bookmark37">Exogenous and endogenous variables</a><a name="bookmark23">&zwnj;</a><a name="bookmark22">&zwnj;</a></p><p style="padding-top: 5pt;padding-left: 64pt;text-indent: 0pt;text-align: justify;">Exogenous variables are parallel time series variables that are not modeled directly for output but used to help us model the time series that we are interested in. Typically, exogenous variables are not affected by other variables in the system. Endogenous variables are variables that are affected by other variables in the system. A purely endogenous variable is a variable that is entirely dependent on the other variables in the system. Relaxing the strict assumptions a bit, we can consider the target variable as the endogenous variable and the explanatory regressors we include in the model as exogenous variables.</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Forecast combination</p><p style="padding-top: 5pt;padding-left: 64pt;text-indent: 0pt;text-align: justify;">Forecast combinations in the time series world are similar to ensembling from the ML world. It is a process by which we combine multiple forecasts by using some function, either learned or heuristic-based, such as a simple average of three forecast models.</p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are a lot more terms specific to time series, some of which we will be covering throughout the book. But to start with a basic familiarity in the field, these terms should be enough.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We had our first dip into time series as we understood the different types of time series, looked at how a DGP generates a time series, and saw how we can think about the important question: <i>How well can we forecast a time series? </i>We also had a quick review of the terminology and notation required to understand the rest of the book. In the next chapter, we will be getting our hands dirty and will learn how to work with time series data, how to preprocess a time series, how to handle missing data and outliers, and so on. If you have not set up the environment yet, take a break and put some time into doing that.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Further reading</p></li><li><p class="s4" style="padding-top: 7pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series: From Discretization to Attention and Invariance <a href="https://arxiv.org/abs/2012.00168" class="s140" target="_blank">by S.N. Shukla and B.M. Marlin (2020): </a><a href="https://arxiv.org/abs/2012.00168" class="a" target="_blank">https:// </a><a href="https://arxiv.org/abs/2012.00168" target="_blank">arxiv.org/abs/2012.00168</a></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Learning from Irregularly-Sampled Time Series: A Missing Data Perspective <a href="https://arxiv.org/abs/2008.07599" class="s140" target="_blank">by S.C. Li and B.M. Marlin (2020), ICML: </a><a href="https://arxiv.org/abs/2008.07599" target="_blank">https://arxiv.org/abs/2008.07599</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark38">2</a><a name="bookmark39">&zwnj;</a><a name="bookmark41">&zwnj;</a><a name="bookmark40">&zwnj;</a></h2><h4 style="padding-top: 2pt;text-indent: 0pt;text-align: right;">Acquiring and Processing </h4><h4 style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Time Series Data </h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous chapter, we learned what a time series is and established a few standard notations and terminologies. Now, let’s switch tracks from theory to practice. In this chapter, we are going to get our hands dirty and start working with data. Although we said time series data is everywhere, we are still yet to get our hands dirty with a few time series datasets. We are going to start working on the dataset we have chosen to work on throughout this book, process it in the right way, and learn about a few techniques for dealing with missing values.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will cover the following topics:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Understanding the time series dataset</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">pandas datetime operations, indexing, and slicing – a refresher</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Handling missing data</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Mapping additional information</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Saving and loading files to disk</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Handling longer periods of missing data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p class="s46" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter02" class="s140" target="_blank">The code for this chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter02" class="s45" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter02<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark62">Handling time series data is like handling other tabular datasets, but with a focus on the temporal dimension. As with any tabular dataset, pandas is perfectly equipped to handle time series data as well.</a><a name="bookmark42">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s start getting our hands dirty and work through a dataset from the beginning. We are going to use the <i>London Smart Meters </i>dataset throughout this book. If you have not downloaded the data already as part of the environment setup, go to the <i>Preface </i>and do that now.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Understanding the time series dataset</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is the key first step in any new dataset you come across, even before <span class="s5">Exploratory Data Analysis </span>(<span class="s5">EDA</span><a href="#bookmark87" class="s140">), which we will be covering in </a><i>Chapter 3</i>, <i>Analyzing and Visualizing Time Series Data</i>. Understanding where the data is coming from, the data generating process behind it, and the source domain is essential to having a good understanding of the dataset.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">London Data Store, a free and open data-sharing portal, provided this dataset, which was collected and enriched by Jean-Michel D and uploaded on Kaggle.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The dataset contains energy consumption readings for a sample of 5,567 London households that took part in the UK Power Networks-led Low Carbon London project between November 2011 and February 2014. Readings were taken at half-hourly intervals. Some metadata about the households is also available as part of the dataset. Let’s look at what metadata is available as part of the dataset:</p><ul id="l10"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">CACI UK segmented the UK’s population into demographic types, called Acorn. For each household in the data, we have the corresponding Acorn classification. The Acorn classes (Lavish Lifestyles, City Sophisticates, Student Life, and so on) are grouped into parent classes (Affluent Achievers, Rising Prosperity, Financially Stretched, and so on). A full list of Acorn classes can be found in <i>Table 2.1</i><a href="https://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf" class="s140" target="_blank">. The complete documentation detailing each class is available at </a><span class="s46">https://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf</span>.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">The dataset contains two groups of customers – one group who was subjected to <span class="s5">dynamic time-of-use </span>(<span class="s5">dToU</span>) energy prices throughout 2013, and another group who were on flat-rate tariffs. The tariff prices for the dToU were given a day ahead via the Smart Meter IHD or via text message.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Jean-Michel D also enriched the dataset with weather and UK bank holidays data.</p><p class="s47" style="padding-top: 4pt;padding-left: 265pt;text-indent: 0pt;text-align: left;"><a name="bookmark63">Understanding the time series dataset 21</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_046.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">The following table shows the Acorn classes:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 79pt;text-indent: 0pt;text-align: left;"><span><img width="388" height="539" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_047.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="118" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_048.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 90%;text-align: justify;">The Kaggle dataset also preprocesses the time series data daily and combines all the separat files. Here, we will ignore those files and start with the raw files, which can be found in th <span class="s48">hhblock_dataset </span>folder. Learning to work with the raw files is an integral part of workin with real-world datasets in the industry.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 91%;text-align: justify;">e e g</p><p style="text-indent: 0pt;text-align: left;"/><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Table 2.1 – ACORN classification</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark64">Preparing a data model</a><a name="bookmark43">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Once we understand where the data is coming from, we can look at the data, understand the information present in the different files, and figure out a mental model of how to relate the different files. You may call it old school, but Microsoft Excel is an excellent tool for gaining this first-level understanding. If the file is too big to open in Excel, we can also read it in Python and save a sample of the data to an Excel file and open it. However, keep in mind that Excel sometimes messes with the format of the data, especially dates, so we need to take care to not save the file and write back the formatting changes Excel made. If you are allergic to Excel, you can do it in Python as well, albeit with a lot more keystrokes. The purpose of this exercise is to <i>see </i>what the different data files contain, explore the relationship between the different files, and so on. We can make this more formal and explicit by drawing a data model, similar to the one shown in the following diagram:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 47pt;text-indent: 0pt;text-align: left;"><span><img width="496" height="251" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_049.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.1 – Data model of the London Smart Meters dataset</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The data model is more for us to understand the data rather than any data engineering purpose. Therefore, it only contains bare-minimum information, such as the key columns on the left and the sample data on the right. We also have arrows connecting different files, with keys used to link the files.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s look at a few key column names and their meanings:</p></li><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">LCLid<span class="p">: The unique consumer ID for a household</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">stdorTou<span class="p">: Whether the household has dToU or standard</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Acorn<span class="p">: The ACORN class</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Acorn_grouped<span class="p">: The ACORN group</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">file<span class="p">: The block number</span></p></li></ul></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="88" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_050.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 92%;text-align: left;">To follow along with the complete code, use the <span class="s48">01 - Pandas Refresher &amp; Missing Values Treatment.ipynb </span>notebook in the <span class="s48">chapter01 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: left;"><a name="bookmark65">Each </a><span class="s48">LCLid </span>has a unique time series attached to it. The time series file is formatted in a slightly tricky format – each day, there will be 48 observations at a half-hourly frequency in the columns of the file.<a name="bookmark44">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_051.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">If you are familiar with the datetime manipulations in pandas, feel free to skip ahead to the next section.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Before we start working with our dataset, there are a few concepts we need to establish. One of them is a concept in pandas DataFrames, which is of utmost importance – the pandas datetime properties and index. Let’s quickly look at a few pandas concepts that will be useful.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">pandas datetime operations, indexing, and slicing</p><ul id="l11"><li><p class="s3" style="padding-top: 3pt;padding-left: 39pt;text-indent: -11pt;text-align: left;">a refresher</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Instead of using our dataset, which is slightly complex, let’s pick an easy, well-formatted stock exchange price dataset from the UCI Machine Learning Repository and look at the functionality of pandas:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">df = pd.read_excel(&quot;https://archive.ics.uci.edu/ml/machine- learning-databases/00247/data_akbilgic.xlsx&quot;, skiprows=1)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The DataFrame that we read looks as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 33pt;text-indent: 0pt;text-align: left;"><span><img width="517" height="114" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_052.gif"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.2 – The DataFrame with stock exchange prices</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Now that we have read the DataFrame, let’s start manipulating it.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark66">Converting the date columns into pd.Timestamp/DatetimeIndex</a><a name="bookmark45">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">First, we must convert the date column (which may not always be parsed as dates automatically by pandas) into pandas datetime. For that, pandas has a handy function called <span class="s48">pd.to_datetime</span>. It infers the datetime format automatically and converts the input into a <span class="s48">pd.Timestamp</span>, if the input is a <span class="s48">string</span>, or into a <span class="s48">DatetimeIndex </span>if the input is a <span class="s48">list </span>of strings. So, if we pass a single date as a string, <span class="s48">pd.to_datetime </span>converts it into <span class="s48">pd.Timestamp</span>, while if we pass a list of dates, it converts it into <span class="s48">DatetimeIndex</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">&gt;&gt;&gt; pd.to_datetime(&quot;13-4-1987&quot;).strftime(&quot;%d, %B %Y&quot;) &#39;13, April 1987&#39;</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, let’s look at a case where the automatic parsing fails. The date is January 4, 1987. Let’s see what happens when we pass the string to the function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">&gt;&gt;&gt; pd.to_datetime(&quot;4-1-1987&quot;).strftime(&quot;%d, %B %Y&quot;) &#39;01, April 1987&#39;</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Well, that wasn’t expected, right? But if you think about it, anyone can make that mistake because we are not telling the computer whether the month or the day comes first, and pandas assumes the month comes first. Let’s rectify that:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt; pd.to_datetime(&quot;4-1-1987&quot;, dayfirst=True).strftime(&quot;%d, %B</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">%Y&quot;)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">&#39;04, January 1987&#39;</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Another case where automatic date parsing fails is when the date string is in a non-standard form. In that case, we can provide a <span class="s48">strftime </span>formatted string to help pandas parse the dates correctly:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt; pd.to_datetime(&quot;4|1|1987&quot;, format=&quot;%d|%m|%Y&quot;).strftime(&quot;%d,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">%B %Y&quot;)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">&#39;04, January 1987&#39;</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">A full list of <span class="s48">strftime </span><a href="https://strftime.org/" class="s140" target="_blank">conventions can be found at </a><span class="s46">https://strftime.org/</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="244" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_053.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Practitioner’s tip</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 92%;text-align: justify;">Because of the wide variety of data formats, pandas may infer the time incorrectly. While reading a file, pandas will try to parse the dates automatically and create an error. There are many ways we can control this behavior: we can use the <span class="s48">parse_dates </span>flag to turn off date parsing, the <span class="s48">date_parser </span>argument to pass in a custom date parser, and <span class="s48">year_first </span>and <span class="s48">day_first </span>to easily denote two popular formats of dates.</p><p style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 92%;text-align: justify;">Out of all these options, I prefer to use <span class="s48">parse_dates=False </span>in both <span class="s48">pd.read_csv </span>and <span class="s48">pd.read_excel </span>to make sure pandas is not parsing the data automatically. After that, you can convert the date using the <span class="s48">format </span>parameter, which lets you explicitly set the date format of the column using <span class="s48">strftime </span>conventions. There are two other parameters in <span class="s48">pd.to_datetime </span>that will also make inferring dates less error-prone – <span class="s48">yearfirst </span>and <span class="s48">dayfirst</span>. If you don’t provide an explicit date format, at least provide one of these.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark67">Now, let’s convert the date column in our stock prices dataset into datetime:</a><a name="bookmark46">&zwnj;</a></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><span class="s49" style=" background-color: #F3F2F1;">  df[&#39;date&#39;] </span><span class="s23" style=" background-color: #F3F2F1;">= </span><span class="s49" style=" background-color: #F3F2F1;">pd.to_datetime(df[&#39;date&#39;], yearfirst</span><span class="s23" style=" background-color: #F3F2F1;">=True</span><span class="s49" style=" background-color: #F3F2F1;">)         </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: left;">Now, the <span class="s48">&#39;date&#39; </span>column, <span class="s48">dtype</span>, should be either <span class="s48">datetime64[ns] </span>or <span class="s48">&lt;M8[ns]</span>, which are both pandas/NumPy native datetime formats. But why do we need to do this?</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">It’s because of the wide range of additional functionalities this unlocks. The traditional <span class="s48">min() </span>and</p><p class="s48" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">max() <span class="p">functions will start working because pandas knows it is a datetime column:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt; df.date.min(),df.date.max()</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">(Timestamp(&#39;2009-01-05 00:00:00&#39;), Timestamp(&#39;2011-02-22</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">00:00:00&#39;))</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s look at a few cool features the datetime format gives us.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Using the .dt accessor and datetime properties</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Since the column is now in date format, all the semantic information that is encoded in the date can be used through pandas datetime properties. We can access many datetime properties, such as <span class="s48">month</span>, <span class="s48">day_of_week</span>, <span class="s48">day_of_year</span>, and so on, using the <span class="s48">.dt </span>accessor:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt; print(f&quot;&quot;&quot;</p><p class="s28" style="padding-top: 3pt;padding-left: 39pt;text-indent: 0pt;text-align: left;">Date: {df.date.iloc[0]}</p><p class="s28" style="padding-top: 3pt;padding-left: 39pt;text-indent: 0pt;line-height: 131%;text-align: left;">Day of year: {df.date.dt.day_of_year.iloc[0]} Day of week: {df.date.dt.dayofweek.iloc[0]} Week of Year: {df.date.dt.weekofyear.iloc[0]} Month: {df.date.dt.month.iloc[0]}</p><p class="s28" style="padding-left: 39pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Month Name: {df.date.dt.month_name().iloc[0]}</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 39pt;text-indent: 0pt;line-height: 131%;text-align: left;">Quarter: {df.date.dt.quarter.iloc[0]} Year: {df.date.dt.year.iloc[0]}</p><p class="s28" style="padding-left: 39pt;text-indent: 0pt;line-height: 131%;text-align: left;">ISO Week: {df.date.dt.isocalendar().week.iloc[0]} &quot;&quot;&quot;)</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Date: 2009-01-05 00:00:00</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">Day of year: 5 Day of week: 0 Week of Year: 2 Month: 1</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">Month Name: January Quarter: 1</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Year: 2009</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ISO Week: 2</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark68"/><a name="bookmark47">&zwnj;</a></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">As of pandas 1.1.0, <span class="s48">week_of_year </span>has been deprecated because of the inconsistencies it produces at the end/start of the year. Instead, the ISO Calendar standards (which are commonly used in government and business) have been adopted and we can access the ISO calendar to get the ISO weeks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Slicing and indexing</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The real fun starts when we make the date column the index of the DataFrame. By doing this, you can use all the fancy slicing operations that pandas supports but on the datetime axis. Let’s take a look at few of them:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Setting the index as the datetime column</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">df.set_index(&quot;date&quot;, inplace=True)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Select all data after 2010-01-04(including)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">df[&quot;2010-01-04&quot;:]</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Select all data between 2010-01-04 and 2010-02-06(not including)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">df[&quot;2010-01-04&quot;: &quot;2010-02-06&quot;]</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Select data 2010 and before</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">df[: &quot;2010&quot;]</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Select data between 2010-01 and 2010-06(both including)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">df[&quot;2010-01&quot;: &quot;2010-06&quot;]</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">In addition to the semantic information and intelligent indexing and slicing, pandas also provides tools for creating and manipulating date sequences.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark69">Creating date sequences and managing date offsets</a><a name="bookmark48">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">If you are familiar with <span class="s48">range </span>in Python and <span class="s48">np.arange </span>in NumPy, then you will know they help us create <span class="s48">integer</span>/<span class="s48">float </span>sequences by providing a start point and an end point. pandas has something similar for datetime – <span class="s48">pd.date_range</span>. The function accepts start and end dates, along with a frequency (daily or monthly, and so on) and creates the sequence of dates in between. Let’s look at a couple of ways of creating a sequence of dates:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="576" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_054.png"/></span></p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;text-align: left;"># Specifying start and end dates with frequency</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">pd.date_range(start=&quot;2018-01-20&quot;, end=&quot;2018-01-23&quot;, freq=&quot;D&quot;). astype(str).tolist()</p><p class="s38" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"># Output: [&#39;2018-01-20&#39;, &#39;2018-01-21&#39;, &#39;2018-01-22&#39;, &#39;2018-01-</p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">23&#39;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Specifying start and number of periods to generate in the given frequency</p><p class="s28" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">pd.date_range(start=&quot;2018-01-20&quot;, periods=4, freq=&quot;D&quot;). astype(str).tolist()</p><p class="s38" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"># Output: [&#39;2018-01-20&#39;, &#39;2018-01-21&#39;, &#39;2018-01-22&#39;, &#39;2018-01-</p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">23&#39;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;text-align: left;"># Generating a date sequence with every 2 days</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">pd.date_range(start=&quot;2018-01-20&quot;, periods=4, freq=&quot;2D&quot;). astype(str).tolist()</p><p class="s38" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"># Output: [&#39;2018-01-20&#39;, &#39;2018-01-22&#39;, &#39;2018-01-24&#39;, &#39;2018-01-</p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">26&#39;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Generating a date sequence every month. By default it starts with Month end</p><p class="s28" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">pd.date_range(start=&quot;2018-01-20&quot;, periods=4, freq=&quot;M&quot;). astype(str).tolist()</p><p class="s38" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"># Output: [&#39;2018-01-31&#39;, &#39;2018-02-28&#39;, &#39;2018-03-31&#39;, &#39;2018-04-</p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">30&#39;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;text-align: left;"># Generating a date sequence every month, but month start</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">pd.date_range(start=&quot;2018-01-20&quot;, periods=4, freq=&quot;MS&quot;). astype(str).tolist()</p><p class="s38" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"># Output: [&#39;2018-02-01&#39;, &#39;2018-03-01&#39;, &#39;2018-04-01&#39;, &#39;2018-05-</p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">01&#39;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark70">We can also add or subtract days, months, and other values to/from dates using </a><span class="s48">pd.TimeDelta</span>:<a name="bookmark49">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Add four days to the date range</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">(pd.date_range(start=&quot;2018-01-20&quot;, end=&quot;2018-01-23&quot;, freq=&quot;D&quot;)</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">+ pd.Timedelta(4, unit=&quot;D&quot;)).astype(str).tolist()</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Output: [&#39;2018-01-24&#39;, &#39;2018-01-25&#39;, &#39;2018-01-26&#39;, &#39;2018-01-</p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">27&#39;]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;text-align: left;"># Add four weeks to the date range</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">(pd.date_range(start=&quot;2018-01-20&quot;, end=&quot;2018-01-23&quot;, freq=&quot;D&quot;)</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">+ pd.Timedelta(4, unit=&quot;W&quot;)).astype(str).tolist()</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Output: [&#39;2018-02-17&#39;, &#39;2018-02-18&#39;, &#39;2018-02-19&#39;, &#39;2018-02-</p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">20&#39;]</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">There are a lot of these aliases in pandas, including <span class="s48">W</span>, <span class="s48">W-MON</span>, <span class="s48">MS</span><a href="https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases" class="s140" target="_blank">, and others. The full list can be found at </a><a href="https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases" class="s45" target="_blank">https://pandas.pydata.org/docs/user_guide/timeseries. </a><span class="s46">html#timeseries-offset-aliases</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In this section, we looked at a few useful features and operations we can perform on datetime indices and know how to manipulate DataFrames with datetime columns. Now, let’s review a few techniques we can use to deal with missing data.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Handling missing data</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">While dealing with large datasets in the wild, you are bound to encounter missing data. If it is not part of the time series, it may be part of the additional information you collect and map. Before we jump the gun and fill it with a mean value or drop those rows, let’s think about a few aspects:</p><ul id="l12"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">The first consideration should be whether the missing data we are worried about is missing or not. For that, we need to think about the <span class="s5">Data Generating Process </span>(<span class="s5">DGP</span>) (the process that is generating the time series). As an example, let’s look at sales at a local supermarket. You have been given the <span class="s5">point-of-sale </span>(<span class="s5">POS</span>) transactions for the last 2 years and you are processing the data into a time series. While analyzing the data, you found that there are a few products where there aren’t any transactions for a few days. Now, what you need to think about is whether the missing data is missing or whether there is some information that this missingness is giving you. If you don’t have any transactions for a particular product for a day, it will appear as missing data while you are processing it, even though it is not missing. What that tells us you that there were no sales for that item, and that you should fill such missing data with zeros.</p></li></ul></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l13"><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a name="bookmark71">Now, what if you see that, every Sunday, the data is missing – that is, there is a pattern to the missingness. This becomes tricky because how you fill in such gaps depends on the model that you intend to use. If you fill in such gaps with zeros, a model that looks at the immediate past to predict the future will predict inaccurate results on Monday. However, if you tell the model that the previous day was Sunday, then the model still can learn to tell the difference.</a></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Lastly, what if you see zero sales on one of the best-selling products that always gets sold? This can happen because of something such as a POS machine malfunction, a data entry mistake, or an out-of-stock situation. These types of missing values can be imputed with a few techniques.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="122" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_055.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Practitioner’s tip</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 92%;text-align: justify;">When reading data using a method such as <span class="s48">read_csv</span>, pandas provides a few handy ways to handle missing values. pandas treats many values such as <span class="s48">#N/A</span>, <span class="s48">null</span>, and so on as <span class="s48">NaN </span>by default. We can control this list of allowable <span class="s48">NaN </span>values using the <span class="s48">na_values </span>and <span class="s48">keep_default_na </span>parameters.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s46" style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a href="https://www.data.act.gov.au/Environment/Air-Quality-Monitoring-Data/94a5-zqnn" class="s140" target="_blank">Let’s look at an Air Quality dataset published by the ACT Government, Canberra, Australia under the CC by Attribution 4.0 International License (</a><a href="https://www.data.act.gov.au/Environment/Air-Quality-Monitoring-Data/94a5-zqnn" class="s45" target="_blank">https://www.data.act.gov.au/ </a>Environment/Air-Quality-Monitoring-Data/94a5-zqnn<span class="p">) and see how we can impute such values using pandas (there are more sophisticated techniques available, all of which will be covered later in this chapter).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">We have chosen region <span class="s5">Monash </span>and <span class="s5">PM2.5 </span>readings and introduced some missing values, as shown in the following diagram:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><span><img width="482" height="257" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_056.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.3 – Missing values in the Air Quality dataset</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark72">Now, let’s look at a few simple techniques we can use to fill the missing values:</a></p><ul id="l14"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Last Observation Carried Forward or Forward Fill<span class="p">: This imputation technique takes the last observed value and uses that to fill all the missing values until it finds the next observation. It is also called forward fill. We can do this like so:</span></p><p style="padding-top: 9pt;padding-left: 64pt;text-indent: 0pt;text-align: left;"><span class="s49" style=" background-color: #F3F2F1;">  df[&#39;pm2_5_1_hr&#39;].ffill()                                   </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s5" style="padding-left: 64pt;text-indent: -13pt;text-align: justify;">Next Observation Carried Backward of Backward Fill<span class="p">: This imputation technique takes the next observation and backtracks to fill in all the missing values with this value. This is also called backward fill. Let’s see how we can do this in pandas:</span></p><p style="padding-top: 9pt;padding-left: 64pt;text-indent: 0pt;text-align: left;"><span class="s49" style=" background-color: #F3F2F1;">  df[&#39;pm2_5_1_hr&#39;].bfill()                                   </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s5" style="padding-left: 64pt;text-indent: -13pt;text-align: justify;">Mean Value Fill<span class="p">: This imputation technique is also pretty simple. We calculate the mean of the entire series and wherever we find missing values, we fill it with the mean value:</span></p><p style="padding-top: 9pt;padding-left: 64pt;text-indent: 0pt;text-align: left;"><span class="s49" style=" background-color: #F3F2F1;">  df[&#39;pm2_5_1_hr&#39;].fillna(df[&#39;pm2_5_1_hr&#39;].mean())           </span></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s plot the imputed lines we get from using these three techniques:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 73pt;text-indent: 0pt;text-align: left;"><span><img width="435" height="234" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_057.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.4 – Imputed missing values using forward, backward, and mean value fill</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Another family of imputation techniques covers interpolation:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Linear Interpolation<span class="p">: Linear interpolation is just like drawing a line between the two observed points and filling the missing values so that they lie on this line. This is how we do it:</span></p><p style="padding-top: 9pt;padding-left: 64pt;text-indent: 0pt;text-align: left;"><span class="s49" style=" background-color: #F3F2F1;">  df[&#39;pm2_5_1_hr&#39;].interpolate(method=&quot;linear&quot;)              </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a name="bookmark73">Nearest Interpolation</a><span class="p">: This is intuitively like a combination of the forward and backward fill. For each missing value, the closest observed value is found and is used to fill in the missing value:</span></p><p style="padding-top: 9pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><span class="s49" style=" background-color: #F3F2F1;">  df[&#39;pm2_5_1_hr&#39;].interpolate(method=&quot;nearest&quot;)             </span></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s plot the two interpolated lines:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 50pt;text-indent: 0pt;text-align: left;"><span><img width="481" height="259" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_058.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.5 – Imputed missing values using linear and nearest interpolation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">There are a few non-linear interpolation techniques as well:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s5">Spline, Polynomial, and Other Interpolations</span>: In addition to linear interpolation, pandas also supports non-linear interpolation techniques that call a SciPy routine at the backend. Spline and polynomial interpolations are similar. They fit a spline/polynomial of a given order to the data and use that to fill in missing values. While using <span class="s48">spline </span>or <span class="s48">polynomial </span>as the method in <span class="s48">interpolate</span>, we should always provide <span class="s48">order </span>as well. The higher the order, the more flexible the function that is used will be to fit the observed points. Let’s see how we can use spline and polynomial interpolation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">df[&#39;pm2_5_1_hr&#39;].interpolate(method=&quot;spline&quot;, order=2)</p><p class="s28" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;line-height: 106%;text-align: left;">df[&#39;pm2_5_1_hr&#39;].interpolate(method=&quot;polynomial&quot;, order=5)</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 64pt;text-indent: 0pt;text-align: left;"><a name="bookmark74">Let’s plot these two non-linear interpolation techniques:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 48pt;text-indent: 0pt;text-align: left;"><span><img width="500" height="267" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_059.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.6 – Imputed missing values using spline and polynomial interpolation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s46" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="p">For a complete list of interpolation techniques supported by </span><span style=" color: #000;">interpolate</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html" class="s140" target="_blank">, go to </a><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html" class="s45" target="_blank">https:// pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series. </a>interpolate.html <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d" class="s140" target="_blank">and </a><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d" class="s45" target="_blank">https://docs.scipy.org/doc/scipy/reference/ generated/scipy.interpolate.interp1d.html#scipy.interpolate. </a>interp1d<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="88" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_060.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 92%;text-align: left;">To follow along with the complete code for pre-processing, use the <span class="s48">02 - Preprocessin London Smart Meter Dataset.ipynb </span>notebook in the <span class="s48">chapter01 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s48" style="padding-top: 7pt;text-indent: 0pt;text-align: left;">g</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we are more comfortable with the way pandas manages datetime, let’s go back to our dataset and convert the data into a more manageable form.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark75">Converting the half-hourly block-level data (hhblock) into time series data</a><a name="bookmark51">&zwnj;</a><a name="bookmark50">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Before we start processing, let’s understand a few general categories of information we will find in a time series dataset:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Time Series Identifiers<span class="p">: These are identifiers for a particular time series. It can be a name, an ID, or any other unique feature – for example, the SKU name or the ID of a retail sales dataset or the consumer ID in the energy dataset that we are working with are all time series identifiers.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Metadata or Static Features<span class="p">: This information does not vary with time. An example of this is the ACORN classification of the household in our dataset.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Time-Varying Features<span class="p">: This information varies with time – for example, the weather information. For each point in time, we have a different value for weather, unlike the Acorn classification.</span></p></li></ul><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Next. Let’s discuss formatting of a dataset.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Compact, expanded, and wide forms of data</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are many ways to format a time series dataset, especially a dataset with many related time series, like the one we have now. Apart from the standard terminology of <span class="s5">wide </span>data, we can also look at two non-standard ways of formatting time series data. Although there is no standard nomenclature for those, we will refer to them as <span class="s5">compact </span>and <span class="s5">expanded </span>in this book.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Compact form data is when any particular time series occupies only a single row in the pandas DataFrame – that is, the time dimension is managed as an array within a DataFrame row. The time series identifiers and the metadata occupy the columns with scalar values and then the time series values; other time-varying features occupy the columns with an array. Two additional columns are included to extrapolate time – <span class="s48">start_datetime </span>and frequency. If we know the start datetime and the frequency of the time series, we can easily construct the time and recover the time series from the DataFrame. This only works for regularly sampled time series. The advantage is that the DataFrames take up much less memory and are easy and faster to work with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="525" height="138" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_061.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.7 – Compact form data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark76">The expanded form is when the time series is expanded along the rows of a DataFrame. If there are </a><i>n </i>steps in the time series, it occupies <i>n </i>rows in the DataFrame. The time series identifiers and the metadata get repeated along all the rows. The time-varying features also get expanded along the rows. And instead of the start date and frequency, we have the datetime as a column:<a name="bookmark52">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><span><img width="455" height="143" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_062.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 40pt;text-indent: 0pt;text-align: center;">Figure 2.8 – Expanded form data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">If the compact form had a time series identifier as the key, the time series identifier and the datetime column would be combined and become the key.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Wide-format data is more common in traditional time series literature. It can be considered a legacy format, which is limiting in many ways. Do you remember the stock data we saw earlier (<i>Figure 2.2</i>)? We have the date as an index or as one of the columns and the different time series as different columns of the DataFrame. As the number of time series increases, they become wider and wider, hence the name. This data format does not allow us to include any metadata about the time series. For instance, in our data, we have information about whether a particular household is under standard or dynamic pricing. There is no way for us to include such metadata in the wide format. From an operational perspective, the wide format also does not play well with relational databases because we have to keep adding columns to the table when we get new time series. We won’t be using this format in this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Enforcing regular intervals in time series</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="101" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_063.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Best practice</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">While working with datasets with multiple time series, it is best practice to check the end date of all the time series. If they are not uniform, we can align them with the latest date across a the time series in the dataset.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 89%;text-align: left;">s ll</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">One of the first things you should check and correct is whether the regularly sampled time series data that you have has equal intervals of time. In practice, even regularly sampled time series have some samples missing in between because of some data collection error or some other peculiar way data is collected. So, while working with the data, we will make sure we enforce regular intervals in the time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a name="bookmark77">In our smart meters dataset, some </a><span class="s48">LCLid </span>columns end much earlier than the rest. Maybe the household opted out of the program, or they moved out and left the house empty; the reason could be anything. However, we need to handle that while we enforce regular intervals.<a name="bookmark53">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We will learn how to convert the dataset into a time series format in the next section. The code for this process can be found in the <span class="s48">02 - Preprocessing London Smart Meter Dataset. ipynb </span>notebook.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Converting the London Smart Meters dataset into a time series format</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">For each dataset that you come across, the steps you would have to take to convert it into either a compact or expanded form would be different. It depends on how the original data is structured. Here, we will look at how the London Smart Meters dataset can be transformed so that we can transfer those learnings to other datasets.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are two steps we need to do before we can start processing the data into either compact or expanded form:</p><ol id="l15"><li><p class="s5" style="padding-top: 9pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Find the Global End Date<span class="p">: We must find the maximum date across all the block files so that we know the global end date of the time series.</span></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;"><span class="s5">Basic Preprocessing</span>: If you remember how <span class="s48">hhblock_dataset </span>is structured, you will remember that each row had a date and that along the columns, we have half-hourly blocks. We need to reshape that into a long form, where each row has a date and a single half-hourly block. It’s easier to handle that way.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Now, let’s define separate functions for converting the data into compact and expanded forms and <span class="s48">apply </span>that function to each of the <span class="s48">LCLid </span>columns. We will do this for each <span class="s48">LCLid </span>separately since the start date for each <span class="s48">LCLid </span>is different.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Expanded form</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The function for converting into expanded form does the following:</p><ol id="l16"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Finds the start date.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Create a standard DataFrame using the start date and the global end date.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;line-height: 94%;text-align: left;">Left merges the DataFrame for <span class="s48">LCLid </span>to the standard DataFrame, leaving the missing data as <span class="s48">np.nan</span>.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Returns the merged DataFrame.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a name="bookmark78">Once we have all the </a><span class="s48">LCLid </span>DataFrames, we must perform a couple of additional steps to complete the expanded form processing:<a name="bookmark54">&zwnj;</a></p><ol id="l17"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Concatenate all the DataFrames into a single DataFrame.</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: left;"><span class="p">Create a column called offset, which is the numerical representation of the half-hour blocks; for example, </span>hh_3 <span class="s47">→ </span>3<span class="p">.</span></p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Create a timestamp by adding a 30-minute offset to the day and dropping the unnecessary columns.</p></li></ol></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">For one block, this representation takes up ~47 MB of memory.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Compact form</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">The function for converting into compact form does the following:</p><ol id="l18"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Finds the start date and time series identifiers.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Creates a standard DataFrame using the start date and the global end date.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;line-height: 94%;text-align: left;">Left merges the DataFrame for <span class="s48">LCLid </span>to the standard DataFrame, leaving the missing data as <span class="s48">np.nan</span>.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Sorts the values on the date.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Returns the time series array, along with the time series identifier, start date, and the length of the time series.</p></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Once we have this information for each <span class="s48">LCLid</span>, we can compile it into a DataFrame and add <span class="s48">30min</span></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">as the frequency.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">For one block, this representation takes up only ~0.002 MB of memory.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">We are going to use the compact form because it is easy to work with and much less resource hungry.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Mapping additional information</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">From the data model that we prepared earlier, we know that there are three key files that we have to map: <i>Household Information</i>, <i>Weather</i>, and <i>Bank Holidays</i>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The <span class="s48">informations_households.csv </span>file contains metadata about the household. There are static features that are not dependent on time. For this, we just need to left merge <span class="s48">informations_ households.csv </span>to the compact form based on <span class="s48">LCLid</span>, which is the time series identifier.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="174" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_064.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s29" style="padding-left: 13pt;text-indent: 0pt;text-align: justify;">Best practice</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 91%;text-align: justify;">While doing a pandas merge, one of the most common and unexpected outcomes is that the number of rows before and after the operation is not the same (even if you are doing a left merge). This typically happens because there are duplicates in the keys on which you are merging. As a best practice, you can use the <span class="s48">validate </span>parameter in the pandas merge, which takes in inputs such as <span class="s48">one_to_one </span>and <span class="s48">many_to_one </span><a href="https://pandas.pydata.org/docs/reference/api/pandas.merge.html" class="s140" target="_blank">so that this check is done while merging and will throw an error if the assumption is not met. For more information, go to </a><a href="https://pandas.pydata.org/docs/reference/api/pandas.merge.html" class="s45" target="_blank">https:// </a><span class="s46">pandas.pydata.org/docs/reference/api/pandas.merge.html</span>.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s47" style="padding-top: 4pt;padding-left: 290pt;text-indent: 0pt;text-align: left;"><a name="bookmark79">Mapping additional information 37</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Bank Holidays and Weather, on the other hand, are time-varying features and should be dealt with accordingly. The most important aspect to keep in mind is that while we map this information, it should perfectly align with the time series that we have already stored as an array.</p><p class="s48" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">uk_bank_holidays.csv <span class="p">is a file that contains the dates of the holidays and the kind of holiday. The holiday information is quite important here because the energy consumption patterns would be different on a holiday when the family members are at home spending time with each other or watching television, and so on. Follow these steps to process this file:</span></p><ol id="l19"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Convert the date column into the datetime format and set it as the index of the DataFrame.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;line-height: 94%;text-align: left;">Using the <span class="s48">resample </span>function we saw earlier, we must ensure that the index is resampled every 30 minutes, which is the frequency of the times series.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Forward fill the holidays within a day and fill in the rest of the <span class="s48">NaN </span>values with <span class="s48">NO_HOLIDAY</span>.</p></li></ol><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, we have converted the holiday file into a DataFrame that has a row for each 30-minute interval. On each row, we have a column that specifies whether that day was a holiday or not.</p><p class="s48" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">weather_hourly_darksky.csv <span class="p">is a file that is, once again, at the daily frequency. We need to downsample it to a 30-minute frequency because the data that we need to map to this is at a half- hourly frequency. If we don’t do this, the weather will only be mapped to the hourly timestamps, leaving the half-hourly timestamps empty.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The steps we must follow to process this file are also similar to the way we processed holidays:</p><ol id="l20"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Convert the date column into the datetime format and set it as the index of the DataFrame.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;line-height: 94%;text-align: left;">Using the <span class="s48">resample function</span>, we must ensure that the index is resampled every 30 minutes, which is the frequency of the times series.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Forward fill the weather features to fill the missing values that were created while resampling.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now that you have made sure the alignment between the time series and the time-varying features is ensured, you can loop over each of the time series and extract the weather and bank holiday array before storing it in the corresponding row of the DataFrame.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark80">Saving and loading files to disk</a><a name="bookmark56">&zwnj;</a><a name="bookmark55">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The fully merged DataFrame in its compact form takes up only ~10 MB. But saving this file requires a little bit of engineering. If we try to save the file in CSV format, it will not work because of the way we have stored arrays in pandas columns (since the data is in its compact form). We can save it in <span class="s48">pickle </span>or <span class="s48">parquet </span>format, or any of the binary forms of file storage. This can work, depending on the size of the RAM available in our machines. Although the fully merged DataFrame is just ~10 MB, saving it in <span class="s48">pickle </span>format will make the size explode to ~15 GB.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">What we can do is save this as a text file while making a few tweaks to accommodate the column names, column types, and other metadata that is required to read the file back into memory. The resulting file size on disk still comes out to ~15 GB but since we are doing it as an I/O operation, we are not keeping all that data in our memory. We call this the time series (<span class="s48">.ts</span>) format. The functions for saving a compact form in <span class="s48">.ts </span>format, reading the <span class="s48">.ts </span>format, and converting the compact form into expanded form are available in this book’s GitHub repository under <span class="s48">src/data_utils.py</span>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">If you don’t need to store all of the DataFrame in a single file, you can split it into multiple chunks and save them individually in a binary format, such as <span class="s48">parquet</span>. For our datasets, let’s follow this route and split the whole DataFrame into chunks of blocks and save them as <span class="s48">parquet </span>files. This is the best route for us because of a few reasons:</p><ul id="l21"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">It leverages the compression that comes with the format</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">It reads in parts of the whole data for quick iteration and experimentation</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">The data types are retained between the read and write operations, leading to less ambiguity</p></li></ul></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have processed the dataset and stored it on disk, let’s read it back into memory and look at a few more techniques to handle missing data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Handling longer periods of missing data</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="88" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_065.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 92%;text-align: left;">To follow along with the complete code for missing data imputation, use the <span class="s48">03 - Handlin Missing Data (Long Gaps).ipynb </span>notebook in the <span class="s48">chapter02 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s48" style="padding-top: 7pt;text-indent: 0pt;text-align: left;">g</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We saw some techniques for handling missing data earlier – forward and backward filling, interpolation, and so on. Those techniques usually work if there are one or two missing data points. But if a large section of data is missing, then these simple techniques fall short.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s read blocks 0-7 <span class="s48">parquet </span>from memory:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">block_df = pd.read_parquet(&quot;data/london_smart_meters/ preprocessed/london_smart_meters_merged_block_0-7.parquet&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The data that we have saved is in compact form. We need to convert it into expanded form because it is easier to work with time series data in that form. Since we only need a subset of the time series (for faster demonstration purposes), we are just extracting one block from these seven blocks. To convert compact form into expanded form, we can use a helpful function in <span class="s48">src/utils/data_utils. py </span>called <span class="s48">compact_to_expanded</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Converting to expanded form</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">exp_block_df = compact_to_expanded(block_df[block_ df.file==&quot;block_7&quot;], timeseries_col = &#39;energy_consumption&#39;,</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">static_cols = [&quot;frequency&quot;, &quot;series_length&quot;, &quot;stdorToU&quot;, &quot;Acorn&quot;, &quot;Acorn_grouped&quot;, &quot;file&quot;],</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">time_varying_cols = [&#39;holidays&#39;, &#39;visibility&#39;, &#39;windBearing&#39;, &#39;temperature&#39;, &#39;dewPoint&#39;,</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 42pt;line-height: 106%;text-align: left;">&#39;pressure&#39;, &#39;apparentTemperature&#39;, &#39;windSpeed&#39;, &#39;precipType&#39;, &#39;icon&#39;,</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 42pt;line-height: 131%;text-align: left;">&#39;humidity&#39;, &#39;summary&#39;], ts_identifier = &quot;LCLid&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">One of the best ways to visualize the missing data in a group of related time series is by using a very helpful package called <span class="s48">missingno</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Pivot the data to set the index as the datetime and the different time series along the columns</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">plot_df = pd.pivot_table(exp_block_df, index=&quot;timestamp&quot;, columns=&quot;LCLid&quot;, values=&quot;energy_consumption&quot;)</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Generate Plot. Since we have a datetime index, we can mention the frequency to decide what do we want on the X axis</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">msno.matrix(plot_df, freq=&quot;M&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">The preceding code produces the following output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="214" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_066.gif"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.9 – Visualization of the missing data in block 7</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="102" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_067.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 89%;text-align: left;">n ll</p><p style="text-indent: 0pt;text-align: left;"/><p class="s29" style="padding-top: 5pt;text-indent: 0pt;text-align: justify;">mportant note</p><p style="padding-top: 3pt;text-indent: 7pt;line-height: 88%;text-align: justify;">ly attempt the <span class="s48">missingno </span>visualization on related time series where there are less tha time series. If you have a dataset that contains thousands of time series (such as in our fu ataset), applying this visualization will give us an illegible plot and a frozen computer.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">I</p><p style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">On 25</p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 12pt;text-align: left;">d</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This visualization tells us a lot of things at a single glance. The <i>Y</i>-axis contains the dates that we are plotting the visualization for, while the <i>X</i>-axis contains the columns, which in this case are the different households. We know that all the time series are not perfectly aligned – that is, not all of them start at the same time and end at the same time. The big white gaps we can see at the beginning of many of the time series show that data collection for those consumers started later than the others. We can also see that a few time series finish earlier than the rest, which means either they stopped being consumers or the measurement phase stopped. There are also a few smaller white lines in many time series, which are real missing values. We can also notice a sparkline to the right, which is a compact representation of the number of missing columns for each row. If there are no missing values (all time series have some value), then the sparkline would be at the far right. Finally, if there are a lot of missing values, the line will be to the left.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Just because there are missing values, we are not going to fill/impute them because the decision of whether to impute missing data or not comes later in the workflow. For some models, we do not need to do the imputation, while for others, we do. There are multiple ways of imputing missing data and which one to choose is another decision we cannot make beforehand.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: left;">So, for now, let’s pick one <span class="s48">LCLid </span>and dig deeper. We already know that there are some missing values between 2012-09-30 and 2012-10-31. Let’s visualize that period:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Taking a single time series from the block</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df = exp_block_df[exp_block_df.LCLid==&quot;MAC000193&quot;].set_ index(&quot;timestamp&quot;)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">msno.matrix(ts_df[&quot;2012-09-30&quot;: &quot;2012-10-31&quot;], freq=&quot;D&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The preceding code produces the following output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="518" height="215" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_068.gif"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 49pt;text-indent: 0pt;text-align: left;">Figure 2.10 – Visualization of missing data of MAC000193 between 2012-09-30 and 2012-10-31</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, we can see that the missing data is really between 2012-10-18 and 2012-10-19. Normally, we would go ahead and impute the missing data in this period, but since we are looking at this with an academic lens, we will take a slightly different route. Let’s introduce an artificial missing data section and see how the different techniques we are going to look at impute the missing data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># The dates between which we are nulling out the time series</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">window = slice(&quot;2012-10-07&quot;, &quot;2012-10-08&quot;)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Creating a new column and artificially creating missing values</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">ts_df[&#39;energy_consumption_missing&#39;] = ts_df.energy_consumption ts_df.loc[window, &quot;energy_consumption_missing&quot;] = np.nan</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark81">Now, let’s plot the missing area in the time series:</a><a name="bookmark57">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="521" height="268" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_069.gif"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.11 – The energy consumption of MAC000193 between 2012-10-05 and 2012-10-10</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We are missing 2 whole days of energy consumption readings, which means there are 96 missing data points (half-hourly). If we use one of the techniques we saw earlier, such as interpolation, we will see that it will mostly be a straight line because none of the methods are complex enough to capture the pattern over a long time.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are a few techniques that we can use to fill in such large missing gaps in data. We will cover these now.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Imputing with the previous day</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Since this is a half-hourly time series of energy consumption, it stands to reason that there might be a pattern that is repeating day after day. The energy consumption between 9:00 A.M. and 10:00 A.M. might be higher as everybody gets ready to go to the office and a slump during the day when most houses may be empty. So, the simplest way to fill in the missing data would be to use the last day energy readings so that the energy reading at 10:00 A.M. 2012-10-18 can be filled with the energy reading at 10:00 A.M. 2012-10-17:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Shifting 48 steps to get previous day</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ts_df[&quot;prev_day&quot;] = ts_df[&#39;energy_consumption&#39;].shift(48)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Using the shifted column to fill missing</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df[&#39;prev_day_imputed&#39;] = ts_df[&#39;energy_consumption_ missing&#39;]</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df.loc[null_mask,&quot;prev_day_imputed&quot;] = ts_df.loc[null_ mask,&quot;prev_day&quot;]</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">mae = mean_absolute_error(ts_df.loc[window, &quot;prev_day_ imputed&quot;], ts_df.loc[window, &quot;energy_consumption&quot;])</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark82"/><a name="bookmark58">&zwnj;</a></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s see what the imputation looks like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="521" height="285" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_070.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.12 – Imputing with the previous day</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">While this looks better, this is also very brittle. When we are copying the previous day, we are also assuming that any kind of variation or anomalous behavior is also repeated. We can already see that the patterns for the day before and the day after are not the same.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Hourly average profile</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">A better approach would be to calculate an hourly profile from the data – the mean consumption for every hour – and use the average to fill the missing data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Create a column with the Hour from timestamp</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ts_df[&quot;hour&quot;] = ts_df.index.hour</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Calculate hourly average consumption</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">hourly_profile = ts_df.groupby([&#39;hour&#39;])[&#39;energy_consumption&#39;]. mean().reset_index()</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">hourly_profile.rename(columns={&quot;energy_consumption&quot;: &quot;hourly_</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">profile&quot;}, inplace=True)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Saving the index because it gets lost in merge</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">idx = ts_df.index</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Merge the hourly profile dataframe to ts dataframe</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df = ts_df.merge(hourly_profile, on=[&#39;hour&#39;], how=&#39;left&#39;, validate=&quot;many_to_one&quot;)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ts_df.index = idx</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">#Using the hourly profile to fill missing</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df[&#39;hourly_profile_imputed&#39;] = ts_df[&#39;energy_consumption_ missing&#39;]</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df.loc[null_mask,&quot;hourly_profile_imputed&quot;] = ts_df.loc[null_ mask,&quot;hourly_profile&quot;]</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">mae = mean_absolute_error(ts_df.loc[window, &quot;hourly_profile_ imputed&quot;], ts_df.loc[window, &quot;energy_consumption&quot;])</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s see if this is better:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span><img width="509" height="283" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_071.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.13 – Imputing with an hourly profile</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark83">This is giving us a much more generalized curve that does not have the spikes that we saw for the individual days. The hourly ups and downs have also been captured as per our expectations. The </a><span class="s5">mean absolute error </span>(<span class="s5">MAE</span>) is also lower than before.<a name="bookmark59">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The hourly average for each weekday</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can further refine this rule by introducing a specific profile for each weekday. It stands to reason that the usage pattern on a weekday is not going to be the same on a weekend. Hence, we can calculate the average hourly consumption for each weekday separately so that we have one profile for Monday, another for Tuesday, and so on:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="400" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_072.png"/></span></p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;line-height: 131%;text-align: left;">#Create a column with the weekday from timestamp <span class="s28">ts_df[&quot;weekday&quot;] = ts_df.index.weekday </span>#Calculate weekday-hourly average consumption</p><p class="s28" style="padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">day_hourly_profile = ts_df.groupby([&#39;weekday&#39;,&#39;hour&#39;])[&#39;energy_ consumption&#39;].mean().reset_index()</p><p class="s28" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">day_hourly_profile.rename(columns={&quot;energy_consumption&quot;: &quot;day_ hourly_profile&quot;}, inplace=True)</p><p class="s38" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">#Saving the index because it gets lost in merge</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">idx = ts_df.index</p><p class="s38" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">#Merge the day-hourly profile dataframe to ts dataframe</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df = ts_df.merge(day_hourly_profile, on=[&#39;weekday&#39;, &#39;hour&#39;], how=&#39;left&#39;, validate=&quot;many_to_one&quot;)</p><p class="s28" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">ts_df.index = idx</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">#Using the day-hourly profile to fill missing</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df[&#39;day_hourly_profile_imputed&#39;] = ts_df[&#39;energy_ consumption_missing&#39;]</p><p class="s28" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df.loc[null_mask,&quot;day_hourly_profile_imputed&quot;] = ts_ df.loc[null_mask,&quot;day_hourly_profile&quot;]</p><p class="s28" style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">mae = mean_absolute_error(ts_df.loc[window, &quot;day_hourly_ profile_imputed&quot;], ts_df.loc[window, &quot;energy_consumption&quot;])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark84">Let’s see what this looks like:</a><a name="bookmark60">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 54pt;text-indent: 0pt;text-align: left;"><span><img width="482" height="263" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_073.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.14 – Imputing the hourly average for each weekday</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">This looks very similar to the other one, but this is because the day we are imputing is a weekday and the weekday profiles are similar. The MAE is also lower than the day profile. The weekend profile is slightly different, which you can see in the associated Jupyter Notebook.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Seasonal interpolation</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Although calculating seasonal profiles and using them to impute works well, there are instances, especially when there is a trend in the time series, where such a simple technique falls short. The simple seasonal profile doesn’t capture the trend at all and ignores it completely. For such cases, we can do the following:</p><ol id="l22"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Calculate the seasonal profile, similar to how we calculated the averages earlier.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Subtract the seasonal profile and apply any of the interpolation techniques we saw earlier.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Add the seasonal profile back to the interpolated series.</p></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">This process has been implemented in this book’s GitHub repository in the <span class="s48">src/imputation/ interpolation.py </span>file. We can use it as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">from src.imputation.interpolation import SeasonalInterpolation</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Seasonal interpolation using 48*7 as the seasonal period.</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">recovered_matrix_seas_interp_weekday_half_hour = SeasonalInterpolation(seasonal_period=48*7,decomposition_</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">strategy=&quot;additive&quot;, interpolation_strategy=&quot;spline&quot;, interpolation_args={&quot;order&quot;:3}, min_value=0).fit_transform(ts_ df.energy_consumption_missing.values.reshape(-1,1))</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_df[&#39;seas_interp_weekday_half_hour_imputed&#39;] = recovered_ matrix_seas_interp_weekday_half_hour</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark85"/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="103" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_074.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional information</p><p style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;line-height: 90%;text-align: justify;">Internally, we use something called seasonal decomposition (<span class="s48">statsmodels.tsa. seasonal.seasonal_decompose</span><a href="#bookmark87" class="s140">), which will be covered in </a><a href="#bookmark87" class="s21">Chapter </a><i>3</i>, <i>Analyzing and Visualizing Time Series Data</i>, to isolate the seasonality component.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The key parameter here is <span class="s48">seasonal_period</span>, which tells the algorithm to look for patterns that repeat every <span class="s48">seasonal_period</span>. If we mention <span class="s48">seasonal_period=48</span>, it will look for patterns that repeat every 48 data points. In our case, they are after each day (because we have 48 half-hour timesteps in a day). In addition to this, we need to specify what kind of interpolation we need to perform.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Here, we have done seasonal interpolation using 48 (half-hourly) and 48*7 (weekday to half-hourly) and plotted the resulting imputation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;text-align: left;"><span><img width="514" height="288" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_075.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 2.15 – Imputing with seasonal interpolation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark86">Here, we can see that both have captured the seasonality patterns, but the half-hourly profile every weekday has captured the peaks in the first day better, so they have a lower MAE. There is no improvement in terms of hourly averages, mostly because there is no strong increasing or decreasing patterns in the time series.</a><a name="bookmark61">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">With this, we have come to the end of this chapter. We are now officially into the nitty-gritty of juggling time series data, cleaning it, and processing it. Congratulations on finishing this chapter!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">After a short refresher on pandas DataFrames, especially on the datetime manipulations and simple techniques for handling missing data, we learned about the two forms of storing and working with time series data – compact and expanded. With all this knowledge, we took our raw dataset and built a pipeline to convert it into compact form. If you have run the accompanying notebook, you should have the preprocessed dataset saved on disk. We also had an in-depth look at some techniques for handling long gaps of missing data.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have the processed datasets, in the next chapter, we will learn how to visualize and analyze a time series dataset.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark87">3</a><a name="bookmark88">&zwnj;</a><a name="bookmark90">&zwnj;</a><a name="bookmark89">&zwnj;</a></h2><h4 style="padding-top: 2pt;text-indent: 0pt;text-align: right;">Analyzing and Visualizing</h4><h4 style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Time Series Data</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous chapter, we learned where to obtain time series datasets, as well as how to manipulate time series data using <span class="s20">pandas</span>, handle missing values, and so on. Now that we have the processed time series data, it’s time to understand the dataset, which data scientists call <span class="s5">Exploratory Data Analysis </span>(<span class="s5">EDA</span>). It is a process by which the data scientist analyzes the data by looking at aggregate statistics, feature distributions, visualizations, and so on to try and uncover patterns in the data that they can leverage in modeling. In this chapter, we will look at a couple of ways to analyze a time series dataset, a few specific techniques that are tailor-made for time series, and review some of the visualization techniques for time series data.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will cover the following topics:</p><ul id="l23"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Components of a time series</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Visualizing time series data</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Decomposing a time series</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Detecting and treating outliers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">You will need to run <span class="s20">02 - Preprocessing London Smart Meter Dataset.ipynb</span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">notebook from <span class="s20">Chapter02 </span>folder.</p><p class="s27" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter03" class="s140" target="_blank">The code for this chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter03" class="a" target="_blank">https://github.com/PacktPublishing/Modern- </a>Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter03<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark115">Components of a time series</a><a name="bookmark92">&zwnj;</a><a name="bookmark91">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Before we start analyzing and visualizing time series, we need to understand the structure of a time series. Any time series can contain some or all of the following components:</p><ul id="l24"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Trend</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Seasonal</p></li><li><p class="s5" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Cyclical</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Irregular</p></li></ul></li></ul></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">These components can be mixed in different ways, but two very commonly assumed ways are <i>additive </i>(<i>Y</i></p><p class="s4" style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">= Trend + Seasonal + Cyclical + Irregular<span class="p">) and </span>multiplicative <span class="p">(</span>Y = Trend * Seasonal * Cyclical * Irregular<span class="p">).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The trend component</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The <span class="s5">trend </span>is a long-term change in the mean of a time series. It is the smooth and steady movement of a time series in a particular direction. When the time series moves upward, we say there is an <i>upward or increasing trend</i>, while when it moves downward, we say there is a <i>downward or decreasing trend</i>. At the time of writing, if we think about the revenue of Tesla over the years, as shown in the following figure, we can see that it has been increasing consistently for the last few years:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="521" height="263" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_076.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.1 – Tesla’s revenue in millions of USD</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Looking at the preceding figure, we can say that Tesla’s revenue is having an increasing trend. The trend doesn’t need to be linear; it can also be non-linear.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark116">Components of a time series 51</a><a name="bookmark95">&zwnj;</a><a name="bookmark94">&zwnj;</a><a name="bookmark93">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_077.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">The seasonal component</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">When a time series exhibits regular, repetitive, up-and-down fluctuations, we call that <span class="s5">seasonality</span>. For instance, retail sales typically shoot up during the holidays, specifically Christmas in western countries. Similarly, electricity consumption peaks during the summer months in the tropics and the winter months in colder countries. In all these examples, you can see a specific up-and-down pattern repeating every year. Another example is sunspots, as shown in the following figure:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span><img width="516" height="267" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_078.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.2 – Number of sunspots from 1749 to 2017</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">As you can see, sunspots peak every 11 years.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">The cyclical component</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The <span class="s5">cyclical </span>component is often confused with seasonality, but it stands apart due to a very subtle difference. Like seasonality, the cyclical component also exhibits a similar up-and-down pattern around the trend line, but instead of repeating the pattern every period, the cyclical component is irregular. A good example of this is economic recession, which happens over a 10-year cycle. However, this doesn’t happen like clockwork; sometimes, it can be fewer or more than every 10 years.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">The irregular component</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This component is left after removing the trends, seasonality, and cyclicity from a time series. Traditionally, this component is considered <i>unpredictable </i>and is also called the <i>residual </i>or <i>error term</i>. In common classical statistics-based models, the point of any “model” is to capture all the other components to the point that the only part that is not captured is the irregular component. In modern</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark117">machine learning, we do not consider this component entirely unpredictable. We try to capture this component, or parts of it, by using exogenous variables. For instance, the irregular component of retail sales may be explained as the different promotional activities they run. When we have this additional information, the “unpredictable” component starts to become predictable again. But no matter how many additional variables you add to the model, there will always be some component, which is the true irregular component (or true error), that is left behind.</a><a name="bookmark97">&zwnj;</a><a name="bookmark96">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we know what the different components of a time series are, let’s see how we can visualize them.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Visualizing time series data</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark38" class="s140">In </a>Chapter 2<span class="p">, </span>Acquiring and Processing Time Series Data<span class="p">, we learned how to prepare a data model as a first step toward analyzing a new dataset. If preparing a data model is like approaching someone you like and making that first contact, then EDA is like dating that person. At this point, you have the dataset, and you are trying to get to know them, trying to figure out what makes them tick, what the person likes and dislikes, and so on.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">EDA often employs visualization techniques to uncover patterns, spot anomalies, form and test hypotheses, and so on. Spending some time understanding your dataset will help you a lot when you are trying to squeeze out every last bit of performance from the models. You may understand what sort of features you must create, or what kind of modeling techniques should be applied, and so on.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_079.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code for visualizing time series, use the <span class="s20">01-Visualizing Time Series.ipynb </span>notebook in the <span class="s20">chapter03 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In this chapter, we will cover a few visualization techniques that are well suited for time series datasets.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Line charts</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is the most basic and common visualization that is used for understanding a time series. We just plot the time on the <i>X</i>-axis and the time series value on the <i>Y</i>-axis. Let’s see what it looks like if we plot one of the households from our dataset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 34pt;text-indent: 0pt;text-align: left;"><span><img width="510" height="270" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_080.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.3 – Line plot of household MAC000193</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">When you have a long time series with high variation, as we have, the line plot can get a bit chaotic. One of the options to get a macroview of the time series in terms of trends and movement is to plot a smoothed version of the time series. Let’s see what a rolling monthly average of the time series look like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;text-align: left;"><span><img width="518" height="267" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_081.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.4 – Rolling monthly average energy consumption of household MAC000193</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see the macro patterns much more clearly now. The seasonality is clear – the series peaks in winter and troughs during summer. And if you think about it critically, it makes sense. This is London we are talking about, and the energy consumption would be higher during the winter because of lower temperatures and subsequent heating system usage. For a household in the tropics, for example, the pattern may be reversed, with the peaks coming in summer when air conditioners come into play.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another use for the line chart is to visualize two or more time series together and investigate any correlations between them. In our case, let’s try plotting the temperature along with the energy consumption and see if the hypothesis we have about temperature influencing energy consumption holds good:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 61pt;text-indent: 0pt;text-align: left;"><span><img width="461" height="518" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_082.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.5 – Temperature and energy consumption (zoomed-in plot at the bottom)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark118">Here, we can see a clear negative correlation in yearly resolution between energy consumption and temperature. Winters show higher energy consumption on a macro scale. We can also see the daily patterns that are loosely correlated with temperature, but maybe because of other factors such as people coming back home after work and so on.</a><a name="bookmark98">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are a few other visualizations that are more suited to bringing out seasonality in a time series. Let’s take a look.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Seasonal plots</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">A <span class="s5">seasonal plot </span>is very similar to a line plot, but the key difference here is that the <i>X</i>-axis denotes the “seasons”, the <i>Y</i>-axis denotes the time series value, and the different seasonal cycles are represented in different colors or line types. For instance, the yearly seasonality at a monthly resolution can be depicted with months on the <i>X</i>-axis and different years in different colors.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s see what this looks like for our household in question. Here, we have plotted the average monthly energy consumption across multiple years:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 34pt;text-indent: 0pt;text-align: left;"><span><img width="513" height="255" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_083.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.6 – Seasonal plot at a monthly resolution</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can instantly see the appeal in this visualization because it lets us visualize the seasonality pattern easily. We can see that the consumption goes down in the summer months and we can also see that it happens consistently across multiple years. In the 2 years that we have data for, we can see that in October, the behavior in 2013 slightly deviated from 2012. Maybe there is something else that can help us explain this difference – what about temperature? We can also plot the seasonal plots with another variable of interest, such as the temperature:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 42pt;text-indent: 0pt;text-align: left;"><a name="bookmark119"><span><img width="509" height="278" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_084.jpg"/></span></a><a name="bookmark99">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.7 – Seasonal plot at a monthly resolution (energy consumption versus temperature)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Notice October? In October 2013, the temperature stayed warmer for 1 month more, hence why the energy consumption pattern was slightly different from last year.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can plot these kinds of plots at other resolutions as well, such as hourly seasonality. But when there are too many seasonal cycles to be plotted, it increases visual clutter. An alternative to a seasonal plot is a seasonal box plot.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Seasonal box plots</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Instead of plotting the different seasonal cycles in different colors or line types, we can represent them as a box plot. This instantly clears up the clutter in the plot. The additional benefit you get from this representation is that it lets us understand the variability across seasonal cycles:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><a name="bookmark120"><span><img width="487" height="514" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_085.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.8 – Seasonal plot (top) and seasonal box plot (bottom) at an hourly resolution</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, we can see that the seasonal plot at this resolution is too cluttered to make out the pattern and the variation across seasonal cycles. However, the seasonal box plot is much more informative. The horizontal line in the box tells us about the median, the box is the <span class="s5">interquartile range </span>(<span class="s5">IQR</span>), and the points that are marked are the outliers. By looking at the medians, we can see that the peak consumption occurs from 9 A.M. onward. But the variability is also higher from 9 A.M. If you plot separate box plots for each week, for example, you will see that the patterns are slightly different on Sundays (additional visualizations are in the associated notebook).</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">However, there is another visualization that lets you inspect these patterns along two dimensions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark121">Calendar heatmaps</a><a name="bookmark101">&zwnj;</a><a name="bookmark100">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Instead of having separate box plots or separate line charts for each week of the day, it would be useful if we could condense that information into a single plot. This is where <span class="s5">calendar heatmaps </span>come in. A calendar heatmap uses colored cells in a rectangular block to represent the information. Along the two sides of the rectangle, we can find two separate granularities of time, such as month and year. In each intersection, the cell is colored relative to the value of the time series at that intersection.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s look at the hourly average energy consumption across the different weekdays in a calendar heatmap:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="517" height="243" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_086.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.9 – A calendar heatmap for energy consumption</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">From the color scale on the right, we know that lighter colors mean higher values. We can see how Monday to Saturday have similar peaks – that is, once in the morning and once in the evening. However, Sunday has a slightly different pattern, with higher consumption throughout the day.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">So far, we’ve reviewed a lot of visualizations that can bring out seasonality. Now, let’s look at a visualization for inspecting autocorrelation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Autocorrelation plot</p><p class="s4" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark137" class="s140">If correlation indicates the strength and direction of the linear relationship between two variables, autocorrelation is the correlation between the values of a time series in successive periods. Most time series have a heavy dependence on the value in the previous period, and this is a critical component in a lot of the forecasting models we will be seeing as well. Something such as ARIMA (which we will briefly look at in </a>Chapter 4<span class="p">, </span>Setting a Strong Baseline Forecast<span class="p">) is built on autocorrelation. So, it’s always helpful to just visualize and understand how strong the dependence on previous time steps is.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark122">This is where </a><span class="s5">autocorrelation plots </span>come in handy. In such plots, we have the different lags (<i>t-1</i>, <i>t-2</i>, <i>t-3</i>, and so on) on the <i>X</i>-axis and the correlations between <i>t </i>and the different lags on the <i>Y</i>-axis. In addition to autocorrelation, we can also look at <span class="s5">partial autocorrelation</span>, which is very similar to autocorrelation but with one key difference: partial autocorrelation removes any indirect correlation that may be present before presenting the correlations. Let’s look at an example to understand this. If <i>t </i>is the current time step, let’s assume <i>t-1 </i>is highly correlated to <i>t</i>. So, by extending this logic, <i>t-2 </i>will be highly correlated with <i>t-1 </i>and because of this correlation, the autocorrelation between <i>t </i>and <i>t-2 </i>would be high. However, partial autocorrelation corrects this and extracts the correlation, which can be purely attributed to <i>t-2 </i>and <i>t</i>.</p><p class="s4" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark211" class="s140">One thing we need to keep in mind is that the autocorrelation and partial autocorrelation analysis works best if the time series is stationary (we will talk about stationarity in detail in </a>Chapter 6<span class="p">, </span>Feature Engineering for Time Series Forecasting<span class="p">).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="106" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_087.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Best practice</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">There are many ways of making a series stationary, but a quick and dirty way is to use seasonal decomposition and just pick the residuals. It should be devoid of trends and seasonality, which are the major drivers of non-stationarity in a time series. But as we will see later in this book, this is not a foolproof method of making a series stationary in the truest sense.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Now, let’s see what these plots look like for our household from the dataset (after making it stationary):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><span><img width="516" height="240" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_088.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.10 – Autocorrelation and partial autocorrelation plots</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark123">Here, we can see that the first lag (</a><i>t-1</i>) has the most influence and that its influence quickly drops down to close to zero in the partial autocorrelation plot. This means that the energy consumption of a day is highly correlated with the energy consumption the day before.<a name="bookmark103">&zwnj;</a><a name="bookmark102">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">With that, we’ve looked at the different components of a time series and learned how to visualize a few of them. Now, let’s see how we can decompose a time series into its components.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Decomposing a time series</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Seasonal decomposition is the process by which we deconstruct a time series into its components – typically, trend, seasonality, and residuals. The general approach for decomposing a time series is as follows:</p><ol id="l25"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Detrending<span class="p">: Here, we estimate the </span>trend component <span class="p">(which is the smooth change in the time series) and remove it from the time series, giving us a </span>detrended time series<span class="p">.</span></p></li><li><p class="s5" style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Deseasonalizing<span class="p">: Here, we estimate the seasonality component from the detrended time series. After removing the seasonal component, what is left is the residual.</span></p></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s discuss them in detail.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Detrending</p><p class="s5" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Detrending <span class="p">can be done in a few different ways. Two popular ways of doing it are by using </span>moving averages <span class="p">and </span>locally estimated scatterplot smoothing <span class="p">(</span>LOESS<span class="p">) </span>regression<span class="p">.</span></p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Moving averages</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">One of the easiest ways of estimating trends is by using a moving average along the time series. It can be seen as a window that is moved along the time series in steps, and at each step, the average of all the values in the window is recorded. This moving average is a smoothed-out time series and helps us estimate the slow change in time series, which is the trend. The downside is that the technique is quite noisy. Even after smoothing out a time series using this technique, the extracted trend will not be smooth; it will be noisy. The noise should ideally reside with the residuals and not the trend (see the trend line shown in <i>Figure 3.11</i>).</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">LOESS</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The <span class="s5">LOESS </span>algorithm, which is also called <i>locally weighted polynomial regression</i>, was developed by Bill Cleveland through the 70s to the 90s. It is a non-parametric method that is used to fit a smooth curve onto a noisy signal. We use an ordinal variable that moves between the time series as the independent variable and the time series signal as the dependent variable. For each value in the ordinal variable, the algorithm uses a fraction of the closest points and estimates a smoothed trend using only those points in a weighted regression. The weights in the weighted regression are the closest points to the point in question. This is given the highest weight and it decays as we move farther away from it. This gives us a very effective tool for modeling the smooth changes in the time series (trend).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark124">Deseasonalizing</a><a name="bookmark104">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The seasonality component can also be estimated in a few different ways. The two most popular ways of doing this are by using period-adjusted averages or a Fourier series.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Period adjusted averages</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This is a pretty simple technique wherein we calculate a seasonality index for each period in the expected cycle by taking the average values of all such periods over all the cycles. To make that clear, let’s look at a monthly time series where we expect an annual seasonality in this time series. So, the up-and-down pattern would complete a full cycle in 12 months, or the seasonality period is 12. In other words, every 12 points in the time series have similar seasonal components. So, we take the average of all January values as the period-adjusted average for January. In the same way, we calculate the period average for all 12 months. At the end of the exercise, we have 12 period averages, and we can also calculate an <i>average </i>period average. Now, we can make these period averages into an index by either subtracting the average of all period averages from each of the period averages (for additive) or dividing the average of all period averages from each of the period averages (multiplicative).</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Fourier series</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the late 1700s, Joseph Fourier, a mathematician and physicist, while studying heat flow, realized something profound – <i>any </i>periodic function can be broken down into a simple series of sine and cosine waves. Let’s dwell on that for a minute. Any periodic function, no matter the shape, curve, or absence of it, or how wildly it oscillates around the axis, can be broken down into a series of sine and cosine waves.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="122" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_089.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional information</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">For the more mathematically inclined, the original theory proposes to decompose any periodic function into an integral of exponentials. Using Euler’s identity,   <span class="s50">𝑒𝑒𝑖𝑖𝑖𝑖 = 𝑐𝑐𝑐𝑐𝑐𝑐(𝑦𝑦) + 𝑖𝑖 ⋅ 𝑐𝑐𝑖𝑖𝑠𝑠(𝑦𝑦)   </span>, we can consider them as a summation of sine and cosine waves. The <i>Further reading </i>section contains a few resources if you want to delve deeper and explore related concepts, such as the Fourier transform.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">It is this property that we use to extract seasonality from a time series because seasonality is a periodic function, and any periodic function can be approximated by a combination of sine and cosine waves. The sine-cosine form of a Fourier series is as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s50" style="text-indent: 0pt;line-height: 5pt;text-align: right;">𝑠𝑠</p><p class="s51" style="padding-top: 2pt;text-indent: 0pt;line-height: 10pt;text-align: right;"> <span class="s52">𝑎𝑎</span><span class="s53">0</span></p><p class="s50" style="padding-left: 2pt;text-indent: 0pt;line-height: 2pt;text-align: center;">=</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s50" style="padding-left: 3pt;text-indent: 0pt;line-height: 5pt;text-align: left;">+  Σ𝑁𝑁     (𝑎𝑎</p><p class="s50" style="padding-top: 2pt;padding-left: 168pt;text-indent: 0pt;line-height: 9pt;text-align: center;">2π</p><ul id="l26"><li><p class="s50" style="padding-left: 9pt;text-indent: -4pt;line-height: 3pt;text-align: left;">𝑐𝑐𝑐𝑐𝑠𝑠 (      ⋅ 𝑛𝑛 ⋅ 𝑥𝑥) + 𝑏𝑏</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="14" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_090.png"/></span></p><p class="s50" style="padding-top: 2pt;padding-left: 27pt;text-indent: 0pt;line-height: 9pt;text-align: left;">2π</p></li><li><p class="s50" style="padding-left: 9pt;text-indent: -4pt;line-height: 3pt;text-align: left;">𝑠𝑠𝑠𝑠𝑛𝑛 (      ⋅ 𝑛𝑛 ⋅ 𝑥𝑥))</p><p style="padding-left: 27pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="14" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_091.png"/></span></p><p class="s54" style="text-indent: 0pt;line-height: 44%;text-align: right;">𝑁𝑁(𝑥𝑥) <span class="s55">2</span></p><p class="s54" style="padding-left: 21pt;text-indent: 0pt;line-height: 5pt;text-align: left;">𝑛𝑛=1</p><p class="s54" style="text-indent: 0pt;line-height: 5pt;text-align: right;">𝑛𝑛</p><p class="s50" style="text-indent: 0pt;line-height: 10pt;text-align: right;">𝑃𝑃</p><p class="s54" style="text-indent: 0pt;line-height: 5pt;text-align: right;">𝑛𝑛</p><p class="s50" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">𝑃𝑃</p><p style="text-indent: 0pt;text-align: left;"><span><img width="18" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_092.png"/></span></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, is the <i>N</i>-term approximation of the signal, <i>S</i>. Theoretically, when <i>N </i>is infinite, the resulting approximation is equal to the original signal. <i>P </i>is the maximum length of the cycle.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_093.png"/></span></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can use this Fourier series, or a few terms from the Fourier series, to model our seasonality. In our application, <i>P </i>is the maximum length of the cycle we are trying to model. For instance, for a yearly seasonality for monthly data, the maximum length of the cycle (<i>P</i>) is 12. <i>x </i>would be an ordinal variable that increases from <i>1 </i>to <i>P</i>. In this example, <i>x </i>would be <i>1</i>, <i>2</i>, <i>3</i>, … <i>12</i>. Now, with these terms, all that is left to do is find   and <span><img width="13" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_094.png"/></span>, which we can do by regressing on the signal.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We’ve seen that with the right combination of Fourier terms, we can replicate any signal. But the question is, should we? What we want to learn from data is a generalized seasonality profile that does well with unseen data as well. So, we use <i>N </i>as a hyperparameter to extract as complex a signal as we want from the data.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is a good time to brush up on your trigonometry and remember what sine and cosine waves look like. The first Fourier term (<i>n=1</i>) is your age-old sine and cosine waves, which complete one full cycle in the maximum cycle length (<i>P</i>). As we increase <span class="s20">n</span>, we get sine and cosine waves that have multiple cycles in the maximum cycle length (<i>P</i>). This can be seen in the following figure:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span><img width="520" height="263" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_095.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.11 – Cosine Fourier terms (n=1, 2, 3)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The sine and cosine waves are complementary to each other, as shown in the following figure:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><a name="bookmark125"><span><img width="520" height="263" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_096.jpg"/></span></a><a name="bookmark105">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.12 – Sine and cosine Fourier terms (n=1)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, let’s see how we can use this in practice.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Implementations</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_097.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code for decomposing time series, use the <span class="s20">02-Decomposing Time Series.ipynb </span>notebook in the <span class="s20">chapter03 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are four implementations that we will cover here in the following subsections.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">seasonal_decompose from statsmodel</p><p class="s20" style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">statsmodels.tsa.seasonal <span class="p">has a function called </span>seasonal_decompose<span class="p">. This is an implementation that uses moving averages for the trend component and period-adjusted averages for the seasonal component. It supports both additive and multiplicative modes of decomposition. However, it doesn’t tolerate missing values. Let’s see how we can use it:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Does not support missing values, so using imputed ts instead</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">res = seasonal_decompose(ts, period=7*48, model=&quot;additive&quot;, extrapolate_trend=&quot;freq&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark126">A few key parameters to keep in mind are as follows:</a></p><ul id="l27"><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">period <span class="p">is the seasonal period you expect the pattern to repeat.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">model <span class="p">takes </span>additive <span class="p">or </span>multiplicative <span class="p">as arguments to determine the type of decomposition.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">filt <span class="p">takes in an array that is used as the weights in the moving average (convolution, to be specific). It can also be used to define the window over which we need our moving average. We can increase it to smooth out the trend component to some extent.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">extrapolate_trend <span class="p">is a parameter that we can use to extend the trend component to both sides to avoid the missing values that are generated when applying the moving average filter.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">two_sided <span class="p">is a parameter that lets us define how the moving averages are calculated. If </span>True<span class="p">, which it is by default, the moving average is calculated using the past as well as future values because the window for the moving average is centered. If </span>False<span class="p">, it only uses past values to calculate the moving average.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see how well we have been able to decompose one of the time series in our datasets. We used <span class="s20">period=7*48 </span>to capture a weekday-hourly profile and <span class="s20">filt=np.repeat(1/(30*48), 30*48) </span>to make the moving average over 30 days with uniform weights:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="288" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_098.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.13 – Seasonal decomposition using statsmodels</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark127">We can’t see the seasonal pattern because it’s too small in the grand scale of the plot. The associated notebook has zoomed-in plots to help you understand the seasonal pattern. Even with a large window (for example, 20 days) of smoothing, the trend still has some noise in it. We may be able to reduce this a bit more by increasing the window, but there is a better alternative, as we will see now.</a></p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Seasonality and trend decomposition using LOESS (STL)</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">As we saw earlier, LOESS is much more suited for trend estimation. <span class="s5">Seasonality and trend decomposition using LOESS </span>(<span class="s5">STL</span>) is an implementation that uses LOESS for trend estimation and period averages for seasonality. Although <span class="s20">statsmodels </span>has an implementation, we have reimplemented it for better performance and flexibility. This implementation can be found in this book’s GitHub repository under <span class="s20">src.decomposition.seasonal.py</span>. It expects a pandas DataFrame or series with a datetime index as an input. Let’s see how we can use this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">stl = STL(seasonality_period=7*48, model = &quot;additive&quot;) res_new = stl.fit(ts_df.energy_consumption)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The key parameters here are as follows:</p><ul id="l28"><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">seasonality_period <span class="p">is the seasonal period you expect the pattern to repeat.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">model <span class="p">takes </span>additive <span class="p">or </span>multiplicative <span class="p">as arguments to determine the type of decomposition.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">lo_frac <span class="p">is the fraction of the data that will be used to fit the LOESS regression.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">lo_delta <span class="p">is the fractional distance within which we use a linear interpolation instead of weighted regression. Using a non-zero </span>lo_delta <span class="p">significantly decreases computation time.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s see what this decomposition looks like. Here, we used <span class="s20">seasonality_period=7*48 </span>to capture a weekday-hourly profile:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 42pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="288" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_099.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.14 – STL decomposition</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: center;">Let’s also look at the decomposition for just 1 month to see the extracted seasonality patterns clearer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="294" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_100.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.15 – STL decomposition (zoomed-in for a month)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark128">The trend is smooth enough now and seasonality has also been captured. Here, we can clearly see the hourly peaks and valleys and the higher peaks on weekends. But since we are relying on averages to derive the seasonality, it is also highly influenced by outliers. A few very high or very low values in the time series will skew your seasonality profile that’s been derived from period averages. Another disadvantage of this technique is that the “goodness” of the seasonality that’s been extracted suffers when the difference between the resolution of the data and the expected seasonality cycle is greater. For instance, when extracting a yearly seasonality on daily or sub-daily data, this would make the extracted seasonality very noisy. This technique will also not work if you have less than two cycles of the expected seasonality – for instance, if we want to extract a yearly seasonality, but we have less than 2 years of data.</a></p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Fourier decomposition</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can find the Python implementation for decomposing a time series using Fourier terms in <span class="s20">src. decomposition.seasonal.py</span>. It uses LOESS for trend detection and Fourier terms for seasonality extraction. There are two ways we can use it. First, we can specify <span class="s20">seasonality_period </span>as one of the <span class="s20">pandas </span>datetime properties (such as <span class="s20">hour</span>, <span class="s20">week_of_day</span>, and so on):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">stl = FourierDecomposition(seasonality_period=&quot;hour&quot;, model = &quot;additive&quot;, n_fourier_terms=5)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">res_new = stl.fit(pd.Series(ts.squeeze(), index=ts_df.index))</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Alternatively, we can create any custom seasonality array that’s the same length as the time series that has an ordinal representation of the seasonality. If it is an annual seasonality of daily data, the array would have a minimum value of <span class="s20">1 </span>and a maximum value of <span class="s20">365 </span>as it increases by one every day of the year:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">#Making a custom seasonality term <span class="s28">ts_df[&quot;dayofweek&quot;] = ts_df.index.dayofweek ts_df[&quot;hour&quot;] = ts_df.index.hour</span></p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">#Creating a sorted unique combination df</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">map_df = ts_df[[&quot;dayofweek&quot;,&quot;hour&quot;]].drop_duplicates().sort_ values([&quot;dayofweek&quot;, &quot;hour&quot;])</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Assigning an ordinal variable to capture the order</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">map_df[&quot;map&quot;] = np.arange(1, len(map_df)+1)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># mapping the ordinal mapping back to the original df and getting the seasonality array</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">seasonality = ts_df.merge(map_df, on=[&quot;dayofweek&quot;,&quot;hour&quot;], how=&#39;left&#39;, validate=&quot;many_to_one&quot;)[&#39;map&#39;]</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">stl = FourierDecomposition(model = &quot;additive&quot;, n_fourier_ terms=50)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">res_new = stl.fit(pd.Series(ts, index=ts_df.index), seasonality=seasonality)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark129">The key parameters that are involved in this process are as follows:</a></p><ul id="l29"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s20">seasonality_period </span>is the seasonality to be extracted from the <i>datetime index</i>. <span class="s20">pandas</span><span class="s27"> </span>datetime properties such as <span class="s20">week_of_day</span>, <span class="s20">month</span>, and so on can be used to specify the most prominent seasonality. If left set to <span class="s20">None</span>, you need to provide the seasonality array while calling <span class="s20">fit</span>.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">model <span class="p">takes </span>additive <span class="p">or </span>multiplicative <span class="p">as arguments to determine the type of decomposition.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">n_fourier_terms <span class="p">determines the number of Fourier terms to be used to extract the seasonality. The more we increase this parameter, the more complex the seasonality that is extracted from the data.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">lo_frac <span class="p">is the fraction of the data that will be used to fit the LOESS regression.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">lo_delta <span class="p">is the fractional distance within which we use linear interpolation instead of weighted regression. Using a non-zero </span>lo_delta <span class="p">significantly decreases computation time.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s see the zoomed-in plot for the decomposition using <span class="s20">FourierDecomposition</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="294" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_101.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.16 – Decomposition using Fourier terms (zoomed-in for a month)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark130">The trend is going to be the same as the STL one because we are using LOESS here as well. The seasonality profile may be slightly different and robust to outliers because we are doing regularized regression using the Fourier terms on the signal. Another advantage is that we have decoupled the resolution of the data and the expected seasonality. Now, extracting a yearly seasonality on sub-daily data is not as challenging as with period averages.</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">So far, we have only seen techniques that extract one seasonality per series; mostly, we extract the major seasonality. So, what do we do when we have multiple seasonal patterns?</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Multiple seasonality decomposition using LOESS (MSTL)</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Time series with high-frequency data (such as daily, hourly, or minutely data) are prone to exhibit multiple seasonal patterns. For instance, there may be an hourly seasonality pattern, a weekly seasonality pattern, and a yearly seasonality pattern. But if we extract only the dominant pattern and leave the rest to residuals, we are not doing justice to the decomposition. Kasun Bandara et al. proposed an extension of STL decomposition for multiple seasonality, known as <span class="s5">multiple seasonal-trend decomposition using LOESS </span>(<span class="s5">MSTL</span>), and a corresponding implementation is present in the R ecosystem. A very similar implementation in Python can be found in <span class="s20">src.decomposition.seasonal.py</span>. In addition to MSTL, the implementation extracts multiple seasonality using Fourier terms.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_102.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Kasun Bandara et al. is cited in the <i>References </i>section as reference <i>1</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s look at an example of how we can use this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">stl = MultiSeasonalDecomposition(seasonal_ model=&quot;fourier&quot;,seasonality_periods=[&quot;day_of_year&quot;, &quot;day_of_ week&quot;, &quot;hour&quot;], model = &quot;additive&quot;, n_fourier_terms=10)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">res_new = stl.fit(pd.Series(ts, index=ts_df.index))</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The key parameters here are as follows:</p></li></ul></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">seasonality_periods <span class="p">is the list of expected seasonalities. For </span>STL<span class="p">, it is a list of seasonal periods, while for </span>FourierDecomposition<span class="p">, it is a list of strings that denotes </span>pandas <span class="p">datetime properties.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">seasonality_model <span class="p">takes </span>fourier <span class="p">or </span>averages <span class="p">as arguments to determine the type of seasonality decomposition.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">model <span class="p">takes </span>additive <span class="p">or </span>multiplicative <span class="p">as arguments to determine the type of decomposition.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">n_fourier_terms <span class="p">determines the number of Fourier terms to be used to extract the seasonality. As we increase this parameter, the more complex the seasonality that is extracted from the data.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l30"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;"><a name="bookmark131">lo_frac </a><span class="p">is the fraction of the data that will be used to fit the LOESS regression.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">lo_delta <span class="p">is the fractional distance within which we use linear interpolation instead of weighted regression. Using a non-zero </span>lo_delta <span class="p">significantly decreases computation time.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s see what the decomposition looks like when using Fourier decomposition:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="288" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_103.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.17 – Multiple seasonality decomposition using Fourier terms</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Here, we can see that the <span class="s20">day_of_week </span>seasonality has been extracted. To see the <span class="s20">day_of_week</span></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">and <span class="s20">hour </span>seasonal components, we need to zoom in a bit:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a name="bookmark132"><span><img width="501" height="283" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_104.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.18 – Multiple seasonality decomposition using Fourier terms (zoomed-in for a month)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, we can observe that the <span class="s20">hour </span>seasonality has been extracted well and that it has also isolated the <span class="s20">day_of_week </span>seasonal component, which peaks on weekends. The <span class="s5">discrete step </span>nature of the <span class="s20">day_of_week </span>seasonal component is because the frequency of the data is half-hourly, and for 48 data points, <span class="s20">day_of_week </span>will be the same.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We have summarized the four techniques we’ve covered in the following table:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 60pt;text-indent: 0pt;text-align: left;"><span><img width="443" height="236" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_105.gif"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Table 3.1 – Different seasonal decomposition techniques</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s understand and analyze a time series dataset.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark133">Detecting and treating outliers</a><a name="bookmark107">&zwnj;</a><a name="bookmark106">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">An <span class="s5">outlier</span>, as its name suggests, is an observation that lies at an abnormal distance from the rest of the observations. If we are looking at a <span class="s5">data generating process </span>(<span class="s5">DGP</span>) as a stochastic process that generates the time series, the outliers are the points that have the least probability of being generated from the DGP. This can be for many reasons, including faulty measurement equipment, incorrect data entry, and black-swan events, to name a few. Being able to detect such outliers and <i>treat </i>them may help your forecasting model understand the data better.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Outlier/anomaly detection is a specialized field itself in time series, but in this book, we are going to restrict ourselves to simpler techniques of identifying and treating outliers. This is because our main aim is not to detect outliers, but to clean the data for our forecasting models to perform better. If you want to learn more about anomaly detection, head over to the <i>Further reading </i>section for a few resources to get started.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_106.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code for detecting outliers, use the <span class="s20">03-Outlier Detection. ipynb </span>notebook in the <span class="s20">chapter03 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at a few techniques for identifying outliers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Standard deviation</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;line-height: 90%;text-align: justify;">This is a rule of thumb that almost everyone who has worked with data for some time would have heard of – if <span class="s56">μ </span>is the mean of the time series and <span class="s57">σ </span>is the standard deviation, then anything that falls beyond <span class="s58">μ ± 3 σ </span>is an <span class="s5">outlier</span>. The underlying theory is deeply rooted in statistics. If we assume that the values of the time series follow a normal distribution (which is a symmetrical distribution with very desirable</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">properties), using probability theory, we can derive that 68% of the area under the normal distribution lies within one standard deviation on either side of the mean, about 95% of the area within two standard deviations, and about 99% of the area within three standard deviations. So, when we make the bounds as three standard deviations (by using the rule of thumb), what we are saying is that if any observation whose probability of belonging to the probability distribution is less than 1%, then they are an outlier. Moving slightly to more practical issues, this cutoff of three standard deviations is in no way sacrosanct. We need to try out different values of this multiple and determine the right multiple by subjectively evaluating the results we get. The higher the multiple is, the fewer outliers there will be.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">For highly seasonal data, the naïve way of applying the rule to the raw time series will not work well. In such cases, we must deseasonalize the data using any of the techniques we discussed earlier and then apply the outliers to the residuals. If we don’t do that, we may flag a seasonal peak as an outlier, which is not what we want.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another key assumption here is the normal distribution. However, in reality, a lot of the time series we come across may not be normal and hence the rule will lose its theoretical guarantees fast.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark134">Detecting and treating outliers 73</a><a name="bookmark110">&zwnj;</a><a name="bookmark109">&zwnj;</a><a name="bookmark108">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_107.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Interquartile range (IQR)</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Another very similar technique is using the IQR instead of the standard deviation to define the bounds beyond which we mark the observations as outliers. IQR is the difference between the 3rd quartile (or the 75th percentile or 0.75 quantile) and the 1st quartile (or the 25th percentile or 0.25 quantile). The upper and lower bounds are defined as follows:</p></li></ul></li><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Upper bound = Q3 + n x IQR</p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Lower bound = Q1 - n x IQR</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, <i>IQR = Q3-Q2</i>, and <i>n </i>is the multiple of IQRs that determines the width of the acceptable area.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 91%;text-align: justify;">For datasets where we observe high occurrences of outliers and wild variations, this is slightly more robust than the standard deviation. This is because the standard deviation and the mean are highly influenced by individual points in the dataset. If 2 <span class="s56">σ </span>was the rule of thumb in the earlier method, here, it is 1.5 times the IQR. This also ties back to the same normal distribution assumption, and 1.5 times the IQR is equivalent to ~3 <span class="s57">σ </span>(2.7 <span class="s56">σ </span>to be exact). The point about deseasonalizing before applying the rule applies here as well. It applies to all the techniques we will see here.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Isolation Forest</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s5">Isolation Forest </span>is an unsupervised anomaly detection algorithm based on decision trees. A typical anomaly detection algorithm models the <i>normal </i>points and profiles outliers as any points that do not fit the <i>normal. </i>But Isolation Forest takes a different path and models the outliers directly. It does this by creating a forest of decision trees by randomly splitting the feature space. This technique works on the assumption that the outlier points fall in the outer periphery and are easier to fall into a leaf node of a tree. Therefore, you can find the outliers in short branches, whereas normal points, which are closer together, will require longer branches. The “anomaly score” of any point is determined by the depth of the tree to be traversed before reaching that particular point. scikit-learn has an implementation of the algorithm under <span class="s20">sklearn.ensemble.IsolationForest</span>. Apart from the standard parameters for decision trees, the key parameter here is contamination. It is set to <span class="s20">auto </span>by default but can be set to any value between 0 and 0.5. This parameter specifies what percentage of the dataset you expect to be anomalous. But one thing we have to keep in mind is that <span class="s20">IsolationForest </span>does not consider time at all and just highlights values that fall <i>outside the norm</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Extreme studentized deviate (ESD) and seasonal ESD (S-ESD)</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This statistics-based technique is more sophisticated than the basic <span class="s59">± σ </span>technique, but still uses the same assumption of normality. It is based on another statistical test, called Grubbs&#39;s Test, which is used to find a <i>single outlier </i>in a normally distributed dataset. ESD iteratively uses Grubbs&#39;s test by identifying and removing an outlier at each step. It also adjusts the critical value based on the number of points left. For a more detailed understanding of the test, go to the <i>Further reading </i>section, where we have provided a couple of resources about ESD and S-ESD. In 2017, Hochenbaum et al. from Twitter Research proposed to use the generalized ESD with deseasonalization as a method of detecting outliers for time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark135">We have adapted an existing implementation of the algorithm for our use case, and it is available in this book’s GitHub repository. While all the other methods leave it to the user to determine the right level of outliers by tweaking a few parameters, S-ESD only takes in an upper bound on the number of expected outliers and then identifies the outliers independently. For instance, we set the upper bound to 800 and the algorithm identified ~400 outliers in the data we are working with.</a><a name="bookmark111">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_108.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Hochenbaum et al. is cited in the <i>References </i>section as reference <i>2</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see how the outliers were detected using all the techniques we have reviewed:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 76pt;text-indent: 0pt;text-align: left;"><span><img width="417" height="300" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_109.gif"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 3.19 – Outliers detected using different techniques</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we’ve learned how to detect outliers, let’s talk about how we can treat them and clean the dataset.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Treating outliers</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The first question that we must answer is whether or not we should correct the outliers we have identified. The statistical tests that identify outliers automatically should go through another level of human verification. If we blindly “treat” outliers, we might be chopping off a valuable pattern that will help us forecast the time series. If you are only forecasting a handful of time series, then it still makes sense to look at the outliers and anchor them to reality by looking at the causes for such outliers.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark136">Summary 75</a><a name="bookmark114">&zwnj;</a><a name="bookmark113">&zwnj;</a><a name="bookmark112">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_110.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">But when you have thousands of time series, a human can’t inspect all the outliers, so we will have to resort to automated techniques. A common practice is to replace an outlier with a heuristic such as the maximum, minimum, 75th percentile, and so on. A better method is to consider the outliers as missing data and use any of the techniques we discussed earlier to impute the outliers.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">One thing we must keep in mind is that outlier correction is not a necessary step in forecasting, especially when using modern methods such as machine learning or deep learning. Whether we do outlier correction or not is something we have to experiment with and figure out.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Well done! This was a pretty busy chapter, with a lot of concepts and code, so congratulations on finishing it. Feel free to head back and revise a few topics as needed.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we learned about the key components of a time series and familiarized ourselves with terms such as trend, seasonality, and so on. We also reviewed a few time series-specific visualization techniques that will come in handy during EDA. Then, we learned about techniques that let you decompose a time series into its components and saw techniques for detecting outliers in the data. Finally, we learned how to treat the identified outliers. Now, you are all set to start forecasting the time series, which we will start in the next chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The following are the references for this chapter:</p><ol id="l31"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Kasun Bandara and Rob J Hyndman and Christoph Bergmeir. (2021). <i>MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns</i><a href="https://arxiv.org/abs/2107.13462" class="s140" target="_blank">. arXiv:2107.13462 [stat.AP]. </a><span class="s27">https://arxiv.org/abs/2107.13462</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Hochenbaum, J., Vallis, O., &amp; Kejariwal, A. (2017). <i>Automatic Anomaly Detection in the Cloud Via Statistical Learning</i><a href="https://arxiv.org/abs/1704.07706" class="s140" target="_blank">. ArXiv, abs/1704.07706. </a><span class="s27">https://arxiv.org/abs/1704.07706</span>.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">To learn more about the topics that were covered in this chapter, take a look at the following resources:</p></li><li><p class="s27" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><a href="https://www.setzeus.com/public-blog-post/the-fourier-series" class="s140" target="_blank">Fourier Series: </a><a href="https://www.setzeus.com/public-blog-post/the-fourier-series" class="a" target="_blank">https://www.setzeus.com/public-blog-post/the-fourier- </a>series<span class="p">.</span></p></li><li><p class="s27" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><a href="https://www.youtube.com/watch?v=UKHBWzoOKsY" class="s140" target="_blank">Fourier Series from Khan Academy: </a><a href="https://www.youtube.com/watch?v=UKHBWzoOKsY" class="a" target="_blank">https://www.youtube.com/ </a>watch?v=UKHBWzoOKsY<span class="p">.</span></p></li><li><p class="s27" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/" class="s140" target="_blank">Fourier Transform - </a><a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/" class="a" target="_blank">https://betterexplained.com/articles/an-interactive- </a>guide-to-the-fourier-transform/<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l32"><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Ane Blázquez-García, Angel Conde, Usue Mori, and Jose A. Lozano. (2021). <i>A Review on Outlier/Anomaly Detection in Time Series Data. </i><a href="https://arxiv.org/abs/2002.04236" class="s140" target="_blank">arXiv:2002.04236. </a><a href="https://arxiv.org/abs/2002.04236" class="a" target="_blank">https://arxiv.org/ </a><span class="s27">abs/2002.04236</span>.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Braei, M., &amp; Wagner, S. (2020). <i>Anomaly Detection in Univariate Time-series: A Survey on the State-of-the-Art</i><a href="https://arxiv.org/abs/2004.00433" class="s140" target="_blank">. ArXiv, abs/2004.00433. </a><span class="s27">https://arxiv.org/abs/2004.00433</span>.</p></li><li><p class="s27" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm" class="s140" target="_blank">Generalized ESD Test for Outliers: </a><a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm" class="a" target="_blank">https://www.itl.nist.gov/div898/handbook/ </a>eda/section3/eda35h3.htm<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark137">4</a><a name="bookmark138">&zwnj;</a><a name="bookmark139">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 178pt;text-indent: 16pt;line-height: 114%;text-align: left;">Setting a Strong Baseline Forec ast </h4><p style="padding-top: 24pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous chapter, we saw some techniques we can use to understand <span class="s5">time series data</span>, do some <span class="s5">Exploratory Data Analysis </span>(<span class="s5">EDA</span>), and so on. But now, let’s get to the crux of the matter – <span class="s5">time series forecasting</span>. The point of understanding the dataset and looking at patterns, seasonality, and so on was to make the job of forecasting that series easier. And with any machine learning exercise, one of the first things we need to establish before going further is a <span class="s5">baseline</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">A baseline is a simple model that provides reasonable results without requiring a lot of time to come up with them. Many people think of baselines as something that is derived from common sense, such as an average or some rule of thumb. But as a best practice, a baseline can be as sophisticated as we want it to be, so long as it is quickly and easily implemented. Any further progress we want to make will be in terms of the performance of this baseline.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will look at a few classical techniques that can be used as baselines, and a strong baseline at that. Some may feel that the forecasting techniques we will be discussing in this chapter shouldn’t be baselines, but we are keeping them in here because these techniques have stood the test of time – and for good reason. They are also very mature and can be applied with very little effort, thanks to the awesome open source libraries that implement them. There can be many types of problems/ datasets where it is difficult to beat the baseline techniques we will discuss in this chapter, and in those cases, there is no shame in just sticking to one of these baseline techniques.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will cover the following topics:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Setting up a test harness</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Generating strong baseline forecasts</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Assessing the forecastability of a time series</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark161">Technical requirements</a><a name="bookmark142">&zwnj;</a><a name="bookmark141">&zwnj;</a><a name="bookmark140">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">You will need to set up the <span class="s5">Anaconda </span>environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">To run the notebooks for this chapter, you need to run the <span class="s48">02-Preprocessing London Smart Meter Dataset.ipynb </span>preprocessing notebook from <span class="s48">Chapter02</span>.</p><p class="s46" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter04" class="s140" target="_blank">The code for this chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter04" class="s45" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter04<span class="p">.</span></p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Setting up a test harness</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Before we start forecasting and setting up baselines, we need to set up a <span class="s5">test harness</span>. In software testing, a test harness is a collection of code and the inputs that have been configured to test a program under various situations. In terms of machine learning, a test harness is a set of code and data that can be used to evaluate algorithms. It is important to set up a test harness so that we can evaluate all future algorithms in a standard and quick way.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The first thing we need is <span class="s5">holdout </span>(<span class="s5">test</span>) and <span class="s5">validation </span>datasets.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Creating holdout (test) and validation datasets</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">As a standard practice, in machine learning, we set aside two parts of the dataset, name them <i>validation data </i>and <i>test data</i>, and don’t use them at all to train the model. The validation data is used in the modeling process to assess the quality of the model. To select between different model classes, tune the hyperparameters, perform feature selection, and so on, we need a dataset. Test data is like the final test of your chosen model. It tells you how well your model is doing in unseen data. If validation data is like the mid-term exams, the test data is your final exam.</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark38" class="s140">In regular regression or classification, we usually sample a few records at random and set them aside. But while dealing with time series, we need to respect the temporal aspect of the dataset. Therefore, a best practice is to set aside the latest part of the dataset as the test data. Another rule of thumb is to set equal-sized validation and test datasets so that the key modeling decisions we make based on the validation data are as close as possible to the test data. The dataset that we introduced in </a>Chapter 2<span class="p">, </span>Acquiring and Processing Time Series Data <span class="p">(the London Smart Energy dataset), contains the energy consumption readings of households in London from November 2011 to February 2014. So, we are going to put aside January 2014 as the validation data and February 2014 as the test data.</span></p><p style="padding-top: 3pt;padding-bottom: 2pt;padding-left: 306pt;text-indent: 0pt;text-align: left;"><a name="bookmark162">Setting up a test harness 79</a><a name="bookmark143">&zwnj;</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_111.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Let’s open <span class="s48">01-Setting up Experiment Harness.ipynb </span>from the <span class="s48">chapter04 </span>folder and run it. In the notebook, we must create the train-test split both before and after filling the missing values with <span class="s48">SeasonalInterpolation </span>and save them accordingly. Once the notebook finishes running, you will have created the following files in the preprocessed folder with the 2014 data saved separately:</p></li><li><p class="s48" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">selected_blocks_train.parquet</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">selected_blocks_val.parquet</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">selected_blocks_test.parquet</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">selected_blocks_train_missing_imputed.parquet</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">selected_blocks_val_missing_imputed.parquet</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">selected_blocks_test_missing_imputed.parquet</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now that we have a fixed dataset that can be used to fairly evaluate multiple algorithms, we need a way to evaluate the different forecasts.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Choosing an evaluation metric</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In machine learning, we have a handful of metrics that can be used to measure continuous outputs, mainly <span class="s5">Mean Absolute Error </span>(<span class="s5">MAE</span>) and <span class="s5">Mean Squared Error </span>(<span class="s5">MSE</span><a href="#bookmark711" class="s140">). But in the time series forecasting realm, there are scores of metrics with no real consensus on which ones to use. One of the reasons for this overwhelming number of metrics is that no one metric measures every characteristic of a forecast. Therefore, we have a whole chapter devoted to this topic (</a><i>Chapter 18</i>, <i>Evaluating Forecasts – Forecast Metrics</i>). For now, we will just review a few metrics, all of which we are going to use to measure the forecasts. We are just going to consider them at face value:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_112.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_113.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_114.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_115.png"/></span></p></li><li><p style="padding-top: 9pt;padding-left: 55pt;text-indent: -13pt;line-height: 91%;text-align: justify;"><span class="s5">Mean Absolute Error </span>(<span class="s5">MAE</span>): MAE is a very simple metric. It is the average of the unsigned error between the forecast at timestep <span class="s60">( </span><span class="s37">) </span>and the observed value at time <span class="s61">(</span><span class="s62"> </span><span class="s63">).</span>. The formula is as follows:</p><p class="s64" style="padding-top: 2pt;text-indent: 0pt;line-height: 6pt;text-align: right;">𝑁𝑁</p><p class="s65" style="text-indent: 0pt;line-height: 6pt;text-align: right;">1</p><p class="s64" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">𝐿𝐿</p><p style="text-indent: 0pt;text-align: left;"><span><img width="29" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_116.png"/></span></p><p class="s65" style="padding-left: 12pt;text-indent: 0pt;line-height: 78%;text-align: center;">𝑀𝑀𝑀𝑀𝑀𝑀 = <span class="s66">𝑁𝑁</span> × 𝐿𝐿 × ∑ ∑|𝑓𝑓𝑖𝑖,𝑗𝑗 − 𝑦𝑦𝑖𝑖,𝑗𝑗|</p><p class="s64" style="padding-left: 173pt;text-indent: 0pt;line-height: 5pt;text-align: center;">𝑖𝑖          𝑗𝑗</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 55pt;text-indent: 0pt;text-align: left;">Here, <i>N </i>is the number of time series, <i>L </i>is the length of time series (in this case, the length of the test period), and <i>f </i>and <i>y </i>are the forecast and observed values, respectively.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_117.png"/></span></p></li><li><p class="s5" style="padding-top: 1pt;padding-left: 55pt;text-indent: -13pt;line-height: 14pt;text-align: left;">Mean Squared Error <span class="p">(</span>MSE<span class="p">): MSE is the average of the squared error between the forecast </span><span class="s67">(</span><span class="s68"> </span><span class="s47">)</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_118.png"/></span></p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 14pt;text-align: left;">and observed <span class="s69">( </span><span class="s70">). </span>values:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s54" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝑁𝑁</p><p class="s50" style="text-indent: 0pt;line-height: 7pt;text-align: right;">1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s54" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">𝐿𝐿</p><p class="s54" style="padding-left: 59pt;text-indent: 0pt;line-height: 5pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"><span><img width="31" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_119.png"/></span></p><p class="s50" style="padding-left: 12pt;text-indent: 0pt;line-height: 83%;text-align: center;">𝑀𝑀𝑀𝑀𝑀𝑀  = <span class="s71">𝑁𝑁</span> × 𝐿𝐿 × ∑ ∑(𝑓𝑓𝑖𝑖,𝑗𝑗 − 𝑦𝑦𝑖𝑖,𝑗𝑗)</p><p class="s54" style="padding-left: 173pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝑖𝑖         j</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l33"><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a name="bookmark163">Mean Absolute Scaled Error </a><span class="p">(</span>MASE<span class="p">): MASE is slightly more complicated than MSE or MAE but gives us a slightly better measure to overcome the scale-dependent nature of the previous two measures. If we have multiple time series with different average values, MAE and MSE will show higher errors for the high-value time series as opposed to the low-valued time series. MASE overcomes this by scaling the errors based on the in-sample MAE from the </span>naïve forecasting method <span class="p">(which is one of the most basic forecasts possible; we will review it later in this chapter). Intuitively, MASE gives us the measure of how much better our forecast is as compared to the naïve forecast:</span><a name="bookmark144">&zwnj;</a></p><p class="s72" style="padding-top: 3pt;padding-left: 52pt;text-indent: 0pt;line-height: 12pt;text-align: center;"> <span class="s73">1</span><span class="s74"> × ∑𝐿𝐿|𝑓𝑓  − 𝑦𝑦 |</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="141" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_120.png"/></span></p><p class="s74" style="padding-top: 3pt;text-indent: 0pt;line-height: 6pt;text-align: right;">𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀 =</p><p class="s74" style="text-indent: 0pt;line-height: 10pt;text-align: right;">𝐿𝐿</p><p class="s75" style="text-indent: 0pt;line-height: 8pt;text-align: right;">𝑖𝑖</p><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑖𝑖</p><p class="s75" style="padding-left: 16pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑖𝑖</p><p class="s76" style="padding-top: 2pt;text-indent: 0pt;line-height: 29%;text-align: right;">   <span class="s73">1</span><span class="s74"> </span><span class="s77">× </span><span class="s78">∑</span><span class="s79">𝐿𝐿</span></p><p class="s74" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 6pt;text-align: left;">|𝑦𝑦</p><p class="s74" style="padding-top: 5pt;padding-left: 3pt;text-indent: 0pt;line-height: 6pt;text-align: left;">− 𝑦𝑦 |</p><p class="s74" style="text-indent: 0pt;line-height: 11pt;text-align: right;">𝐿𝐿 − 1</p><p class="s75" style="padding-left: 19pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝑗𝑗=2</p><p class="s75" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑗𝑗</p><p class="s75" style="padding-left: 15pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑗𝑗−1</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s5">Forecast Bias </span>(<span class="s5">FB</span>): This is a metric with slightly different aspects from the other metrics we’ve seen. While the other metrics help assess the <i>correctness </i>of the forecast, irrespective of the direction of the error, forecast bias lets us understand the overall <i>bias </i>in the model. Forecast bias is a metric that helps us understand whether the forecast is continuously over- or under- forecasting. We calculate forecast bias as the difference between the sum of the forecast and the sum of the observed values, expressed as a percentage over the sum of all actuals:</p><p class="s80" style="padding-top: 5pt;padding-left: 193pt;text-indent: 0pt;line-height: 10pt;text-align: left;">∑<span class="s81">𝑁𝑁 </span>∑<span class="s81">𝐿𝐿 </span>𝑓𝑓<span class="s82">𝑖𝑖,𝑗𝑗 </span>− ∑<span class="s81">𝑁𝑁 </span>∑<span class="s81">𝐿𝐿 </span>𝑦𝑦<span class="s82">𝑖𝑖,𝑗𝑗</span></p><p class="s80" style="padding-top: 2pt;text-indent: 0pt;text-align: right;">𝐹𝐹𝐹𝐹 =</p><p class="s50" style="text-indent: 0pt;line-height: 9pt;text-align: right;">𝑖𝑖</p><p class="s50" style="text-indent: 0pt;line-height: 9pt;text-align: right;">𝑗𝑗</p><p class="s50" style="text-indent: 0pt;line-height: 9pt;text-align: right;">𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"><span><img width="155" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_121.png"/></span></p><p class="s83" style="text-indent: 0pt;line-height: 10pt;text-align: left;">∑<span class="s84">𝑁𝑁 </span>∑<span class="s84">𝐿𝐿 </span><span class="s85">𝑦𝑦</span><span class="s50">𝑖𝑖,𝑗𝑗</span></p><p class="s50" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑗𝑗</p><p class="s50" style="padding-left: 173pt;text-indent: 0pt;line-height: 9pt;text-align: center;">𝑖𝑖          𝑗𝑗</p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, our test harness is ready. We also know how to evaluate and compare forecasts that have been generated from different models on a single, fixed holdout dataset with a set of predetermined metrics. Now, it’s time to start forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Generating strong baseline forecasts</p><p class="s5" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Time series forecasting <span class="p">has been around since the early 1920s, and through the years, many brilliant people have come up with different models, some statistical and some heuristic-based. I refer to them collectively as </span>classical statistical models <span class="p">or </span>econometrics models<span class="p">, although they are not strictly statistical/econometric.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">In this section, we are going to review a few such models that can form really strong baselines when we want to try modern techniques in forecasting. As an exercise, we are going to use an excellent open source library for time series forecasting – <span class="s48">darts </span><a href="https://github.com/unit8co/darts" class="s140" target="_blank">(</a><a href="https://github.com/unit8co/darts" class="s45" target="_blank">https://github.com/unit8co/ </a><span class="s46">darts</span>). The <span class="s48">02-Baseline Forecasts using darts.ipynb </span>notebook contains the code for this section so that you can follow along.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Before we start looking at forecasting techniques, let’s quickly understand how to use the <span class="s48">darts </span>library to generate the forecasts. We are going to pick one consumer from the dataset and try out all the baseline techniques on the validation dataset one by one.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The first thing we need to do is select the consumer we want using the unique ID for each customer, the</p><p class="s48" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">LCLid <span class="p">column (from the expanded form of data), and set the timestamp as the index of the DataFrame:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_train = train_df.loc[train_df.LCLid==&quot;MAC000193&quot;, [&quot;timestamp&quot;,&quot;energy_consumption&quot;]].set_index(&quot;timestamp&quot;)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ts_test = val_df.loc[val_df.LCLid==&quot;MAC000193&quot;, [&quot;timestamp&quot;,&quot;energy_consumption&quot;]].set_index(&quot;timestamp&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Now that we have a single time series, we need to put it into a data structure that the <span class="s48">darts </span>library expects – a <span class="s48">TimeSeries </span>data structure, which is native to <span class="s48">darts</span>. <span class="s48">TimeSeries </span>can be built easily using a few factory methods:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">From a <span class="s48">pandas </span>DataFrame using <span class="s48">TimeSeries.from_dataframe()</span></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: left;">From a separate array of time index and observed values using <span class="s48">TimeSeries.from_times_ and_values()</span></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">From a <span class="s48">numpy </span>array using <span class="s48">TimeSeries.from_values()</span></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">From a <span class="s48">pandas </span>series using <span class="s48">TimeSeries.from_series()</span></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">From a CSV file using <span class="s48">TimeSeries.from_csv()</span></p></li></ul><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">In our case, since we have the time series as a <span class="s48">pandas </span>series, we can just use the <span class="s48">from_series()</span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">ts_train = TimeSeries.from_series(ts_train) ts_test = TimeSeries.from_series(ts_test)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Once we have the <span class="s48">TimeSeries </span>data structure, we can just initialize the model and call <span class="s48">.fit </span>and</p><p class="s48" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">.predict <span class="p">to get the forecast:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">model = &lt;initialize the model&gt; model.fit(ts_train)</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">y_pred = model.predict(len(ts_test))</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">When we call <span class="s48">.predict</span>, we have to tell the model how long into the future we have to predict. This is called the horizon of the forecast. In our case, we need to predict our test period, which we can easily do by just taking the length of the <span class="s48">ts_test </span>array.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can also calculate the metrics we discussed earlier in the test harness easily:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">mae(actual_series = ts_test, pred_series = y_pred) mse(actual_series = ts_test, pred_series = y_pred)</p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># For MASE calculation, the training set is also needed</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">mase(actual_series = ts_test, pred_series = y_pred,</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">insample=ts_train)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Forecast Bias is not part of darts, but an own implementation available in src.utils.ts_utils</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">forecast_bias(actual_series = ts_test, pred_series = y_pred)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark164"/><a name="bookmark145">&zwnj;</a></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: left;">For ease of experimentation, we have encapsulated all of this into a handy function, <span class="s48">eval_model</span>, in the notebook. This returns the predictions and the calculated metrics in a dictionary.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, let’s start looking at a few very simple methods of forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Naïve forecast</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">A naïve forecast is as simple as you can get. The forecast is just the last/most recent observation in a time series. If the latest observation in a time series is 10, then the forecast for all future timesteps is</p><ol id="l34"><li><p style="padding-left: 51pt;text-indent: -14pt;line-height: 13pt;text-align: left;">This can be implemented as follows using the <span class="s48">NaiveSeasonal </span>class in <span class="s48">darts</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from darts.models import NaiveSeasonal naive_model = NaiveSeasonal(K=1)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: left;">Once we have initialized the model, we can call our helpful <span class="s48">eval_model </span>function in the notebook to run and record the forecast and metrics.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s visualize the forecast we just generated:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 56pt;text-indent: 0pt;text-align: left;"><span><img width="476" height="252" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_122.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">Figure 4.1 – Naïve forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Here, we can see that the forecast is a straight line and completely ignores any pattern in the series. This is by far the simplest way to forecast, hence why it is naïve. Now, let’s look at another simple method.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark165">Moving average forecast</a><a name="bookmark147">&zwnj;</a><a name="bookmark146">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">While a naïve forecast memorizes the most recent past, it also memorizes the noise at any timestep. <span class="s5">A moving average forecast </span>is another simple method that tries to overcome the pure memorization of the naïve method. Instead of taking the latest observation, it takes the mean of the latest <i>n </i>steps as the forecast. Moving average is not one of the models present in <span class="s48">darts</span>, but we have implemented a darts-compatible model in this book’s GitHub repository in the <span class="s48">chapter04 </span>folder:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: justify;">from src.forecasting.baselines import NaiveMovingAverage <span class="s38">#Taking a moving average over 48 timesteps, i.e, one day </span>naive_model = NaiveMovingAverage(window=48)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s look at the forecast we generated:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><span><img width="427" height="226" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_123.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 4.2 – Moving average forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">This forecast is also almost a straight line. Now, let’s look at another simple method, but one that considers seasonality as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Seasonal naive forecast</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 86%;text-align: justify;"><span class="s5">A seasonal naive forecast </span>is a twist on the simple naive method. Whereas in the naive method, we took the last observation (<span class="s86">Y</span><span class="s87">t −1</span>), in seasonal naïve, we take the <span class="s41">Y</span><span class="s88">t</span><span class="s89">−k </span>observation. So, we look back <i>k </i>steps for each forecast. This enables the algorithm to mimic the last seasonality cycle. For instance, if</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">we set <span class="s48">k=48*7</span>, we will be able to mimic the latest seasonal weekly cycle. This method is implemented in darts and we can use it like so:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from darts.models import NaiveSeasonal naive_model = NaiveSeasonal(K=48*7)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark166">Let’s see what this forecast looks like:</a><a name="bookmark148">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 47pt;text-indent: 0pt;text-align: left;"><span><img width="495" height="262" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_124.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 4.3 – Seasonal naïve forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, we can see that the forecast is trying to mimic the seasonality pattern. However, it’s not very accurate because it is blindly following the last seasonal cycle.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we’ve looked at a few simple methods, let’s look at a few statistical models.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Exponential smoothing (ETS)</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s5">Exponential smoothing </span>(<span class="s5">ETS</span>) is one of the most popular methods for generating forecasts. It has been around since the late 1950s and has proved its mettle and stood the test of time. There are a few different variants of ETS – <span class="s5">single exponential smoothing</span>, <span class="s5">double exponential smoothing</span>, <span class="s5">Holt-Winters’ seasonal smoothing</span>, and so on. But all of them have one key idea that has been used in different ways. In the naïve method, we were just using the latest observation, which is like saying only the most recent data point in history matters and no data point before that matters. On the other hand, the moving average method considers the last <i>n </i>observations to be equally important and takes the mean of them.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">ETS combines both these intuitions and says that all the history is important, but the recent history is more important. Therefore, the forecast is generated using a weighted average where the weights decrease exponentially as we move farther into the history:</p><p class="s80" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑓𝑓<span class="s82">𝑡𝑡</span><span class="s50">  </span>= α ⋅ 𝑦𝑦<span class="s82">𝑡</span><span class="s50">𝑡−1  </span>+ α ⋅ (1 − α) ⋅ 𝑦𝑦<span class="s82">𝑡</span><span class="s50">𝑡−2  </span>+ α ⋅ (1 − α)2 ⋅ 𝑦𝑦<span class="s82">𝑡</span><span class="s50">𝑡−3  </span>+ ⋯</p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_125.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_126.png"/></span></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;">Here, <span class="s90">0 ≤ α ≤ 1 </span>is the smoothing parameter that lets us decide how fast or slow the weights should decay, is the actuals at timestep <i>t</i>, and is the forecast at timestep <i>t</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark167">Simple exponential smoothing </a><span class="p">(</span>SES<span class="p">) is when you simply apply this smoothing procedure to the history. This is more suited for time series that have no trends or seasonality, and the forecast is going to be a flat line. The forecast is generated using the following formula:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="320" height="30" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_127.gif"/></span></p><p class="s5" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Double exponential smoothing <span class="p">(</span>DES<span class="p">) extends the smoothing idea to model trends as well. It has two smoothing equations – one for the level and the other for the trend. Once you have the estimate of the level and trend, you can combine them. This forecast is not necessarily flat because the estimated trend is used to extrapolate it into the future. The forecast is generated according to the following formula:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 91pt;text-indent: 0pt;text-align: left;"><span><img width="359" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_128.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_129.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_130.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_131.png"/></span></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">First, we estimate the level ( ) using the <i>Level Equation </i>with the available observations. Then, we estimate the trend using the <i>Trend Equation</i>. Finally, to get the forecast, we combine and using the <i>Forecast Equation</i>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 92%;text-align: justify;">Researchers have found empirical evidence that this kind of constant extrapolation can result in over-forecasts over the long-term forecast. This is because, in the real world, time series data doesn’t increase at a constant rate forever. Motivated by this, an addition to this has also been introduced that dampens the trend by a factor of <span class="s44">0 &lt; </span><span class="s91">ϕ </span><span class="s44">&lt; </span><span class="s91">1 </span>, such that when <span class="s92">ϕ</span><span class="s93">= 1</span>, there is no damping, and it is identical to double exponential smoothing.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><span class="s5">Triple exponential smoothing </span>or <span class="s5">Holt -Winters </span>(<span class="s5">HW</span>) takes this one step forward by including another smoothing term to model the seasonality. This has three parameters ( <span class="s94">α, β, γ</span>) for the smoothing and uses a seasonality period (<i>m</i>) as input parameters. You can also choose between additive or multiplicative seasonality. The forecast equations for the additive model are as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;"><span><img width="488" height="142" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_132.gif"/></span></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">These formulae are also used like in the double exponential case. Instead of estimating level and trend, we estimate level, trend, and seasonality separately.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark168">The family of exponential smoothing methods is not limited to the three that we just discussed. A way to think about the different models is in terms of the trend and seasonal components of these models. The trend can either be no trend, additive, or additive damped. The seasonality can be no seasonality, additive, or multiplicative. Every combination of these parameters is a different technique in the family, as shown in the following table:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;"><span><img width="511" height="184" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_133.gif"/></span></p><p class="s37" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Table 4.1 – Exponential smoothing family</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">This entire family of methods has been wrapped in a single implementation from <span class="s48">statsmodels </span>under <span class="s48">statsmodels.tsa.holtwinters.ExponentialSmoothing</span>. The <span class="s48">darts </span>library has a wrapper implementation around the <span class="s48">statsmodels </span>implementation to make it work in the standardized way we have adopted in this chapter. Let’s see how we can initialize the ETS model in <span class="s48">darts</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">from darts.models import ExponentialSmoothing</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">from darts.utils.utils import ModelMode, SeasonalityMode</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">ets_model = ExponentialSmoothing(trend=ModelMode.ADDITIVE, damped=True, seasonal=SeasonalityMode.ADDITIVE, seasonal_ periods=48*7)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: left;">The key parameters are <span class="s48">trend</span>, <span class="s48">damped</span>, <span class="s48">seasonal</span>, and <span class="s48">seasonal_periods</span>. They help you decide what kind of model you want to fit the data.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s see what the forecast that we generated using ETS looks like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;text-align: left;"><a name="bookmark169"><span><img width="518" height="262" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_134.jpg"/></span></a><a name="bookmark149">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 4.4 – Exponential smoothing forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The forecast has captured the seasonality but has failed to capture the peaks. But we can see the improvement in MAE already.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, let’s look at one of the most popular forecasting methods out there.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">ARIMA</p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_135.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_136.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_137.png"/></span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s5">Autoregressive Integrated Moving Average </span>(<span class="s5">ARIMA</span>) models are the other class of methods that, like ETS, have stood the test of time and are one of the most popular classical methods of forecasting. The ETS family of methods is modeled around trend and seasonality, while ARIMA relies on <span class="s5">autocorrelation </span>(the correlation of with <span class="s95">−1</span>, <span class="s96">−</span><span class="s97">2 </span>, and so on).</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The simplest in the family are the <i>AR(p) </i>models, which use <span class="s5">linear regression </span>with <i>p </i>previous timesteps or, in other words, <i>p </i>lags. Mathematically, it can be written as follows:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_138.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_139.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_140.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_141.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_142.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="3" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_143.png"/></span></p><p class="s68" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">=    + ϕ<span class="s98">1       </span><span class="s99">−1 </span>+ ϕ<span class="s98">2      −2  </span>+ ⋯ ϕ <span><img width="18" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_144.png"/></span><span class="s98">−    </span>+ ϵ</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_145.png"/></span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Here, <i>c </i>is the intercept and <span class="s100">ϵ </span>is the noise or error at timestep <i>t</i>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The next in the family are <i>MA(q) </i>models, in which instead of past observed values, we use the past <i>q</i></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">errors in the forecast (which is assumed to be pure white noise) to come up with a forecast:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_146.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_147.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_148.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_149.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_150.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_151.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_152.png"/></span></p><p class="s58" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">=     + θ<span class="s101">1</span>ϵ  <span class="s101">−1  </span>+ θ<span class="s101">2</span>ϵ <span class="s101">−2  </span>+ ⋯ θ  ϵ <span class="s101">−</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Here, <span class="s102">ϵ </span>is white noise and <i>c </i>is the intercept.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_153.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_154.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_155.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="19" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_156.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_157.png"/></span></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 91%;text-align: justify;"><a name="bookmark170">This is not typically used on its own but in conjunction with AR(p) models, which makes the next one on our list </a><i>ARMA(p,q) </i>models. ARMA models are defined as <span class="s103">= ( ) + ( )</span>.</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="p">In all the ARIMA models, there is one underlying assumption – the </span>time series is stationary <a href="#bookmark10" class="s140">(we talked about stationarity in </a>Chapter 1<span class="p">, </span>Introducing Time Series<a href="#bookmark211" class="s140">, and will elaborate on this in </a>Chapter 6<span class="p">, </span>Feature Engineering for Time Series Forecasting<span class="p">). There are many ways to make the series stationary but taking the difference of successive values is one such technique. This is known as </span><span class="s5">differencing</span><span class="p">. Sometimes, we need to do differencing once, while other times, we have to perform successive differencing before the time series becomes stationary. The number of times we do the differencing operation is called the </span>order of differencing<span class="p">. The I in ARIMA, and the final piece of the puzzle, stands for </span>Integrated<span class="p">. It defines the order of differencing we need to do before the series becomes stationary and is denoted by </span>d<span class="p">.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">So, the complete <i>ARIMA(p,d,q) </i>model says that we do <i>d</i>th order of differencing and then consider the last <i>p </i>terms in an autoregressive manner, and then include the last <i>q </i>moving average terms to come up with the forecast.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_158.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_159.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_160.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_161.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_162.png"/></span></p><p style="padding-top: 7pt;padding-left: 36pt;text-indent: 0pt;text-align: justify;">The ARIMA models we have discussed so far only handle non-seasonal time series. But using the same concepts we discussed, but on a seasonal cycle, we get <span class="s5">Seasonal ARIMA</span>. <i>p</i>, <i>d</i>, and <i>q </i>are slightly tweaked so that they work on the seasonal period, <i>m</i>. To differentiate them from the normal <i>p</i>, <i>d</i>, and<i>q</i>, we call the seasonal values <i>P</i>, <i>D</i>, and <i>Q</i>. For instance, if <i>p </i>meant taking the last <i>p </i>lags, <i>P </i>means taking the last <i>P </i>seasonal lags. If <span class="s104">1 </span>is <span class="s105">−1</span>, <span class="s104">1 </span>would be <span class="s106">− </span>. Similarly, <i>D </i>means the order of seasonal differencing.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="234" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_163.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Practical considerations</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 90%;text-align: justify;">Although ARIMA and Auto ARIMA can give you good-performing models in many cases, they can be quite slow when you have long seasonal periods and a long time series. In our case, where we have almost 27k observations in the history, ARIMA becomes very slow and a memory hog. Even when using just the latest 8,000 observations, a single ARIMA fit takes around 6 minutes. Letting go of the seasonal parameters brings down the runtime drastically, but for a seasonal time series such as energy consumption, it doesn’t make sense. Auto ARIMA includes many such fits to identify the best parameters and therefore becomes impractical for long time series datasets. Almost all the implementations in the Python ecosystem suffer from this drawback except for <span class="s48">statsforecast </span><a href="https://github.com/Nixtla/statsforecast" class="s140" target="_blank">(</a><a href="https://github.com/Nixtla/statsforecast" class="s45" target="_blank">https://github.com/Nixtla/ </a><span class="s46">statsforecast</span>). At the time of writing, the new library has just been released, but it shows a lot of promise in having a fast implementation of <span class="s48">ARIMA </span>and <span class="s48">AutoARIMA</span>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 36pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Picking the right <i>p</i>, <i>d</i>, and <i>q </i>and <i>P</i>, <i>D</i>, and <i>Q </i>values is not very intuitive, and we will have to resort to statistical tests to find them. However, this becomes a bit impractical when you are forecasting many time series. An automatic way of iterating through the different parameters and finding the best <i>p</i>, <i>d</i>, and <i>q</i>, and <i>P</i>, <i>D</i>, and <i>Q </i>values for the data is called <span class="s5">Auto ARIMA</span>. In Python, <span class="s48">pmdarima </span>is a library that has implemented this, and the <span class="s48">darts </span>library, once again, has a wrapper around it to make our work easy. <span class="s48">darts </span>also has a normal ARIMA implementation that is a wrapper around <span class="s48">statsmodels.tsa.arima.model.ARIMA</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark171">Let’s see how we can apply </a><span class="s48">ARIMA </span>and <span class="s48">AutoARIMA </span>using <span class="s48">darts</span>:<a name="bookmark150">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#ARIMA model by specifying parameters</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">arima_model = ARIMA(p=2, d=1, q=1, seasonal_order=(1, 1, 1, 48))</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">#AutoARIMA model by specifying max limits for parameters and letting the algorithm find the best ones</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">auto_arima_model = AutoARIMA(max_p=5, max_q=3, m=48, seasonal=True)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">For the entire list of parameters for <span class="s48">AutoARIMA</span>, head over to the <span class="s48">pmdarima </span><a href="https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.AutoARIMA.html" class="s140" target="_blank">documentation at </a><a href="https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.AutoARIMA.html" class="s45" target="_blank">https://alkaline-ml.com/pmdarima/modules/generated/pmdarima. </a><span class="s46">arima.AutoARIMA.html</span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s see what the ETS and ARIMA forecasts look like for the households we were experimenting with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 49pt;text-indent: 0pt;text-align: left;"><span><img width="470" height="249" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_164.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 4.5 – ETS and ARIMA forecasts</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The ARIMA forecast has captured the seasonality and the peaks better than ETS and it reflects in a lower MAE score.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at another method – the Theta Forecast.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Theta Forecast</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 92%;text-align: justify;">The <span class="s5">Theta Forecast </span>was the top-performing submission in the M3 forecasting competition that was held in 2002. The method relies on a parameter, <span class="s107">θ</span>, that amplifies or smooths the local curvature of a time series, depending on the value chosen. Using <span class="s108">θ</span>, we smooth or amplify the original time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="168" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_165.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Side note</p><p class="s46" style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 90%;text-align: justify;"><a href="https://mofc.unic.ac.cy/the-m6-competition/" class="s140" target="_blank">The M-competitions are forecasting competitions organized by Spyros Makridakis, a leading forecasting researcher. They typically curate a dataset of time series, lay down the metrics with which the forecasts will be evaluated, and open these competitions to researchers all around the world to get the best forecast possible. These competitions are considered to be some of the biggest and most popular time series forecasting competitions in the world. At the time of writing, five such competitions have already been completed and the sixth one has been announced: </a>https://mofc.unic.ac.cy/the-m6-competition/<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark172">These smoothed lines are called </a><span class="s5">theta lines</span>. V. Assimakopoulos and K. Nikolopoulos proposed this method as a decomposition approach to forecasting. Although in theory any number of theta lines can be used, the originally proposed method used two theta lines, <span class="s109">=0 </span>and <span class="s110">=2</span>, and took an average of the forecast of the two theta lines as the final forecast.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In 2002, Rob Hyndman et al. simplified the Theta method and showed that we can use ETS with a drift term to get equivalent results to the original Theta method, which is what is adapted into most of the implementations of the method that exist today. The major steps that are involved in the Theta Forecast (which is implemented in <span class="s48">darts</span>) are as follows:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="29" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_166.png"/></span></p><ol id="l35"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;line-height: 94%;text-align: justify;">Check for seasonality and use <span class="s48">statsmodels.tsa.seasonal.seasonal_decompose</span><span class="s46"> </span>to extract seasonality if the series is seasonal. This step creates a new deseasonalized time series, .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_167.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Use SES on the deseasonalized time series, , and retrieve the estimated smoothing parameter, <span class="s111">α</span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="19" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_168.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_169.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Fit a linear trend on <span class="s112">(1 − θ) × </span>and retrieve the estimated coefficient, <span class="s104">θ</span>.</p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_170.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_171.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="22" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_172.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_173.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_174.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_175.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_176.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_177.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_178.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_179.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_180.png"/></span></p><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">4. <span class="s113">+ </span><span class="s114">= </span><span class="s113">+ </span><span class="s114">+ </span><span class="s113">θ </span><span class="s114">× </span><span class="s113">+ </span>, where <span class="s115">+ </span>is the SET forecast for timesteps <i>t </i>to <i>t+H </i>and <span class="s116">+ </span>is an array denoting time, <i>[t, t+1, t+2, … t+H]</i>.</p><ol id="l36"><li><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 9pt;line-height: 162%;text-align: justify;">Reseasonalize if the data was deseasonalized in the beginning. Let’s see how we can use it practically:</p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">theta_model = Theta(theta=3, seasonality_period=48*7, season_ mode=SeasonalityMode.ADDITIVE)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">The key parameters here are as follows:</p><ul id="l37"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;"><span class="s48">theta</span>: This functions as a dampening of the trend and is not to be confused with <span class="s59">θ </span>in the context of theta lines. The implementation takes <span class="s62">θ = 0 </span>and <span class="s117">θ </span><span class="s118">= 2 </span>as a fixed configuration. The higher the theta value, the higher the dampening of the trend.</p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">season_mode <span class="p">and </span>seasonality_period<span class="p">: These parameters are used for the initial seasonal decomposition. If left empty, the implementation automatically tests for seasonality and deseasonalizes the time series automatically. It is recommended to set these parameters with our domain knowledge if we know them.</span></p></li></ul></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark173">Let’s visualize the forecast we just generated using the Theta Forecast:</a><a name="bookmark151">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: left;"><span><img width="444" height="235" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_181.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 4.6 – The Theta Forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="101" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_182.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The research paper in which V. Assimakopoulos and K. Nikolopoulos proposed the Thet method is cited as reference <i>1 </i>in the <i>References </i>section, while subsequent simplification b Rob Hyndman is cited as reference <i>2</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 89%;text-align: left;">a y</p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Fast Fourier Transform forecast</p><p class="s4" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark87" class="s140">We used a Fourier series in </a>Chapter 3<span class="p">, </span>Analyzing and Visualizing Time Series Data<span class="p">, to decompose seasonality. Fourier Transform is a very related concept.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Fourier Transform decomposes a time series, which is in the time domain, to temporal frequencies, which is in the frequency domain. It breaks apart a time series and returns the information about the frequency of all the sine (or cosine) waves that constitute the time series. The period of a sine wave is the time it takes to perform one complete cycle, while the frequency of a sine wave is as follows:</p><p class="s57" style="padding-top: 7pt;padding-bottom: 3pt;text-indent: 0pt;text-align: center;">1</p><p style="padding-left: 206pt;text-indent: 0pt;text-align: left;"><span><img width="53" height="21" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_183.png"/></span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This is the number of complete cycles that can happen upon every unit of time. Therefore, by knowing the frequencies of all the sine waves of a time series, we can reconstruct the time series perfectly, allowing a seamless transition from the frequency domain to the time domain.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark174">The following diagram shows what Fourier Transform strives to do. The time series, which is in the time domain, is split into several frequencies in the frequency domain so that when we add all of those frequencies to the time domain, we get the original time series:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 79pt;text-indent: 0pt;text-align: left;"><span><img width="414" height="233" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_184.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">Figure 4.7 – Fourier Transform</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The theory is quite beautiful and if you want to know more, go to the <i>Further reading </i>section. But for now, let’s take it at face value and move on.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">For sequences that are evenly spaced, <span class="s5">Discrete Fourier Transform </span>is applicable, which does away with the integral and makes it into a summation. However, this is also very slow to compute. Fortunately, there is an algorithm called <span class="s5">Fast Fourier Transform </span>(<span class="s5">FFT</span>) that makes it computationally feasible. Coupled with this, there is an <span class="s5">Inverse Fast Fourier Transform </span>(<span class="s5">IFFT</span>) to go back to the time domain.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">While FFT will let us reconstruct the time series exactly, that is not something we want when we are forecasting. We only want to capture the signal in the time series and exclude the noise. Therefore, we can filter out noise by choosing a few prominent frequencies from FFT and only use this smaller set in IFFT. Since FFT needs the time series to be detrended, we must apply a detrending step before the FFT step and add the trend once we have reconstructed the time series.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is implemented in <span class="s48">darts</span>, and we can use the implementation shown here:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">fft_model = FFT(nr_freqs_to_keep=35, trend=”poly”, trend_poly_ degree=2)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: left;">The <span class="s48">nr_freqs_to_keep </span>and <span class="s48">trend </span>hyperparameters must be tweaked to get the best forecast possible.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s see what the FFT forecast looks like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="494" height="262" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_185.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 173pt;text-indent: 0pt;text-align: center;">Figure 4.8 – FFT forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Again, the seasonality pattern has been replicated, although it is not capturing the peaks in the forecast. Let’s also take a look at how the different metrics that we chose did for each of these forecasts for the household we were experimenting with (from the notebook):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="494" height="234" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_186.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 4.9 – Summary of all the baseline algorithms</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark175">Out of all the baseline algorithms we tried, ARIMA is performing the best on both MAE as well as MSE. But if you look at the </a><span class="s5">Time Elapsed </span>column, it stands out. Even after taking only the latest 8,000 observations to train, it took much more time than the other baseline algorithms. The next best from MAE and MSE are Theta, EST, and FFT, out of which Theta and FFT are orders of magnitude faster than EST.<a name="bookmark152">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">So, we can take these two algorithms as our baseline and run them on all 399 households in the dataset (both validation and test) we’ve chosen (the code for this is available in the <span class="s48">02-Baseline Forecasts using darts.ipynb </span>notebook).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Evaluating the baseline forecasts</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Since we have the baseline forecasts generated from Theta as well as FFT, we should also evaluate these forecasts. The aggregate metrics for all the selected households for both these methods are as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 54pt;text-indent: 0pt;text-align: left;"><span><img width="486" height="215" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_187.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 4.10 – The aggregate metrics of all the selected households (both validation and test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">It looks like FFT is performing much better in all three metrics. We also have these metrics calculated at a household level. Let’s look at the distribution of these metrics in the validation dataset for all the selected households:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="516" height="558" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_188.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 28pt;text-indent: 4pt;text-align: justify;">Figure 4.11 – The distribution of MASE and forecast bias of the baseline forecast in the validation dataset</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The MASE histogram of FFT seems to have a smaller spread than Theta. FFT also has a lower median MASE than Theta. We can see a similar pattern for forecast bias as well, with the forecast bias of FFT centered around zero and much less spread.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark10" class="s140" name="bookmark176">Back in </a>Chapter 1<span class="p">, </span>Introducing Time Series<span class="p">, we saw why every time series is not equally predictable and saw three factors to help us think about the issue – understanding the Data Generating Process (DGP), the amount of data, and adequately repeating the pattern. In most cases, the first two are pretty easy to evaluate, but the third one requires some analysis. Although the performance of baseline methods gives us some idea about how predictable any time series is, they still are model-dependent. So, instead of measuring how well a time series is forecastable, we might be better measuring how well the chosen model can approximate the time series. This is where a few techniques that are more fundamental (relying on the statistical properties of a time series) come in.</span><a name="bookmark154">&zwnj;</a><a name="bookmark153">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Assessing the forecastability of a time series</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Although there are many statistical measures that we can use to assess the predictability of a time series, we will just look at a few that are easier to understand and practical when dealing with large time series datasets. The associated notebook (<span class="s48">02-Forecastability.ipynb</span>) contains the code to follow along.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Coefficient of Variation (CoV)</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The <span class="s5">Coefficient of Variation </span>(<span class="s5">CoV</span>) relies on the intuition that the more variability that you find in a time series, the harder it is to predict it. And how do we measure variability in a random variable? <span class="s5">Standard deviation</span>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In many real-world time series, the variation we see in the time series is dependent on the scale of the time series. Let’s imagine that there are two retail products, <i>A </i>and <i>B</i>. <i>A </i>has a mean monthly sale of 15, while <i>B </i>has 50. If we look at a few real-world examples like this, we will see that if <i>A </i>and <i>B </i>have the same standard deviation, <i>B</i>, which has a higher mean, is much more forecastable than <i>A</i>. To accommodate this phenomenon and to make sure we bring all the time series in a dataset to a common scale, we can use the CoV:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_189.png"/></span></p><p class="s58" style="padding-left: 36pt;text-indent: 0pt;line-height: 10pt;text-align: center;">σ</p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_190.png"/></span></p><p class="s58" style="padding-left: 16pt;text-indent: 0pt;line-height: 8pt;text-align: center;">=</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_191.png"/></span></p><p class="s58" style="padding-left: 36pt;text-indent: 0pt;line-height: 10pt;text-align: center;">μ</p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_192.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_193.png"/></span></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, is the standard deviation and <span class="s119">μ </span>is the mean of the time series, <i>n</i>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The CoV is the relative dispersion of data points around the mean, which is much better than looking at the pure standard deviation.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The larger the value for the CoV, the worse the predictability of the time series. There is no hard cutoff, but a value of 0.49 is considered a rule of thumb to separate time series that are relatively easier to forecast from the hard ones. But depending on the general <i>hardness </i>of the dataset, we can tweak this cutoff. Something I have found useful is to plot a histogram of CoV values in a dataset and derive cutoffs based on that.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark177">Even though the CoV is widely used in the industry, it suffers from a few key issues:</a><a name="bookmark155">&zwnj;</a></p><ul id="l38"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">It doesn’t consider seasonality. A sine or cosine wave will have a higher CoV than a horizontal line, but we know both are equally predictable.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">It doesn’t consider the trend. A linear trend will make a series have a higher CoV, but we know it is equally predictable like a horizontal line is.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">It doesn’t handle negative values in the time series. If you have negative values, it makes the mean smaller, thereby inflating the CoV.</p></li></ul><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">To overcome these shortcomings, we propose another derived measure.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Residual variability (RV)</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The thought behind residual variability (<span class="s5">RV</span>) is to try and measure the same kind of variability that we were trying to capture with the CoV but without the shortcomings. I was brainstorming on ways to avoid the problems of using the CoV, typically the seasonality issue, and was applying the CoV to the residuals after seasonal decomposition. It was then I realized that the residuals would have a few negative values and that the CoV wouldn’t work well. Stefan de Kok, who is a thought leader in demand forecasting and probabilistic forecasting, suggested using the mean of the original actuals, which worked.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">To calculate RV, you must perform the following steps:</p><ol id="l39"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Perform seasonal decomposition.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Calculate the standard deviation of the residuals or the irregular component.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Divide the standard deviation by the mean of the original observed values (before decomposition).</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The key assumption here is that seasonality and trend are components that can be predicted. Therefore, our assessment of the predictability of a time series should only look at the variability of the residuals. But we cannot use CoV on the residuals because the residuals can have negative and positive values, so the mean of the residuals loses the interpretation of the level of the series and tends to zero. When residuals tend to zero, the CoV measure tends to infinity because of the division by mean. Therefore, we use the mean of the original series as the scaling factor.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s see how we can calculate RV for all the time series in our dataset (which are in a compact form):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">block_df[&quot;rv&quot;] = block_df.progress_apply(lambda x: calc_norm_ sd(x[&#39;residuals&#39;],x[&#39;energy_consumption&#39;]), axis=1)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">In this section, we looked at two measures that are based on the standard deviation of the time series. Now, let’s look at assessing the forecastability of a time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark178">Entropy-based measures</a><a name="bookmark156">&zwnj;</a></p><p class="s5" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Entropy <span class="p">is a ubiquitous term in science. We see it popping up in Physics, quantum mechanics, social sciences, and information theory. And everywhere, it is used to talk about a measure of chaos or lack of predictability in a system. The entropy we are most interested in now is the one from Information Theory. Information Theory involves quantifying, storing, and communicating digital information.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_194.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p class="s4" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">A Mathematical Theory of Communication <span class="p">by Claude E. Shannon is cited as reference </span>3 <span class="p">in th</span></p><p class="s4" style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">References <span class="p">section.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">e</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Claude E. Shannon presented the qualitative and quantitative model of communication as a statistical process in his seminal paper <i>A Mathematical Theory of Communication</i>. While the paper introduced a lot of ideas, some of the concepts that are relevant to us are Information Entropy and the concept of a <i>bit </i>– a fundamental unit of measurement of information.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The theory in itself is quite a lot to cover, but to summarize the key bits of information, take a look at the following short glossary:</p><ul id="l40"><li><p style="padding-top: 9pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Information is nothing but a sequence of <i>symbols</i>, which can be transmitted from the <i>receiver </i>to the <i>sender </i>through a medium, which is called a <i>channel</i>. For instance, when we are texting somebody, the sequence of symbols are the letters/words of the language in which we are texting; the channel is the electronic medium.</p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Entropy <span class="p">can be thought of as the amount of </span>uncertainty <span class="p">or </span>surprise <span class="p">in a sequence of symbols given some distribution of the symbols.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">A bit<span class="p">, as we mentioned earlier, is a unit of information and is a binary digit. It can either be 0 or 1.</span></p></li></ul></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, if we were to transfer 1 bit of information, it would reduce the uncertainty of the receiver by 2. To understand this better, let’s consider a coin toss. We toss the coin in the air, and as it is spinning through the air, we don’t know whether it is going to be heads or tails. But we know it is going to be one of these two. When the coin hits the ground and finally comes to rest, we find that it is heads. We can represent whether the coin toss is heads or tails with 1 bit of information (0 for heads and 1 for tails). So, the information that was passed to us when the coin fell reduced the possible outcomes from two to one (heads). This transfer was possible with 1 bit of information.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In Information Theory, the entropy of a discrete random variable is the average level of <i>information</i>, <i>surprise</i>, or <i>uncertainty </i>inherent in the variable’s possible outcomes. In more technical parlance, it is the expected number of bits required for the best possible encoding scheme of the information present in the random variable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="101" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_195.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional reading</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">If you want to intuitively understand entropy, cross-entropy, Kullback-Leibler divergence, and so on, head over to the <i>Further reading </i>section. There are a couple of links to blogs (one of which is my own) where we try to lay down the intuition behind these metrics).</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-bottom: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Entropy is formally defined as follows:</p><p style="padding-left: 211pt;text-indent: 0pt;line-height: 4pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_196.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_197.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_198.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_199.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_200.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="42" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_201.png"/></span></p><p class="s111" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">(   ) = − ∑       (    ) ⋅           (  )</p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_202.png"/></span></p><p class="s99" style="padding-top: 3pt;padding-left: 152pt;text-indent: 0pt;text-align: center;">=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_203.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_204.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="14" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_205.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_206.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_207.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_208.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_209.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_210.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_211.png"/></span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Here, <i>X </i>is the discrete random variable with possible outcomes, and <span class="s106">1 </span><span class="s120">, </span><span class="s106">2</span><span class="s120">, … , </span>. Each of those outcomes has a probability of occurring, which is denoted by and <span class="s62">( </span><span class="s121">1</span><span class="s62">), ( </span><span class="s121">2</span><span class="s62">), … , ( ) </span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">To develop some intuition around this, we can think that the more spread out a probability distribution is, the more chaos is in the distribution, and thus more entropy. Let’s quickly check this in code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Creating an array with a well balanced probability distribution</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">flat = np.array([0.1,0.2, 0.3,0.2, 0.2])</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Calculating Entropy</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">print((-np.log2(flat)* flat).sum())</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">&gt;&gt; 2.2464393446710154</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Creating an array with a peak in probability</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">sharp = np.array([0.1,0.6, 0.1,0.1, 0.1])</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Calculating Entropy</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">print((-np.log2(sharp)* sharp).sum())</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">&gt;&gt; 1.7709505944546688</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, we can see that the probability distribution that spread its mass has higher entropy.</p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the context of a time series, <i>n </i>is the total number of time series observations, and <span class="s122">P</span><span class="s123">(</span><span class="s122">x</span><span class="s124">i</span><span class="s123">) </span>is the probability for each symbol of the time series alphabet. A sharp distribution means that the time series values are concentrated on a small area and should be easier to predict. On the other hand, a wide or flat distribution means that the time series value can be equally likely across a wider range of values and hence is difficult to predict.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">If we have two time series – one containing the result of a coin toss and the other containing the result of a dice throw – the dice throw would have any output between one and six, whereas the coin toss would be either zero or one. The coin toss time series would have lower entropy and be easier to predict than the dice throw time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark179">But since time series is typically continuous, and entropy requires a discrete random variable, we can resort to a few strategies to convert the continuous time series into a discrete one. Many strategies, such as quantization or binning, can be applied, which leads to a myriad of complexity measures. Let’s review one such measure that is useful and practical.</a></p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Spectral entropy</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">To calculate the entropy of a time series, we need to discretize the time series. One way to do that is by using FFT and <span class="s5">power spectral density (PSD)</span>. This discretization of the continuous time series is used to calculate spectral entropy.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We learned what Fourier Transform is earlier in this chapter and used it to generate a baseline forecast. But using FFT, we can also estimate a quantity called power spectral density. This answers the question, <i>How much of the signal is at a particular frequency? </i>There are many ways of estimating power spectral density from a time series, but one of the easiest ways is by using the <span class="s5">Welch method</span>, which is a non-parametric method based on Discrete Fourier Transform. This is also implemented as a handy function with the <span class="s48">periodogram(x) </span>signature in <span class="s48">scipy</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The returned <i>PSD </i>will have a length equal to the number of frequencies estimated, but these are densities and not well-defined probabilities. So, we need to normalize <i>PSD </i>to be between zero and one:</p><p class="s51" style="padding-top: 2pt;padding-left: 45pt;text-indent: 0pt;line-height: 10pt;text-align: center;">    <span class="s52">𝑛𝑛𝑛𝑛𝐷𝐷</span><span class="s53">𝑖          𝑖</span><span class="s125">       </span></p><p class="s50" style="padding-left: 149pt;text-indent: 0pt;line-height: 3pt;text-align: center;">𝑛𝑛𝑛𝑛𝑛𝑛𝐷𝐷   =</p><p class="s50" style="text-indent: 0pt;line-height: 9pt;text-align: left;">∑</p><p style="text-indent: 0pt;text-align: left;"/><p class="s54" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑖𝑖</p><p class="s54" style="padding-left: 17pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝐹𝐹</p><p class="s54" style="padding-left: 17pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑗𝑗=1</p><p class="s50" style="padding-top: 1pt;text-indent: 0pt;text-align: left;">𝑛𝑛𝑛𝑛𝐷𝐷<span class="s126">𝑗𝑗</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here, <i>F </i>is the number of frequencies that are part of the returned power spectrum density.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now that we have the probabilities, we can just plug this into the entropy formula and arrive at the spectral entropy:</p><p class="s127" style="padding-top: 1pt;padding-left: 146pt;text-indent: 0pt;text-align: center;">𝑛𝑛</p><p class="s90" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐻𝐻<span class="s128">𝑠</span><span class="s127">𝑠(𝑋𝑋)  </span>= − ∑ 𝑛𝑛𝑛𝑛𝑛𝑛𝐷𝐷<span class="s128">𝑖𝑖</span><span class="s127">  </span>⋅ 𝑙𝑙𝑙𝑙𝑙𝑙(𝑛𝑛𝑛𝑛𝑛𝑛𝐷𝐷<span class="s128">𝑖𝑖</span><span class="s127"> </span>)</p><p class="s127" style="padding-top: 2pt;padding-left: 146pt;text-indent: 0pt;text-align: center;">𝑖𝑖=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">When we introduced <span class="s5">entropy-based measures</span>, we saw that the more spread out the probability mass of a distribution is, the higher the entropy is. In this context, the more frequencies across which the spectral density is spread, the higher the spectral entropy. So, a higher spectral entropy means the time series is more complex and therefore more difficult to forecast.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Since FFT has an assumption of stationarity, it is recommended that we make the series stationary before using spectral entropy as a metric. We can even apply this metric to a detrended and deseasonalized time series, which we can refer to as <span class="s5">residual spectral entropy</span>. This book’s GitHub repository contains an implementation of spectral entropy under <span class="s48">src.forecastability.entropy.spectral_ entropy</span>. This implementation also has a parameter, <span class="s48">transform_stationary</span>, which, if set to <span class="s48">True</span>, will detrend the series before we apply spectral entropy. Let’s see how we can calculate spectral entropy for our dataset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from src.forecastability.entropy import spectral_entropy block_df[&quot;spectral_entropy&quot;] = block_df.energy_consumption.</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">progress_apply(lambda x: spectral_entropy(x, transform_ stationary=True))</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">block_df[&quot;residual_spectral_entropy&quot;] = block_df.residuals. progress_apply(spectral_entropy)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark180"/><a name="bookmark157">&zwnj;</a></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are other entropy-based measures such as approximate entropy and sample entropy, but we will not cover those in this book. They are more computationally intensive and don’t tend to work for time series that contain fewer than 200 values. If you are interested in learning more about these measures, head over to the <i>Further reading </i>section.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Another metric that takes a slightly different path is the Kaboudan metric.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Kaboudan metric</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In 1999, Kaboudan defined a metric for time series predictability, calling it the <span class="s129">η</span>-metric. The idea behind it is very simple. If we block -shuffle a time series, we are essentially destroying the information in the time series. <span class="s5">Block shuffling </span>is the process of dividing the time series into blocks and then shuffling those blocks. So, if we calculate the <span class="s5">sum of squared errors </span>(<span class="s5">SSE</span>) of a forecast that’s been trained on a time series and then contrast it with the SSE of a forecast trained on a shuffled time series, we can infer the predictability of the time series. The formula to calculate this is as follows:</p><p class="s111" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">η = 1 −  <span><img width="28" height="30" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_212.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="31" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_213.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="29" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_214.png"/></span></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, is the SSE of the forecast that was generated from the original time series, while is the SSE of the forecast that was generated from the block-shuffled series.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_215.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="29" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_216.png"/></span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">If the time series contains some predictable signals, would be lower than and <span class="s130">η </span>would approach zero. This is because there was some information or patterns that were broken due to the block shuffling. On the other hand, if a series is just white noise (which is unpredictable by definition)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="20" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_217.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_218.png"/></span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">there would be hardly any difference between and , and <span class="s100">η </span>would approach one.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="114" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_219.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="46" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_220.png"/></span></p><p class="s131" style="text-indent: 0pt;line-height: 11pt;text-align: left;">η</p><p style="text-indent: 0pt;text-align: left;"/><p class="s131" style="text-indent: 0pt;line-height: 11pt;text-align: left;">= 1 − √</p><p style="text-indent: 0pt;text-align: left;"/><p class="s29" style="text-indent: 0pt;line-height: 13pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The research paper that proposed the Kaboudan metric is cited as reference <i>4 </i>in the <i>Reference</i></p><p style="text-indent: 0pt;line-height: 13pt;text-align: left;">section. The subsequent modification that Duan suggested is cited as reference <i>5</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">s</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_221.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_222.png"/></span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;">In 2002, Duan investigated this metric and suggested some modifications in his thesis. One of the problems he identified, especially in long time series, is that the <span class="s130">η </span>values are found in a narrow band around 1 and suggested a slight modification to the formula. We call this the <span class="s5">modified Kaboudan metric</span>. The measure on the lower side is also clipped to zero. Sometimes, the metric can go below zero because is lower than , which is because the series is unpredictable and, by pure chance, block shuffling made the SSE lower:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 255pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="30" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_223.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark181">This modified version, as well as the original, has been implemented in this book’s GitHub repository.</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There is no restriction on the forecasting model you use to generate the forecast, which makes it a bit more flexible. Ideally, we can choose one of the classical statistical methods that is fast enough to be applied to the whole dataset. But this also makes the Kaboudan metric dependent on the model, and the limitations of the model are inherent in the metric. The metric measures a combination of how difficult a series is to forecast and how difficult it is for the model to forecast the series.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Again, both metrics are implemented in this book’s GitHub repository. Let’s see how we can use them:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.forecastability.kaboudan import kaboudan_metric, modified_kaboudan_metric</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">model = Theta(theta=3, seasonality_period=48*7, season_ mode=SeasonalityMode.ADDITIVE)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">block_df[&quot;kaboudan_metric&quot;] = [kaboudan_metric(r[0], model=model, block_size=5, backtesting_start=0.5, n_folds=1) for r in tqdm(zip(*block_df[[&quot;energy_consumption&quot;]].to_ dict(&quot;list&quot;).values()), total=len(block_df))]</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">block_df[&quot;modified_kaboudan_metric&quot;] = [modified_kaboudan_ metric(r[0], model=model, block_size=5, backtesting_start=0.5, n_folds=1) for r in tqdm(zip(*block_df[[&quot;energy_consumption&quot;]]. to_dict(&quot;list&quot;).values()), total=len(block_df))]</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="182" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_224.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional reading</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">If you want to delve a little deeper and analyze the behavior of these metrics, how similar the are to each other, and how effective they are in measuring forecastability, go to the end of th <span class="s48">03-Forecastability.ipynb </span>notebook. We compute rank correlations among thes metrics to understand how similar these metrics are. We can also find rank correlations wit the computed metrics from the best performing baseline method to understand how well thes metrics did in estimating the forecastability of a time series. I strongly encourage you to pla around with the notebook and understand the differences between the different metrics. Pic a few time series and check how the different metrics give you slightly different interpretation</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 90%;text-align: justify;">y e e h e y k s.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Although there are many more metrics we can use for this purpose, the metrics we just reviewed for assessing forecastability cover a lot of the popular use cases and should be more than enough to gauge any time series dataset in regards to the difficulty of forecasting it. We can use these metrics to compare onetime series with another time series or to profile a whole set of related time series in a dataset with another dataset for benchmarking purposes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-bottom: 2pt;text-indent: 0pt;text-align: right;"><a name="bookmark182">Summary 103</a><a name="bookmark159">&zwnj;</a><a name="bookmark158">&zwnj;</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_225.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Congratulations on generating your baseline forecasts – the first set of forecasts we have generated using this book! Feel free to head over to the notebooks and play around with the parameters of the methods and see how forecasts change. It’ll help you develop an intuition around what the baseline methods are doing. If you are interested in learning more about how to make these baseline methods better, head over to the <i>Further reading </i>section, where we have provided a link to the paper <i>The Wisdom of the Data: Getting the Most Out of Univariate Time Series Forecasting</i>, by F. Petropoulos and E. Spiliotis.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And with this, we have come to the end of <i>Section 1</i>, <i>Getting Familiar with Time Series</i>. We have come a long way from just understanding what a time series is to generating competitive baseline forecasts. Along the way, we learned how to handle missing values and outliers and how to manipulate time series data using pandas. We used all those skills on a real-world dataset regarding energy consumption. We also looked at ways to visualize and decompose time series. In this chapter, we set up a test harness, learned how to use the <span class="s48">darts </span>library to generate a baseline forecast, and looked at a few metrics that can be used to understand the forecastability of a time series. For some of you, this may be a refresher, and we hope this chapter added some value in terms of some subtleties and practical considerations. For the rest of you, we hope you are in a good place, foundationally, to start venturing into modern techniques using machine learning in the next section of the book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the next chapter, we will discuss the basics of machine learning and delve into time series forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The following references were provided in this chapter:</p><ol id="l41"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Assimakopoulos, Vassilis and Nikolopoulos, K.. (2000). <i>The theta model: A decomposition approach to forecasting</i><a href="https://www.researchgate.net/publication/223049702_The_theta_model_A_decomposition_approach_to_forecasting" class="s140" target="_blank">. International Journal of Forecasting. 16. 521-530. </a><a href="https://www.researchgate.net/publication/223049702_The_theta_model_A_decomposition_approach_to_forecasting" class="s45" target="_blank">https://www. researchgate.net/publication/223049702_The_theta_model_A_ </a><span class="s46">decomposition_approach_to_forecasting</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Rob J. Hyndman, Baki Billah. (2003). <i>Unmasking the Theta method</i><a href="https://robjhyndman.com/papers/Theta.pdf" class="s140" target="_blank">. International Journal of Forecasting. 19. 287-290. </a><span class="s46">https://robjhyndman.com/papers/Theta.pdf</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;line-height: 94%;text-align: justify;">Shannon, C.E. (1948), <i>A Mathematical Theory of Communication</i><a href="https://people.math.harvard.edu/%7Ectm/home/text/others/shannon/entropy/entropy.pdf" class="s140" target="_blank">. Bell System Technical Journal, 27: 379-423. </a><a href="https://people.math.harvard.edu/%7Ectm/home/text/others/shannon/entropy/entropy.pdf" class="s45" target="_blank">https://people.math.harvard.edu/~ctm/home/text/others/ </a><span class="s46">shannon/entropy/entropy.pdf</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;line-height: 94%;text-align: justify;">Kaboudan, M. (1999). <i>A measure of time series’ predictability using genetic programming applied to stock returns</i><a href="http://www.aiecon.org/conference/efmaci2004/pdf/GP_Basics_paper.pdf" class="s140" target="_blank">. Journal of Forecasting, 18, 345-357: </a><a href="http://www.aiecon.org/conference/efmaci2004/pdf/GP_Basics_paper.pdf" class="s45" target="_blank">http://www.aiecon.org/ </a><span class="s46">conference/efmaci2004/pdf/GP_Basics_paper.pdf</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;line-height: 94%;text-align: justify;">Duan, M. (2002). <i>TIME SERIES PREDICTABILITY</i><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.1898&amp;rep=rep1&amp;type=pdf" class="s140" target="_blank">: </a><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.1898&amp;rep=rep1&amp;type=pdf" class="s45" target="_blank">https://citeseerx.ist.psu. </a><span class="s46">edu/viewdoc/download?doi=10.1.1.68.1898&amp;rep=rep1&amp;type=pdf</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark183">Further reading</a><a name="bookmark160">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">To learn more about the topics that were covered in this chapter, take a look at the following resources:</p><ul id="l42"><li><p class="s4" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">Information Theory and Entropy<a href="https://deep-and-shallow.com/2020/01/09/deep-learning-and-information-theory/" class="s140" target="_blank">, by Manu Joseph: </a><a href="https://deep-and-shallow.com/2020/01/09/deep-learning-and-information-theory/" class="s45" target="_blank">https://deep-and-shallow. </a><span class="s46">com/2020/01/09/deep-learning-and-information-theory/</span><span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">Visual Information<a href="https://colah.github.io/posts/2015-09-Visual-Information" class="s140" target="_blank">, by Chris Olah: </a><a href="https://colah.github.io/posts/2015-09-Visual-Information" class="s45" target="_blank">https://colah.github.io/posts/2015-09- </a><span class="s46">Visual-Information</span><span class="p">.</span></p></li><li><p class="s46" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;"><a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/" class="s140" target="_blank">Fourier Transform: </a><a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/" class="s45" target="_blank">https://betterexplained.com/articles/ </a>an-interactive-guide-to-the-fourier-transform/<span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">Fourier Transform <a href="https://www.youtube.com/watch?v=spUNpyF58BY&amp;vl=en" class="s140" target="_blank">by 3blue1brown – a visual introduction: </a><a href="https://www.youtube.com/watch?v=spUNpyF58BY&amp;vl=en" class="s45" target="_blank">https://www.youtube. </a><span class="s46">com/watch?v=spUNpyF58BY&amp;vl=en</span><span class="p">.</span></p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Understanding Fourier Transform by Example<a href="https://www.ritchievink.com/blog/2017/04/23/understanding-the-fourier-transform-by-example/" class="s140" target="_blank">, by Richie Vink: </a><a href="https://www.ritchievink.com/blog/2017/04/23/understanding-the-fourier-transform-by-example/" class="s45" target="_blank">https://www.ritchievink. com/blog/2017/04/23/understanding-the-fourier-transform-by- </a><span class="s46">example/</span><span class="p">.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Delgado-Bonal A, Marshak A. <i>Approximate Entropy and Sample Entropy: A Comprehensive Tutorial</i><a href="https://www.mdpi.com/1099-4300/21/6/541" class="s140" target="_blank">. Entropy. 2019; 21(6):541: </a><span class="s46">https://www.mdpi.com/1099-4300/21/6/541</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Yentes, J.M., Hunt, N., Schmid, K.K. et al. <i>The Appropriate Use of Approximate Entropy and Sample Entropy with Short Data Sets</i><a href="https://doi.org/10.1007/s10439-012-0668-3" class="s140" target="_blank">. Ann Biomed Eng 41, 349–365 (2013): </a><a href="https://doi.org/10.1007/s10439-012-0668-3" class="s45" target="_blank">https://doi. org/10.1007/s10439-012-0668-3</a></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Ponce-Flores M, Frausto-Solís J, Santamaría-Bonfil G, Pérez-Ortega J, González-Barbosa JJ. <i>Time Series Complexities and Their Relationship to Forecasting Performance</i><a href="https://www.mdpi.com/1099-4300/22/1/89" class="s140" target="_blank">. Entropy. 2020; 22(1):89. </a><a href="https://www.mdpi.com/1099-4300/22/1/89" class="s45" target="_blank">https://www.mdpi.com/1099-4300/22/1/89</a></p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Petropoulos F, Spiliotis E. <i>The Wisdom of the Data: Getting the Most Out of Univariate Time Series Forecasting</i><a href="https://doi.org/10.3390/forecast3030029" class="s140" target="_blank">. Forecasting. 2021; 3(3):478-497. </a><a href="https://doi.org/10.3390/forecast3030029" class="s45" target="_blank">https://doi.org/10.3390/ forecast3030029</a></p></li></ul></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s31" style="padding-top: 5pt;padding-left: 158pt;text-indent: 171pt;line-height: 114%;text-align: right;"><a name="bookmark184">Part 2 – Machine Learning for</a><a name="bookmark185">&zwnj;</a></p><p class="s31" style="text-indent: 0pt;line-height: 30pt;text-align: right;">Time Series</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s32" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this part, we will be looking at ways of applying modern machine learning techniques for time series forecasting. This part also covers powerful forecast combination methods and the exciting new paradigm of global models. By the end of this part, you will be able to set up a modeling pipeline using modern machine learning techniques for time series forecasting.</p><p class="s32" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This part comprises the following chapters:</p><ul id="l43"><li><p class="s34" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 5<span class="s32">, </span>Time Series Forecasting as Regression</p></li><li><p class="s34" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 6<span class="s32">, </span>Feature Engineering for Time Series Forecasting</p></li><li><p class="s34" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 7<span class="s32">, </span>Target Transformations for Time Series Forecasting</p></li><li><p class="s34" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 8<span class="s32">, </span>Forecasting Time Series with Machine Learning Models</p></li><li><p class="s34" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 9<span class="s32">, </span>Ensembling and Stacking</p></li><li><p class="s34" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 10<span class="s32">, </span>Global Forecasting Models</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark186">5</a><a name="bookmark187">&zwnj;</a><a name="bookmark189">&zwnj;</a><a name="bookmark188">&zwnj;</a></h2><h4 style="padding-top: 2pt;text-indent: 0pt;text-align: right;">Time Series Forec asting </h4><h4 style="padding-top: 4pt;text-indent: 0pt;text-align: right;">as Regression </h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous part of the book, we have developed a fundamental understanding of time series and equipped ourselves with tools and techniques to analyze and visualize time series and even generate our first baseline forecasts. We have mainly covered classical and statistical techniques in this book so far. Let’s now dip our toes into modern <span class="s5">machine learning </span>and learn how we can leverage this comparatively newer field for <span class="s5">time series forecasting</span>. Machine learning is a field that has grown in leaps and bounds in recent times, and being able to leverage these newer techniques for time series forecasting is a skill that will be invaluable in today’s world.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Understanding the basics of machine learning</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Time series forecasting as regression</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Local versus global models</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Understanding the basics of machine learning</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Before we get started with using machine learning for time series, let’s spend some time establishing what machine learning is and setting up a framework to demonstrate what it does (if you are already very comfortable with machine learning, feel free to skip ahead, or just stay with us and refresh the concepts). In 1959, Arthur Samuel defined machine learning as a “<i>field of study that gives computers the ability to learn without being explicitly programmed</i>.” Traditionally, programming has been a paradigm under which we know a set of rules/logic to perform an action, and that action is performed on the given data to get the output that we want. But, machine learning flipped this on its head. In machine learning, we start with data and the output, and we ask the computer to tell us about the rules with which the desired output can be achieved from the data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"><a name="bookmark200"><span><img width="495" height="167" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_226.gif"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 5.1 – Traditional programming versus machine learning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are many kinds of problem settings in machine learning, such as supervised learning, unsupervised learning, self-supervised learning, and so on, but we will stick to supervised learning, which is the most popular one and the most applicable one to the contents of this book.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s start our discussion small and slowly build up to the whole schematic, which encapsulates most of the key components of a supervised machine learning problem:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 75pt;text-indent: 0pt;text-align: left;"><span><img width="432" height="118" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_227.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 5.2 – Supervised machine learning schematic, part 1 – the ideal function</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">As we already discussed, what we want from machine learning is to <i>learn </i>from the data and come up with a set of rules/logic. The closest analogy in mathematics for logic/rules is a function, which takes in an input (here, data) and provides an output. Mathematically, it can be written as follows:</p><p class="s132" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑦𝑦 = 𝑔𝑔(𝑋𝑋)</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">where <i>X </i>is the set of features and <i>g </i>is the <span class="s5">ideal target function </span>(denoted by <span class="s5">1 </span>in <i>Figure 5.2</i>) that maps the <i>X </i>input (denoted by <span class="s5">2 </span>in the schematic) to the target (ideal) output, <i>y </i>(denoted by <span class="s5">3 </span>in the schematic). The ideal target function is largely an unknown function, similar to the <span class="s5">data generating process </span>(<span class="s5">DGP</span><a href="#bookmark10" class="s140">) we saw in </a><i>Chapter 1</i>, <i>Introducing Time Series</i>, which is not in our control.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;text-align: left;"><a name="bookmark201"><span><img width="344" height="136" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_228.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 5.3 – Supervised machine learning schematic, part 2 – the learned approximation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_229.png"/></span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">But,  we  want  the  computer  to  <i>learn  </i>this  ideal  target  function.  This  approximation  of  the  ideal target function is denoted by another function, <i>h </i>(<span class="s5">4 </span>in the schematic), which takes in the same set of features, <i>X</i>, and outputs a predicted target,  <span class="s133">̂</span><span class="s95">    </span>(<span class="s5">5 </span>in the schematic). <span class="s134">Φ  </span>are the parameters of the <i>h </i>function (or model parameters):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;text-align: left;"><span><img width="351" height="144" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_230.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 5.4 – Supervised machine learning schematic, part 3 – putting it all together</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, how do we find this approximation <i>h </i>function and its parameters, <span class="s135">Φ</span>? With the dataset of examples (<span class="s5">6 </span>in the schematic). The supervised machine learning problem works on the premise that we are able to collect a set of examples that shows the features, <i>X</i>, and the corresponding target, <i>y</i>, which is also referred to as <i>labels </i>in literature. It is from this set of examples (the dataset) that the computer <i>learns </i>the approximation function, <i>h</i>, and the optimal model parameters, <span class="s58">Φ</span>. In the preceding diagram, the only real unknown entity is the ideal target function, <i>g</i>. So, we can use the training dataset, <i>D</i>, to get predicted targets for every sample in the dataset. We already know the ideal target for all the examples. We need a way to compare the ideal targets and predicted targets, and this is where the loss function (<span class="s5">7 </span>in the schematic) comes in. This loss function tells us how far away from the real truth we are with the approximated function, <i>h</i>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although <i>h </i>can be any function, it is typically chosen from a set of a well-known class of functions, <span class="s103">ℋ</span>. <span class="s136">ℋ  </span>is the finite set of functions that can be fit to the data. This class of functions is what we colloquially call <span class="s5">models</span>. For instance, <i>h </i>can be chosen from all the linear functions or all the tree- based functions, and so on. Choosing an <i>h </i>from <span class="s136">ℋ </span>is done by a combination of hyperparameters (which the modeler specifies) and the model parameter, which is learned from data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark202">Now, all that is left is to run through the different functions so that we find the best approximation function, </a><i>h</i>, which gives us the lowest loss. This is an optimization process that we call <span class="s5">training</span>.<a name="bookmark191">&zwnj;</a><a name="bookmark190">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s also take a look at a few key concepts, which will be important in all our discussions ahead.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Supervised machine learning tasks</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Machine learning can be used to solve a wide variety of tasks such as <span class="s5">regression</span>, <span class="s5">classification</span>, and <span class="s5">recommendation</span>. But, since classification and regression are the most popular classes of problems, we will spend just a little bit of time reviewing what they are.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The difference between classification and regression tasks is very simple. In the machine learning schematic (<i>Figure 5.2</i>), we talked about <i>y</i>, the target. This target can be either a real-valued number or a class of items. For instance, we could be predicting the stock price for next week or we could just predict whether the stock was going to go up or down. In the first case, we are predicting a real- valued number, which is called <span class="s5">regression</span>. In the other case, we are predicting one out of two classes (<i>up </i>or <i>down</i>), and this is called <span class="s5">classification</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Overfitting and underfitting</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The biggest challenge in machine learning systems is that the model we trained must perform well on a new and unseen dataset. The ability of a machine learning model to do that is called the <span class="s5">generalization capability </span>of the model. The training process in a machine learning setup is akin to mathematical optimization, with one subtle difference. The aim of mathematical optimization is to arrive at the global maxima in the provided dataset. But in machine learning, the aim is to achieve low test error by using the training error as a proxy. How well a machine learning model is doing on training error and testing error is closely related to the concepts of overfitting and underfitting. Let’s use an example to understand these terms.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The learning process of a machine learning model has many parallels to how humans learn. Suppose three students, <i>A</i>, <i>B</i>, and <i>C</i>, are studying for an examination. <i>A </i>is a slacker and went clubbing the night before. <i>B </i>decided to double down and memorize the textbook end to end. And, <i>C </i>paid attention in class and understood the topics up for the examination.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">As expected, <i>A </i>flunked the examination, <i>C </i>got the highest score, and <i>B </i>did OK.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><i>A </i>flunked the examination because they didn’t learn enough. This happens to machine learning models as well when they don’t learn enough patterns, and this is called <span class="s5">underfitting</span>. This is characterized by high training errors and high test errors.</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">B <span class="p">didn’t score as highly as expected; after all, they did memorize the whole text, word for word. But many questions in the examination weren’t directly from the textbook and </span>B <span class="p">wasn’t able to answer them correctly. In other words, the questions in the examination were </span>new and unseen<span class="p">. And because </span>B <span class="p">memorized everything but didn’t make an effort to understand the underlying concepts, </span>B <span class="p">wasn’t</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark203">able to </a><i>generalize </i>the knowledge they had to new questions. This situation, in machine learning, is called <span class="s5">overfitting</span>. This is typically characterized by a big delta in training and test errors. Typically, we will see very low training errors and high test errors.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The third student, <i>C</i>, learned the right way and understood the underlying concepts and because of that, was able to <i>generalize </i>to <i>new and unseen </i>questions. This is the ideal state for a machine learning model, as well. This is characterized by reasonably low test errors and a small delta between training and test errors.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We just saw the two greatest challenges in machine learning. Now, let’s also look at a few ways we have that can be used to tackle these challenges.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There is a close relationship between the <span class="s5">capacity </span>of a model and underfitting or overfitting. A model’s capacity is its ability to be flexible enough to fit a wide variety of functions. Models with low capacity may struggle to fit the training data, leading to underfitting. Models with high capacity may overfit by memorizing the training data too much. Just to develop an intuition around this concept of capacity, let’s look at an example. When we move from linear regression to polynomial regression, we are adding more capacity to the model. Instead of fitting just straight lines, we are letting the model fit curved lines as well. Machine learning models generally do well when their capacity is appropriate for the learning problem at hand.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 61pt;text-indent: 0pt;text-align: left;"><span><img width="439" height="354" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_231.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 5.5 – Underfitting versus overfitting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark204">Figure 5.5 </a><span class="p">shows a very popular case to illustrate overfitting and underfitting. We create a few random points using a known function and try to learn that by using those data samples. We can see that the linear regression, which is one of the simplest models, has underfitted the data by drawing a straight line through those points. Polynomial regression is linear regression, but with some higher-order features. For now, you can consider the move from linear regression to polynomial regression with higher degrees as increasing the capacity of the model. So, when we use a degree of 4, we see that the learned function fits the data well and matches our ideal function. But if we keep increasing the capacity of the model and reach </span>degree = 15<span class="p">, we see that the learned function is still passing through the training samples, but has learned a very different function, overfitting to the training data. Finding the optimal capacity to learn a generalizable function is one of the core challenges of machine learning.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">While capacity is one aspect of the model, another aspect is <span class="s5">regularization</span>. Even with the same capacity, there are multiple functions a model can choose from the hypothesis space of all functions. With regularization, we try to give preference to a set of functions in the hypothesis space over the others. While all these functions are valid functions that can be chosen, we nudge the optimization process in such a way that we end up with a kind of function toward which we have a preference. Although regularization is a general term used to refer to any kind of constraint we place on the learning process to reduce the complexity of the learned function, more commonly, it is used in the form of a weight decay. Let’s take an example of linear regression, which is when we fit a straight line to the input features by learning a weight associated with each feature.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">A linear regression model can be written mathematically as follows:</p><p class="s127" style="padding-top: 6pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑁𝑁</p><p class="s137" style="padding-top: 4pt;padding-left: 39pt;text-indent: 0pt;text-align: center;">𝑦𝑦̂ = 𝑐𝑐 + ∑ 𝑤𝑤<span class="s128">𝑖𝑖   </span>× 𝑥𝑥<span class="s128">𝑖𝑖</span></p><p class="s127" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑖𝑖=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_232.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_233.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_234.png"/></span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <i>N </i>is the number of features, <i>c </i>is the intercept,     is the <i>i</i>th feature, and     is the weight associated with the <i>i</i>th feature. We estimate the right weight (<i>L</i>) by considering this as an optimization problem <span class="s138">t</span>hat minimizes the error between  <span class="s131">̂   </span><span class="s138">a</span>nd <i>y </i>(real output).</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, with regularization, we add an additional term to <i>L</i>, which forces the weights to become smaller. Commonly, this is done using an <i>l1 </i>or <i>l2 </i>regularizer. An <i>l1 </i>regularizer is when you add the sum of squared weights to <i>L</i>:</p><p class="s75" style="padding-top: 1pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑁𝑁</p><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"/><p class="s74" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝐿𝐿  + 𝜆𝜆 ∑ 𝑤𝑤2</p><p class="s75" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑖𝑖=1</p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">where   <span><img width="9" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_235.png"/></span><span class="s139">  </span>is the regularization coefficient that determines how strongly we penalize the weights. An <i>l2 </i>regularizer is when you add the sum of absolute weights to <i>L</i>:</p><p class="s128" style="padding-top: 3pt;padding-left: 233pt;text-indent: 0pt;text-align: left;">𝑁𝑁 <span class="s127">2</span></p><p class="s90" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝐿𝐿 + 𝜆𝜆 ∑|𝑤𝑤<span class="s128">𝑖</span><span class="s127">𝑖</span>|</p><p class="s127" style="padding-top: 3pt;padding-left: 230pt;text-indent: 0pt;text-align: left;">𝑖𝑖=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark205">In both cases, we are enforcing a preference for smaller weights over larger weights because it keeps the function from relying too much on any one feature from the ones used in the machine learning model. Regularization is an entire topic unto itself and if you want to learn more, head over to the </a><i>Further reading </i>section for a few resources on regularization.<a name="bookmark192">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Another really effective way to reduce overfitting is to simply train the model with more data. With a larger dataset, the chances of the model overfitting become less because of the sheer variety that can be captured in a large dataset.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, how do we tune the knobs to strike a balance between underfitting and overfitting? Let’s look at it in the following section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Hyperparameters and validation sets</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Almost all machine learning models have a few hyperparameters associated with them. <span class="s5">Hyperparameters </span>are parameters of the model that are not learned from data but rather are set before the start of training. For instance, the weight of the regularization is a hyperparameter. Most hyperparameters either help us control the capacity of the model or apply regularization to the model. By controlling either capacity or regularization or both, we can travel the frontier between underfitting and overfitting models and arrive at a model that is <i>just right</i>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="169" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_236.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Suggested reading</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Although we have touched the surface of machine learning in the book, there is a lot more, and to truly appreciate the rest of the book better, we suggest gaining more understanding of machine learning. We suggest starting with <i>Machine Learning </i>by <i>Stanford (Andrew Ng)</i></p><p style="padding-left: 13pt;text-indent: 0pt;text-align: justify;"><a href="https://www.coursera.org/learn/machine-learning" class="s140" target="_blank">– </a><span class="s46">https://www.coursera.org/learn/machine-learning</span>. If you are in a hurry, the <i>Machine Learning Crash Course </i>by <i>Google </i><a href="https://developers.google.com/machine-learning/crash-course/ml-intro" class="s140" target="_blank">is also a good starting point – </a><a href="https://developers.google.com/machine-learning/crash-course/ml-intro" class="s45" target="_blank">https://developers.google.com/machine-learning/crash-</a></p><p class="s46" style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">course/ml-intro<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">But since these hyperparameters have to be set outside of the algorithm, how do we estimate the best hyperparameters? Although it is not part of the core <i>learning process</i>, we learn the hyperparameters also from the data. But if we just use the training data to learn the hyperparameters, it will just choose the maximum possible model capacity, which results in overfitting. This is where we need a <span class="s5">validation set</span>, a part of the data that the training process does not have access to. But when the dataset is small (not hundreds of thousands of samples), the performance on a single validation set doesn’t guarantee a fair evaluation. In such cases, we rely on <span class="s5">cross-validation</span>. The general trick is to repeat the training and evaluation procedure on different subsets of the original dataset. A common way of doing this is called <span class="s5">k-fold cross validation</span>, when the original dataset is divided into <i>k </i>equal, non-overlapping, and random subsets, and each subset is evaluated after training on all the other subsets. We have provided a link in the <i>Further reading </i>section if you want to read up about cross- validation techniques. Later in the book, we will also be covering this topic, but from the time series perspective, which has a few differences from the standard way of doing cross-validation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark206">Now that we have a basic understanding of machine learning, let’s start looking at how we can use it to do time series forecasting.</a><a name="bookmark194">&zwnj;</a><a name="bookmark193">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Time series forecasting as regression</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark10" class="s140">A time series, as we saw in </a>Chapter 1<span class="p">, </span>Introducing Time Series<span class="p">, is a set of observations taken sequentially in time. And typically, time series forecasting is about trying to predict what these observations will be in the future. Given a sequence of observations of arbitrary length of history, we predict the future to an arbitrary horizon.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We saw that regression, or machine learning to predict a continuous variable, works on a dataset of examples, and each example is a set of input features and targets. We can see that regression, which is tasked with predicting a single output provided with a set of inputs, is fundamentally incompatible with forecasting, where we are given a set of historical values and asked to predict the future values. This fundamental incompatibility between the time series and machine learning regression paradigms is why we cannot use regression for time series forecasting directly.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Moreover, time series forecasting, by definition, is an extrapolation problem, whereas regression, most of the time, is an interpolation one. Extrapolation is typically harder to solve using data-driven methods. Another key assumption in regression problems is that the samples used for training are <span class="s5">independent and identically distributed </span>(<span class="s5">IID</span>). But time series break that assumption as well because subsequent observations in a time series display considerable dependence.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">However, to use the wide variety of techniques from machine learning, we need to cast time series forecasting as a regression. Thankfully, there are ways to convert a time series into a regression and get over the IID assumption by introducing some memory to the machine learning model through some features. Let’s see how it can be done.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Time delay embedding</p><p class="s4" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark137" class="s140">We talked about the ARIMA model in </a>Chapter 4<span class="p">, </span>Setting a Strong Baseline Forecast<span class="p">, and saw how it is an autoregressive model. We can use the same concept to convert a time series problem into a regression one. Let’s use the following diagram to make the concept clear:</span></p><p style="padding-top: 3pt;padding-left: 278pt;text-indent: 0pt;text-align: left;"><a name="bookmark207">Time series forecasting as regression 115</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_237.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span><img width="525" height="198" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_238.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 5.6 – Time series to regression conversion using a sliding window</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_239.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_240.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_241.png"/></span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 92%;text-align: justify;">Let’s assume we have a time series with time steps,  <span><img width="11" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_242.png"/></span>. Consider we are at time <i>t</i>, and we have a time series of the length of history as <i>L</i>. So, our time series will look something like in the diagram with as the latest observation in the time series, and <span class="s68">−1, </span><span class="s98">−2</span><span class="s68">, </span>and so on as we move backward in time.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_243.png"/></span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In an ideal world, each observation in should be conditioned on all the previous observations when we forecast. But, it is not practical because <i>L </i>can be arbitrarily long. We often restrict the forecasting function to use only the most recent <i>M </i>observations of the series, where <i>M &lt; L</i>. These are called finite memory models or <span class="s5">Markov models </span>and <i>M </i>is called the order of autoregression, memory size, or the receptive field.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Therefore, in time delay embedding, we assume a window of arbitrary length <i>M &lt; L </i>and extract fixed-length subsequences from the time series by sliding the window over the length of the time series.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_244.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_245.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_246.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_247.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_248.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_249.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_250.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_251.png"/></span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;">In the diagram, we have taken a sliding window with a memory size of 3. So, the first subsequence we can extract (if we are starting from the most recent and working backward) is <span class="s68">−3</span><span class="s141">, −2, −1 </span>. And is the observation that comes right after the subsequence. This becomes our first example in the dataset (row 1 in the table in the diagram). Now, we slide the window one time step to the left (backward <span class="s138">in time) and extract the new subsequence, </span><span class="s68">−4</span><span class="s131">, </span><span class="s142">−3</span><span class="s131">, </span><span class="s142">−2 </span><span class="s138">. The corresponding target would </span>become <span class="s68">−1</span>. We repeat this process as we move back to the beginning of the time series, and at each step of the sliding window, we add one more example to the dataset.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">At the end of it, we have an aligned dataset with a fixed vector size of features (which will be equal to the window size) and a single target, which is what a typical machine learning dataset looks like.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark208">Now that we have a table with three features, let’s also assign semantic meaning to the three features. If we look at the right-most column in the table in the diagram, we can see that the time step present in the column is always one time step behind the target. We call it </a><span class="s5">Lag 1</span>. The second column from the right is always two time steps behind the target, and this is called <span class="s5">Lag 2</span>. Generalizing this, the feature that has observations that are <i>n </i>time steps behind the target, we call <span class="s5">Lag n</span>.<a name="bookmark196">&zwnj;</a><a name="bookmark195">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This transformation from time series to regression using <span class="s5">time-delay embedding </span>encodes the autoregressive structure of a time series in a way that can be utilized by standard regression frameworks. Another way we can think about using regression for time series forecasting is to perform <span class="s5">regression on time</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Temporal embedding</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">If we rely on previous observations in autoregressive models, we rely on the concept of time for temporal embedding models. The core idea is that we forget the autoregressive nature of the time series and assume that any value in the time series is only dependent on time. We derive features that capture time, the passage of time, periodicity of time, and so on, from the timestamps associated with the time series, and then we use these features to predict the target using a regression model. There are many ways to do this, from simply aligning a monotonically and uniformly increasing numerical column that captures the passage of time to sophisticated <span class="s5">Fourier </span><a href="#bookmark211" class="s140">terms to capture the periodic components in time. We will talk about those techniques in detail in </a><i>Chapter 6</i>, <i>Feature Engineering for Time Series Forecasting</i>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Before we wind up the chapter, let’s also talk about a key concept that is gaining ground steadily in the time series forecasting space. A large part of this book embraces this new paradigm of forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Global forecasting models – a paradigm shift</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Traditionally, each time series was treated in isolation. Because of that, traditional forecasting has always looked at the history of a single time series alone in fitting a forecasting function. But recently, because of the ease of collecting data in today’s digital-first world, many companies have started collecting large amounts of time series from similar sources, or related time series.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">For example, retailers such as Walmart collect data on sales of millions of products across thousands of stores. Companies such as Uber or Lyft collect the demand for rides from all the zones in a city. In the energy sector, energy consumption data is collected across all consumers. All these sets of time series have shared behavior and are hence called <span class="s5">related time series</span>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can consider that all the time series in a related time series come from separate <span class="s5">data generating processes </span>(<span class="s5">DGPs</span>), and thereby model them all separately. We call these the <span class="s5">local </span>models of forecasting. An alternative to this approach is to assume that all the time series are coming from a single DGP. Instead of fitting a separate forecast function for each time series individually, we fit a single forecast function to all the related time series. This approach has been called <span class="s5">global </span>or <span class="s5">cross- learning </span>in literature.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="89" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_252.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s29" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The terminology <i>global </i>was introduced by <i>David Salinas et al. </i>in the <i>DeepAR </i>paper (referenc number 1) and <i>Cross-learning </i>by <i>Slawek Smyl </i>(reference number 2).</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">e</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 245pt;text-indent: 0pt;text-align: left;">Global forecasting models – a paradigm shift 117</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We saw earlier that having more data will lead to lower chances of overfitting and, therefore, lower generalization error (the difference between training and testing errors). This is exactly one of the shortcomings of the local approach. Traditionally, time series are not very long, and in many cases, it is difficult and time-consuming to collect more data as well. Fitting a machine learning model (with all its expressiveness) on small data is prone to overfitting. This is why time series models that enforce strong priors were used to forecast such time series, traditionally. But these strong priors, which restrict the fitting of traditional time series models, can also lead to a form of underfitting and limit accuracy.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Strong and expressive data-driven models, as in machine learning, require a larger amount of data to have a model that generalizes to new and unseen data. A time series, by definition, is tied to time, and sometimes, collecting more data means waiting for months or years and that is not desirable. So, if we cannot increase the <i>length </i>of the time-series dataset, we can increase the <i>width </i>of the time series dataset. If we add multiple time series to the dataset, we increase the width of the dataset, and there by increase the amount of data the model is getting trained with. <i>Figure 5.7 </i>shows the concept of increasing the width of a time series dataset visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;"><span><img width="356" height="347" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_253.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 5.7 – The length and width of a time series dataset</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark209">This works in favor of machine learning models because with higher flexibility in fitting a forecast function and the addition of more data to work with, the machine learning model can learn a more complex forecast function than traditional time series models, which are typically shared between the related time series, in a completely data-driven way.</a><a name="bookmark197">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another shortcoming of the local approach revolves around scalability. In the case of Walmart we mentioned earlier, there are millions of time series that need to be forecasted and it is not possible to have human oversight on all these models. If we think about this from an engineering perspective, training and maintaining millions of models in a production system would give any engineer a nightmare. But under the global approach, we only train a single model for all these time series, which drastically reduces the number of models we need to maintain and yet can generate all the required forecasts.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This new paradigm of forecasting has gained traction and has consistently been shown to improve the local approaches in multiple time series competitions, mostly in datasets of related time series. In Kaggle competitions, such as <i>Rossman Store Sales </i>(2015), <i>Wikipedia WebTraffic Time Series Forecasting </i>(2017), <i>Corporación Favorita Grocery Sales Forecasting </i>(2018), and <i>M5 Competition </i>(2020), the winning entries were all global models—either machine learning or deep learning or a combination of both. The <i>Intermarché Forecasting Competition </i>(2021) also had global models as the winning submissions. Links to these competitions are provided in the <i>Further reading </i>section.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_254.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The <i>Montero-Manson and Hyndman </i>(2020) research paper is cited in <i>References </i>und reference number 3.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">er</p><p style="text-indent: 0pt;text-align: left;"/><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="p">Although we have many empirical findings where the global models have outperformed local models for related time series, global models are still a relatively new area of research. </span>Montero-Manson and Hyndman <a href="#bookmark367" class="s140">(2020) showed a few very interesting results and showed that any local method can be approximated by a global model with required complexity, and the most interesting finding they put forward is that the global model will perform better, even with unrelated time series. We will talk more about global models and strategies for global models in </a>Chapter 10<span class="p">, </span>Global Forecasting Models<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have started our journey beyond baseline forecasting methods and dipped our toes into the world of machine learning. After a brief refresher on machine learning, where we looked at key concepts such as overfitting, underfitting, regularization, and so on, we saw how we can convert a time series forecasting problem into a regression problem from the machine learning world. We also developed a conceptual understanding of different embeddings, such as time delay embedding and temporal embedding, which can be used to convert a time series problem into a regression problem. To wrap things up, we also learned about a new paradigm in time series forecasting – global models – and contrasted them with local models on a conceptual level. In the next few chapters, we will start putting these concepts into practice, and see techniques for feature engineering, and strategies for global models.</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: right;"><a name="bookmark210">References 119</a><a name="bookmark199">&zwnj;</a><a name="bookmark198">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_255.png"/></span></p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Following are the references that we used in this chapter:</p><ol id="l44"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). <i>DeepAR: Probabilistic forecasting with autoregressive recurrent networks</i><a href="https://doi.org/10.1016/j.ijforecast.2019.07.001" class="s140" target="_blank">. International Journal of Forecasting. 36-3. 1181-1191: </a><a href="https://doi.org/10.1016/j.ijforecast.2019.07.001" class="s45" target="_blank">https://doi.org/10.1016/j.ijforecast.2019.07.001</a></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Slawek Smyl (2020). <i>A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting</i><a href="https://doi.org/10.1016/j.ijforecast.2019.03.017" class="s140" target="_blank">. International Journal of Forecasting. 36-1: 75-85 </a><a href="https://doi.org/10.1016/j.ijforecast.2019.03.017" class="s45" target="_blank">https://doi. org/10.1016/j.ijforecast.2019.03.017</a></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Montero-Manso, P., Hyndman, R.J.. (2020), <i>Principles and algorithms for forecasting groups of time series: Locality and globality</i><a href="https://arxiv.org/abs/2008.00444" class="s140" target="_blank">. arXiv:2008.00444[cs.LG]: </a><a href="https://arxiv.org/abs/2008.00444" class="s45" target="_blank">https://arxiv.org/ abs/2008.00444</a></p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">You can check out the following resources for further reading:</p></li><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: justify;">Regularization for Sparsity from Google Machine Learning Crash Course<a href="https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization" class="s140" target="_blank">: </a><a href="https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization" class="s45" target="_blank">https:// developers.google.com/machine-learning/crash-course/ regularization-for-sparsity/l1-regularization</a></p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">L1 and L2 Regularization from Foundations of Machine Learning, Bloomberg ML EDU<span class="p">:</span></p><p style="padding-top: 1pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><a href="https://www.youtube.com/watch?v=d6XDOS4btck" class="s45">https://www.youtube.com/watch?v=d6XDOS4btck</a></p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: justify;">Cross-validation: evaluating estimator performance from scikit-learn<a href="https://scikit-learn.org/stable/modules/cross_validation.html" class="s140" target="_blank">: </a><a href="https://scikit-learn.org/stable/modules/cross_validation.html" class="s45" target="_blank">https://scikit- learn.org/stable/modules/cross_validation.html</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: left;">Rossmann Store Sales<a href="https://www.kaggle.com/c/rossmann-store-sales" class="s140" target="_blank">: </a><a href="https://www.kaggle.com/c/rossmann-store-sales" class="s45" target="_blank">https://www.kaggle.com/c/rossmann-store- sales</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: left;">Web Traffic Time Series Forecasting <a href="https://www.kaggle.com/c/web-traffic-time-series-forecasting" class="s140" target="_blank">– </a><a href="https://www.kaggle.com/c/web-traffic-time-series-forecasting" class="s45" target="_blank">https://www.kaggle.com/c/ web-traffic-time-series-forecasting</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: left;">Corporación Favorita Grocery Sales Forecasting <a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting" class="s140" target="_blank">– </a><a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting" class="s45" target="_blank">https://www.kaggle.com/c/ favorita-grocery-sales-forecasting</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: left;">M5 Forecasting – Accuracy <a href="https://www.kaggle.com/c/m5-forecasting-accuracy" class="s140" target="_blank">– </a><a href="https://www.kaggle.com/c/m5-forecasting-accuracy" class="s45" target="_blank">https://www.kaggle.com/c/m5-forecasting- accuracy</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark211">6</a><a name="bookmark212">&zwnj;</a><a name="bookmark214">&zwnj;</a><a name="bookmark213">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 90pt;text-indent: -1pt;line-height: 114%;text-align: left;">Feature Engineering for Time Series Forec asting </h4><p style="padding-top: 24pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous chapter, we started looking at <span class="s5">machine learning </span>(<span class="s5">ML</span>) as a tool to solve the problem of <span class="s5">time series forecasting</span>. We also talked about a few techniques such as <span class="s5">time delay embedding </span>and <span class="s5">temporal embedding</span>, which cast time series forecasting problems as classical regression problems from the ML paradigm. In this chapter, we’ll look at those techniques in detail and go through them in a practical sense using the dataset we have been working with throughout this book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will cover the following topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Feature engineering</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Avoiding data leakage</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Setting a forecast horizon</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Time delay embedding</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Temporal embedding</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to run these notebooks:</p></li><li><p class="s48" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">02-Preprocessing London Smart Meter Dataset.ipynb from Chapter02</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">01-Setting up Experiment Harness.ipynb from Chapter04</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s46" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter06" class="s140" target="_blank" name="bookmark228">The code for this chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter06" class="s45" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter06<span class="p">.</span><a name="bookmark215">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Feature engineering</p><p class="s5" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Feature engineering<span class="p">, as the name suggests, is the process of engineering features from the data, mostly using domain knowledge, to make the learning process smoother and more efficient. In a typical ML setting, engineering good features is essential to get good performance from any ML model. Feature engineering is a highly subjective part of ML where each problem at hand has a different path – one that is hand-crafted to that problem.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="p">When we are casting a time series problem as a regression problem, there are a few standard techniques that we can apply. This is a key step in the process because how well an ML model acquires an understanding of </span>time <span class="p">is dependent on how well we engineer features to capture </span>time<a href="#bookmark137" class="s140">. The baseline methods we covered in </a>Chapter 4<span class="p">, </span>Setting a Strong Baseline Forecast<span class="p">, are the methods that are created for the specific use case of time series forecasting and because of that, the temporal aspect of the problem is built into those models. For instance, ARIMA doesn’t need any feature engineering to understand time because it is built into the model. But a standard regression model has no explicit understanding of time, so we need to create good features to embed the temporal aspect of the problem.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="89" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_256.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">To follow along with the complete code, use the <span class="s48">01-Feature Engineering.ipynb</span></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">notebook in the <span class="s48">chapter06 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">In the previous chapter (</a><i>Chapter 5</i>, <i>Time Series Forecasting as Regression</i>), we talked about two main ideas to encode time into the regression framework: <span class="s5">time delay embedding </span>and <span class="s5">temporal embedding</span>. Although we touched on these concepts at a high level, it is time to dig deeper and see them in action.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have already split the dataset that we were working on into train, validation, and test datasets. But since we are generating features that are based on previous observations, operationally, it is better when we have the train, validation, and test datasets combined. It will be clearer why shortly, but for now, let’s take it on faith and move ahead. Now, let’s go ahead and combine the two datasets:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Reading the missing value imputed and train test split data</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">train_df = pd.read_parquet(preprocessed / &quot;selected_blocks_ train_missing_imputed.parquet&quot;)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">val_df = pd.read_parquet(preprocessed / &quot;selected_blocks_val_ missing_imputed.parquet&quot;)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">test_df = pd.read_parquet(preprocessed / &quot;selected_blocks_test_ missing_imputed.parquet&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p class="s47" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark229">Avoiding data leakage 123</a><a name="bookmark216">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_257.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">#Adding train, validation and test tags to distinguish them before combining</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">train_df[&#39;type&#39;] = &quot;train&quot; val_df[&#39;type&#39;] = &quot;val&quot; test_df[&#39;type&#39;] = &quot;test&quot;</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">full_df = pd.concat([train_df, val_df, test_df]).sort_ values([&quot;LCLid&quot;, &quot;timestamp&quot;])</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">del train_df, test_df, val_df</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Now, we have a <span class="s48">full_df </span>that combines the train, validation, and test datasets. Some of you may already have alarm bells ringing in your head at combining the train and test sets. What about <span class="s5">data leakage</span>? Let’s check it out.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Avoiding data leakage</p><p class="s5" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Data leakage <span class="p">occurs when the model is trained with some information that would not be available at the time of prediction. Typically, this leads to high performance in the training set, but very poor performance in unseen data. There are two types of data leakage:</span></p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Target leakage <span class="p">is when the information about the target (that we are trying to predict) leaks into some of the features in the model, leading to an overreliance of the model on those features, ultimately leading to poor generalization. This includes features that use the target in any way.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Train-test contamination <span class="p">is when there is some information leaking between the train and test datasets. This can happen because of careless handling and splitting of data. But it can also happen in more subtle ways, such as scaling the dataset before splitting the train and test sets.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">When we are working with time series forecasting problems, the biggest and most common mistake that we can make is target leakage. We will have to think hard about each of the features to ensure we are not using any data that will not be available during prediction. The following diagram will help us remember and internalize this concept:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><img width="531" height="118" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_258.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 6.1 – Usable and not-usable information to avoid data leakage</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="152" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_259.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Best practices</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">There are many ways of identifying target leakage, apart from thinking hard about the feature</p><ul id="l45"><li><p style="padding-top: 3pt;padding-left: 19pt;text-indent: -6pt;text-align: left;">If the model you’ve built is too good to be true, you most likely have a leakage problem</p></li><li><p style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">If any single feature has too much weightage in the feature importance of the model, tha feature may have a problem with leakage</p></li><li><p style="padding-top: 3pt;padding-left: 19pt;text-indent: -6pt;text-align: left;">Double-check the features that are highly correlated with the target</p></li></ul><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 1pt;text-indent: -1pt;line-height: 245%;text-align: left;">s: t</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark230">To make this concept clearer and more relevant to the time series forecasting context, let’s look at an example. Let’s say we are forecasting sales for shampoo, and we are using sales for conditioner as a feature. We developed the model, trained it on the training data, and tested it on the validation data. The model is doing very well. The moment we start predicting for the future, we will see the problem. We don’t know what the sales for conditioner are in the future either. While this example is pretty straightforward, there will be times when this becomes not so obvious. And that is why we need to exercise a fair amount of caution while creating features and always evaluate the features through the lens of, </a><i>will this feature be available at the time of prediction?</i><a name="bookmark217">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s learn about forecast horizons.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Setting a forecast horizon</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Although we generated forecasts earlier in this book, we never explicitly discussed <span class="s5">forecast horizons</span><a href="#bookmark186" class="s140">. A forecast horizon is the number of time steps into the future we want to forecast at any point in time. For instance, if we want to forecast the next 24 hours for the electricity consumption dataset that we have been working with, the forecast horizon becomes 48 (because the data is half-hourly). In </a><a href="#bookmark186" class="s21">Chapter </a><i>5</i>, <i>Time Series Forecasting as Regression, </i>where we generated baselines, we just predicted the entire test data at once. In such cases, the forecast horizon becomes equal to the length of the test data.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We never had to worry about this until now because, in the classical statistical methods of forecasting, this decision is decoupled from modeling. If we train a model, we can use that model to predict any future point without retraining. But with <i>time series forecasting as regression</i>, we have a constraint on the forecast horizon and it has its roots in data leakage. For now, let’s only look at single-step-ahead forecasting. In the context of the dataset we are working with, this means that we will be answering the question, <i>what is the energy consumption in the next half an hour? </i>We will talk about multi-step forecasting and other mechanics of forecasting in <i>Section 4 – The Mechanics of Forecasting</i>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Now that we have set some ground rules, let’s start looking at the different feature engineering techniques. To follow along with the Jupyter notebook, head over to the <span class="s48">chapter06 </span>folder and use the <span class="s48">01-Feature Engineering.ipynb </span>file.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark231">Time delay embedding</a><a name="bookmark219">&zwnj;</a><a name="bookmark218">&zwnj;</a></p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">The basic idea behind time delay embedding is to embed time in terms of recent observations. If you want to head back to </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, and review this concept (</span>Figure 5.1<span class="p">), please go ahead and do so now.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In <i>Figure 5.1</i>, we talked about including previous observations of a time series as <span class="s5">lags</span>. However, there are a few more ways to capture recent and seasonal information using this concept. Let’s take a look.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Lags or backshift</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Let’s assume we have a time series with time steps, <span class="s90">𝑌𝑌</span><span class="s128">𝐿</span><span class="s127">𝐿</span>. Consider that we are at time <i>t </i>and that we have a time series where the length of history is <i>L</i>. So, our time series will have <span class="s90">𝑦𝑦</span><span class="s128">𝑡𝑡</span><span class="s127"> </span>as the latest observation in the time series, and then <span class="s90">𝑦𝑦</span><span class="s128">𝑡</span><span class="s127">𝑡−1</span><span class="s90">, 𝑦𝑦</span><span class="s128">𝑡</span><span class="s127">𝑡−2</span><span class="s90">, </span><a href="#bookmark186" class="s140">and so on as we move back in time. So, lags, as explained in </a><a href="#bookmark186" class="s21">Chapter </a><i>5</i>, <i>Time Series Forecasting as Regression</i>, are features that include the previous observations in the time series, as shown in the following diagram:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 64pt;text-indent: 0pt;text-align: left;"><span><img width="429" height="300" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_260.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 173pt;text-indent: 0pt;text-align: center;">Figure 6.2 – Lag features</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can create multiple lags by including observations that are <i>a </i>timesteps before (<span class="s143">𝑦</span><span class="s90">𝑦</span><span class="s127">𝑡𝑡−𝑎𝑎</span>); we will call this <i>Lag a</i>. In the preceding diagram, we have shown <span class="s5">Lag 1</span>, <span class="s5">Lag 2</span>, and <span class="s5">Lag 3</span>. However, we can add any number of lags we like. Now, let’s learn how to do that in code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: center;"><span class="s49" style=" background-color: #F3F2F1;">  df[&quot;lag_1&quot;]=df[&quot;column&quot;].shift(1)                               </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark232">Remember when we combined the train and test datasets and I asked you to take it in good faith? It’s time to put a reason to that faith. If we consider the lag operation (or any autoregressive feature), it relies on a continuous representation along the time axis. If we consider the test dataset, for the first few rows (or earliest dates), the lags would be missing because it is part of the training dataset. So, by combining the two, we create a continuous representation along the time axis where standard functions in pandas such as </a><span class="s48">shift </span>can be utilized to create these features easily and efficiently.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">It is as simple as that, but we need to perform the lag operation for each <span class="s48">LCLid </span>separately. We have included a helpful method in <span class="s48">src.feature_engineering.autoregressive_features </span>called <span class="s48">add_lags </span>that adds all the lags you want for each <span class="s48">LCLid </span>in a fast and efficient manner. Let’s see how we can use that.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We are going to import the method and use a few of its parameters to configure the lag operation the way we want:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.feature_engineering.autoregressive_features import add_lags</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Creating first 5 lags and then same 5 lags but from previous day and previous week to capture seasonality</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">lags = (</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">(np.arange(5) + 1).tolist()</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">+ (np.arange(5) + 46).tolist()</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">+ (np.arange(5) + (48 * 7) - 2).tolist()</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">full_df, added_features = add_lags(</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">full_df, lags=lags, column=&quot;energy_consumption&quot;, ts_ id=&quot;LCLid&quot;, use_32_bit=True</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, let’s look at the parameters that we used in the previous code snippet:</p><ul id="l46"><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">lags<span class="p">: This parameter takes in a list of integers denoting all the lags we need to create as features.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">column<span class="p">: The name of the column to be lagged. In our case, this is </span>energy_consumption<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">ts_id<span class="p">: The name of the column that contains the unique ID of a time series. If </span>None<span class="p">, it assumes the DataFrame only contains a single time series. In our case, </span>LCLid <span class="p">is the name of that column.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">use_32_bit<span class="p">: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</span></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">This method returns the DataFrame with the lags added and a list with column names of the newly added features.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark233">Rolling window aggregations</a><a name="bookmark220">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">With lags, we were connecting the present points to single points in the past, but with rolling window features, we are connecting the present with an aggregate statistic of a window from the past. Instead of looking at the observation from previous time steps, we would be looking at an average of the observations from the last three timesteps. Take a look at the following diagram to understand this better:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span><img width="521" height="338" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_261.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 6.3 – Rolling window aggregation features</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="69" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_262.png"/></span></p><p class="s29" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">mportant note</p><p style="text-indent: 0pt;text-align: left;">We are <i>not including </i><span class="s144">𝑦𝑦</span><span class="s145">𝑡𝑡</span><span class="s127"> </span>in the vector of past observations because that leads to data leakage.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s29" style="padding-top: 5pt;text-indent: 0pt;text-align: right;">I</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can calculate rolling statistics with different widows, and each of them would capture slightly different aspects of the history. In the preceding diagram, we can see an example of a window of three and a window of four. When we are at timestep <i>t</i>, a rolling window of three would have <span class="s90">𝑦𝑦𝑡𝑡−3, 𝑦𝑦𝑡𝑡−2, 𝑦𝑦𝑡𝑡−1 </span>as the vector of past observations. Once we have these, we can apply any aggregation functions, such as the mean, standard deviation, min, max, and so on. Once we have a scalar value after the aggregation function, we can include that as a feature for timestep <i>t</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s see how we can do this with pandas:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># We shift by one to make sure there is no data leakage</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">df[&quot;rolling_3_mean&quot;] = df[&quot;column&quot;].shift(1).rolling(3).mean()</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Similar to the lags, we need to do this operation for each <span class="s48">LCLid </span>column separately. We have included a helpful method in <span class="s48">src.feature_engineering.autoregressive_features </span>called <span class="s48">add_rolling_features </span>that adds all the rolling features you want for each <span class="s48">LCLid </span>in a fast and efficient manner. Let’s see how we can use that.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We are going to import this method and use a few of its parameters to configure the rolling operation the way we want:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.feature_engineering.autoregressive_features import add_rolling_features</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">full_df, added_features = add_rolling_features( full_df,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">rolls=[3, 6, 12, 48],</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">column=&quot;energy_consumption&quot;, agg_funcs=[&quot;mean&quot;, &quot;std&quot;], ts_id=&quot;LCLid&quot;, use_32_bit=True,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, let’s look at the parameters that we used in the previous code snippet:</p></li><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">rolls<span class="p">: This parameter takes in a list of integers denoting all the windows over which we need to calculate the aggregate statistics.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">column<span class="p">: The name of the column to be lagged. In our case, this is </span>energy_consumption<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: left;">agg_funcs<span class="p">: This is a list of aggregations that we want to do for each window we declared in</span></p><p class="s48" style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: left;">rolls<span class="p">. Allowable aggregation functions include </span>{mean, std, max, min}<span class="p">.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s48">n_shift</span>: This is the number of timesteps we need to shift before doing the rolling operation. This parameter avoids data leakage. Although we are shifting by one now, there are cases where we need to shift by more than one as well. This is typically used in multi-step forecasting, which we will cover in <i>Section 3 – Mechanics of Forecasting</i>.</p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">ts_id<span class="p">: The name of the column name that contains the unique ID of a time series. If </span>None<span class="p">, it assumes that the DataFrame only has a single time series. In our case, </span>LCLid <span class="p">is the name of that column.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">use_32_bit<span class="p">: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark234">This method returns the DataFrame with the rolling features added and a list with column names of the newly added features.</a><a name="bookmark221">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Seasonal rolling window aggregations</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Seasonal rolling window aggregations are very similar to rolling window aggregations, but instead of taking past <i>n </i>consecutive observations in the window, this takes a seasonal window, skipping a constant number of timesteps between each item in a window. The following diagram will make this clearer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 89pt;text-indent: 0pt;text-align: left;"><span><img width="368" height="531" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_263.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 6.4 – Seasonal rolling window aggregations</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="69" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_264.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Important note</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">We are <i>not including </i><span class="s90">𝑦𝑦𝑡𝑡 </span>as an element in the seasonal rolling window vector to avoid data leakage.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The key parameter here is the seasonality period, which is commonly referred to as <i>m</i>. This is the number of timesteps after which we expect the seasonality pattern to repeat. When we are at timestep <i>t</i>, a rolling window of three would have <span class="s90">𝑦𝑦𝑡𝑡−3, 𝑦𝑦𝑡𝑡−2, 𝑦𝑦𝑡𝑡−1 </span>as the vector of past observations. But the seasonal rolling window would skip <i>m </i>timesteps between each item in the window. This means that the observations that are there in the seasonal rolling window would be <span class="s144">𝑦𝑦</span><span class="s90">𝑡𝑡−𝑚𝑚, 𝑦𝑦𝑡𝑡−2𝑚𝑚, 𝑦𝑦𝑡𝑡−3𝑚𝑚</span>. And as usual, once we have the window vector, we just need to apply the aggregation function to get a scalar value and include that as a feature.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s46" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a href="http://github.com/jmoralez/window_ops/" class="s140" target="_blank">This is not an operation that you can do easily and efficiently with pandas. Some fancy NumPy indexing and Python loops should do the trick. We are using an implementation from </a><a href="http://github.com/jmoralez/window_ops/" class="s45" target="_blank">github.com/ jmoralez/window_ops/</a> <span class="p">that uses NumPy and Numba to make the operation fast and efficient.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Just like the features we saw earlier, we need to do this operation for each <span class="s48">LCLid </span>separately. We have included a helpful method in <span class="s48">src.feature_engineering.autoregressive_features </span>called <span class="s48">add_seasonal_rolling_features </span>that adds all the seasonal rolling features you want for each <span class="s48">LCLid </span>in a fast and efficient manner. Let’s see how we can use that.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We are going to import the method and use a few parameters of the method to configure the seasonal rolling operation the way we want:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.feature_engineering.autoregressive_features import add_seasonal_rolling_features</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">full_df, added_features = add_seasonal_rolling_features( full_df,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">rolls=[3], seasonal_periods=[48, 48 * 7], column=&quot;energy_consumption&quot;, agg_funcs=[&quot;mean&quot;, &quot;std&quot;], ts_id=&quot;LCLid&quot;, use_32_bit=True,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark235">Now, let’s look at the parameters that we used in the previous code snippet:</a><a name="bookmark222">&zwnj;</a></p></li></ul></li><li><p class="s48" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">seasonal_periods<span class="p">: This is a list of seasonal periods that should be used in the seasonal rolling windows. In the case of multiple seasonalities, we can include the seasonal rolling features of all the seasonalities.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">rolls<span class="p">: This parameter takes in a list of integers denoting all the windows over which we need to calculate aggregate statistics.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">column<span class="p">: The name of the column to be lagged. In our case, this is </span>energy_consumption<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">agg_funcs<span class="p">: This is a list of aggregations that we want to do for each window we declared in</span></p><p class="s48" style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">rolls<span class="p">. The allowable aggregation functions are </span>{mean, std, max, min}<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">n_shift<span class="p">: This is the number of seasonal timesteps we need to shift before doing the rolling operation. This parameter avoids data leakage.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">ts_id<span class="p">: The name of the column name that contains the unique ID of a time series. If </span>None<span class="p">, it assumes the DataFrame only contains a single time series. In our case, </span>LCLid <span class="p">is the name of that column.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">Use_32_bit<span class="p">: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">As always, the method returns the DataFrame with seasonal rolling features and a list containing the column names of the newly added features.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Exponentially weighted moving averages (EWMA)</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">With the rolling window mean operation, we were calculating the average of the window, and it works synonymously with the <span class="s5">moving average</span>. EWMA is the slightly smarter cousin of the moving average. While the moving average considers a rolling window and considers each item in the window equally on the computed average, EWMA tries to do a weighted average on the window, and the weights decay at an exponential rate. There is a parameter, <span class="s90">α</span>, that determines how fast the weights decay. And because of this, we can consider all the history available as a window and let the parameter decide how much recency is included in EWMA. This can be written simply and recursively, as follows:</p><p class="s90" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐸𝐸𝐸𝐸𝐸𝐸𝐴𝐴<span class="s128">𝑡𝑡</span><span class="s127">  </span>= α × 𝑦𝑦<span class="s128">𝑡𝑡</span><span class="s127">  </span>+ (1 − α) × 𝐸𝐸𝐸𝐸𝐸𝐸𝐴𝐴<span class="s128">𝑡</span><span class="s127">𝑡−1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, we can see that the larger the value of , the more the average is skewed toward recent values (see <i>Figure 6.6 </i>to get a visual intuition of how the weights would be). If we expand the recursion, the weights of each term work out to be:</p><p class="s90" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑤𝑤_{𝑡𝑡 − 𝑘𝑘}   =  α × (1 − α)^𝑘𝑘</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark236">where </a><i>k </i>is the number of timesteps behind <i>t</i>. If we plot the weights, we can see them in an exponential decay; determines how fast the decay happens. Another way to think about is in terms of <span class="s5">span</span>. Span is the number of periods at which the decayed weights approach zero (not in a strictly mathematical way, but intuitively). and span are related through this equation:</p><p class="s146" style="padding-top: 3pt;padding-left: 25pt;text-indent: 0pt;line-height: 9pt;text-align: center;">2</p><p style="text-indent: 0pt;text-align: left;"><span><img width="50" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_265.png"/></span></p><p class="s146" style="padding-left: 151pt;text-indent: 0pt;line-height: 7pt;text-align: center;">α =</p><p class="s146" style="padding-left: 173pt;text-indent: 0pt;line-height: 9pt;text-align: center;">1 + 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This will become clearer in the following diagram, where we have plotted how the weights decay for different values of :</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="519" height="200" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_266.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 6.5 – Exponential weight decay for different values of</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, we can see that the weight becomes small by the time we reach the span.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Intuitively, we can think of EWMA as an average of the entire history of the time series, but with parameters such as <span class="s90">𝜶𝜶 </span>and <span class="s5">span</span>, we can make different periods of history more representative of the average. If we define a 60-period span, we can think that the last 60 time periods are what majorly drive the average. So, making EWMAs with different spans or s gives us representative features that capture different periods of history.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The overall process is depicted in the following diagram:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 56pt;text-indent: 0pt;text-align: left;"><span><img width="448" height="284" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_267.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 173pt;text-indent: 0pt;text-align: center;">Figure 6.6 – EWMA features</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, let’s see how we can do this in pandas:</p><p style="padding-top: 11pt;text-indent: 0pt;text-align: center;"><span class="s49" style=" background-color: #F3F2F1;">  df[&quot;ewma&quot;]=df[&#39;column&#39;].shift(1).ewm(alpha=0.5).mean()          </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Like the other features we discussed earlier, EWMA also needs to be done for each <span class="s48">LCLid </span>separately. We have included a helpful method in <span class="s48">src.feature_engineering.autoregressive_ features </span>called <span class="s48">add_ewma </span>that adds all the EWMA features you want for each <span class="s48">LCLid </span>in a fast and efficient manner. Let’s see how we can use that.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We are going to import the method and use a few parameters of the method to configure EWMA the way we want to:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.feature_engineering.autoregressive_features import add_ewma</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">full_df, added_features = add_ewma( full_df,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">spans=[48 * 60, 48 * 7, 48], column=&quot;energy_consumption&quot;, ts_id=&quot;LCLid&quot;, use_32_bit=True,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark237">Now, let’s look at the parameters that we used in the previous code snippet:</a><a name="bookmark224">&zwnj;</a><a name="bookmark223">&zwnj;</a></p><ul id="l47"><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">alphas<span class="p">: This is a list of all s we need to calculate the EWMA features for.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">spans<span class="p">: Alternatively, we can use this to list all the spans we need to calculate the EWMA features for. If you use this feature, </span>alphas <span class="p">will be ignored.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">column<span class="p">: The name of the column to be lagged. In our case, this is </span>energy_consumption<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">n_shift<span class="p">: This is the number of seasonal timesteps we need to shift before doing the rolling operation. This parameter avoids data leakage.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">ts_id<span class="p">: The name of the column name that has a unique ID for a time series. If </span>None<span class="p">, it assumes the DataFrame only contains a single time series. In our case, </span>LCLid <span class="p">is the name of that column.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">use_32_bit<span class="p">: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</span></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">As always, the method returns the DataFrame containing EWMA features and a list with the column names of the newly added features.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">These are a few standard ways of including time delay embedding in your ML model, but you are not restricted to just these. As always, feature engineering is a space that is not bound by rules and we can get as creative as we want and inject domain knowledge into the model. Apart from the features we have seen, we can include the difference in lag as custom lags that inject domain knowledge, and so on.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at the other class of features we can add via <span class="s5">temporal embedding</span>.</p><p class="s3" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Temporal embedding</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">In </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, we briefly talked about temporal embedding as a process where we try to embed </span>time <span class="p">into features that the ML model can leverage. If we think about </span>time <span class="p">for a second, we will realize that there are two aspects of time that are important to us in the context of time series forecasting – </span>passage of time <span class="p">and </span>periodicity of time<span class="p">.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s look at a few features that can help us capture these aspects in an ML model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Calendar features</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The first set of features that we can extract are features based on calendars. Although the strict definition of time series is a set of observations taken sequentially in time, more often than not, we will have the timestamps of these collected observations alongside the time series. We can utilize these timestamps and extract calendar features such as the month, quarter, day of the year, hour, minutes, and so on. These features capture the periodicity of time and help the ML model capture seasonality well. Only the calendar features that are temporally higher than the frequency of the time series make sense. For instance, an hour feature in a time series with a weekly frequency doesn’t make sense, but a month</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark238">feature and week feature make sense. We can utilize inbuilt datetime functionalities in pandas to create these features and treat these as categorical features in the model.</a><a name="bookmark225">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Time elapsed</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This is another feature that captures the passage of time in an ML model. This feature increases monotonically as time increases, giving the ML model a sense of the passage of time. There are many ways to create this feature, but one of the easiest and most efficient ways is to use the integer representation of dates in NumPy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">df[&#39;time_elapsed&#39;] = df[&#39;timestamp&#39;].values.astype(np.int64)/ (10**9)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We have included a helpful method in <span class="s48">src.feature_engineering.temporal_features </span>called <span class="s48">add_temporal_features </span>that adds all relevant temporal features in an automated way. Let’s see how we can use it.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We are going to import the method and use a few parameters of this method to configure and create the temporal features:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">full_df, added_features = add_temporal_features( full_df,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">field_name=&quot;timestamp&quot;, frequency=&quot;30min&quot;, add_elapsed=True, drop=False, use_32_bit=True,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, let’s look at the parameters that we used in the previous code snippet:</p></li></ul></li><li><p class="s48" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: left;">field_name<span class="p">: This is the column name that contains the datetime that should be used to create features.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: left;">frequency<span class="p">: We should provide the frequency of the time series as input so that the method automatically extracts the relevant features. These are standard pandas frequency strings.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">add_elapsed<span class="p">: This flag turns the creation of the time elapsed feature on or off.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: left;">use_32_bit<span class="p">: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</span></p></li></ul><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Just like the previous methods we discussed, this also returns the new DataFrame with the temporal features added and a list containing the column names of the newly added features.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark239">Fourier terms</a><a name="bookmark226">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Previously, we extracted a few calendar features such as the month, year, and so on and talked about using them as categorical variables in the ML model. Another way we can represent the same information, but on a continuous scale, is by using <span class="s5">Fourier terms</span><a href="#bookmark87" class="s140">. We discussed Fourier series in </a><i>Chapter 3</i>, <i>Analyzing and Visualizing Time Series Data</i>. Just to recall, the sine-cosine form of the Fourier series is as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s137" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝑠𝑠</p><p class="s147" style="padding-top: 4pt;padding-left: 20pt;text-indent: 0pt;line-height: 36%;text-align: left;">=<span class="s137">   𝑎𝑎</span><span class="s128">0</span></p><p style="padding-left: 36pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="15" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_268.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s137" style="padding-left: 4pt;text-indent: 0pt;line-height: 6pt;text-align: left;">+  Σ𝑁𝑁     (𝑎𝑎</p><p class="s137" style="padding-top: 3pt;padding-left: 165pt;text-indent: 0pt;line-height: 11pt;text-align: center;">2π</p></li></ul></li><li><p class="s137" style="padding-left: 11pt;text-indent: -5pt;line-height: 4pt;text-align: left;">𝑐𝑐𝑐𝑐𝑠𝑠 (      ⋅ 𝑛𝑛 ⋅ 𝑥𝑥) + 𝑏𝑏</p><p style="padding-left: 34pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="16" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_269.png"/></span></p><p class="s137" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2π</p></li><li><p class="s137" style="padding-left: 11pt;text-indent: -5pt;line-height: 4pt;text-align: left;">𝑠𝑠𝑠𝑠𝑛𝑛 (      ⋅ 𝑛𝑛 ⋅ 𝑥𝑥))</p><p style="padding-left: 33pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="17" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_270.png"/></span></p><p class="s127" style="padding-left: 91pt;text-indent: 0pt;line-height: 45%;text-align: left;">𝑁𝑁(𝑥𝑥) <span class="s148">2</span></p><p class="s127" style="padding-left: 23pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑛𝑛=1</p><p class="s127" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝑛𝑛</p><p class="s137" style="text-indent: 0pt;line-height: 12pt;text-align: right;">𝑃𝑃</p><p class="s127" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝑛𝑛</p><p class="s137" style="padding-left: 31pt;text-indent: 0pt;line-height: 12pt;text-align: left;">𝑃𝑃</p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <span class="s90">𝑠𝑠𝑁𝑁 </span>is the <i>N</i>-term approximation of the signal, <i>S</i>. Theoretically when <i>N </i>is infinite, the resulting approximation is equal to the original signal. <i>P </i>is the maximum length of the cycle. We can create these cosine and sine functions as features to represent the seasonal cycle. If we are encoding the month, we know that the month goes from 1 to 12 and then repeats itself. So, <i>P</i>, in this case, will be 12 and <i>x </i>will be 1, 2, …12. Therefore, for each <i>x</i>, we can calculate the cosine and sine terms and add them as features to the ML model. The following diagram shows the difference in representations between the month on an ordinal scale and as a Fourier series:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 64pt;text-indent: 0pt;text-align: left;"><span><img width="457" height="421" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_271.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 6.7 – Month as an ordinal step function (top) versus Fourier terms (bottom)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The preceding diagram shows just a single Fourier term; we can add multiple Fourier terms to help capture complex seasonality.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We cannot say that continuous representation of seasonality is better than categorical because it depends on the type of model you are using and the dataset. This is something we will have to find out empirically.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">To make the process of adding Fourier features easy, we have made some easy-to-use methods available in <span class="s48">src.feature_engineering.temporal_features </span>in a file called <span class="s48">bulk_ add_fourier_features </span>that adds Fourier features for all the calendar features we want in an automated way. Let’s see how we can use that.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We are going to import the method and use a few parameters of the method to configure and create the Fourier series-based features:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">full_df, added_features = bulk_add_fourier_features( full_df,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">[&quot;timestamp_Month&quot;, &quot;timestamp_Hour&quot;, &quot;timestamp_Minute&quot;], max_values=[12, 24, 60],</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">n_fourier_terms=5, use_32_bit=True,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, let’s look at the parameters that we used in the previous code snippet:</p><ul id="l48"><li><p class="s48" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">columns_to_encode<span class="p">: This is the list of calendar features we need to encode using Fourier terms.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">max_values<span class="p">: This is a list of max values for the seasonal cycle for the calendar features in the same order as they are given in </span>columns_to_encode<span class="p">. For instance, for </span>month <span class="p">as a column to encode, we give </span>12 <span class="p">as the corresponding </span>max_value<span class="p">. If not given, </span>max_value <span class="p">will be inferred. This is only recommended if the data you have contains at least a single complete seasonal cycle.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">n_fourier_terms<span class="p">: The number of Fourier terms to be added. This is synonymous to </span><span class="s4">n</span></p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">in the equation for the Fourier series mentioned previously.</p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">use_32_bit<span class="p">: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</span></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Just like the previous methods we’ve discussed, this also returns a new DataFrame with the Fourier features added and a list with column names of the newly added features.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: left;"><a name="bookmark240">After executing the </a><span class="s48">01-Feature Engineering.ipynb </span>notebook in <span class="s48">chapter06</span>, we will have the following feature engineered files written to disk:<a name="bookmark227">&zwnj;</a></p><ul id="l49"><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">selected_blocks_train_missing_imputed_feature_engg.parquet</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">selected_blocks_val_missing_imputed_feature_engg.parquet</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">selected_blocks_test_missing_imputed_feature_engg.parquet</p></li></ul></li></ul></li></ul><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="155" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_272.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional information</p><p class="s46" style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><a href="https://github.com/Nixtla/tsfeatures" class="s140" target="_blank">The world of feature engineering is vast and there are a few open source libraries that make exploring that space easier. A few of them include </a><a href="https://github.com/Nixtla/tsfeatures" class="s45" target="_blank">https://github.com/Nixtla/ </a>tsfeatures<a href="https://tsfresh.readthedocs.io/en/latest/" class="s140" target="_blank">, </a>https://tsfresh.readthedocs.io/en/latest/<a href="https://github.com/DynamicsAndNeuralSystems/catch22" class="s140" target="_blank">, and </a>https://github.com/DynamicsAndNeuralSystems/catch22<span class="p">. A  preprint by </span><span class="s4">Ben D. Fulcher </span><span class="p">titled </span><span class="s4">Feature-based time-series analysis </span><a href="https://arxiv.org/abs/1709.08055" class="s140" target="_blank">at </a><a href="https://arxiv.org/abs/1709.08055" class="s45" target="_blank">https://arxiv.org/ abs/1709.08055</a> <span class="p">also gives a nice summary of the space.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">In this section, we looked at a few popular and effective ways of generating features for time series. But there are many more and depending on your problem and the domain, many of them will be relevant.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">After a brief overview of the ML for time series forecasting paradigm in the previous chapter, in this chapter, we looked at this practically and saw how we can prepare the dataset with the required features to start using these models. We reviewed a few time series-specific feature engineering techniques such as lags, rolling, and seasonal features. All the techniques we learned in this chapter are tools with which we can quickly iterate through experiments to find out what works for our dataset. However, we only talked about feature engineering, which affects one side of the standard regression equation (<span class="s90">𝑦𝑦  =  𝑚𝑚𝑿𝑿  +  𝑐𝑐</span>). The other side, which is the target (<span class="s90">𝑦𝑦</span>) we are predicting, is also equally important. In the next chapter, we’ll look at a few concepts such as stationarity and some transformations that affect the target.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark241">7</a><a name="bookmark242">&zwnj;</a><a name="bookmark244">&zwnj;</a><a name="bookmark243">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 90pt;text-indent: -43pt;line-height: 114%;text-align: left;">Target Transformations for Time Series Forec asting </h4><p style="padding-top: 24pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous chapter, we delved into how we can do temporal embedding and time delay embedding by making use of feature engineering techniques. But that was just one side of the regression equation</p><ul id="l50"><li><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">the features. Often, we see that the other side of the equation – the target – does not behave the way we want. In other words, the target doesn’t have some desirable properties that make forecasting easier. One of the major culprits in this area is <span class="s5">stationarity </span>– or more specifically, the lack of it. And it creates problems with the assumptions we make while developing a <span class="s5">machine learning </span>(<span class="s5">ML</span>)/statistical model. In this chapter, we will look at some techniques for handling such problems with the target.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will cover the following topics:</p><ul id="l51"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Handling non-stationarity in time series</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Detecting and correcting for unit roots</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Detecting and correcting for trends</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Detecting and correcting for seasonality</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Detecting and correcting for heteroscedasticity</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">AutoML approach to target transformation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The following are the technical requirements for this chapter:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l52"><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;"><a name="bookmark266">You will need to run the following notebooks in this chapter:</a><a name="bookmark245">&zwnj;</a></p><ul id="l53"><li><p class="s48" style="padding-top: 8pt;padding-left: 74pt;text-indent: -11pt;line-height: 13pt;text-align: left;">02-Preprocessing London Smart Meter Dataset.ipynb <span class="p">in the</span></p><p class="s48" style="padding-left: 74pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Chapter02 <span class="p">folder</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">01-Setting up Experiment Harness.ipynb <span class="p">in </span>Chapter04 <span class="p">folder</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">01-Feature Engineering.ipynb <span class="p">in </span>Chapter06 <span class="p">folder</span></p></li></ul></li><li><p class="s46" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter07" class="s140" target="_blank">The associated code for this chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter07" class="s45" target="_blank">https://github.com/ PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/ </a>tree/main/notebooks/Chapter07<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Handling non-stationarity in time series</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s5">Stationarity </span>is a prevalent assumption in econometrics and is a rigorous and mathematical concept. But without getting into a lot of math, we can intuitively think about stationarity as the state where the statistical properties of the distribution from which the time series is sampled remain constant over time. This is relevant in time series as regression as well because we are estimating a single forecasting function across time. And if the <i>behavior </i>of the time series changes with time, the single function that we estimate may not be relevant all the time. For instance, if we think about the number of visitors to the nearby park in a day as a time series, we know that those patterns are going to be very different for pre- and post-pandemic periods. In the ML world, this phenomenon is called <span class="s5">concept drift</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Intuitively, we can understand that it is easier to forecast a stationary series than a non-stationary series. But here comes the punchline: in the real world, almost all time series do not satisfy the stationarity assumption – more specifically, the <span class="s5">strict stationarity </span>assumption. Strict stationarity is when all the statistical properties such as the mean, variance, skewness, and so on do not change with time. Many times, this strict stationarity assumption is relaxed in favor of <span class="s5">weak stationarity</span>, where we only stipulate that the mean and the variance of the time series do not change with time.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are four main questions we can ask ourselves to check whether our time series is stationary or not:</p></li><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Does the mean change over time? Or in other words, is there a trend in the time series?</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Does the variance change over time? Or in other words, is the time series heteroscedastic?</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Does the time series exhibit periodic changes in mean? Or in other words, is there seasonality in the time series?</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Does the time series have a unit root?</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Out of these questions, the first three can be ascertained using a simple visual inspection. <span class="s5">Unit roots </span>are more difficult to understand. Let’s take a look at a few time series and check whether we can tell whether they are stationary or not via visual inspection (you can note your answers):</p><p class="s47" style="padding-top: 4pt;padding-left: 259pt;text-indent: 0pt;text-align: left;"><a name="bookmark267">Handling non-stationarity in time series 141</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_273.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span><img width="518" height="251" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_274.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 7.1 – Testing your understanding of stationarity</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, check your responses and see how many of them you guessed correctly. If you got at least four out of six, you are doing great with your intuition of stationarity:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s5">Timeseries 1 </span>is <i>stationary </i>as it is a white noise process that, by definition, has zero mean and a constant variance. It checks our checklist for the first three questions.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s5">Timeseries 2 </span>is <i>non-stationary </i>as it has an obvious downward linear trend. This means that the mean of the series at the beginning of the series is not the same toward the end. So, it fails our first question in the checklist.</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Timeseries 3 <span class="p">may look stationary at first because it is essentially oscillating around 0, but the oscillations are wider as we progress through time. This means that it has an increasing variance</span></p><p style="padding-left: 55pt;text-indent: 0pt;text-align: justify;">– or in other words, it is heteroscedastic. So, although this checks our first question, it doesn’t pass our second check of having constant variance. Hence, it is <i>non-stationary</i>.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Now, we are coming to the problem child – <span class="s5">Timeseries 4</span>. At first glance, we may think it is stationary because even though it had a trend in the beginning, it also reversed it, making the mean almost constant. And it’s not obvious that the variance is also widely varying. But this is a time series with a unit root (we will talk about this in detail later in the <i>Unit roots </i>section) and typically unit root time series are difficult to judge visually.</p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">Timeseries 5 <span class="p">checks out on the first two questions – the constant mean and constant variance</span></p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">– but it has a very obvious seasonal pattern and hence is <i>non-stationary</i>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s5">Timeseries 6 </span>is another white noise process, included just to trick you. This is also <i>stationary</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="240" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_275.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 92%;text-align: left;">To follow along with the complete code, use the <span class="s48">02-Dealing with Non-Stationarity. ipynb </span>notebook in the <span class="s48">chapter06 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Although we are talking about correcting or making a time series stationary, it is not always essential to do that in the ML paradigm because some of these can be handled by using the right kind of features in the model. Whether to make a series stationary or not is a decision we will have to make after experimenting with the techniques. This is because, as you will see, while there are advantages to making a series stationary, there are also disadvantages to using some of these techniques, as we will see when we discuss each transformation in detail.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark268">When we have hundreds, or even millions, of time series, we can’t practically do a visual inspection to ascertain whether they are stationary or not. So, now, let’s look at a few ways of detecting these key properties using statistical tests and also how to try and correct them.</a><a name="bookmark247">&zwnj;</a><a name="bookmark246">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Detecting and correcting for unit roots</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s talk about unit roots first since this is what is most commonly tested for stationarity. Time series analysis has its roots in econometrics and statistics and unit root is a concept derived directly from those fields.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Unit roots</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Unit roots are quite complicated to understand fully but to develop some intuition, we can look at a simplification. Let’s consider an autoregressive model of order 1(AR(1) model):</p><p class="s90" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑦𝑦<span class="s128">𝑡𝑡</span><span class="s127">  </span>= ϕ𝑦𝑦<span class="s128">𝑡</span><span class="s127">𝑡−1  </span>+ ϵ<span class="s128">𝑡𝑡</span><span class="s127">  </span>, where ϵ<span class="s128">𝑡𝑡</span><span class="s127">  </span>is white noise and ϕ is the AR coefficient</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">If we think about the different values of <span class="s143">ϕ </span>in the equation, we can come up with three scenarios (<i>Figure 7.2</i>):</p><ul id="l54"><li><p class="s90" style="padding-top: 9pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">|𝜙𝜙| &gt; 1<span class="p">: When </span><span class="s144">|</span>ϕ<span class="s144">|</span> <span class="p">is greater than 1, every successive value in the time series is multiplied by a number greater than 1, which means it will have a strong and rapidly increasing/decreasing trend and thereby be non-stationary.</span></p></li><li><p class="s149" style="padding-top: 3pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">|<span class="s143">𝜙𝜙</span>|<span class="s90"> </span><span class="s143">&lt;</span><span class="s90"> 1</span><span class="p">: When </span>|<span class="s143">ϕ</span>|<span class="s90"> </span><span class="p">is less than 1, every successive value in the time series is multiplied by a number less than 1, which means over the long term, the mean of the series trends to zero and will oscillate around it. Therefore, it is stationary.</span></p></li><li><p class="s90" style="padding-top: 5pt;padding-left: 62pt;text-indent: -12pt;text-align: justify;"><span class="s144">|</span>𝜙𝜙<span class="s144">|</span> = 1<span class="p">: When </span><span class="s144">|</span>ϕ<span class="s144">|</span> <span class="p">is equal to 1, things become trickier. When </span>|𝜙𝜙| = 1 <span class="p">for an AR(1) model,</span></p><p class="s47" style="padding-top: 4pt;padding-left: 263pt;text-indent: 0pt;text-align: left;"><a name="bookmark269">Detecting and correcting for unit roots 143</a><a name="bookmark248">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_276.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s90" style="padding-left: 55pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><span class="p">this is known as it having a unit root and the equation becomes </span>𝑦𝑦<span class="s128">𝑡𝑡</span><span class="s127">  </span>= 𝑦𝑦<span class="s128">𝑡</span><span class="s127">𝑡−1  </span>+ ϵ<span class="s128">𝑡𝑡</span><span class="s127">  </span><span class="p">. This is called random walk in econometrics and is a very popular kind of time series in financial and economic domains. Mathematically, we can prove that such a series will have a constant mean but a non-constant variance:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 68pt;text-indent: 0pt;text-align: left;"><span><img width="423" height="367" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_277.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 7.2 – Autoregressive time series with different <span class="s90">𝝓𝝓 </span>parameters</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">While we discussed unit roots in an AR(1) process, we can extend the same intuition to multiple lags or an AR(p) model. Calculating and testing unit roots is more complicated there, but still possible.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">So, now that we know what a unit root is, how can we statistically test this? This is where the Dickey- Fuller test comes in.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The Augmented Dickey-Fuller (ADF) test</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The null hypothesis in this test is that the <span class="s90">𝝓𝝓 </span>in an AR(1) model of the time series is equal to 1, and by extension non-stationary. The alternate hypothesis is that the <span class="s90">ϕ </span>in the AR(1) model is less than 1. The <span class="s5">ADF </span>test takes the Dickey-Fuller test and extends it to an AR(p) model because most time series are not defined by just one lag of the time series. This is the standard and most popular statistical test to check for unit roots.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark270">Let’s see how we can do this in Python using </a><span class="s48">statsmodels</span>:<a name="bookmark249">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from statsmodels.tsa.stattools import adfuller result = adfuller(y)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p class="s48" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">result <span class="p">from </span>adfuller <span class="p">is a tuple that contains the test statistic, p-value, and critical values at different confidence levels. Here, we are most interested in the p-value, which is an easy and practical way to check whether the null hypothesis is rejected or not. If </span>p&lt;0.05<span class="p">, there is a 95% probability that the series does not have a unit root; the series is stationary from a unit root perspective.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">To make this process even easier, we have included a method called <span class="s48">check_unit_root </span>in <span class="s48">src. transforms.stationary_utils </span>that does the inference for you and returns a <span class="s48">namedtuple </span>with a Boolean attribute called <span class="s48">stationary</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">from src.transforms.stationary_utils import check_unit_root</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># We pass the time series along with the confidence with which we need the results</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">check_unit_root(y, confidence=0.05)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we’ve learned how to check whether a series has a unit root or not, how do we make it stationary? Let’s look at a few transforms that help us do that.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Differencing transform</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The differencing transform is a very popular transform to make a time series stationary, or at least get rid of unit roots. The concept is simple: we transform the time series from the domain of observation to the domain of change in observations. The differencing transform subtracts subsequent observations from one another:</p><p class="s80" style="padding-top: 5pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑧𝑧<span class="s82">𝑡𝑡</span><span class="s50">  </span>= 𝑦𝑦<span class="s82">𝑡𝑡</span><span class="s50">  </span>− 𝑦𝑦<span class="s82">𝑡</span><span class="s50">𝑡−1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Differencing helps us stabilize the mean of the time series and, with that, reduce or eliminate trend and seasonality. Let’s see how differencing can make a series stationary.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 90%;text-align: justify;">Let the time series in question be <span class="s144">𝑦</span><span class="s90">𝑦</span><span class="s145">𝑡𝑡</span><span class="s127">  </span><span class="s144">=</span><span class="s90"> β</span><span class="s145">0</span><span class="s127">  </span><span class="s144">+</span><span class="s90"> β</span><span class="s145">1</span><span class="s144">𝑡𝑡</span><span class="s90"> + ϵ</span><span class="s145">𝑡</span><span class="s127">𝑡</span>, where <span class="s90">β</span><span class="s128">0</span><span class="s127"> </span>and <span class="s90">β1 </span>are the coefficients and    is white noise. From this equation, we can see that time, <i>t</i>, is part of the equation, making <span class="s137">𝑦𝑦𝑡𝑡 </span>a time series with a trend. So, the differenced time series <i>z </i>would be as follows:</p><p class="s90" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;"><span class="s150">𝑧𝑧</span><span class="s41">𝑡𝑡  = 𝑦𝑦𝑡𝑡  − 𝑦𝑦𝑡𝑡−1 </span>= <span class="s144">(</span>β0 + β1𝑡𝑡 + ϵ𝑡𝑡<span class="s144">) </span>− <span class="s144">(</span>β0 + β1<span class="s144">(</span>𝑡𝑡 − 1<span class="s144">) </span>+ ϵ𝑡𝑡−1<span class="s144">) </span>= β<span class="s128">1  </span>+ (ϵ<span class="s128">𝑡𝑡  </span>− ϵ<span class="s128">𝑡𝑡−1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">What we need to look for in this new equation is that there is no mention of <i>t</i>. This means that the dependence on <i>t</i>, which created the trend, has been removed, and now the time series has constant mean and variance at any point in time.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark271">Differencing does not remove all kinds of non-stationarity but works for the majority of time series. But there are a few drawbacks to this approach as well. One of them is that we lose the scale of the time series while modeling. Many times, the scale of the time series holds some information that is useful for forecasting. For instance, in a supply chain, SKUs with higher sales exhibit a different kind of pattern from the SKUs with lower sales and when we do differencing, this information about the distinction is lost.</a><a name="bookmark250">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Another drawback is more from an operational point of view. When we use differencing for forecasting, we also need to inverse the transform after we get the differenced output from the model. This is an additional layer of complexity that we have to manage. One way is to keep the most recent observation in memory and keep adding the differences to it to inverse the transform. Another way is to have <span class="s90">𝑦𝑦𝑡𝑡−1 </span>ready for every <i>t </i>that we need to inverse transform and keep adding the difference to <span class="s90">𝑦𝑦𝑡𝑡−1</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We have implemented the latter using the datetime index as a key to align and fetch the <span class="s90">𝑦𝑦𝑡𝑡−1 </span>observation in <span class="s48">src.transforms.target_transformations.py </span>in this book’s GitHub repository. Let’s see how we can use it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.transforms.target_transformations import AdditiveDifferencingTransformer</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">diff_transformer = AdditiveDifferencingTransformer()</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># [1:] because differencing reduces the length of the time series by one</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">y_diff = diff_transformer.fit_transform(y, freq=&quot;1D&quot;)[1:]</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p class="s48" style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">y_diff <span class="p">will have the transformed series. To get back to the original time series, we can call</span></p><p class="s48" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">inverse_transform <span class="p">using </span>diff_transformer<span class="p">.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Here, we saw differencing as the process of subtracting subsequent values in the time series. But we can also do differencing with other operators such as division (<span class="s90">𝑦𝑦𝑡𝑡/𝑦𝑦𝑡𝑡−1 </span>), which is implemented in the <span class="s48">src.transforms.target_transformations.py </span>file as <span class="s48">MultiplicativeDifferencingTransformer</span>. We can also experiment with these transforms to check whether these work best for your dataset.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although differencing solves the majority of stationarity issues, it’s not guaranteed to take care of all kinds of trends (non-linear or piecewise trends), seasonality, and so on. Sometimes, we may not want to difference the series but still handle trends and seasonality. So, let’s see how we can detect and remove trends in a time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Detecting and correcting for trends</p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">In </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, we talked about forecasting being a difficult problem because it is intrinsically an extrapolation problem. Trends are one of the major contributors to forecasting being an extrapolation problem. If we have a time series that is trending upward, any</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark272">model that attempts to forecast it needs to extrapolate beyond the range of values it has seen during training. ARIMA handles this using autoregression, whereas exponential smoothing handles it by modeling the trend explicitly. But standard regression may not be naturally suited to extrapolation. However, with suitable features, such as lags, it can start to do that. But if we can confidently estimate and extract a trend in the time series, we can simplify the problem we have to apply regression to by detrending the time series.</a><a name="bookmark251">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">But before we move ahead, it is worth learning about two major types of trends.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Deterministic and stochastic trends</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s take the simple <i>AR(1) </i>model we saw earlier to develop intuitions about this one too. Earlier, we saw that having <span class="s90">ϕ &gt; 1 </span>in an <i>AR(1) </i>model leads to a trend in the time series. But another way we can think about a trending time series is if we include time as an ordinal variable in the equation defining the time series. For instance, let’s consider two time series:</p><p class="s90" style="padding-top: 4pt;padding-bottom: 2pt;padding-left: 37pt;text-indent: 0pt;line-height: 142%;text-align: left;"><span class="s151">T</span><span class="s4">ime series 1: </span><span class="s143">𝑦</span>𝑦<span class="s127">𝑡𝑡  </span><span class="s143">=</span> ϕ𝑦𝑦<span class="s127">𝑡𝑡−1  </span><span class="s143">+</span> ϵ<span class="s127">𝑡𝑡 </span><span class="s143">;</span> ϵ<span class="s127">𝑡𝑡  </span><span class="s143">∼</span> 𝒩𝒩<span class="s143">(0</span>, σ<span class="s152">2</span><span class="s143">) </span><span class="s153">T</span><span class="s4">ime series 2: </span>𝑦𝑦<span class="s128">𝑡𝑡</span><span class="s127">  </span>= β<span class="s128">0</span><span class="s127">  </span>+ β<span class="s128">1</span>𝑡𝑡 + 𝜖𝜖<span class="s128">𝑡𝑡</span><span class="s127"> </span>; 𝜖𝜖<span class="s128">𝑡𝑡</span><span class="s127">  </span>∼ 𝒩𝒩(0, 𝜎𝜎2) <span class="p">These can be seen in the following graphs:</span></p><p style="padding-left: 85pt;text-indent: 0pt;text-align: left;"><span><img width="403" height="253" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_278.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 7.3 – Time series 1 (stochastic trend) and Time series 2 (deterministic trend)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We saw both of these equations earlier; <i>Time series 1 </i>is the <i>AR(1) </i>model while <i>Time series 2 </i>is the time series equation we chose to illustrate differencing. We already know that for <span class="s144">𝜙𝜙 </span><span class="s90">&gt; 1</span>, both <i>Time series 1 </i>and <i>Time series 2 </i>have trends. But there is a difference between the two trends. In <i>Time series 2</i>, the trend is constant and can be perfectly modeled. In this case, just a linear fit would explain the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark273">trend perfectly. But in </a><i>Time series 1</i>, the trend is not something that can be explained by a simple linear fit. It is inherently dependent on the previous value of the time series that has <span class="s90">ϵ𝑡𝑡−1 </span>and hence is stochastic. Therefore, <i>Time series 2 </i>has a deterministic trend and <i>Time series 1 </i>has a stochastic trend.<a name="bookmark252">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can use the same ADF test we saw earlier in this chapter to check whether a time series has deterministic or stochastic trends. Without going into the math of the statistical test, we know that it tests for a unit root by fitting an <i>AR(p) </i>model to the time series. There are a few variants of this test that we can specify using the <span class="s48">regression </span>parameter in the <span class="s48">statsmodels </span>implementation. This parameter takes in the following values:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;"><span class="s48">c</span>: This means we are including a constant intercept in the <i>AR(p) </i>model. Practically, this means that we will be considering a time series as stationary even if the series is not around zero. This is the default setting in <span class="s48">statsmodels</span>.</p></li><li><p style="padding-top: 5pt;padding-left: 57pt;text-indent: -15pt;text-align: justify;"><span class="s48">n</span>: This means we do not even include a constant intercept in the <i>AR(p) </i>model.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;"><span class="s48">ct</span>: If we supply this option, the <i>AR(p) </i>model will also have a constant intercept and a linear, deterministic trend component. What this means is that even if there is a deterministic trend in the time series, it will be ignored and the series will be tested as stationary.</p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">ctt<span class="p">: This is when we include a constant intercept – that is, a linear and quadratic trend.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">So, if we run an ADF test with <span class="s48">regression=&quot;c&quot;</span>, it will be non-stationary. Now, if we run the ADF test with <span class="s48">regression=&quot;ct&quot;</span>, it will come out as stationary. This means that when we removed a deterministic trend from the time series, it became stationary. This test is what we can use to determine whether a trend that we observe in the time series is deterministic or stochastic. In the <i>Further reading </i>section, we have provided a link to a blog post by <i>Fabian Kostadinov</i>, where he experiments with a few time series to make the distinction between the different variants of ADF tests clear.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We have implemented this test in <span class="s48">src.transforms.stationary_utils </span>as <span class="s48">check_ deterministic_trend</span>, which does the inference for you and returns a <span class="s48">namedtuple </span>with a Boolean attribute of <span class="s48">deterministic_trend</span>. Let’s see how we can use this test:</p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  check_deterministic_trend(y, confidence=0.05)                   </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">This will tell us whether the trend is stationary or deterministic. Now, let’s look at a couple of ways to identify and statistically test trends (irrespective of whether it is deterministic or not) in a time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Kendall’s Tau</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Kendall’s Tau is a measure of correlation but carried out on the ranks of the data. Similar to Spearman’s correlation, which also calculates correlation on ranked data, Kendall’s Tau is a non-parametric test and therefore does not make assumptions about the data. The correlation coefficient, Tau, returns a value between -1 and 1, where 0 shows no relationship and 1 or -1 is a perfect relationship. We will not dive into the details of how Kendall’s Tau is calculated and how the significance test is done as this is outside the scope of this book. The <i>Further reading </i>section contains a link that explains this well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark274">In this section, we will see how we can use Kendall’s Tau to measure the trend in our time series. As mentioned earlier, Kendall’s Tau calculates a rank correlation between two variables. If we chose one of those variables as the time series and set the other as the ordinal representation of time, the resulting Kendall’s Tau would represent the trend in the time series. An additional benefit is that the higher the value of Kendall’s Tau, the stronger we expect the trend to be.</a></p><p class="s48" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">scipy <span class="p">has an implementation of Kendall’s Tau that we can use as follows:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">import scipy.stats as stats</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">tau, p_value = stats.kendalltau(y, np.arange(len(y)))</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We can compare the returned p-value to our required confidence (typically, this is <span class="s48">0.05</span>) and say that if <span class="s48">p_value </span>&lt; <span class="s48">confidence</span>, we conclude that the trend is statistically significant. The sign of <span class="s48">tau </span>tells us whether this is an increasing trend or a decreasing one.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We have made an implementation of Kendall’s Tau in <span class="s48">src.transforms.stationary_utils </span>as <span class="s48">check_trend</span>, which checks the presence of a trend for you. The only parameters we need to provide are as follows:</p><ul id="l55"><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">y<span class="p">: The time series to check</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">confidence<span class="p">: The confidence level against which the resulting p-value will be checked</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">A few more parameters are there, but those are for the <span class="s5">Mann-Kendall </span>(<span class="s5">M-K</span>) test, which will be explained next.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see how we can use this test:</p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  check_trend(y, confidence=0.05)                                 </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">This method also checks whether the trend that has been identified is deterministic or stochastic and calculates the direction of the trend. The result is returned as a <span class="s48">namedtuple </span>with the following parameters:</p></li><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">trend<span class="p">: A Boolean flag signifying the presence of a trend.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">direction<span class="p">: This will be either </span>increasing <span class="p">or </span>decreasing<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">slope<span class="p">: The slope of the estimated trend line. For Kendall’s Tau, it will be the Tau.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">p<span class="p">: The p-value of the statistical test.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 37pt;text-indent: 13pt;line-height: 160%;text-align: left;">deterministic<span class="p">: A Boolean flag signifying the deterministic trend. Now, let’s look at the Mann-Kendall test.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark275">Mann-Kendall test (M-K test)</a><a name="bookmark253">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The Mann-Kendall test is used to check for the presence of a monotonic upward or downward trend. And since the M-K test is a non-parametric test, like Kendall’s Tau, there is no assumption of normality or linearity. The test is done by analyzing the signs between consecutive points in the time series. The crux of the test is the idea that in the presence of a trend, the sign values, if summed up, increase or decrease constantly.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although non-parametric, there were a few assumptions in the original test:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">There is no auto-correlation in the time series</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">There is no seasonality in the time series</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a href="https://github.com/mmhs013/pyMannKendall" class="s140" target="_blank">Numerous alterations have been made to the original tests to tackle these problems over the years and a lot of such alterations, along with the original test, have been implemented at </a><a href="https://github.com/mmhs013/pyMannKendall" class="s45" target="_blank">https://github. </a><span class="s46">com/mmhs013/pyMannKendall</span>. They are available in <span class="s48">pypi </span>as <span class="s48">pymannkendall</span>.</p><p class="s5" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Pre-whitening <span class="p">is a common technique used to remove the autocorrelation in a time series. In a nutshell, the idea is as follows:</span></p><p class="s127" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"/><p class="s90" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;"><span class="p">1.    Identify </span><span class="s144">ϕ</span> <span class="p">with an AR(1) model </span><span class="s154">2.</span><span class="p">     </span><span class="s155">𝑦</span>𝑦<span class="s127">𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝ℎ𝑖𝑖𝑡𝑡𝑝𝑝𝑖𝑖  </span><span class="s155">=</span> 𝑦𝑦<span class="s156">𝑡𝑡</span><span class="s127">  </span><span class="s155">−</span> 𝜙𝜙 ∗ 𝑦𝑦<span class="s156">𝑡</span><span class="s127">𝑡−1</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_279.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference Check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The research paper by M. Bayazit and B. Önöz is cited in the <i>References </i>section under referenc number <i>1</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">e</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">M. Bayazit and B. Önöz (2007) suggested to not use pre-whitening before doing the M-K test if the sample size is larger than 50 and if the trend is strong enough (slope&gt;0.01). For seasonal data, a seasonal variant of the M-K test has also been implemented in <span class="s48">pymannkendall</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The same method we discussed earlier, <span class="s48">check_trend</span>, also implements M-K tests that can be enabled by setting <span class="s48">mann_kendall=True</span>. However, one thing we need to keep in mind is that the M-K test is considerably slower than Kendall&#39;s Tau, especially for long time series. There are a few more parameters specific to the M-K test:</p><ul id="l56"><li><p class="s48" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">seasonal_period<span class="p">: The default value is </span>None<span class="p">. But if there is seasonality, we can provide</span></p><p class="s48" style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">seasonal_period <span class="p">here and the seasonal variant of the M-K test will be retrieved.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">prewhiten<span class="p">: This is a Boolean flag that’s used to pre-whiten the time series before applying the M-K test. The default value is </span>None<span class="p">. In that case, using the condition we discussed earlier (N&gt;50), we decide whether to pre-whiten or not. If we explicitly pass </span>True <span class="p">or </span>False <span class="p">here, it will be respected.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark276">Let’s see how we can use this test:</a><a name="bookmark254">&zwnj;</a></p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  check_trend(y, confidence=0.05, mann_kendall=True)              </span></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The result is returned as a <span class="s48">namedtuple </span>with the following parameters:</p><ul id="l57"><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">trend<span class="p">: A Boolean flag signifying the presence of a trend.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">direction<span class="p">: This will be either </span>increasing <span class="p">or </span>decreasing<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">slope<span class="p">: The slope of the estimated trend line. For the M-K test, it will be the slope estimated using the Theil-Sen estimator.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">p<span class="p">: The p-value of the statistical test.</span></p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 37pt;text-indent: 13pt;line-height: 160%;text-align: left;">deterministic<span class="p">: A Boolean flag signifying the deterministic trend. Now that we know how to detect a trend, let’s look at detrending.</span></p><p class="s22" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Detrending transform</p><p class="s4" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark87" class="s140">If the trend is deterministic, removing the trend would add some value to the modeling procedure. In </a>Chapter 3<span class="p">, </span>Analyzing and Visualizing Time Series Data<span class="p">, we discussed detrending as it was an integral part of the decomposition we were doing. But techniques such as moving average or LOESS regression have one drawback – they can’t extrapolate. But if we are considering a deterministic linear (or even polynomial) trend, it can be easily estimated by using linear regression. The added advantage here is that the trend that is identified can easily be extrapolated.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The procedure is simple: we regress the time series on the ordinal representation of time and extract the parameters. Once we have these parameters, using the dates, we can extrapolate the trend to any point in the future.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We have made and implemented a detrender as a transformer in <span class="s48">src.transforms.target_ transformations.py </span>as <span class="s48">DetrendingTransformer</span>. Let’s see how we can use it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.transforms.target_transformations import DetrendingTransformer</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">detrending_transformer = DetrendingTransformer(degree=1)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">y_detrended = detrending_transformer.fit_transform(y, freq=&quot;1D&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p class="s48" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">y_detrended <span class="p">will contain the detrended series. To get the original time series back, we can call</span></p><p class="s48" style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">inverse_transform <span class="p">using </span>detrending_transformer<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="133" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_280.png"/></span></p><p style="text-indent: 0pt;line-height: 13pt;text-align: left;">.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s90" style="text-indent: 0pt;line-height: 11pt;text-align: left;">× 𝜙𝜙   , 𝑤𝑤ℎ𝑒𝑒𝑒𝑒𝑒𝑒 𝜙𝜙 &lt;  1</p><p style="text-indent: 0pt;text-align: left;"/><p class="s127" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑡𝑡+ℎ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s90" style="text-indent: 0pt;line-height: 11pt;text-align: left;">= 𝑓𝑓</p><p style="text-indent: 0pt;text-align: left;"/><p class="s127" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑡𝑡+ℎ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s127" style="text-indent: 0pt;line-height: 8pt;text-align: left;">ℎ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s127" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑</p><p style="text-indent: 0pt;text-align: left;"/><p class="s90" style="text-indent: 0pt;line-height: 11pt;text-align: left;">𝑓𝑓</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;line-height: 13pt;text-align: left;">as</p><p style="text-indent: 0pt;text-align: left;"/><p class="s29" style="text-indent: 0pt;line-height: 13pt;text-align: justify;">Best practice</p><p style="padding-top: 2pt;text-indent: 0pt;line-height: 88%;text-align: justify;">We have to be careful with the trend assumptions, especially if we are forecasting for the long term. Even a linear trend assumption can lead to an unrealistic forecast because trends don’t continue the same forever in the real world. It is always advisable to dampen the trend by some factor, <span class="s144">𝜙𝜙</span>, to be conservative in our extrapolation of the trend. This dampening can be as simple</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark277">Another key aspect that makes a time series non-stationary is seasonality. Let’s look at how to identify seasonality and remove it.</a><a name="bookmark256">&zwnj;</a><a name="bookmark255">&zwnj;</a></p><p class="s3" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Detecting and correcting for seasonality</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">A vast majority of real-world time series have seasonality such as retail sales, energy consumption, and so on. And generally, the presence or absence of seasonality comes as part of the domain knowledge. But when we are working with a time series dataset, the domain knowledge becomes slightly diluted. The majority of time series may exhibit seasonality, but that doesn’t mean every time series in the dataset is seasonal. For instance, within a retail dataset, there might be items that are seasonal and some items that are not. Therefore, when working with a time series dataset, being able to determine whether a particular time series is seasonal or not has some value.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Detecting seasonality</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are two popular ways to check for seasonality, apart from just eyeballing it: autocorrelation and fast Fourier transform. Either is equally capable of identifying the seasonality period automatically. For our discussion, we’ll cover the autocorrelation method and examine how we can use that to determine seasonality.</p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark87" class="s140">Autocorrelation, as explained in </a>Chapter 3<span class="p">, </span>Analyzing and Visualizing Time Series Data<span class="p">, is the correlation of a time series to its lagged values. Typically, we expect the correlation to be higher in the immediate lags (lag 1, lag 2, and so on) and gradually die down as we move farther into the past. But for time series with seasonality, we will also see a spike in the seasonal periods.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s understand this by looking at an example. Consider a synthetic time series that is just white noise combined with a sinusoidal signal with a seasonality cycle of 25 (identical to the seasonal time series we saw earlier in <i>Figure 7.1</i>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#WhiteNoise + Seasonal</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">y_random = pd.Series(np.random.randn(length), index=index) t = np.arange(len(y_random))</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">y_seasonal = (y_random+1.9*np.cos((2*np.pi*t)/(length/4)))</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a name="bookmark278">If we plot the </a><span class="s5">autocorrelation function </span>(<span class="s5">ACF</span>) for this time series, it will look as follows (the code to calculate and plot this can be found in the <span class="s48">02-Dealing with Non-Stationarity. ipynb </span>notebook):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span><img width="523" height="267" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_281.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 7.4 – Autocorrelation plot of the synthetic time series with a seasonality cycle of 25</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We can see that apart from the first few lags, the autocorrelation increases as we approach the seasonal cycle and peaks at the exact seasonality. We can use this property of the ACF to detect seasonality. <span class="s48">darts</span>, the library we used to generate our baseline forecasts, has an implementation of this technique that identifies seasonality. But since it was designed to work for the time series data structure of <span class="s48">darts</span>, we have adapted the same logic to work on regular <span class="s48">pandas </span>series in <span class="s48">src. transforms.stationary_utils.py </span>under the name <span class="s48">check_seasonality</span>. The implementation can do two kinds of seasonality checks. It can take a <span class="s48">seasonality_period </span>as input and verify whether a seasonality corresponding to that <span class="s48">seasonality_period </span>exists in the data or not. If we do not give a <span class="s48">seasonality_period </span>ahead of time, it will return to you the shortest <span class="s48">seasonality_period </span>that is statistically significant.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The procedure, at a high level, does the following:</p><ol id="l58"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Calculates the ACF.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Finds all the relative maxima in the ACF. A relative maximum is a point where the function changes direction from increasing to decreasing.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;line-height: 94%;text-align: left;">Checks whether the provided <span class="s48">seasonal_period </span>is a relative maximum. If not, we conclude there is no seasonality associated with <span class="s48">seasonality_period</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: left;"><a name="bookmark279">Now, we take the assumption that the ACF is normally distributed and compute the upper limit at the specified confidence level. The upper bound is given by:</a><a name="bookmark257">&zwnj;</a></p><p class="s80" style="padding-top: 2pt;padding-left: 12pt;text-indent: 0pt;line-height: 10pt;text-align: center;">𝑈𝑈𝑈𝑈 = 𝑧𝑧    <span class="s51"> </span><span class="s52">𝛼𝛼</span><span class="s50">  </span>× 𝑆𝑆𝑆𝑆(𝑟𝑟<span class="s82">ℎ</span>)</p><p class="s50" style="padding-left: 161pt;text-indent: 0pt;line-height: 83%;text-align: center;">1−<span class="s55">2</span></p><p style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;line-height: 14pt;text-align: center;">where <span class="s80">𝑟𝑟</span><span class="s82">ℎ</span><span class="s50"> </span>is the estimated autocorrelation at lag <i>h</i>, <i>SE </i>is the standard error, and <span class="s157">𝑧𝑧</span><span class="s158">1</span><span class="s65">−</span><span class="s159"> </span><span class="s160">𝛼𝛼</span><span class="s65"> </span>is the</p><p style="padding-left: 12pt;text-indent: 0pt;line-height: 13pt;text-align: center;">quantile of the normal distribution based on the required confidence,   . The SE is <span class="s161">2</span></p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;">approximated using Bartlett’s formula (for the math behind this, head over to the <i>Further reading </i>section).</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;line-height: 94%;text-align: left;">Each of our candidates for <span class="s48">seasonality_period </span>is checked against this upper limit and the ones that are above this limit are deemed statistically significant.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">There are only three parameters for this function, apart from the time series itself:</p></li></ol></li></ul></li><li><p class="s48" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: left;">max_lag<span class="p">: This specifies the maximum lag that should be included in the ACF and subsequent search for seasonality. This should be at least one more than the expected seasonality period.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: left;">seasonal_period<span class="p">: This is where we give our intuition of the seasonality period from domain knowledge and the function verifies that assumption for us.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 28pt;text-indent: 13pt;line-height: 160%;text-align: left;">confidence<span class="p">: This is the standard statistical confidence level. The default value is </span>0.05<span class="p">. Let’s see how we can use this function with an example:</span></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">check_seasonality(y_seasonal, max_lag=30, seasonal_period=25, confidence=0.05)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">This will give you a <span class="s48">namedtuple </span>with <span class="s48">seasonal</span>, a Boolean flag to indicate seasonality, and</p><p class="s48" style="padding-left: 28pt;text-indent: 0pt;line-height: 146%;text-align: left;">seasonal_periods<span class="p">, the seasonal periods with significant seasonality, as parameters. Now that we know how to identify and test for seasonality, let’s talk about deseasonalizing.</span></p><p class="s22" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Deseasonalizing transform</p><p class="s4" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark87" class="s140">In </a>Chapter 3<span class="p">, </span>Analyzing and Visualizing Time Series Data<span class="p">, we reviewed techniques for seasonal decomposition. We can use the same techniques here as well, but with just one tweak. Earlier, we were not concerned with projecting the seasonality into the future. But when we are using deseasonalizing in forecasting, it is essential to be able to project it into the future as well. We are in luck since projecting the seasonal cycle forward is trivial. This is because we are looking at a fixed seasonality profile that will always keep repeating in the seasonal cycle. For instance, if we identified a seasonality profile for the 12 months of a year (yearly seasonality at monthly frequency data), the seasonality that’s extracted for these 12 months will just be repeating itself in chunks of 12 months.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Using this property, we have implemented a transformer in <span class="s48">src.transforms.target_ transformations.py </span>as <span class="s48">DeseasonalizingTransformer</span>. There are a few parameters and properties that we need to be aware of:</p><ul id="l59"><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">seasonality_extraction<span class="p">: This transformer supports two ways of extracting seasonality</span></p><p style="padding-left: 64pt;text-indent: 0pt;line-height: 94%;text-align: justify;">– <span class="s48">&quot;period_averages&quot;</span>, where the seasonality profile is estimated using seasonal averaging, and <span class="s48">&quot;fourier_terms&quot;</span>, where we regress on Fourier terms to extract the seasonality.</p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">seasonality_period<span class="p">: Depending on the technique we use for seasonality extraction, this can either be an integer or a string. If </span>&quot;period_averages&quot;<span class="p">, this parameter denotes the number of periods after which the seasonal cycle repeats. If </span>&quot;fourier_terms&quot;<span class="p">, this denotes the seasonality to be extracted from the datetime index. pandas datetime properties such as </span>week_of_day<span class="p">, </span>month<span class="p">, and so on can be used to specify the most prominent seasonality. Similar to </span>FourierDecomposition<span class="p">, which we saw earlier, we can also omit this parameter and provide custom seasonality in the </span>fit<span class="p">/</span>transform <span class="p">methods in the implementation.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">n_fourier_terms<span class="p">: This parameter specifies the number of Fourier terms to be included in the regression. Increasing this parameter makes the fitted seasonality more complex.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;">There is no detrending in this implementation because we already saw a <span class="s48">DetrendingTrans- former</span>. This implementation expects any trend to be removed before using the <span class="s48">fit </span>function.</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see how we can use it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.transforms.target_transformations import DeseasonalizingTransformer</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">deseasonalizing_transformer = DeseasonalizingTransformer(seasonality_extraction=&quot;period_ averages&quot;,seasonal_period=25)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">y_deseasonalized = deseasonalizing_transformer.fit_transform(y, freq=&quot;1D&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p class="s48" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">y_deseasonalized <span class="p">will have the deseasonalized time series. To get back to the original time series, we can use the </span>inverse_transform <span class="p">function. Typically, this can be used to add the seasonality back after making predictions.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="225" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_282.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Best practice</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Modeling seasonality can be done either separately, as discussed here, or by using the seasonal features that we discussed earlier in this chapter. Although the final evaluation on which one works better has to be found out empirically for each dataset, we can have a few rules of thumb/ guidelines to decide on priority.</p><p style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">When we have enough data, letting the model learn seasonality as part of the main forecasting problem seems to work better. But in cases where data is not that rich, extracting seasonality separately before feeding it to an ML model works well.</p><p style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">When the dataset has varied seasonality (different seasonal cycles for different time series), then it should be treated accordingly. Either deseasonalize each time series separately or split the global ML model into different local models each with its own seasonality pattern.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark280">The last aspect that we talked about earlier is heteroscedasticity. Let’s quickly take a look at that as well.</a><a name="bookmark259">&zwnj;</a><a name="bookmark258">&zwnj;</a></p><p class="s3" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Detecting and correcting for heteroscedasticity</p><p style="padding-top: 7pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">Despite having a scary name, heteroscedasticity is a simple enough concept. It is derived from ancient Greek, where <i>hetero </i>means <i>different </i>and <i>skedasis </i>means <i>dispersion</i>. True to its name, heteroscedasticity is defined when the variability of a variable is different across another variable. In the context of a time series, we say a time series is heteroscedastic when the variability or dispersion of the time series varies with time. For instance, let’s think about the spending of a household through the years. In these years, this particular household went from being poor to middle class and finally upper middle class. When the household was poor, the spending was less and only on essentials, and because of that, the variability in spending was less. But as they approached upper middle class, the household could afford luxuries, which created spikes in the time series and therefore higher variability. If we refer back to <i>Figure 7.1</i>, we can see what a heteroscedastic time series looks like.</p><p style="padding-top: 6pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">But in addition to visual inspection, it would be neat if we could carry out an automated statistical test to ascertain heteroscedasticity.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 27pt;text-indent: 0pt;text-align: justify;">Detecting heteroscedasticity</p><p style="padding-top: 8pt;padding-left: 27pt;text-indent: 0pt;line-height: 94%;text-align: justify;">There are many ways to detect heteroscedasticity, but we will be using one of the most popular techniques, known as the <span class="s5">White test</span>, proposed by Halbert White in 1980. The White test uses an auxiliary regression task to check for constant variance. We run an initial regression using some covariates and calculate the residuals of this regression. Then, we fit another regression model with these residuals as the target and the covariates used in the first regression, and their squares and cross products. The final statistic is estimated by using the <span class="s162">𝑅</span><span class="s90">𝑅</span><span class="s163">2</span><span class="s127"> </span>value of this auxiliary regression. For a detailed account of the test, head over to the <i>Further reading </i>section; for the rigorous mathematical procedure, the research paper is cited in the <i>References </i>section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_283.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To learn more about the rigorous mathematical procedure of the White test, take a look at the research paper cited in the <i>References </i>section under reference number <i>2</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a name="bookmark281">In the context of a time series, we adapt this formulation by using a deterministic trend model. The initial regression is done by using time as an ordinal variable and the residuals are used to carry out the White test. The White test has an implementation in </a><span class="s48">statsmodels </span>of <span class="s48">het_white</span>, which we will be using to carry out this test. We have wrapped all of this in a helpful function in <span class="s48">src. transforms.stationary_utils </span>as <span class="s48">check_heteroscedasticity</span>, which has only one additional parameter – <span class="s48">confidence</span>. Let’s see how we can use that:<a name="bookmark260">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.transforms.stationary_utils import check_ heteroscedastisticity</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">check_heteroscedastisticity(y, confidence=0.05)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">This returns a <span class="s48">namedtuple </span>with the following parameters:</p></li><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Heteroscedastic<span class="p">: A Boolean flag indicating the presence of heteroscedasticity</span></p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;"><span class="s48">lm_statistic</span>: The <span class="s5">Lagrangian Multiplier </span>(<span class="s5">LM</span>) statistic</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="101" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_284.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Best practice</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The heteroscedasticity test we are doing only considers a trend in the regression and therefore, in the presence of seasonality, may not work very well. It is advised to deseasonalize the data before applying the function.</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">lm_p_value<span class="p">: The p-value associated with the LM statistic</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Detecting heteroscedasticity was the easier part. There are a few transforms that attempt to remove heteroscedasticity but with advantages and disadvantages. Let’s take a look at a few such transforms.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Log transform</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Log transform, as the name suggests, is about applying a logarithm to the time series. There are two main properties of a log transform – variance stabilization and reducing skewness – thereby making the data distribution more <i>normal</i>. And out of these, we are more interested in the first property because that is what combats heteroscedasticity.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Log transforms are typically known to reduce the variance of the data and thereby remove heteroscedasticity in the data. Intuitively, we can think of a log transform as something that <i>pulls in </i>the extreme values on the right of the histogram, at the same time stretching back the very low values on the left of the histogram.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark282">But it has been shown that the log transform does not always stabilize the variance. In addition to that, the log transform poses another challenge in ML. The optimization of loss now happens on the log scale. This means that while optimizing the loss, the ML model might think that the loss is small enough, but when we inverse the transformation, a very small loss will blow up into a large number. Another key disadvantage is that the log transform can only be applied to strictly positive data. And if any of your data is zero or less than zero, then you will need to offset the whole distribution by adding some constant, </a><i>M</i>, and then applying the transform. This will also create some disturbance in the data, which can have adverse effects.<a name="bookmark261">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The bottom line is that we should be careful when applying a log transform. We have implemented a transformer in <span class="s48">src.transforms.target_transformations.py </span>as <span class="s48">LogTransformer </span>with just one parameter, <span class="s48">add_one</span>, which adds one before the transform and subtracts one after the inverse. Let’s see how we can use it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.transforms.target_transformations import LogTransformer</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">log_transformer = LogTransformer(add_one=True) y_log = log_transformer.fit_transform(y)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p class="s48" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: left;">y_log <span class="p">is the log-transformed time series. We can call </span>inverse_transform <span class="p">to get the original time series back.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Box-Cox transform</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The log transform, although effective and common, is very <i>strong</i>. But the log is not the only monotonic transform that we can use. There are many other transforms, such as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s90" style="text-indent: 0pt;text-align: right;">𝑦𝑦</p><p class="s87" style="padding-top: 1pt;text-indent: 0pt;line-height: 75%;text-align: left;">2<span class="s164">,</span><span class="s90"> 1</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_285.png"/></span></p><p class="s90" style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">𝑦𝑦</p><p class="s90" style="padding-top: 1pt;text-indent: 0pt;line-height: 10pt;text-align: right;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="18" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_286.png"/></span></p><p class="s90" style="text-indent: 0pt;line-height: 8pt;text-align: right;">,    <span class="s165">  </span></p><p class="s90" style="text-indent: 0pt;line-height: 11pt;text-align: right;">√<span class="s144">𝑦𝑦</span></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">and so on, which are collectively part of the family of power transforms. One set of transforms that is very famous and widely used in this family is the Box-Cox transforms:</p><p class="s90" style="padding-top: 3pt;padding-left: 168pt;text-indent: 0pt;line-height: 9pt;text-align: center;">𝑦𝑦λ − 1</p><p class="s90" style="text-indent: 0pt;line-height: 11pt;text-align: right;">y(λ) =</p><p class="s90" style="padding-left: 32pt;text-indent: 0pt;line-height: 9pt;text-align: left;">, if λ ≠ 0</p><p style="text-indent: 0pt;text-align: left;"><span><img width="39" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_287.png"/></span></p><p class="s90" style="padding-left: 13pt;text-indent: 0pt;line-height: 10pt;text-align: left;">λ</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_288.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The original research paper by Box and Cox is cited in the <i>References </i>section under referenc number <i>3</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">e</p><p style="text-indent: 0pt;text-align: left;"/><p class="s74" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝐴𝐴𝐴𝐴𝐴𝐴,  𝑙𝑙𝑙𝑙𝑙𝑙(𝑦𝑦), 𝑖𝑖𝑖𝑖 λ = 0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: left;">Intuitively, we can see that the Box-Cox transform is a generalized logarithm transform. The log transform is just a special case of the Box-Cox transform (when <span class="s90">λ = 0</span>). At d<u>i</u>fferent values of <span class="s90">𝜆𝜆</span>, it</p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="3" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_289.png"/></span></p><p class="s75" style="text-indent: 0pt;line-height: 8pt;text-align: left;">√𝑦𝑦</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">approximates other transforms such as <span class="s90">𝑦𝑦</span><span class="s167">2</span><span class="s127"> </span>when <span class="s168">𝜆𝜆 = 2</span>,  <span class="s74">1  </span>when <span class="s162">𝜆𝜆</span><span class="s90"> = −0.5</span>, <span class="s162">√</span><span class="s90">𝑦𝑦 </span>when <span class="s90">𝜆𝜆 = 0.5</span>, and</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">so on. When <span class="s90">λ = 1</span>, there is no major transformation.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">A lot of the disadvantages that we mentioned for log transforms apply here as well, but the degree to which those effects are there varies, and we have a parameter, <span class="s90">λ</span>, to help us decide on the right level of those effects. Like log transforms, Box-Cox transforms also only use strictly positive data. The same addition of a constant to offset the data distribution has to be done here as well. The flip side of the parameter that there is one more hyperparameter to tune.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are a few automated methods to find the optimum <span class="s90">𝜆𝜆 </span>for any data distribution. One of them is by minimizing the log-likelihood of the data distribution, assuming normality. So, essentially, what we will be doing is finding the optimal <span class="s90">𝜆𝜆 </span>that makes the data distribution most <i>normal</i>. This optimization is already implemented in popular implementations such as the <span class="s48">boxcox </span>function in the <span class="s48">scipy. special </span>module in <span class="s48">scipy</span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_290.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The research paper proposing Guerrero’s method is cited in the <i>References </i>section und reference number <i>4</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">er</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another way to find the optimal <span class="s137">𝜆𝜆 </span>is to use Guerrero’s method, which is typically suited for a time series. In this method, instead of trying to conform the data distribution to a normal distribution, we try to minimize the variability of the time series across different sub-series in the time series that are homogenous. The definition of this sub-series is slightly subjective but usually, we can safely assume the sub-series as the seasonal length. Therefore, what we will be trying to minimize is the variability of the time series across different seasonality cycles.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are stark differences in the way both these optimization methods work and we need to be careful when using them. If our main concern is to remove the heteroscedastic behavior of the time series, Guerrero’s method is what we can use.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">We have made a transformer available in <span class="s48">src.transforms.target_transformations. py </span>called <span class="s48">BoxCoxTransformer</span>. There are a few parameters and properties that we need to be aware of:</p></li><li><p style="padding-top: 9pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;"><span class="s48">box_cox_lambda</span>: This is the <span class="s90">λ </span>parameter to be used for the Box-Cox transform. If left set to <span class="s48">None</span>, the implementation will find an optimal <span class="s90">𝜆𝜆</span>.</p></li><li><p class="s48" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: left;">optimization<span class="p">: This can either be </span>guerrero<span class="p">, which is the default setting, or</span></p><p style="padding-left: 63pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s48">loglikelihood</span>. This determines how the <span class="s90">𝜆𝜆 </span>parameter is estimated.</p></li><li><p style="padding-top: 5pt;padding-left: 63pt;text-indent: -13pt;line-height: 94%;text-align: left;"><span class="s48">seasonal_period</span>: This is an input for finding the optimal <span class="s90">𝜆𝜆 </span>parameter using Guerrero’s method. Technically, this is the length of the sub-series, usually taken as the seasonality period.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: left;"><a name="bookmark283"><span class="s48">bounds</span></a>: This is another parameter that controls the optimization using Guerrero’s method. This is a tuple with lower and upper bounds in the search for the optimal <span class="s90">𝜆𝜆 </span>parameter.<a name="bookmark262">&zwnj;</a></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 28pt;text-indent: 13pt;line-height: 160%;text-align: left;">add_one<span class="p">: This is a flag that adds one to the series before applying a log transform to avoid log 0. Let’s see how we can use it:</span></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.transforms.target_transformations import BoxCoxTransformer</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">boxcox_transformer = BoxCoxTransformer() y_boxcox = boxcox _transformer.fit_transform(y)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p class="s48" style="padding-top: 3pt;padding-left: 28pt;text-indent: 2pt;line-height: 94%;text-align: justify;">y_boxcox <span class="p">will contain the Box-Cox transformed time series. To get back to the original time series, we can use the </span>inverse_transform <span class="p">function.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">When we approach the forecasting problem at scale, we will have hundreds, thousands, or millions of time series that we will need to analyze before forecasting. In such scenarios, an AutoML approach is needed to be practical.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">AutoML approach to target transformation</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">So far, we have discussed many ways to make a series <i>more </i>stationary (we are using the word stationary here in the non-mathematical sense), such as detrending, deseasonalizing, differencing, and monotonic transformations. We’ve also looked at statistical tests to check whether trends, seasonality, and so on are present in a time series. So, the natural next step is to put it all together to carry out these transforms in an automated way while choosing good defaults wherever possible. This is exactly what we did and implemented an <span class="s48">AutoStationaryTransformer </span>in <span class="s48">src.transforms.target_ transformations</span>. The following flow chart explains the logic of this in an automated way:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><span><img width="524" height="188" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_291.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 7.5 – Flow chart for AutoStationaryTransformer</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">We have excluded differencing from this implementation for two reasons:</p><ul id="l60"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Differencing, in the context of predictions, comes with considerable baggage of technical debt. If you do differencing, you are inherently making it difficult to carry out multi-step forecasting. It is possible, but just more difficult and less flexible.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Differencing can be looked at as a different way of doing what we have done here. This is because differencing removes linear trends and seasonal differencing removes seasonality as well. So, for autoregressive time series, differencing can do a lot and deserves to be a standalone transformation.</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, let’s see what parameters we can use to tweak <span class="s48">AutoStationaryTransformer</span>:</p></li><li><p class="s48" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">confidence<span class="p">: The confidence level for the statistical tests. It defaults to </span>0.05<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">seasonal_period<span class="p">: The number of periods after which the seasonality cycle repeats itself. If set to </span>None<span class="p">, </span>seasonal_period <span class="p">will be inferred from the data. It defaults to </span>None<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">seasonality_max_lags<span class="p">: This is only used if </span>seasonality_period <span class="p">is not given. This sets the maximum lags within which we search for seasonality. It defaults to </span>None<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">trend_check_params<span class="p">: These are the parameters that are used in the statistical tests for trend. </span>check_trend <span class="p">defaults to </span>{&quot;mann_kendall&quot;: False}<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">detrender_params<span class="p">: The parameters passed to </span>DetrendingTransformer<span class="p">. This defaults to </span>{&quot;degree&quot;:1}<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">deseasonalizer_params<span class="p">: The parameters passed to </span>DeseasonalizingTrans- former<span class="p">. </span>seasonality_extraction <span class="p">is fixed as </span>period_averages<span class="p">.</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: left;">box_cox_params<span class="p">: The parameters that are passed to </span>BoxCoxTransformer<span class="p">. Defaults to </span>{&quot;optimization&quot;: &quot;guerrero&quot;}<span class="p">.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, let’s apply this automatic transformation to the dataset we have been working with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">train_df = pd.read_parquet(preprocessed/&quot;selected_blocks_train_ missing_imputed_feature_engg.parquet&quot;)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">transformer_pipelines = {}</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">for _id in tqdm(train_df[&quot;LCLid&quot;].unique()):</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">#Initialize the AutoStationaryTransformer with a seasonality period of 48*7</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">auto_stationary = AutoStationaryTransformer(seasonal_ period=48*7)</p><p class="s38" style="padding-top: 2pt;padding-left: 32pt;text-indent: 0pt;text-align: left;">#Creating the timeseries with datetime index</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">y = train_df.loc[train_df[&quot;LCLid&quot;]==_id, [&quot;energy_ consumption&quot;,&quot;timestamp&quot;]].set_index(&quot;timestamp&quot;)</p><p class="s38" style="padding-top: 2pt;padding-left: 32pt;text-indent: 0pt;text-align: left;">#Fitting and transforming the train</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">y_stat = auto_stationary.fit_transform(y, freq=&quot;30min&quot;)</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># Setting the transformed series back to the dataframe</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">train_df.loc[train_df[&quot;LCLid&quot;]==_id, &quot;energy_consumption&quot;]</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">= y_stat.values</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;">#Saving the pipeline</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">transformer_pipelines[_id] = auto_stationary</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark284"/></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The code to execute this is split into two notebooks called <span class="s48">02-Dealing with Non-Sta- tionarity.ipynb </span>and <span class="s48">02a-Dealing with Non-Stationarity-Train+Val. ipynb </span>in the <span class="s48">chapter06 </span>folder. The former does the auto-stationary transformation on the train data, while the latter does it on train and validation data combined. This is to simulate how we would predict for validation data (by just using train data for training) and for test data (where we use the train and validation data for training).</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">This process is slightly time-consuming. I suggest that you run the notebook, grab lunch or a snack, and come back. Once it’s done, the <span class="s48">02-Dealing with Non-Stationarity.ipynb </span>notebook will save a few files:</p></li></ul></li><li><p class="s48" style="padding-top: 9pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">selected_blocks_train_auto_stat_target.parquet<span class="p">: A DataFrame that has </span>LCLid <span class="p">and </span>timestamp <span class="p">as indices and the transformed target</span></p></li><li><p class="s48" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">auto_transformer_pipelines_train.pkl<span class="p">: A Python dictionary of </span>Auto- StationaryTransformer <span class="p">for each </span>LCLid <span class="p">so that we can reverse the transformations in the future</span></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The <span class="s48">02a-Dealing with Non-Stationarity-Train+Val.ipynb </span>notebook also saves the corresponding files for the train and validation datasets.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="134" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_292.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Best Practice</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">This kind of explicit detrending and deseasonalizing before modeling can also be seen as a form of <span class="s5">boosting</span>. This should be considered as just another alternative to modeling all of this together. There can be situations where letting the model learn from end to end in a data-driven manner performs better than injecting these strong inductive biases using explicit detrending and deseasonalization and vice versa. Cross-validated test scores should always have the last word.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The dataset we are working on has almost negligible trends and is pretty stationary throughout. The impact of these transformations will be more evident in time series with strong trends and heteroscedasticity.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark285">Congratulations on making it through a heavy chapter full of new concepts, some statistics, and mathematics. From the point of view of applying ML models for time series, the concepts in this chapter will be really helpful in taking your models to the next level.</a><a name="bookmark265">&zwnj;</a><a name="bookmark264">&zwnj;</a><a name="bookmark263">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">After getting down to a practical level in the previous chapter, we stayed there and plowed on to review concepts such as stationarity and how to deal with such non-stationary time series. We learned about techniques we can use to explicitly handle non-stationary time series such as differencing, detrending, deseasonalizing, and so on. To put this all together, we saw an automatic way of transforming the target, learned how to use the implementation provided, and applied it to our dataset. Now that we have the necessary skills to effectively transform a time series into an ML dataset, in the next chapter, we will start applying a few ML models to the dataset using the features we’ve created.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The following are the references for this chapter:</p><ol id="l61"><ol id="l62"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Bayazit M. and Önöz B. (2007), <i>To prewhiten or not to prewhiten in trend analysis?</i><a href="https://doi.org/10.1623/hysj.52.4.611" class="s140" target="_blank">, Hydrological Sciences Journal, 52:4, 611-624. </a><span class="s46">https://doi.org/10.1623/hysj.52.4.611</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">White, H. (1980), <i>A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity</i><a href="https://doi.org/10.2307/1912934" class="s140" target="_blank">. Econometrica Vol. 48, No. 4 (May 1980), pp. 817-838 (22 pages). </a><span class="s46">https://doi.org/10.2307/1912934</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;line-height: 94%;text-align: justify;">Box, G. E. P. and Cox, D. R. (1964), <i>An analysis of transformations</i><a href="http://www.ime.usp.br/%7Eabe/lista/pdfQWaCMboK68.pdf" class="s140" target="_blank">. Journal of the Royal Statistical Society, Series B, 26, 211-252. </a><a href="http://www.ime.usp.br/%7Eabe/lista/pdfQWaCMboK68.pdf" class="s45" target="_blank">http://www.ime.usp.br/~abe/lista/ </a><span class="s46">pdfQWaCMboK68.pdf</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;line-height: 94%;text-align: justify;">Guerrero, Victor M. (1993), <i>Time-series analysis supported by power transformations</i><a href="https://onlinelibrary.wiley.com/doi/10.1002/for.3980120104" class="s140" target="_blank">. Journal of Forecasting, Volume 12, Issue 1, 37-48. </a><a href="https://onlinelibrary.wiley.com/doi/10.1002/for.3980120104" class="s45" target="_blank">https://onlinelibrary.wiley.com/ </a><span class="s46">doi/10.1002/for.3980120104</span>.</p></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">To learn more about the topics that were covered in this chapter, take a look at the following resources:</p><ul id="l63"><li><p class="s4" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;line-height: 107%;text-align: justify;">Stationarity in time series analysis<a href="https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322" class="s140" target="_blank">, by Shay Palachy: </a><a href="https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322" class="s45" target="_blank">https://towardsdatascience. com/stationarity-in-time-series-analysis-90c94f27322</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 63pt;text-indent: -13pt;text-align: justify;">Comparing ADF Test Functions in R<a href="https://fabian-kostadinov.github.io/2015/01/27/comparing-adf-test-functions-in-r/" class="s140" target="_blank">, by Fabian Kostadinov (the same concepts can be implemented in Python as well): </a><a href="https://fabian-kostadinov.github.io/2015/01/27/comparing-adf-test-functions-in-r/" class="s45" target="_blank">https://fabian-kostadinov.github.io/2015/01/27/ comparing-adf-test-functions-in-r/</a></p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Kendall’s Tau<a href="https://www.statisticshowto.com/kendalls-tau/" class="s140" target="_blank">: </a><a href="https://www.statisticshowto.com/kendalls-tau/" class="s45" target="_blank">https://www.statisticshowto.com/kendalls-tau/</a></p><p class="s47" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Further reading 163</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_293.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p style="padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: left;"><a href="https://www.statisticshowto.com/wp-content/uploads/2016/08/Mann-Kendall-Analysis-1.pdf" class="s140" target="_blank">Mann-Kendall trend test: </a><a href="https://www.statisticshowto.com/wp-content/uploads/2016/08/Mann-Kendall-Analysis-1.pdf" class="s45" target="_blank">https://www.statisticshowto.com/wp-content/ uploads/2016/08/Mann-Kendall-Analysis-1.pdf</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: left;">Theil-Sen estimator<a href="https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator" class="s140" target="_blank">: </a><a href="https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator" class="s45" target="_blank">https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_ estimator</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: left;">Statistical inference with correlograms <a href="https://en.wikipedia.org/wiki/Correlogram#Statistical_inference_with_correlograms" class="s140" target="_blank">– Wikipedia: </a><a href="https://en.wikipedia.org/wiki/Correlogram#Statistical_inference_with_correlograms" class="s45" target="_blank">https://en.wikipedia.org/ wiki/Correlogram#Statistical_inference_with_correlograms</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 107%;text-align: left;">White test for Heteroscedasticity Detection<a href="https://itfeature.com/heteroscedasticity/white-test-for-heteroskedasticity" class="s140" target="_blank">: </a><a href="https://itfeature.com/heteroscedasticity/white-test-for-heteroskedasticity" class="s45" target="_blank">https://itfeature.com/ heteroscedasticity/white-test-for-heteroskedasticity</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark286">8</a><a name="bookmark287">&zwnj;</a><a name="bookmark289">&zwnj;</a><a name="bookmark288">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 104pt;text-indent: -13pt;line-height: 114%;text-align: right;">Forec asting Time Series with M achine Learning </h4><h4 style="text-indent: 0pt;line-height: 30pt;text-align: right;">M odels </h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous chapter, we started looking at machine learning as a tool to solve the problem of time series forecasting. We talked about a few techniques such as time delay embedding and temporal embedding, both of which cast a time series forecasting problem as a classical regression problem from the machine learning paradigm. In this chapter, we’ll look at these techniques in detail and go through them in a practical sense using the London Smart Meters dataset we have been working with throughout this book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will cover the following topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Training and predicting with machine learning models</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Generating single-step forecast baselines</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Standardized code to train and evaluate machine learning models</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Training and predicting for multiple households</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You must run the following notebooks for this chapter:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">02 - Preprocessing London Smart Meter Dataset.ipynb <span class="p">in </span>Chapter02</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">01-Setting up Experiment Harness.ipynb <span class="p">in </span>Chapter04</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l64"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;"><a name="bookmark308">01-Feature Engineering.ipynb </a><span class="p">in </span>Chapter06<a name="bookmark290">&zwnj;</a></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">02-Dealing with Non-Stationarity.ipynb <span class="p">in </span>Chapter07</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">02a-Dealing with Non-Stationarity-Train+Val.ipynb <span class="p">in </span>Chapter07</p><p class="s27" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter08" class="s140" target="_blank">The code for this chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter08" class="a" target="_blank">https://github.com/PacktPublishing/Modern- </a>Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter08<span class="p">.</span></p><p class="s3" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Training and predicting with machine learning models</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_294.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_295.png"/></span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><a href="#bookmark186" class="s140">In </a><a href="#bookmark186" class="s21">Chapter </a><i>5</i>, <i>Time Series Forecasting as Regression</i>, we talked about a schematic for supervised machine learning (<i>Figure 5.2</i>). In the schematic, we mentioned that the purpose of a supervised learning problem is to come up with a function,  <span class="s111">̂   = ℎ</span><span class="s136">(</span><span class="s111">   , ϕ</span><span class="s136">)</span>, where <i>X </i>is the set of features as the input, <span class="s120">ϕ</span><span class="s58"> </span>is the model parameters, and <i>h </i>is the approximation of the ideal function. In this section, we are going to talk about <i>h </i>in more detail and see how we can use different machine learning models to estimate it.</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">h <span class="p">is any function that approximates the ideal function, but it can be thought of as an element of all possible functions from a family of functions. More formally, we can say the following:</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_296.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_297.png"/></span></p><p class="s141" style="padding-top: 8pt;padding-left: 43pt;text-indent: 0pt;text-align: center;">̂   = ℎ(   , ϕ), where ℎ ϵ ℋ</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <span class="s131">ℋ </span>is a family of functions that we also call a model. For instance, linear regression is a type of model or a family of functions. For each value of the coefficients, the linear regression model gives you a different function and <span class="s131">ℋ </span>becomes the set of all possible functions a linear regression model can produce.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are many families of functions, or models, available. For a more complete understanding of the space, we will need to refer to machine learning books or resources. The <i>Further reading </i>section contains a few resources that may help you start the journey. As for the scope of this book, we narrowly define it as the application of machine learning models for forecasting, rather than machine learning in general. And although we can use any regression model, we will only review a few popular and useful ones for time series forecasting and see them in action. We leave it to you to strike out on your own and explore the other algorithms to become familiar with them as well. But before we look at the different models, we need to generate a few baselines again.</p><p class="s47" style="padding-top: 4pt;padding-left: 252pt;text-indent: 0pt;text-align: left;"><a name="bookmark309">Generating single-step forecast baselines 167</a><a name="bookmark292">&zwnj;</a><a name="bookmark291">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_298.png"/></span></p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Generating single-step forecast baselines</p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark137" class="s140">We reviewed and generated a few baseline models back in </a>Chapter 4<span class="p">, </span>Setting a Strong Baseline Forecast<a href="#bookmark211" class="s140">. But there is a small issue – the prediction horizon. In </a>Chapter 6<span class="p">, </span>Feature Engineering for Time Series Forecasting<span class="p">, we talked about how the machine learning model can only predict one target at a time and that we are sticking with a single-step forecast. The baselines we generated earlier were not single- step, but multi-step. Generating a single-step forecast for baseline algorithms such as ARIMA or ETS requires us to fit on history, predict one step ahead, and then fit again using one more day. Predicting in such an iterative fashion for our test or validation period requires us to do this iteration ~1,440 times (48 data points a day for 30 days) and repeat this for all the households in our selected dataset (150, in our case). This would take quite a long time to compute.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark137" class="s140">We have chosen the naïve method and seasonal naïve (</a><i>Chapter 4</i>, <i>Setting a Strong Baseline Forecast</i>), which can be implemented as native <span class="s20">pandas </span>methods, as two baseline methods to generate single- step forecasts. Naïve forecasts perform unreasonably well for single-step ahead forecasts and can be considered a strong baseline. In the <span class="s20">chapter08 </span>folder, there is a notebook named <span class="s20">00-Single Step Backtesting Baselines.ipynb </span>that generates these baselines and saves them to disk. Let’s run the notebook now. The notebook generates the baselines for both the validation and test datasets and saves the predictions, metrics, and aggregate metrics to disk. The aggregate metrics for the test period are as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 92pt;text-indent: 0pt;text-align: left;"><span><img width="357" height="77" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_299.gif"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.1 – Aggregate metrics for a single-step baseline</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">To make training and evaluating these models easier, we have used a standard structure throughout. Let’s quickly review that structure as well so that you can follow along with the notebooks closely.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;line-height: 119%;text-align: justify;">Standardized code to train and evaluate machine learning models</p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are two main ingredients while training a machine learning model – <i>data </i>and the <i>model </i>itself. Therefore, to standardize the pipeline, we defined three configuration classes (<span class="s20">FeatureConfig</span>, <span class="s20">MissingValueConfig</span>, and <span class="s20">ModelConfig</span>) and another wrapper class (<span class="s20">MLForecast</span>) over scikit-learn-style estimators (<span class="s20">.fit </span>- <span class="s20">.predict</span>) to make the process smooth. Let’s look at each of them.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="87" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_300.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the code, use the <span class="s20">01-Forecasting with ML.ipynb </span>notebook i the <span class="s20">chapter08 </span>folder and the code in the <span class="s20">src </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark310">FeatureConfig</a><a name="bookmark293">&zwnj;</a></p><p class="s20" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">FeatureConfig <span class="p">is a Python </span>dataclass <span class="p">that defines a few key attributes and functions that are necessary while processing the data. For instance, continuous, categorical, and Boolean columns need separate kinds of preprocessing before being fed into the machine learning model. Let’s see what </span>FeatureConfig <span class="p">holds:</span></p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">date<span class="p">: A mandatory column that sets the name of the column with </span>date <span class="p">in the DataFrame.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">target<span class="p">: A mandatory column that sets the name of the column with </span>target <span class="p">in the DataFrame.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">original_target<span class="p">: If </span>target <span class="p">contains a transformed target (log, differenced, and so on),</span><span class="s35"> </span>original_target <span class="p">specifies the name of the column with the target without transformation. This is essential in calculating metrics such as MASE, which relies on training history. If not given, it is assumed that </span>target <span class="p">and </span>original_target <span class="p">are the same.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">continuous_features<span class="p">: A list of continuous features.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">categorical_features<span class="p">: A list of categorical features.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">boolean_features<span class="p">: A list of Boolean features. Boolean features are categorical but only have two unique values.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">index_cols<span class="p">: A list of columns that are set as a DataFrame index while preprocessing. Typically, we would give the datetime and, in some cases, the unique ID of a time series as indices.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">exogenous_features<span class="p">: A list of exogenous features. The features in the DataFrame may be from the feature engineering process, such as the lags or rolling features, but also external sources such as the temperature data in our dataset. This is an optional field that lets us bifurcate the exogenous features from the rest of the features. The items in this list should be a subset of </span>continuous_features<span class="p">, </span>categorical_features<span class="p">, or </span>boolean_features<span class="p">.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In addition to a bit of validation on the inputs, there is also a helpful method called <span class="s20">get_X_y </span>in the class, with the following parameters:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">df<span class="p">: A DataFrame that contains all the necessary columns, including the target, if available</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">categorical<span class="p">: A Boolean flag for including categorical features or not</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">exogenous<span class="p">: A Boolean flag for including exogenous features or not</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The function returns a tuple of <span class="s20">(features, target, original_target)</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark311">All we need to do is initialize the class, like any other class, with the feature names separated into the parameters of the class. The entire code that contains all the features is available in the accompanying notebook.</a><a name="bookmark294">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">After setting the <span class="s20">FeatureConfig </span>data class, we can pass any DataFrame with the features defined to the <span class="s20">get_X_y </span>function to get the features, target, and original target:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">train_features, train_target, train_original_target = feat_ config.get_X_y(</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">sample_train_df, categorical=False, exogenous=False</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p class="s4" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark561" class="s140">As you can see, we are not using categorical features or exogenous features here. We will talk about how to handle categorical features in </a>Chapter 15<span class="p">, </span>Strategies for Global Deep Learning Forecasting Models<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">MissingValueConfig</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="134" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_301.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Best practice</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Although filling with zero or mean is the default or go-to method for the majority of the dat scientist community, we should always make an effort to fill the missing values as intelligentl as possible. In terms of lag features, filling with zero can distort the feature. Instead of fillin with zero, a backward fill (using the earliest value in the column to fill backward) might be much better fit.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 89%;text-align: justify;">a y g a</p><p style="text-indent: 0pt;text-align: left;"/><p class="s4" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark87" class="s140">Another key setting is how to deal with missing values. We saw a few ways to fill in missing values from a time series context in </a>Chapter 3<span class="p">, </span>Analyzing and Visualizing Time Series Data<span class="p">, and we have already filled in missing values and prepared our datasets. But a few missing values will be created in the feature engineering required to convert a time series into a regression problem. For instance, when creating lag features, the earliest date in the dataset will not have enough data to create a lag and will be left empty.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Some machine learning models handle empty or <span class="s20">NaN </span>features naturally, while for other machine learning models, we will need to deal with such missing values before training. It’s helpful if we can define <span class="s20">config </span>in which we set for a few columns where we expect <span class="s20">NaN </span>information on how to fill those. <span class="s20">MissingValueConfig </span>is a Python <span class="s20">dataclass </span>that does just that. Let’s see what it holds:</p></li></ul></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">bfill_columns<span class="p">: A list of column names that need to use a backward fill strategy to fill missing values.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">ffill_columns<span class="p">: A list of column names that need to use a forward fill strategy to fill missing values. If a column name is repeated across both </span>bfill_columns <span class="p">and </span>ffill_columns<span class="p">, that column is filled using backward fill first and the rest of the missing values are filled with the forward fill strategy.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">zero_fill_columns<span class="p">: A list of column names that need to be filled with zeros.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark312">The order in which the missing values are filled is </a><span class="s20">bfill_columns </span>then <span class="s20">ffill_columns </span>and then <span class="s20">zero_fill_columns</span>. As the default strategy, the data class uses the column mean to fill in missing values so that even if you have not defined any strategy for a column, the missing value will be filled in by using a column mean. There is a method called <span class="s20">impute_missing_values </span>that takes in the DataFrame and fills the empty cells with a value according to the specified strategy.<a name="bookmark295">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">ModelConfig</p><p class="s20" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">ModelConfig <span class="p">is a Python </span>dataclass <span class="p">that holds a few details regarding the modeling process, such as whether to normalize the data, whether to fill missing values, and so on. Let’s take a detailed look at what it holds:</span></p><ul id="l65"><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">model<span class="p">: This is a mandatory parameter that can be any scikit-learn-style estimator.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">name<span class="p">: A string name or identifier for the model. If it’s not used, it will revert to the name of the class that was passed in as </span>model<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">normalize<span class="p">: A Boolean flag to set whether to apply </span>StandardScaler <span class="p">to the input or not.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">fill_missing<span class="p">: A Boolean flag to set whether to fill empty values before training or not. Some models can handle </span>NaN <span class="p">naturally, while others can’t.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">encode_categorical<span class="p">: A Boolean flag to set whether to encode categorical columns as part of the fitting procedure. If </span>False<span class="p">, categorical encoding is expected to be done separately and included as part of continuous features.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: left;">categorical_encoder<span class="p">: If </span>encode_categorical <span class="p">is </span>True<span class="p">, </span>categorical_encoder</p><p style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: left;">is the scikit-learn-style encoder we can use.</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see how we can define the <span class="s20">ModelConfig </span>data class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">model_config = ModelConfig( model=LinearRegression(), name=&quot;Linear Regression&quot;, normalize=True, fill_missing=True,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">This has just one method, <span class="s20">clone</span>, that clones the estimator, along with the config, into a new instance.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark313">MLForecast</a><a name="bookmark296">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Last but not least, we have the wrapper class around a scikit-learn-style model. It uses the different configurations we have discussed to encapsulate the training and prediction functions. Let’s see what parameters are available when initializing the model:</p></li></ul></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: left;">model_config<span class="p">: The instance of the </span>ModelConfig <span class="p">class we discussed in the </span><span class="s4">ModelConfig</span></p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: left;">section.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">feature_config<span class="p">: The instance of the </span>FeatureConfig <span class="p">class we discussed earlier.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">missing_config<span class="p">: The instance of the </span>MissingValueConfig <span class="p">class we discussed earlier.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">target_transformer<span class="p">: The instance of target transformers from </span>src.transforms<span class="p">. It should support </span>fit<span class="p">, </span>transform<span class="p">, and </span>inverse_transform<span class="p">. It should also return</span><span class="s35"> </span>pd.Series <span class="p">with a datetime index to work without errors. If we have done the target transform separately, then this is also used to perform </span>inverse_transform <span class="p">during prediction.</span></p><p class="s20" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">MLForecast <span class="p">has a few functions that can help us manage the life cycle of a model, once initialized. Let’s take a look.</span></p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The fit function</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The <span class="s20">fit </span>function is similar in purpose to the scikit-learn <span class="s20">fit </span>function but does a little extra by handling the standardization, categorical encoding, and target transformations using the information in the three configs. The parameters of the function are as follows:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">X<span class="p">: The </span>pandas <span class="p">DataFrame with features to be used in the model as columns.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">y<span class="p">: This is the target and can be a </span>pandas <span class="p">DataFrame, </span>pandas <span class="p">Series, or a </span>numpy <span class="p">array.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">is_transformed<span class="p">: This is a Boolean parameter that lets us know whether the target is already transformed or not. If </span>True<span class="p">, the </span>fit <span class="p">method won’t be transforming the target, even if we have initialized the object with </span>target_transformer<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">fit_kwargs<span class="p">: This is a Python dictionary of keyword arguments that need to be passed to the </span>fit <span class="p">function of the estimator.</span></p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The predict function</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The <span class="s20">predict </span>function handles inferencing. It wraps around the <span class="s20">predict </span>function of the scikit- learn estimator, but like <span class="s20">fit</span>, it does a few other things, such as standardization, categorical encoding, and reversing the target transformation. There is only one parameter for this function:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">X<span class="p">: The </span>pandas <span class="p">DataFrame with features to be used in the model as columns. The index of the DataFrame is passed on to the prediction.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark314">The feature_importance function</a><a name="bookmark297">&zwnj;</a></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The <span class="s20">feature_importance </span>function retrieves the feature importance from the model, if available. For linear models, it extracts the coefficients, while for tree-based models, it extracts the built-in importance and returns it in a sorted DataFrame.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Helper functions for evaluating models</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In addition to these standard configurations and wrapper classes, we have also defined a couple of helper functions for evaluating different models in the notebook:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">def evaluate_model( model_config, feature_config, missing_config, train_features, train_target, test_features, test_target,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">):</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: -24pt;line-height: 131%;text-align: left;">ml_model = MLForecast( model_config=model_config, feature_config=feat_config, missing_config=missing_value_config,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">ml_model.fit(train_features, train_target) y_pred = ml_model.predict(test_features) feat_df = ml_model.feature_importance()</p><p class="s28" style="padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">metrics = calculate_metrics(test_target, y_pred, model_ config.name, train_target)</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">return y_pred, metrics, feat_df</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="101" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_302.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The standard implementation that we have provided with this book is in no way a one-size- fits-all approach, but rather something that works best with the flow and dataset of this book. However, it does form a good starting point and guide for designing systems with your data.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">This provides us with a standard way of evaluating all the different models, as well as automating the process at scale.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark315">Now that we have the baselines and a standard way to apply different models, let’s get back to what the different models are. For the discussion ahead, let’s keep </a><i>time </i>out of our mind because we have converted a time series forecasting problem into a regression problem and factored in <i>time </i>as a feature of the problem (the lags and rolling features).<a name="bookmark298">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Linear regression</p><p style="padding-top: 8pt;padding-bottom: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Linear regression is a family of functions that takes the following form:</p><p style="padding-left: 235pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_303.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_304.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="3" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_305.png"/></span></p><p class="s135" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">̂   = β<span class="s169">0</span><span class="s62">  </span>+ ∑   <span><img width="13" height="14" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_306.png"/></span><span class="s170"> </span>β  ,</p><p style="text-indent: 0pt;text-align: left;"><span><img width="3" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_307.png"/></span></p><p class="s62" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">=1</p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 90%;text-align: justify;">Here, <i>k </i>is the number of features in the model and <span class="s171">β </span>are the parameters of the model. There is a <span class="s172">β </span>for each feature and a <span class="s173">β</span><span class="s106">0</span>, which we call the intercept, which is estimated from data. Essentially, the output is a linear combination of the feature vectors, <i>X</i>. As the name suggests, this is a linear function.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="41" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_308.png"/></span></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The model parameters can be estimated from data, <span class="s69">( , )</span>, using an optimization method and loss, but the most popular method of estimation is using <span class="s5">ordinary least squares </span>(<span class="s5">OLS</span>). Here, we find the model parameters, <span class="s174">β</span>, which minimizes the residual sum of squares (<span class="s5">mean squared error </span>(<span class="s5">MSE</span>)):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 216pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="8" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_309.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="26" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_310.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_311.png"/></span></p><p class="s135" style="padding-top: 6pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">= ∑   (     − ŷ            <span class="s169">i</span>)2</p><p style="text-indent: 0pt;text-align: left;"><span><img width="3" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_312.png"/></span></p><p class="s62" style="padding-top: 2pt;padding-left: 164pt;text-indent: 0pt;text-align: center;">=1</p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The loss function here is very intuitive. We are essentially minimizing the distance between the training samples and our predicted points. The square term acts as a technique that does not cancel out positive and negative errors. Apart from the intuitiveness of the loss, another reason why this is widely chosen is that an analytical solution exists for least squares and because of that, we don’t need to resort to more compute-intensive optimization techniques such as gradient descent.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Linear regression has one foot firmly planted in statistics and with the right assumptions, it can be a powerful tool. Commonly, five assumptions are associated with linear regression, as follows:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The relationship between the independent and dependent variables is linear.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The errors are normally distributed.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The variance of the errors is constant across all the values of the independent variable.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">There is no autocorrelation in the errors.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">There is little to no correlation between independent variables (multi-collinearity).</p></li></ul><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">But unless you are concerned about using linear regression to come up with prediction intervals (a band in which the prediction would lie with some probability), we can disregard all but the first assumption to some extent.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_313.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_314.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_315.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_316.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_317.png"/></span></p><p class="s105" style="text-indent: 0pt;line-height: 6pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"/><p class="s105" style="text-indent: 0pt;line-height: 6pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The linearity assumption (the first assumption) is relevant because if the variables are not linearly related, it will result in an underfit and thus poor performance. We can get around this problem to some extent by projecting the inputs into a higher dimensional space. Theoretically, we can project a non-linear problem into a higher-dimensional space, where the problem is linear. For instance, let’s consider a non-linear function, <span class="s62">= 3 2 + 2 2 + 6 </span><span class="s175">1 2</span>. If we run linear regression in the input space</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_318.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_319.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_320.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="14" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_321.png"/></span></p><p class="s176" style="text-indent: 0pt;line-height: 8pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;">of <span class="s68">1 </span>and <span class="s177">2</span>, we know the resulting model will be highly underfitting. But if we project the input</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_322.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_323.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_324.png"/></span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;">space from <span class="s68">1 </span>and <span class="s177">2 </span>to <span class="s178">2</span>, a perfect linear fit.</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s179">2</span>, and by using a polynomial transform, the function for <i>y </i>becomes</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The multi-collinearity assumption (the final assumption) is partly relevant to the fit of the linear function because when we have highly correlated independent variables, the estimated coefficients are highly unstable and difficult to interpret. The fitted function would still be working well, but because we have multi-collinearity, even small changes in the inputs would make the coefficients change magnitude and sign. It is best practice to check for multi-collinearity if you are using a pure linear regression. This is typically a problem in time series because the features we have extracted, such as the lag and rolling features, may be correlated with each other. Therefore, we will have to be careful while using and interpreting linear regression on time series data.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s see how we can use linear regression and evaluate the fit on a sample household from our validation dataset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from sklearn.linear_model import LinearRegression model_config = ModelConfig(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">model=LinearRegression(), name=&quot;Linear Regression&quot;,</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># LinearRegression is sensitive to normalized data</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">normalize=True,</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># LinearRegression cannot handle missing values</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">fill_missing=True,</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">y_pred, metrics, feat_df = evaluate_model( model_config,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">feat_config, missing_value_config, train_features, train_target, test_features, test_target,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark316">The single-step forecast looks good and is already better than the naïve forecast (MAE = 0.173):</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="499" height="262" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_325.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.2 – Linear regression forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The coefficients of the model, <span class="s173">β </span>(which can be accessed using the <span class="s20">coef_ </span>attribute of a trained scikit- learn model), show how much influence each feature has on the output. So, extracting and plotting them gives us our first level of visibility into the model. Let’s take a look at the coefficients of the model:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 33pt;text-indent: 0pt;text-align: left;"><span><img width="512" height="270" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_326.gif"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.3 – Feature importance of linear regression (top 15)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark317">If we look at the </a><i>Y</i>-axis in the feature importance chart, we can see it is in billions as the coefficient for a couple of features is in orders of magnitude in billions. We can also see that those features are Fourier series-based features, which are correlated with each other. Even though we have a lot of coefficients that are in billions, we can find them on both sides of zero, so they will essentially cancel out each other in the function. This is the problem with multi-collinearity that we talked about earlier. We can go about removing multi-collinear features and then perform some sort of feature selection (forward selection or backward elimination) to make the linear model even better.<a name="bookmark299">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">But instead of that, let’s look at a few modifications we can make to the linear model that are a bit more robust to multi-collinearity and feature selection.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Regularized linear regression</p><p class="s4" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">We briefly talked about regularization in </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, and mentioned that regularization, in the general sense, is any kind of constraint we place on the learning process to reduce the complexity of the learned function. One of the ways linear models can become more complex is by having a high magnitude of coefficients. For instance, in the linear fit, we have a coefficient of 20 billion. Any small change in that feature is going to cause a huge fluctuation in the resulting prediction. Intuitively, if we have a large coefficient, the function becomes more flexible and complex. One way we can fix this is to apply regularization in the form of weight decay. Weight decay is when we add a term that penalizes the magnitude of the coefficients to the loss function. The loss function, residual sum of squares, now becomes as follows:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 210pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_327.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_328.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_329.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_330.png"/></span></p><p class="s58" style="padding-top: 6pt;padding-left: 51pt;text-indent: 0pt;text-align: center;">= ∑   (     − ŷ                         <span class="s101">i</span>)2 + λ</p><p style="text-indent: 0pt;text-align: left;"><span><img width="3" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_331.png"/></span></p><p class="s95" style="padding-top: 2pt;padding-left: 151pt;text-indent: 0pt;text-align: center;">=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_332.png"/></span></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, is the weight decay and <span class="s58">λ &gt; 0 </span>is the strength of regularization.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_333.png"/></span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 14pt;text-align: justify;">is typically the norm of the weight matrix. In linear algebra, the norm of a matrix is a measure of how large its elements are. There are many norms for a matrix, but the two most common norms that are used for regularization are the <span class="s5">L1 </span>and <span class="s5">L2 </span>norms. When we use the L1 norm to regularize linear regression, we call it <span class="s5">lasso regression</span>, while when we use the L2 norm, we call it <span class="s5">ridge regression</span>. When we apply weight decay regularization, we are forcing the coefficients to be lower, which means that it also acts as an internal feature selection because the features that don’t add a lot of value will get very low or zero (depending on the type of regularization) coefficients, which means they contribute little to nothing in the resulting function.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The L1 norm is defined as the sum of the absolute values of the matrix. For weight decay regularization, the L1 norm would be as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 238pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="5" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_334.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="14" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_335.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_336.png"/></span></p><p class="s68" style="padding-top: 5pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">= ∑   |β |</p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_337.png"/></span></p><p class="s99" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark318">L2 norm is defined as the sum of squared values of a matrix. For weight decay regularization, the L2 norm would be as follows:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 230pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="5" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_338.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_339.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_340.png"/></span></p><p class="s131" style="padding-top: 6pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">= ∑   β2</p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_341.png"/></span></p><p class="s106" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_342.png"/></span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">By adding this term to the loss function of linear regression, we are forcing the coefficients to be small because while the optimizer is reducing the RSS, it is also incentivized to reduce .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="117" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_343.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The following section discusses the geometric intuition of regularization. Although it would make your understanding of regularization more solid, it is not essential to be able to follow the rest of this book. So, feel free to skip the next section and just read the <i>Key Point </i>callout if you are pressed for time or if you want to come back to it later when you have time.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Another way we can think about regularization is in terms of linear algebra and geometry.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Regularization – a geometric perspective</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">If we look at the L1 and L2 norms from a slightly different perspective, we will see that they are measures of distance.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_344.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_345.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_346.png"/></span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let be the vector of all the coefficients, <span><img width="8" height="14" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_347.png"/></span>, in linear regression. A vector is an array of numbers, but geometrically, it is also an arrow from the origin to a point in the <i>n</i>-dimensional coordinate space. Now, the L2 norm is nothing but the Euclidean distance from the origin on that point in space defined by the vector, . The L1 norm is the Manhattan distance or taxicab distance from the origin on that point in space defined by the vector, . Let’s see this visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 140pt;text-indent: 0pt;text-align: left;"><span><img width="236" height="223" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_348.jpg"/></span></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.4 – Euclidean versus Manhattan distance</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 82%;text-align: justify;">Euclidean distance is the length of the direct path from the origin to the point. But if we can only move parallel to the two axes, we will have to travel the distance of <span class="s180">β</span><span class="s181">0 </span>along the one axis first, and then a distance of <span class="s67">β</span><span class="s96">1 </span>along the other. This is the Manhattan distance.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s say we are in a city (for example, Manhattan) where the buildings are laid out in square blocks where the straight streets intersect at right angles, and we want to travel from point A to point B. Euclidean distance is the direct distance from point A to point B, which in the real sense is only possible if we parkour through the top of the buildings. On the other hand, the Manhattan distance is the actual distance a taxicab would take while traveling the right-angled roads from point A to point B.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">To develop further geometrical intuition about the L1 and L2 norms, let’s do one thought experiment. If we move the point, <span class="s108">(</span><span class="s141">β0, β1</span><span class="s108">)</span>, in the 2D space while keeping the Euclidean distance or the L2 norm the same, we will end up with a circle with its center at the origin. This becomes a sphere in 3D and a hypersphere in <i>n</i>-D. If we trace out the same but keep the L1 norm the same, we will end up with a diamond with its center at the origin. This would become a cube in 3D and a hypercube in <i>n</i>-D.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_349.png"/></span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, when we are optimizing for the weights, in addition to the main objective of reducing the loss function, we are also encouraging the coefficients to stay within a defined distance (norm) from the origin. Geometrically, this means that we are asking the optimization to find a vector, , that minimizes the loss function and stays within the geometric shape (circle or square) defined by the norm. We can see this in the following diagram:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 84pt;text-indent: 0pt;text-align: left;"><span><img width="403" height="263" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_350.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 48pt;text-indent: 0pt;text-align: left;">Figure 8.5 – Regularization with the L1 Norm (lasso regression) versus the L2 Norm (ridge regression)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_351.png"/></span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The concentric circles in the diagram are the contours of the loss function, with the innermost being the lowest. As we move outward, the loss increases. So, instead of selecting a <span><img width="27" height="15" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_352.png"/></span>, regularized regression will select a that intersects with the norm geometry.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="117" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_353.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Key point</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">For an L2 norm, the coefficients of less relevant features are pushed to zero, but not exactly zero. The feature will still play a role in the final function, but its influence will be minuscule. The L1 norm, on the other hand, pushes the coefficients of such features completely to zero, resulting in a sparse solution.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark319">This geometric interpretation also makes understanding another key difference between ridge and lasso regression. Lasso regression, because of the L1 norm, produces a sparse solution. Earlier, we mentioned that weight decay regularization does implicit feature selection. But depending on whether you are applying the L1 or L2 norm, the kind of implicit feature selection differs.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This can be understood better using the geometrical interpretation of regularization. In optimization, the interesting points are usually found in the extrema or <i>corners </i>of a shape. There are no corners in a circle, so an L2 norm is created; the minima can lie anywhere on the edge of the circle. But for the diamond, we have four corners, and the minima would lie in those corners. So, with the L2 norm, the solution can move very close to zero, but not necessarily zero. However, with the L1 norm, the solution would be on the corners, where the coefficient can be pushed to an absolute zero.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s see how we can use ridge regression and evaluate the fit on a sample household from our validation dataset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from sklearn.linear_model import RidgeCV model_config = ModelConfig(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">model=RidgeCV(), name=&quot;Ridge Regression&quot;,</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># RidgeCV is sensitive to normalized data</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">normalize=True,</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># RidgeCV does not handle missing values</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">fill_missing=True</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">y_pred, metrics, feat_df = evaluate_model( model_config,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">feat_config, missing_value_config, train_features, train_target, test_features, test_target,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s look at the single-step ahead forecast from <span class="s20">RidgeCV</span>. It looks very similar to linear regression. Even the MAE is the same for this household:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 36pt;text-indent: 0pt;text-align: left;"><span><img width="530" height="279" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_354.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.6 – Ridge regression forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;">But it is interesting to look at the coefficients with the L2 regularized model. Let’s take a look at the coefficients of the model:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;"><span><img width="509" height="271" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_355.gif"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.7 – Feature importance of ridge regression (top 15)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, the <i>Y</i>-axis looks reasonable and small. The coefficients for the multi-collinear features have shrunk to a more reasonable level. Features such as the lag features, which should ideally be highly influential, have gained the top spots. As you may recall, in the linear regression (<i>Figure 8.3</i>), these features were dwarfed by the huge coefficients on the Fourier features. We have just plotted the top 15 features here, but if you look at the entire list, you will see that there will be a lot of features for which the coefficients are close to zero.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s try lasso regression on the sample household:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from sklearn.linear_model import LassoCV model_config = ModelConfig(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">model=LassoCV(), name=&quot;Lasso Regression&quot;,</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># LassoCV is sensitive to normalized data</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">normalize=True,</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># LassoCV does not handle missing values</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">fill_missing=True</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">y_pred, metrics, feat_df = evaluate_model( model_config,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">feat_config, missing_value_config, train_features, train_target, test_features, test_target,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s look at the single-step ahead forecast from <span class="s20">LassoCV</span>. Like ridge regression, there is hardly any visual difference from linear regression:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark320"><span><img width="529" height="279" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_356.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.8 – Lasso regression forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-bottom: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s look at the coefficients of the model:</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;"><span><img width="511" height="270" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_357.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.9 – Feature importance of lasso regression (top 15)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;">The coefficients are very similar to ridge regression, but if you look at the full list of coefficients (in the notebook), you will see that there are a lot of features where the coefficients will be zero.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark321">Even with the same MAE, MSE, and so on, ridge or lasso regression is preferred to linear regression because of the additional stability and robustness that comes with regularized regression, especially for forecasting, where multi-collinearity is almost always there. But we need to keep in mind that all the linear regression models are still only capturing linear relationships. If the dataset has a non-linear relationship, the resulting fit from linear regression won’t be as good and, sometimes, will be terrible.</a><a name="bookmark300">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s switch tracks and look at another class of models – <span class="s5">decision trees</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Decision trees</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Decision trees are another family of functions that is much more expressive than a linear function. Decision trees split the feature space into different sub-spaces and fit a very simple model (such as an average) to each. Let’s understand how this partitioning works with an example. Let’s consider a regression problem for predicting <i>Y </i>with just one feature, <i>X</i>, as shown in the following diagram:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 84pt;text-indent: 0pt;text-align: left;"><span><img width="379" height="333" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_358.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.10 – The feature space partitioned by a decision tree</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Right away, we can see that fitting a linear function would result in an underfit. But what decision trees do is split the feature space (here, it is just <i>X</i>) into different regions where the target, <i>Y</i>, is similar and then fit a simple function such as an average (because it is a regression problem). In this case, the decision tree has split the feature space into partitions – A, B, and C. Now, for any <i>X </i>that falls into partition A, the prediction function will return the average of all the points in partition A.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark322">These partitions are formed by creating a decision tree using data. Intuitively, a decision tree creates a set of if-else conditions and tries to arrive at the best way to partition the feature space to maximize the homogeneity of the target variable within the partition. One helpful way to understand what a decision tree does is to think of data points as beads flowing down a tree, taking a path that is based on its features, and ending up in a final resting place. Before we talk about how to create a decision tree from data, let’s take a look at its components and understand the terminology surrounding it:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span><img width="416" height="257" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_359.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.11 – Anatomy of a decision tree</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 36pt;text-indent: 0pt;text-align: justify;">There are two types of nodes in a decision tree – a <span class="s5">decision node </span>and a <span class="s5">leaf node</span>. A decision node is the <i>if-else </i>statement we mentioned previously. This node will have a condition based on which the data points that flow down the tree take the left or right <span class="s5">branch</span>. The decision node that sits right at the top has a special name – the <span class="s5">root node</span>. Finally, the process of dividing the data points based on a condition and directing it to the right or left branch is called <span class="s5">splitting</span>. Leaf nodes are nodes that don’t have any other branches below them. These are final resting points in the <i>beads flowing down a tree </i>analogy. These are the partitions we discussed earlier in this section.</p><p style="padding-top: 6pt;padding-left: 36pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Formally, we can define the function that’s been generated by a decision tree that has <i>M </i>partitions,</p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_360.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_361.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_362.png"/></span></p><p class="s142" style="padding-left: 43pt;text-indent: 0pt;line-height: 15pt;text-align: justify;">1<span class="s58">, </span>2<span class="s58">, … , </span><span class="s138">, as follows:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 215pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="8" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_363.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_364.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_365.png"/></span></p><p class="s131" style="padding-top: 5pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">̂   =  ∑     <span><img width="21" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_366.png"/></span>(    ϵ <span><img width="15" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_367.png"/></span>)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_368.png"/></span></p><p class="s106" style="padding-top: 4pt;padding-left: 165pt;text-indent: 0pt;text-align: center;">=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_369.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="14" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_370.png"/></span></p><p style="padding-top: 6pt;padding-left: 36pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Here, <i>x </i>is the input, is the constant response for the region, , and <i>I </i>is a function that is 1 if</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_371.png"/></span></p><p class="s58" style="padding-left: 46pt;text-indent: 0pt;line-height: 13pt;text-align: left;">ϵ <span><img width="15" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_372.png"/></span><span class="p">; otherwise, it’s 0.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_373.png"/></span></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark323">For regression trees, we usually adopt the squared loss as the loss function. In that case,   is usually set as the average of all </a><i>y</i>, where the corresponding <i>x </i>falls in the <span><img width="15" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_374.png"/></span><span class="s139"> </span>partition.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="117" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_375.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Many algorithms have been proposed over the years on how to create a decision tree from data such as ID3, C4.5, CART, and so on. <span class="s5">Classification and Regression Trees </span>(<span class="s5">CART</span>) is one of the most popular methods out of the lot and it supports regression as well. Therefore, we will just stick to CART in this book.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now that we know how a decision tree functions, the only thing left to understand is how to decide which feature to split on and where to split the feature.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_376.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_377.png"/></span></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The most optimal set of binary partitions that minimizes the sum of squares globally is generally intractable. So, we adopt a greedy algorithm to create the decision tree. Greedy optimization is a heuristic that builds up a solution stage by stage, selecting a local optimum at each stage. Therefore, instead of finding the best feature splits globally, we will create the decision tree, decision node by decision node, where we choose the most optimal feature split at each stage. For a regression tree, we choose a split feature, <i>f</i>, and split point, <i>s</i>, so that it creates two partitions, <span class="s95">1 </span>and <span class="s182">2</span>, that minimize, as follows:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_378.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_379.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_380.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_381.png"/></span></p><p class="s58" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">∑     (     −   <span class="s142">1</span>)2 +   ∑     (     − <span class="s142">2</span>)2</p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_382.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_383.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_384.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_385.png"/></span></p><p class="s106" style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">ϵ    <span class="s121">1                                                  </span>ϵ  <span class="s121">2</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_386.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_387.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_388.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_389.png"/></span></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, <span class="s141">1 </span>and <span class="s118">2 </span>is the average of all <i>y</i>, where the corresponding <i>x </i>falls in-between <span class="s133">1 </span>and <span class="s118">2</span>, respectively.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Therefore, by using this criterion, we can keep splitting the regions further and further. With each level we split, we increase the <span class="s5">depth </span>of the tree by one. But at some point, we will start overfitting the dataset. But if we don’t do enough splits, we might be underfitting the data as well. One strategy is to stop creating further splits when we reach a predetermined depth. In the scikit-learn implementation of <span class="s20">DecisionTreeRegressor</span>, this corresponds to the <span class="s20">max_depth </span>parameter. This is a hyperparameter that needs to be estimated using a validation dataset. There are other strategies to stop the splits, such as setting a minimum number of samples required to split (<span class="s20">min_samples_ split</span>), or a minimum decrease in cost to carry out a split (<span class="s20">min_impurity_decrease</span>). For a complete list of parameters in <span class="s20">DecisionTreeRegressor</span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html" class="s140" target="_blank">, please refer to the documentation at </a><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html" class="a" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.tree. </a><span class="s27">DecisionTreeRegressor.html</span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s see how we can use a decision tree and evaluate the fit on a sample household from our validation dataset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from sklearn.tree import DecisionTreeRegressor model_config = ModelConfig(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">model=DecisionTreeRegressor(max_depth=4, random_state=42), name=&quot;Decision Tree&quot;,</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># Decision Tree is not affected by normalization</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">normalize=False,</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;"># Decision Tree in scikit-learn does not handle missing values</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">fill_missing=True,</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">y_pred, metrics, feat_df = evaluate_model( model_config,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">feat_config, missing_value_config, train_features, train_target, test_features, test_target,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s take a look at the single-step forecast from <span class="s20">DecisionTreeRegressor</span>. It’s not doing as well as the linear or regularized linear regression models we have run so far:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span><img width="520" height="275" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_390.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.12 – Decision tree forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark324">For the linear models, some coefficients helped us understand how much each feature was important to the prediction function. In decision trees, we don’t have any coefficients, but the feature importance is still estimated using the mean decrease in the loss function, which is attributed to each feature in the tree construction process. This can be accessed in scikit-learn models by using the </a><span class="s20">feature_ importance_ </span>attribute of the trained model. Let’s take a look at this feature importance:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><span><img width="505" height="270" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_391.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.13 – Feature importance of a decision tree (top 15)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="182" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_392.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Best practice</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Although the default feature importance is a quick and easy way to check how the different features are used, due diligence should be applied before using them for any other purposes, such as feature selection or making business decisions. This way of assessing feature importance gives misleadingly high values for some continuous features and high cardinality categorical features. It is recommended to use permutation importance (<span class="s20">sklearn.inspection. permutation_importance</span>) for an easy but better assessment of feature importance. The <i>Further reading </i>contains some resources regarding the interpretability of models, which can be a good start to understanding what influences the models.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Here, we can see that the important features such as the lag and seasonal rolling features are coming up at the top.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="22" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_393.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="26" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_394.png"/></span></p><p class="s4" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140" name="bookmark325">We talked about overfitting and underfitting in </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">. These are also referred to high bias (underfitting) and high variance (overfitting) in machine learning parlance (the </span>Further reading <span class="p">section contains links if you wish to read up more about bias and variance and the trade-off between them). A decision tree is an algorithm that is highly prone to overfitting or high variance because, unlike the linear function, if given enough expressiveness, it can memorize the training dataset by partitioning the feature space. Another key disadvantage is a decision tree’s inability to extrapolate. Let’s consider a feature, </span>f<span class="p">, that linearly increases our target variable, </span>y<span class="p">. The training data we have has as the maximum value for </span>f <span class="p">and as the maximum value for</span><a name="bookmark301">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_395.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_396.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_397.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_398.png"/></span></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 88%;text-align: justify;"><i>y</i>. Since the decision tree partitions the feature space and assigns a constant value for that partition, even if we provide    <span class="s136">&gt;</span><span class="s111">          </span>, we will still only get a prediction of  <span class="s59">̂</span><span class="s131">   ≤           </span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at a model that uses decision trees, but in an ensemble and doesn’t overfit as much.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Random forest</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s5">Ensemble learning </span>is a process in which we use multiple models, or experts, and combine them in a way to solve the problem at hand. It taps into the <i>wisdom of the crowd </i>approach, which suggests that the decision-making of a group of people is typically better than any individual in that group. In the machine learning context, these individual models are called <span class="s5">base learners</span>. A single model may not perform well because it’s overfitting the dataset, but when we combine multiple such models, they can form a strong learner.</p><p class="s5" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Bagging <span class="p">is a form of ensemble learning where we use bootstrap sampling (sampling repeatedly with replacement from a population) to draw different subsets of the dataset, train weak learners on each of these subsets, and combine them by averaging or voting (for regression and classification, respectively). Bagging works best for high-variance, low-bias weak learners and the decision tree is a prime successful candidate with bagging. Theoretically, bagging maintains the same level of bias on the weak learners but reduces the variance, resulting in a better model. But if the weak learners are correlated with each other, the benefits of bagging will be limited.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="69" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_399.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The original research paper for Random Forest is cited in the <i>References </i>section as reference</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="text-indent: 0pt;text-align: left;">1<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In 2001, Leo Brieman proposed <span class="s5">Random forest</span>, which substantially modifies standard bagging by building a large collection of decorrelated trees. He proposed to alter the tree building procedure slightly to make sure all the trees that are grown on bootstrapped datasets are not correlated with each other.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark326">In the Random Forest algorithm, we decide how many trees to build. Let’s call that </a><i>M </i>trees. Now, for each tree, the following steps are repeated:</p><ol id="l66"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Draw a bootstrap sample from the training dataset.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Select <i>f </i>features at random from all the features.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Pick the best split just using <i>f </i>features and split the node into two child nodes.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Repeat <i>steps 2 </i>and <i>3 </i>until we hit any of the defined stopping criteria.</p></li></ol><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This set of <i>M </i>trees is the Random Forest. The key difference here from regular trees is the random sampling of features at each split, which increases randomness and reduces the correlation in the outputs of different trees. While predicting, we use each of these <i>M </i>trees to get a prediction. For regression problems, we average them, while for classification problems, we take the majority vote. The final prediction function that we learn from the Random Forest for regression is as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 227pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="8" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_400.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_401.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="20" height="23" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_402.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_403.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_404.png"/></span></p><p class="s131" style="padding-left: 173pt;text-indent: 0pt;text-align: center;">̂   =  <span class="s183">1</span>  ∑        (   )</p><p class="s106" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">=1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_405.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_406.png"/></span></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, <span class="s184">( ) </span>is the output of the <i>t</i>th tree in the Random Forest.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">All the hyperparameters that we have to control the complexity of the decision tree are applicable here as well (<span class="s20">RandomForestRegressor </span>from scikit-learn) and in addition to those, we have two other important parameters – the number of trees to build in the ensemble (<span class="s20">n_estimators</span>) and the number of features randomly chosen for each split (<span class="s20">max_features</span>).</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s see how we can use Random Forest and evaluate the fit on a sample household from our validation dataset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from sklearn.ensemble import RandomForestRegressor model_config = ModelConfig(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">model=RandomForestRegressor(random_state=42, max_depth=4), name=&quot;Random Forest&quot;,</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># RandomForest is not affected by normalization</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">normalize=False,</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;"># RandomForest in scikit-learn does not handle missing values</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">fill_missing=True,</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">y_pred, metrics, feat_df = evaluate_model( model_config,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">feat_config, missing_value_config,</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">train_features, train_target, test_features, test_target,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s take a look at this single-step forecast from <span class="s20">RandomForestRegressor</span>. It’s better than the decision tree, but it’s not as good as the linear models. However, we should keep in mind that we have not tuned the model and may be able to get better results by setting the right hyperparameters.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s take a look at the forecast that was generated using Random Forest:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span><img width="520" height="275" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_407.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.14 – Random Forest forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Just like the feature importance in decision trees, Random Forests also have a very similar mechanism for estimating the feature importance. Since we have a lot of trees in the Random Forest, we accumulate the decrease in split criterion across all the trees in the forest and arrive at a single feature of importance for the Random Forest. This can be accessed in scikit-learn models by using the <span class="s20">feature_importance_ </span>attribute of the trained model. Let’s take a look at this feature importance:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><a name="bookmark327"><span><img width="514" height="276" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_408.gif"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.15 – Feature importance of a decision tree (top 15)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, we can see that the feature importance is very much similar to decision trees. The same caveat about this kind of feature importance applies here as well. This is just a quick and dirty way of looking at what the model is using internally.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="199" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_409.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The scikit-learn implementation of Random Forest can get a bit slow for a large number of trees and data sizes. Instead, we can use the Random Forest implementation from <span class="s20">XGBoost</span>, called <span class="s20">XGBRFRegressor</span><a href="https://xgboost.readthedocs.io/en/latest/tutorials/rf.html" class="s140" target="_blank">, and very similar hyperparameters. In most cases, this is a drop-in replacement and gives almost the same results. The minor difference is due to small implementation details. We have used this variant as well in the notebooks. This variant is preferred going forward because of obvious runtime considerations. It also handles missing values natively and saves us from an additional preprocessing step. More details about the implementation and how to use it can be found here: </a><a href="https://xgboost.readthedocs.io/en/latest/tutorials/rf.html" class="a" target="_blank">https://xgboost.readthedocs. </a><span class="s27">io/en/latest/tutorials/rf.html</span>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Typically, Random Forest achieves good performance on many datasets with very little tuning, so Random Forests are a very popular option in machine learning. The fact that it is difficult to overfit with a Random Forest also increases their appeal. But since Random Forest uses decision trees as the weak learners, the inability of decision trees to extrapolate is passed down to Random Forest as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, let’s look at one last family of functions that is one of the most powerful learning methods and has proven exceedingly well in a wide variety of datasets – gradient boosting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark328">Gradient boosting decision trees</a><a name="bookmark302">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Boosting, like bagging, is another ensemble method that uses a few weak learners to produce a powerful committee of models. The key difference between bagging and boosting is in the way the weak learners are combined. Instead of building different models in parallel on bootstrapped datasets, as bagging does, boosting uses the weak learners in a sequential manner, with each weak learner applied to repeatedly modified versions of the data.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">To understand the additive function formulation, let’s consider this function:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_410.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_411.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_412.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="20" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_413.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_414.png"/></span></p><p class="s58" style="padding-top: 10pt;padding-left: 49pt;text-indent: 0pt;text-align: center;">(   ) = 25 +    2 +        ( )</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_415.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_416.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_417.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_418.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_419.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_420.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_421.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_422.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_423.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_424.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_425.png"/></span></p><p class="s187" style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="p">We can break this function into </span><span class="s185">1</span><span class="s186">( ) </span>= 25, <span class="s185">2</span><span class="s186">( ) </span>= <span class="s188">2</span>, <span class="s185">3</span><span class="s186">( ) </span>= <span class="s186">( ) </span><span class="p">and rewrite </span><span class="s173">( ) </span><span class="p">as follows:</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_426.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_427.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="14" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_428.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_429.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_430.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_431.png"/></span></p><p class="s107" style="padding-top: 10pt;padding-left: 49pt;text-indent: 0pt;text-align: center;">(   ) =   <span class="s101">1 </span>(   ) + <span><img width="9" height="14" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_432.png"/></span><span class="s95">2</span>(  ) + <span><img width="9" height="14" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_433.png"/></span><span class="s95">3</span>( )</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is the kind of additive ensemble function we are learning in boosting. Although, in theory, we can use any weak learner, decision trees are the de facto and most popular choice. So, let’s use decision trees to explore how gradient boosting works.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Earlier, when we were discussing decision trees, we saw that a decision tree that has <i>M </i>partitions,</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_434.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_435.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="14" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_436.png"/></span></p><p class="s96" style="padding-left: 41pt;text-indent: 0pt;line-height: 14pt;text-align: left;">1<span class="s69">, </span>2<span class="s69">, … , </span><span class="p">, is as follows:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 220pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="9" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_437.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_438.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_439.png"/></span></p><p class="s135" style="padding-top: 6pt;padding-left: 182pt;text-indent: 0pt;text-align: left;">(x) =  ∑    <span><img width="24" height="14" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_440.png"/></span>(    ϵ <span><img width="17" height="14" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_441.png"/></span>)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_442.png"/></span></p><p class="s62" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">=1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_443.png"/></span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Here, <i>x </i>is the input,    is the constant response for the region<span class="s58">, </span><span><img width="15" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_444.png"/></span><span class="s189"> </span>, and <i>I </i>is a function that is 1 if</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_445.png"/></span></p><p class="s120" style="padding-left: 46pt;text-indent: 0pt;line-height: 14pt;text-align: left;">ϵ<span class="s58"> </span><span><img width="15" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_446.png"/></span><span class="s138">;</span><span class="p"> otherwise, it is 0. A boosted decision tree model is a sum of such trees:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 227pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><span><img width="10" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_447.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_448.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_449.png"/></span></p><p class="s57" style="padding-top: 7pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">̂   = ∑    <span><img width="17" height="16" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_450.png"/></span>(  )</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_451.png"/></span></p><p class="s141" style="padding-top: 5pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Since finding the optimal partitions, <i>P</i>, and the constant value, <i>c</i>, for all the trees in the ensemble is a very difficult optimization problem, we usually adopt a suboptimal, stagewise solution where we optimize each step as we build the ensemble. In gradient boosting, we use the gradient of the loss to direct our optimization, hence the name.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_452.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_453.png"/></span></p><p class="s69" style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a name="bookmark329"><span class="p">Let the loss function we are using in the training be </span></a>ℒ(<span class="s111"> </span>̂<span class="s111">  ,   </span>)<span class="s111"> </span><span class="p">. Since we are looking at a stagewise</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_454.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_455.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_456.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_457.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_458.png"/></span></p><p class="s138" style="padding-left: 28pt;text-indent: 0pt;line-height: 14pt;text-align: left;">add<span class="p">itive functional form, we can replace  </span><span class="s190">̂</span><span class="s68">     </span>w<span class="p">ith      </span><span class="s99">−1  </span><span class="s191">+</span><span class="s68">      </span><span class="s191">(</span><span class="s68">  ) </span>,<span class="p"> where   </span><span class="s130">̂</span><span class="s111">    </span><span class="s99">−1  </span>i<span class="p">s the prediction of</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_459.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_460.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_461.png"/></span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">the sum of all trees until <i>k-1 </i>and <span class="s130">( ) </span>is the prediction of the tree at stage <i>k</i>. Let’s look at what the gradient boosting learning procedure for a training data with <i>N </i>samples is:</p><ol id="l67"><li><p style="padding-top: 7pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Initialize the model with a constant value by minimizing the loss function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 237pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="7" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_462.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_463.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_464.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_465.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_466.png"/></span></p><p class="s142" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">0(   )  <span class="s131">= arg min ∑    ℒ(    ,  </span>0<span class="s131">)</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_467.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_468.png"/></span></p><p class="s192" style="padding-top: 1pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">0              <span class="s106">=1</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_469.png"/></span></p><ul id="l68"><li><p class="s193" style="padding-top: 5pt;padding-left: 65pt;text-indent: -11pt;line-height: 94%;text-align: justify;">	<span class="s194">0</span><span class="s62"> </span><span class="p">is the prediction of the model that minimizes the loss function at the 0th iteration. At this iteration, we do not have any weak learners yet and this optimization is independent of any feature.</span></p></li><li><p style="padding-top: 5pt;padding-left: 65pt;text-indent: -11pt;text-align: justify;">For squared error loss, this works out to be the average of all training samples, while for the absolute error loss, it’s the median.</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Now that we have the initial solution, we can start the tree-building process. For <i>k=1 to M</i>, we must do the following:</p></li></ol><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_470.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="69" height="27" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_471.png"/></span></p><p class="s62" style="text-indent: 0pt;line-height: 9pt;text-align: left;">δℒ( ,</p><p class="s62" style="padding-top: 2pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">δ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s105" style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;">−1( )<span class="s61">)</span></p><p class="s105" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">−1( )</p><p style="text-indent: 0pt;text-align: left;"/><ol id="l69"><li><p style="padding-top: 8pt;padding-left: 79pt;text-indent: -24pt;text-align: left;">Compute <span class="s117">= − [ ] </span>for all the training samples:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_472.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_473.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_474.png"/></span></p><ul id="l70"><li><p style="padding-top: 8pt;padding-left: 73pt;text-indent: -11pt;text-align: left;"><span class="s193">	</span>is the derivative of the loss function with respect to <span class="s103">( ) </span>from the last iteration. It’s also called pseudo-residuals.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_475.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_476.png"/></span></p></li><li><p style="padding-top: 5pt;padding-left: 73pt;text-indent: -11pt;text-align: left;">For squared error loss, this is just the residual, ( <span class="s62">̂   −   )</span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_477.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_478.png"/></span></p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 79pt;text-indent: -24pt;text-align: left;">Build a regular regression tree to the values with  partitions or leaf nodes, <span><img width="23" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_479.png"/></span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 181pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span><img width="7" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_480.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_481.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_482.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_483.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_484.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_485.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="20" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_486.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_487.png"/></span></p></li><li><p class="s59" style="padding-top: 5pt;padding-left: 79pt;text-indent: -24pt;text-align: left;"><span class="p">Compute </span>ρ <span class="p">= </span>arg min ∑ ℒ ( , <span class="s131">−1( ) + </span>( ))<span class="s131"> </span><span class="p">:</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="2" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_488.png"/></span></p><p class="s99" style="padding-top: 2pt;padding-left: 93pt;text-indent: 0pt;text-align: center;">=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_489.png"/></span></p><ul id="l71"><li><p style="padding-left: 86pt;text-indent: -24pt;text-align: left;">is the scaling factor of the leaf or partition values for the current stage.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_490.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_491.png"/></span></p></li><li><p class="s184" style="padding-top: 1pt;padding-left: 84pt;text-indent: -22pt;text-align: left;">(<span class="s58"> ) </span><span class="p">is the function that was learned by the decision tree from the current stage.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_492.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_493.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_494.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_495.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="7" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_496.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_497.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_498.png"/></span></p><p style="padding-top: 8pt;padding-left: 56pt;text-indent: 0pt;text-align: left;">IV. Update <span class="s131">( ) = −1( ) + η × ρ × ( )</span>:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_499.png"/></span></p></li><li><p style="padding-top: 8pt;padding-left: 81pt;text-indent: -19pt;text-align: left;">is the shrinkage parameter or learning rate.</p></li></ul></li></ol><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Boosting, typically, is a high variance algorithm. This means that the chance of overfitting the training dataset is quite high and that enough measures need to be taken to make sure it doesn’t happen. There are many ways regularization and capacity constraining have been implemented in gradient-boosted trees. As always, all the key parameters that decision trees have to reduce capacity to fit the data are valid here because the weak learner is a decision tree. In addition to that, there are two other key <span class="s138">parameters – the number of trees, </span><i>M </i>(<span class="s20">n_estimators </span>in scikit-learn), and the learning rate, <span class="s68">η </span>(<span class="s20">learning_rate </span>in scikit-learn).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark330">When we apply a learning rate in the additive formulation, we are essentially shrinking each weak learner, thus reducing the effect of any one weak learner on the overall function. This was originally referred to as shrinkage, but now, in all the popular implementations of gradient-boosted trees, it is referred to as the learning rate. The number of trees and the learning rate are highly interdependent. For the same problem, we will need a greater number of trees if we reduce the learning rate. It has been empirically shown that a lower learning rate improves the generalization error. Therefore, a very effective and convenient way is to set the learning rate to a very low value (&lt;0.1) and a very high value for the number of trees (&gt;5,000) and train the gradient boosted tree with early stopping. Early stopping is when we use a validation dataset to monitor the out-of-sample performance while training the model. We stop adding more trees to the ensemble when the out-of-sample error stops reducing.</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another key technique a lot of the implementations adopt is subsampling. Subsampling can be done on rows and columns. Row subsampling is similar to bootstrapping, where each candidate in the ensemble is trained on a subsample of the dataset. Column subsampling is similar to random feature selection in Random Forest. Both these techniques introduce a regularization effect to the ensemble and help reduce generalization error. Some implementations of gradient boosted trees, such as <span class="s20">XGBoost </span>and <span class="s20">LightGBM</span>, implement L1 and L2 regularization directly in the objective function as well.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are many implementations of regression gradient-boosted trees. A few popular implementations are as follows:</p><ul id="l72"><li><p class="s20" style="padding-top: 9pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">GradientBoostingRegressor <span class="p">and </span>HistGradientBoostingRegressor <span class="p">in scikit-learn</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">XGBoost by T Chen</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">LightGBM from Microsoft</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">CatBoost from Yandex</p></li></ul><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Each of these implementations offer changes that range from subtle to very fundamental regarding the standard gradient boosting algorithm. We have included a few resources in the <i>Further reading </i>section so that you can read up on these differences and get acquainted with the different parameters they support.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_500.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The original research papers for XGBoost, LightGBM, and CatBoost are cited in the <i>Reference</i></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">section as <i>2</i>, <i>3</i>, and <i>4</i>, respectively.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">s</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">For our exercise, we are going to use LightGBM from Microsoft Research because it is one of the fastest and best-performing implementations. LightGBM and CatBoost also support categorical features out of the box and handle missing values natively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, let’s see how we can use LightGBM and evaluate the fit on a sample household from our validation dataset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from lightgbm import LGBMRegressor model_config = ModelConfig(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">model=LGBMRegressor(random_state=42), name=&quot;LightGBM&quot;,</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># LightGBM is not affected by normalization</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">normalize=False,</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># LightGBM handles missing values</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">fill_missing=False,</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">y_pred, metrics, feat_df = evaluate_model( model_config,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">feat_config, missing_value_config, train_features, train_target, test_features, test_target,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s take a look at the single-step forecast from <span class="s20">LGBMRegressor</span>. It’s already significantly better than all the other models we have tried so far:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: left;"><span><img width="444" height="235" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_501.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.16 – LightGBM forecast</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark331">Just like the feature importance in decision trees, gradient boosting implementations also have a very similar mechanism for estimating the feature importance. The feature importance for the ensemble is given by the average of split criteria reduction attributed to each feature in all the trees. This can be accessed in the scikit-learn API as the </a><span class="s20">feature_importance_ </span>attribute of the trained model. Let’s take a look at this feature importance:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 45pt;text-indent: 0pt;text-align: left;"><span><img width="508" height="271" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_502.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.17 – Feature importance of LightGBM (top 15)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are multiple ways of getting feature importance from the model, and each implementation has slightly different ways of calculating it. This is controlled by parameters. The most common ways of extracting it (sticking to LightGBM terminology) are <span class="s20">split </span>and <span class="s20">gain</span>. If we choose <span class="s20">split</span>, the feature importance is the number of times a feature is used to split nodes in the trees. On the other hand, <span class="s20">gain </span>is the total reduction in the split criterion. This can be attributed to any feature. <i>Figure</i></p><ol id="l73"><ol id="l74"><li><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">shows <span class="s20">split</span>, which is the default value in LightGBM. We can see that the order of the feature importance is very much similar to decision trees, or Random Forests with almost the same features taking the top three spots.</p><p class="s5" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Gradient boosted decision trees <span class="p">(</span>GBDTs<span class="p">) typically give us very good performance on tabular data and time series as regression is no exception. This very strong model has usually been part of almost all winning entries in Kaggle competitions on time series forecasting in the recent past. While it is one of the best machine learning model families, it still has a few disadvantages:</span></p><ul id="l75"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">GBDTs are high variance algorithms and hence prone to overfitting. This is why all kinds of regularization are applied in different ways in most of the successful implementations of GBDTs.</p></li></ul></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l76"><li><p class="s27" style="padding-top: 4pt;padding-bottom: 3pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a href="https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning" class="s140" target="_blank" name="bookmark332">GBDTs usually take longer to train (although many modern implementations have made this faster) and are not easily parallelizable as a Random Forest. In Random Forest, we can train all the trees in parallel because they are independent of each other. But in GBDTs, the sequential nature of the algorithm restricts parallelization. All the successful implementations have clever ways of enabling parallelization when creating a decision tree. LightGBM has many parallelization strategies, such as feature parallel, data parallel, and voting parallel. Details regarding these can be found at </a><a href="https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning" class="a" target="_blank">https://lightgbm.readthedocs.io/en/latest/Features. html#optimization-in-distributed-learning</a> <span class="p">and are worth understanding. The documentation of the library also contains a helpful guide in choosing between these parallelization strategies in a table:</span></p><p style="padding-left: 42pt;text-indent: 0pt;text-align: left;"><span><img width="495" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_503.gif"/></span></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Table 8.1 – Parallelization strategies in LightGBM</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 55pt;text-indent: -13pt;text-align: justify;">Extrapolation is a problem for GBDTs just like it is a problem for all tree-based models. There is some very weak potential for extrapolation in GBDTs, but nothing that solves the problem. Therefore, if your time series has some strong trends, tree-based methods will, most likely, fail to capture the trend. Either training the model on detrended data or switching to another model class would be the way forward. An easy way to do detrending would be to use <span class="s20">AutoStationaryTransformer</span><a href="#bookmark211" class="s140">, which we discussed in </a><a href="#bookmark211" class="s21">Chapter </a><i>6</i>, <i>Feature Engineering for Time Series Forecasting</i>.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">To summarize, let’s look at the metrics and runtime that were taken by these machine learning models. If you have run the notebook along with this chapter, then you will find the following summary table in there as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 90pt;text-indent: 0pt;text-align: left;"><span><img width="358" height="227" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_504.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.18 – Summary of the metrics and runtimes for a sample household</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark333">Right off the bat, we can see that all of the machine learning models we tried have performed better than the baselines in all metrics except the forecast bias. The three linear regression models perform well with almost equal performance on MAE, MASE, and MSE, with a slight increase in runtimes for regularized models. The decision tree has underperformed, but this is usually expected. Decision trees need to be tuned a little better to reduce overfitting. Random Forest (both the scikit-learn and </a><span class="s20">XGBoost </span>implementations) have improved the decision tree’s performance, which is what we would expect. One key thing to note here is that the <span class="s20">XGBoost </span>implementation of Random Forest is almost six times faster than the scikit-learn one. Finally, <span class="s20">LightGWM </span>has the best performance across all metrics and a faster runtime.<a name="bookmark303">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, this was just one household out of all the selected ones. To see how well these models are doing, we need to evaluate them on all selected households.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Training and predicting for multiple households</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have picked a few models (<span class="s20">LassoCV</span>, <span class="s20">XGBRFRegressor</span>, and <span class="s20">LGBMRegressor</span>) that are doing better in terms of metrics, as well as runtime, to run on all the selected households in our validation dataset. The process is straightforward: loop over all the unique combinations, inner loop over the different models to run, and then train, predict, and evaluate. The code is available in the <span class="s20">01-Forecasting with ML.ipynb </span>notebook in <span class="s20">chapter08</span>, under the <i>Running an ML Forecast For All Consumers </i>heading. You can run the code and take a break because this is going to take a little less than an hour. The notebook also calculates the metrics and contains a summary table that will be ready for you when you’re back. Let’s look at the summary now:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 113pt;text-indent: 0pt;text-align: left;"><span><img width="320" height="138" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_505.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.19 – Aggregate metrics on all the households in the validation dataset</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="101" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_506.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">We also need to run another notebook, called <span class="s20">01a-Forecasting with ML for Te Dataset.ipynb</span>, in <span class="s20">chapter08</span>. This notebook follows the same process, generates th forecast, and calculates the metrics on the test dataset.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 7pt;text-indent: 0pt;line-height: 11pt;text-align: left;">st</p><p style="text-indent: 0pt;line-height: 13pt;text-align: left;">e</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, we can see that even at the aggregated level, the different models we used perform as expected. The notebook also saves the predictions for the validation set on disk.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s47" style="padding-top: 4pt;padding-left: 225pt;text-indent: 0pt;text-align: left;"><a name="bookmark334">Training and predicting for multiple households 199</a><a name="bookmark304">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_507.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The aggregate metrics for the test dataset are as follows (from the notebook):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span><img width="325" height="145" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_508.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 8.20 – Aggregate metrics on all the households in the test dataset</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><a href="#bookmark211" class="s140">In </a>Chapter 6<span class="p">, </span>Feature Engineering for Time Series Forecasting<span class="p">, we used </span><span class="s20">AutoStationaryTransformer</span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">on all the households and saved the transformed dataset.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Using AutoStationaryTransformer</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The process is really similar to what we did earlier in this chapter, but with small changes. We read in the transformed targets and joined them to our regular dataset in such a way that the original target is named <span class="s20">energy_consumption </span>and the transformed target is named <span class="s20">energy_consumption_ auto_stat</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Reading the missing value imputed and train test split data</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">train_df = pd.read_parquet(preprocessed/&quot;block_0-7_train_ missing_imputed_feature_engg.parquet&quot;)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">auto_stat_target = pd.read_parquet(preprocessed/&quot;block_0-7_ train_auto_stat_target.parquet&quot;)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">transformer_pipelines = joblib.load(preprocessed/&quot;auto_ transformer_pipelines_train.pkl&quot;)</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Reading in validation as test</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">test_df = pd.read_parquet(preprocessed/&quot;block_0-7_val_missing_ imputed_feature_engg.parquet&quot;)</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Joining the transformed target</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">train_df = train_df.set_index([&#39;LCLid&#39;,&#39;timestamp&#39;]).join(auto_ stat_target).reset_index()</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">And while defining <span class="s20">FeatureConfig</span>, we used <span class="s20">energy_consumption_auto_stat </span>as <span class="s20">target</span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">and <span class="s20">energy_consumption </span>as <span class="s20">original_target</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="117" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_509.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Notebook check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">The <span class="s20">02-Forecasting with ML and Target Transformation.ipynb </span>an <span class="s20">02a-Forecasting with ML and Target Transformation for Tes Dataset.ipynb </span>notebooks use these transformed targets to generate the forecasts for th validation and test datasets, respectively.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: justify;">d <span class="s20">t </span>e</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark335">Let’s look at the summary metrics that were generated by these notebooks on the transformed data:</a><a name="bookmark305">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 91pt;text-indent: 0pt;text-align: left;"><span><img width="384" height="225" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_510.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 37pt;text-indent: 3pt;text-align: justify;">Figure 8.21 – Aggregate metrics on all the households with transformed targets in the validation dataset</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The target transformed models are not performing as well as the original ones. This might be because the dataset doesn’t have any strong trends.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Congratulations on making it through a very heavy and packed chapter full of theory as well as practice. We hope this has enhanced your understanding of machine learning and skills in applying these modern techniques to time series data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This was a very practical and hands-on chapter where we developed some standard code to train and evaluate multiple models. Then, we reviewed a few key machine learning models and how they work behind the hood. To complete and reinforce what we learned, we applied the machine learning models we learned about to the dataset and saw how well they did.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In the next chapter, we will start combining different forecasts into a single forecast and explore concepts such as combinatorial optimization and stacking to achieve state-of-the-art results.</p><p class="s47" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark336">References 201</a><a name="bookmark307">&zwnj;</a><a name="bookmark306">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_511.png"/></span></p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The following references were provided in this chapter:</p><ol id="l77"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Breiman, L. <i>Random Forests</i>, Machine Learning 45, 5–32 (2001): <span class="s27">https://doi. org/10.1023/A:1010933404324</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Chen, Tianqi and Guestrin, Carlos. (2016). <i>XGBoost: A Scalable Tree Boosting System</i><a href="https://doi.org/10.1145/2939672.2939785" class="s140" target="_blank">. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ‘16). Association for Computing Machinery, New York, NY, USA, 785–794: </a><a href="https://doi.org/10.1145/2939672.2939785" class="a" target="_blank">https:// </a><span class="s27">doi.org/10.1145/2939672.2939785</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Ke, Guolin et.al. (2017), <i>LightGBM: A Highly Efficient Gradient Boosting Decision Tree</i><a href="https://dl.acm.org/doi/pdf/10.5555/3294996.3295074" class="s140" target="_blank">. Advances in Neural Information Processing Systems, pages 3149-3157: </a><a href="https://dl.acm.org/doi/pdf/10.5555/3294996.3295074" class="a" target="_blank">https://dl.acm.org/doi/ </a><span class="s27">pdf/10.5555/3294996.3295074</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Prokhorenkova, Liudmila, Gusev, Gleb et al. (2018), <i>CatBoost: unbiased boosting with categorical features</i><a href="https://dl.acm.org/doi/abs/10.5555/3327757.3327770" class="s140" target="_blank">. Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS’18): </a><span class="s27">https://dl.acm.org/doi/abs/10.5555/3327757.3327770</span>.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">To learn more about the topics that were covered in this chapter, take a look at the following resources:</p></li><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">The difference between L1 and L2 regularization<a href="https://explained.ai/regularization/L1vsL2.html" class="s140" target="_blank">, by Terrence Parr: </a><a href="https://explained.ai/regularization/L1vsL2.html" class="a" target="_blank">https://explained. </a><a href="https://explained.ai/regularization/L1vsL2.html" target="_blank">ai/regularization/L1vsL2.html</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">L1 Norms versus L2 Norms<a href="https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms" class="s140" target="_blank">, by Aleksey Bilogur: </a><a href="https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms" class="a" target="_blank">https://www.kaggle.com/ </a><a href="https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms" target="_blank">residentmario/l1-norms-versus-l2-norms</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">Interpretability – Cracking Open the Black Box<a href="https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-ii/" class="s140" target="_blank">, by Manu Joseph: </a><a href="https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-ii/" class="a" target="_blank">https://deep-and- shallow.com/2019/11/13/interpretability-cracking-open-the-black- </a><a href="https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-ii/" target="_blank">box-part-ii/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">The Gradient Boosters – Part III: XGBoost<a href="https://deep-and-shallow.com/2020/02/12/the-gradient-boosters-iii-xgboost/" class="s140" target="_blank">, by Manu Joseph: </a><a href="https://deep-and-shallow.com/2020/02/12/the-gradient-boosters-iii-xgboost/" class="a" target="_blank">https://deep-and-shallow. </a><a href="https://deep-and-shallow.com/2020/02/12/the-gradient-boosters-iii-xgboost/" target="_blank">com/2020/02/12/the-gradient-boosters-iii-xgboost/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">The Gradient Boosters – Part IV: LightGBM<a href="https://deep-and-shallow.com/2020/02/21/the-gradient-boosters-iii-lightgbm/" class="s140" target="_blank">, by Manu Joseph: </a><a href="https://deep-and-shallow.com/2020/02/21/the-gradient-boosters-iii-lightgbm/" class="a" target="_blank">https://deep-and-shallow. </a><a href="https://deep-and-shallow.com/2020/02/21/the-gradient-boosters-iii-lightgbm/" target="_blank">com/2020/02/21/the-gradient-boosters-iii-lightgbm/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">The Gradient Boosters – Part V: CatBoost<a href="https://deep-and-shallow.com/2020/02/29/the-gradient-boosters-v-catboost/" class="s140" target="_blank">, by Manu Joseph: </a><a href="https://deep-and-shallow.com/2020/02/29/the-gradient-boosters-v-catboost/" class="a" target="_blank">https://deep-and-shallow. </a><a href="https://deep-and-shallow.com/2020/02/29/the-gradient-boosters-v-catboost/" target="_blank">com/2020/02/29/the-gradient-boosters-v-catboost/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">The Gradient Boosters – Part II: Regularized Greedy Forest<a href="https://deep-and-shallow.com/2020/02/09/the-gradient-boosters-ii-regularized-greedy-forest/" class="s140" target="_blank">, by Manu Joseph: </a><a href="https://deep-and-shallow.com/2020/02/09/the-gradient-boosters-ii-regularized-greedy-forest/" class="a" target="_blank">https://deep- and-shallow.com/2020/02/09/the-gradient-boosters-ii-regularized- </a><a href="https://deep-and-shallow.com/2020/02/09/the-gradient-boosters-ii-regularized-greedy-forest/" target="_blank">greedy-forest/</a></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;"><a href="https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html" class="s140" target="_blank">LightGBM Distributed Learning Guide: </a><a href="https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html" class="a" target="_blank">https://lightgbm.readthedocs.io/en/ </a><a href="https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html" target="_blank">latest/Parallel-Learning-Guide.html</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark337">9</a><a name="bookmark338">&zwnj;</a><a name="bookmark340">&zwnj;</a><a name="bookmark339">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">Ensembling and Stack ing </h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous chapter, we looked at a few machine learning algorithms and used them to generate forecasts on the London Smart Meters dataset. Now that we have multiple forecasts for all the households in the dataset, how do we come up with a single forecast by choosing or combining these different forecasts? That is what we will be doing in this chapter – we will learn how to leverage combinatorial and mathematical optimization to come up with a single forecast.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will cover the following topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Strategies for combining forecasts</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Stacking or blending</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You need to run the following notebooks for this chapter:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">02 - Preprocessing London Smart Meter Dataset.ipynb <span class="p">in </span>Chapter02</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">01-Setting up Experiment Harness.ipynb <span class="p">in </span>Chapter04</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">02-Baseline Forecasts using darts.ipynb <span class="p">in </span>Chapter04</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">01-Feature Engineering.ipynb <span class="p">in </span>Chapter06</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">02-Dealing with Non-Stationarity.ipynb <span class="p">in </span>Chapter07</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">02a-Dealing with Non-Stationarity-Train+Val.ipynb <span class="p">in </span>Chapter07</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">00-Single Step Backtesting Baselines.ipynb <span class="p">in </span>Chapter08</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l78"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;"><a name="bookmark352">01-Forecasting with ML.ipynb </a><span class="p">in </span>Chapter08<a name="bookmark341">&zwnj;</a></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">01a-Forecasting with ML for Test Dataset.ipynb <span class="p">in </span>Chapter08</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">02-Forecasting with Target Transformation.ipynb <span class="p">in </span>Chapter08</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">02a-Forecasting with Target Transformation(Test).ipynb <span class="p">in </span>Chapter08</p><p class="s27" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter09" class="s140" target="_blank">The code for this chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter09" class="a" target="_blank">https://github.com/PacktPublishing/Modern- </a>Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter09<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Combining forecasts</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have generated forecasts by using many techniques – some univariate, some machine learning, and so on. But at the end of the day, we would need a single forecast, and that means choosing a forecast or combining a variety. The most straightforward option is to choose the algorithm that does the best in the validation dataset, which in our case is LightGBM. We can think of this <i>selection </i>as another function that takes the forecasts that we generated as inputs and combines them into a final forecast. Mathematically, this can be represented as follows:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_512.png"/></span></p><p class="s107" style="padding-top: 6pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">= ℱ(<span><img width="11" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_513.png"/></span>, <span><img width="11" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_514.png"/></span>, … , <span><img width="14" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_515.png"/></span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="87" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_516.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the code, use the <span class="s20">01-Forecast Combinations.ipynb </span>noteboo in the <span class="s20">chapter09 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">k</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <span class="s131">ℱ </span>is the function that combines <i>N </i>forecasts. We can use the <span class="s58">ℱ </span>function to choose the best- performing model in the validation dataset. However, this function can be as complex as it wants to be, and choosing the right <span class="s131">ℱ </span>function while balancing bias and variance is a must.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We will start by loading all the forecasts (both the validation and test forecasts) and the corresponding metrics for all the forecasts we have generated so far and combining them into <span class="s20">pred_val_df </span>and <span class="s20">pred_test_df</span>. Now, we must reshape the DataFrame using <span class="s20">pd.pivot </span>to get it into the shape we want. Up to this point, we have been tracking multiple metrics. But to meet this objective, we will need to choose one. For this exercise, we are going to choose MAE as the metric. The validation metrics can be combined and reshaped into <span class="s20">metrics_combined_df</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark353"><span><img width="522" height="149" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_517.gif"/></span></a><a name="bookmark342">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 9.1 – Reshaped predictions DataFrame</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at some different strategies for combining the forecasts.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Best fit</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This strategy of choosing the best forecast is by far the most popular and is as simple as choosing the best forecast for each time series based on the validation metrics. This strategy has been made popular by many automated forecasting software, which calls this the “best fit” forecast. The algorithm is very simple:</p><ol id="l79"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Find the best-performing forecast for each time series using a validation dataset.</p></li><li><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 9pt;line-height: 162%;text-align: left;">For each time series, select the forecast from the same model for the test dataset. We can do this easily:</p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;"># Finding the lowest metric for each LCLid <span class="s28">best_alg = metrics_combined_df.idxmin(axis=1) </span>#Initialize two columns in the dataframe <span class="s28">pred_wide_test[&quot;best_fit&quot;] = np.nan pred_wide_test[&quot;best_fit_alg&quot;] = &quot;&quot;</span></p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">#For each LCL id</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">for lcl_id in tqdm(pred_wide_test.index.get_level_values(0). unique()):</p><p class="s38" style="padding-top: 2pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># pick the best algorithm</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">alg = best_alg[lcl_id]</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># and store the forecast in the best_fit column</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">pred_wide_test.loc[lcl_id, &quot;best_fit&quot;] = pred_wide_test. loc[lcl_id, alg].values</p><p class="s38" style="padding-top: 2pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># also store which model was chosen for traceability</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">pred_wide_test.loc[lcl_id, &quot;best_fit_alg&quot;] = alg</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark354">This will create a new column called </a><span class="s20">best_fit </span>with the forecasts that have been chosen according to the strategy we discussed. Now, we can evaluate this new forecast and get the metrics for the test dataset. The following table shows the best individual model (<span class="s20">LightGBM</span>) and the new strategy – <span class="s20">best_fit</span>:<a name="bookmark343">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 100pt;text-indent: 0pt;text-align: left;"><span><img width="361" height="82" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_518.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 9.2 – Aggregate metrics for the best fit strategy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, we can see that the best fit strategy is not performing as well as the best individual model overall, which is not what we expect when combining forecasts. One drawback is the fundamental assumption of this strategy – whatever model does best in the validation period also performs the best in the test period. Given the dynamic nature of time series, this is not always the best strategy. Another drawback of this approach is the instability of the final forecast. When we are using such a rule in a live environment, where we retrain and rerun the best fit every week, the forecast for any time series can jump back and forth between different forecast models, which can generate wildly different forecasts. Therefore, the final forecast shows a lot of week-over-week instability, which hampers the downstream actions we use these forecasts for.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Measures of central tendency</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Another prominent strategy is to use either an average or median to combine the forecasts. This is a function, <span class="s59">ℱ</span>, that is independent of validation metrics. This is both the appeal and angst of this method. It is impossible to overfit the validation metrics because we are not using them at all. But on the other hand, without any information from the validation metric, we may be including some very bad models, which pulls down the ensemble. However, empirically, this simple averaging of taking the median has proven to be a very strong combination method for forecast and is hard to outperform. Let’s see how this can be done:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># ensemble_forecasts is a list of column names(forecast) we want to combine</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">pred_wide_test[&quot;average_ensemble&quot;] = pred_wide_test[ensemble_ forecasts].mean(axis=1)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">pred_wide_test[&quot;median_ensemble&quot;] = pred_wide_test[ensemble_ forecasts].median(axis=1)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The preceding code will create two new columns called <span class="s20">average_ensemble </span>and <span class="s20">median_ensemble </span>with the combined forecasts. Now, we can evaluate this new forecast and get the metrics for the test dataset. The following table shows the best individual model (<span class="s20">LightGBM</span>) and the new strategies:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 73pt;text-indent: 0pt;text-align: left;"><a name="bookmark355"><span><img width="407" height="145" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_519.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 9.3 – Aggregate metrics for the mean and median strategies</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, we can see that neither the mean nor median strategy is working better than the best individual model overall. This can be because we are including methods such as Theta and FFT, which are performing considerably worse than the other machine learning methods. But since we are not taking any information from the validation dataset, we do not know this information. We can make an exception and say that we are going to use the validation metrics to choose which models we include in the average or median. But we have to be careful because now, we are moving closer to the assumption that what works in the validation period is going to work in the test period.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are a few manual techniques we can use here, such as <span class="s5">trimming </span>(discarding the worst-performing models in the ensemble) and <span class="s5">skimming </span>(selecting only the best few models in the ensemble). While effective, these are a bit subjective, and often, they become hard to use, especially when we have scores of models to choose from.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">If we think about this problem, it is essentially a combinatorial optimization problem where we have to select the best combination of models that optimizes our metric. If we consider the average for combining the different forecasts, mathematically, it can be thought of as follows:</p><p class="s75" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝑁𝑁</p><p class="s146" style="text-indent: 0pt;line-height: 10pt;text-align: left;">∑</p><p style="text-indent: 0pt;text-align: left;"/><p class="s146" style="padding-left: 12pt;text-indent: 0pt;line-height: 12pt;text-align: center;">𝑌𝑌<span class="s93">̂</span>  = arg min ℒ (      <span class="s195">1</span>       ∑ 𝑤𝑤  × 𝑌𝑌<span class="s93">̂</span>, 𝑌𝑌)</p><p style="padding-left: 214pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="40" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_520.png"/></span></p><p class="s75" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">𝑤𝑤</p><p class="s75" style="padding-left: 32pt;text-indent: 0pt;line-height: 6pt;text-align: left;">𝑁𝑁</p><p class="s75" style="padding-left: 32pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑖𝑖=1</p><p class="s146" style="text-indent: 0pt;line-height: 12pt;text-align: left;">𝑤𝑤<span class="s79">𝑖𝑖</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s75" style="text-indent: 0pt;text-align: left;">𝑖𝑖=1</p><p class="s75" style="padding-left: 6pt;text-indent: 0pt;line-height: 6pt;text-align: left;">𝑖𝑖 i</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Here, <span class="s136">ℒ </span>is the loss or metric that we are trying to minimize. In our case, we chose that to be the MAE.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_521.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_522.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_523.png"/></span></p><p style="padding-left: 28pt;text-indent: 14pt;text-align: left;"><span class="s141">ϵ [0,1] </span>is the binary weights of each of the base forecasts. Finally, is the set of <i>N </i>base forecasts and is the real observed values of the time series.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">But unlike pure optimization, where there is no concept of bias and variance, we need an optimal solution that can be generalized. Therefore, selecting the global minima in the training data is not advisable because in that case, we might be further overfitting the training dataset, increasing the variance of the resulting model. For this minimization, we typically use out-of-sample predictions, which in this case can be the forecast during the validation period.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark356">The most straightforward solution is to find </a><i>w</i>, which minimizes this function on validation data. But there are two problems with this approach:<a name="bookmark344">&zwnj;</a></p><ul id="l80"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">The possible candidates (different combinations of the base forecasts) increase exponentially as we increase the number of base forecasts, <i>N</i>. This becomes computationally intractable very soon.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Selecting the global minima in the validation period may not be the best strategy because of overfitting the validation period.</p></li></ul></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s take a look at a few heuristics-based solutions to this combinatorial optimization problem.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Simple hill climbing</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We briefly talked about greedy algorithms while discussing decision trees, as well as gradient-boosted trees. Greedy optimization is a heuristic that builds up a solution stage by stage, selecting a local optimum at each stage. In both these machine learning models, we adopt a greedy, stagewise approach to finding the solution to a computationally infeasible optimization problem. To select the best subset that gives us the best combination of forecasts, we can employ a simple greedy algorithm called hill climbing. If we consider the objective function surface as a hill, to find the maxima, we would need to climb the hill. As its name suggests, hill climbing ascends the hill, one step at a time, and in each of those steps, it takes the best possible path, which increases the objective function. Let’s see how the algorithm works.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_524.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_525.png"/></span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, is a set of candidates (base forecasts) and is the objective we want to minimize. The algorithm for the simple hill-climb is as follows:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="29" height="24" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_526.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_527.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="31" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_528.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="29" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_529.png"/></span></p><ol id="l81"><li><p style="padding-top: 9pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Initialize a starting solution, , as the candidate that gives the minimum value in , , and remove from .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_530.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">While the length of &gt; 0, do the following:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_531.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_532.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="46" height="28" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_533.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="56" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_534.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_535.png"/></span></p><ol id="l82"><li><p style="padding-top: 8pt;padding-left: 88pt;text-indent: -24pt;text-align: justify;">Evaluate all members of   by averaging the base forecasts in       with each element in   and select the best member (   ) that was added to      to minimize the objective function,   ( <span><img width="66" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_536.png"/></span><span class="s139"> </span>).</p><p style="text-indent: 0pt;text-align: left;"><span><img width="58" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_537.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_538.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 88pt;text-indent: -24pt;text-align: justify;">If <span class="s131">&gt; , </span>then do the following:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_539.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_540.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="51" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_541.png"/></span></p><p style="padding-top: 8pt;padding-left: 73pt;text-indent: 0pt;text-align: left;">i. <span class="s136">= ∪ </span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_542.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="60" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_543.png"/></span></p><p style="padding-top: 1pt;padding-left: 73pt;text-indent: 0pt;text-align: left;">ii. <span class="s120">= </span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_544.png"/></span></p><p style="padding-top: 3pt;padding-left: 73pt;text-indent: 0pt;text-align: left;">iii.  Remove  <span><img width="65" height="14" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_545.png"/></span><span class="s139"> </span>from .</p></li><li><p style="padding-top: 7pt;padding-left: 88pt;text-indent: -24pt;text-align: justify;">Otherwise, exit.</p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_546.png"/></span></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">At the end of the run, we have , which is the best combination of forecasts we got through greedy optimization. We have made an implementation of this available in <span class="s20">src.forecasting. ensembling.py </span>under the <span class="s20">greedy_optimization </span>function. The parameters for this function are as follows:</p></li></ul></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: left;">objective<span class="p">: This is a callable that takes in a list of strings as the candidates and returns a</span></p><p class="s20" style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: left;">float <span class="p">objective value.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">candidates<span class="p">: This is a list of candidates to be included in the optimization.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">verbose<span class="p">: A flag that specifies whether progress is printed or not.</span></p></li></ul><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The function returns a tuple of the best solution as a list of strings and the best score that was obtained through optimization.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s see how we can use this in our example:</p><ol id="l83"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Import all the required libraries/functions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"># Used to partially construct a function call</p><p class="s28" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">from functools import partial</p><p class="s38" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;line-height: 106%;text-align: left;"># calculate_performance is a custom method we defined to calculate the MAE provided a list of candidates and prediction dataframe</p><p class="s28" style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 106%;text-align: left;">from src.forecasting.ensembling import calculate_ performance, greedy_optimization</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Define the objective function and run greedy optimization:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 106%;text-align: left;"># We partially construct the function call by passing the necessary parameters</p><p class="s28" style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">objective = partial(</p><p class="s28" style="padding-top: 3pt;padding-left: 13pt;text-indent: 24pt;line-height: 106%;text-align: left;">calculate_performance, pred_wide=pred_wide_val, target=&quot;energy_consumption&quot;</p><p class="s28" style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">)</p><p class="s38" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"># ensemble forecasts is the list of candidates</p><p class="s28" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;line-height: 106%;text-align: left;">solution, best_score = greedy_optimization(objective, ensemble_forecasts)</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Once we have the best solution, we can create the combination forecast in the test DataFrame:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 106%;text-align: left;">pred_wide_test[&quot;greedy_ensemble&quot;] = pred_wide_ test[solution].mean(axis=1)</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark357">Once we run this code, we will have the combination forecast under the name </a><span class="s20">greedy_ensemble </span>in our prediction DataFrame. The candidates that are part of the optimal solution are <span class="s20">LightGBM</span>, <span class="s20">Lasso Regression</span>, and <span class="s20">LightGBM_auto_stat</span>. Now, let’s evaluate the results and look at the aggregated metrics:<a name="bookmark345">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="390" height="170" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_547.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 9.4 – Aggregate metrics for a simple hill climbing-based ensemble</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">As we can see, the simple hill-climb is performing better than any individual models or any other ensemble techniques we have seen so far. This greedy approach seems to be working well in this case. Now, let’s understand a few limitations of hill climbing, as follows:</p><ul id="l84"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Runtime considerations<span class="p">: Since a simple hill-climb requires us to evaluate all the candidates at any step, this can cause a bottleneck in terms of runtime. If the number of candidates is large, this approach can take longer to finish.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Short-sightedness<span class="p">: Hill climbing optimization is short-sighted. During optimization, it always picks the best in each step. Sometimes, by choosing a slightly worse solution in a step, we may get to a better overall solution.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Forward-only<span class="p">: Hill climbing is a forward-only algorithm. Once a candidate has been admitted into the solution, we can’t go back and remove it.</span></p></li></ul></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The greedy approach may not always get us the best solution, especially when there are scores of models to combine. So, let’s look at a small variation of hill climbing that tries to get over some of the limitations of the greedy approach.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Stochastic hill climbing</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The key difference between simple hill climbing and stochastic hill climbing is in the evaluation of candidates. In a simple hill-climb, we <i>evaluate all possible options </i>and pick the best among them. However, in a stochastic hill-climb, we <i>randomly pick a candidate </i>and add it to the solution if it is better than the current solution. This addition of stochasticity helps the optimization not get the local maxima/ minima. Let’s take a look at the algorithm.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_548.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_549.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_550.png"/></span></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, is a set of candidates (base forecasts), is the objective we want to minimize, and is the maximum number of iterations we want to run the optimization for. The algorithm for stochastic hill climbing is as follows:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_551.png"/></span></p><ol id="l85"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Initialize a starting solution, , as the candidate. This can be done by picking a candidate at random or choosing the best-performing model.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_552.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_553.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="29" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_554.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_555.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_556.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Set the value of the objective function for , , as , and remove from .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_557.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Repeat this for iterations:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_558.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_559.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="35" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_560.png"/></span></p><ol id="l86"><li><p style="padding-top: 8pt;padding-left: 79pt;text-indent: -24pt;text-align: left;">Draw a random sample from , add it to , and store it as .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="35" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_561.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_562.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="35" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_563.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 79pt;text-indent: -24pt;text-align: left;">Evaluate on the objective function, , and store it as .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="35" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_564.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_565.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_566.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_567.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="34" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_568.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_569.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="35" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_570.png"/></span></p></li><li><p style="padding-top: 1pt;padding-left: 79pt;text-indent: -24pt;text-align: left;">If <span class="s120">&gt; </span>, then do the following:</p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:62.3504pt" cellspacing="0"><tr style="height:16pt"><td style="width:66pt"><p class="s196" style="padding-left: 2pt;text-indent: 0pt;line-height: 14pt;text-align: left;">i. <span class="s197">=</span></p></td><td style="width:37pt"><p class="s198" style="text-indent: 0pt;line-height: 11pt;text-align: center;">∪</p></td><td style="width:11pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:9pt"><p class="s196" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">.</p></td></tr><tr style="height:33pt"><td style="width:66pt"><p class="s196" style="padding-left: 2pt;text-indent: 0pt;line-height: 15pt;text-align: left;">ii. <span class="s197">=</span></p><p class="s196" style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">iii. Remove</p></td><td style="width:37pt"><p class="s196" style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;text-align: center;">.</p><p class="s196" style="padding-top: 3pt;padding-left: 10pt;padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: center;">from</p></td><td style="width:11pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s196" style="text-indent: 0pt;line-height: 13pt;text-align: center;">.</p></td><td style="width:9pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_571.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_572.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_573.png"/></span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">At the end of the run, we have , which is the best combination of forecasts we got through stochastic hill climbing. We have made an implementation of this available in <span class="s20">src.forecasting. ensembling.py </span>under the <span class="s20">stochastic_hillclimbing </span>function. The parameters for this function are as follows:</p><ul id="l87"><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: left;">objective<span class="p">: This is a callable that takes in a list of strings as the candidates and returns a</span></p><p class="s20" style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: left;">float <span class="p">objective value.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">candidates<span class="p">: This is a list of candidates to be included in the optimization.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">n_iterations<span class="p">: The number of iterations to run the hill-climb for. If this is not given, a heuristic (twice the number of candidates) is used to set this.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: left;">init<span class="p">: This determines the strategy to be used for the initial solution. This can be </span>random</p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: left;">or <span class="s20">best</span>.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">verbose<span class="p">: A flag that specifies whether progress is printed or not.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">random_state<span class="p">: A seed that gets repeatable results.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The function returns a tuple of the best solution as a list of strings and the best score obtained through optimization.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This can be used in a very similar fashion to <span class="s20">greedy_optimization</span>. We will only show the different parts here. The full code is available in the notebook:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">from src.forecasting.ensembling import stochastic_hillclimbing</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># ensemble forecasts is the list of candidates</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 1pt;padding-left: 33pt;text-indent: -24pt;line-height: 15pt;text-align: left;">solution, best_score = stochastic_hillclimbing( objective, ensemble_forecasts, n_iterations=10,</p><p class="s28" style="padding-top: 1pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">init=&quot;best&quot;, random_state=9</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark358"/><a name="bookmark346">&zwnj;</a></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Once we run this code, we will have the combination forecast called <span class="s20">stochastic_hillclimb ensemble </span>in our prediction DataFrame. The candidates that are part of the optimal solution are <span class="s20">LightGBM</span>, <span class="s20">Lasso Regression_auto_stat</span>, <span class="s20">LightGBM_auto_stat</span>, and <span class="s20">Lasso Regression</span>. Now, let’s evaluate the results and look at the aggregated metrics:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: left;"><span><img width="432" height="188" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_574.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 9.5 – Aggregate metrics for a stochastic hill climbing-based ensemble</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The stochastic hill-climb is not doing better than the greedy approach but is better than the mean, median, and best fit ensembles. We discussed three disadvantages of simple hill climbing earlier – runtime considerations, short-sightedness, and forward-only. Stochastic hill climbing solves the runtime consideration because we are not evaluating all the combinations and selecting the best. Instead, we are randomly evaluating the combinations and adding them to the ensemble as soon as we see a solution that performs better. It partly solves the short-sightedness purely because the randomness in the algorithm may end up choosing a sub-optimal solution for each stage. But it still only chooses solutions that are better than the current solution.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at another modification of hill climbing that handles this issue as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Simulated annealing</p><p class="s5" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Simulated annealing <span class="p">is a modification of hill climbing that is inspired by a physical phenomenon – annealing solids. Annealing is the process of heating a solid to a predetermined temperature (usually above its melting point), holding it for a while, and then slowly cooling it. This is done to ensure that the atoms assume a new globally minimum energy state, which induces desirable properties to some metals, such as iron.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In 1952, Metropolis proposed simulated annealing as an optimization technique. The annealing analogy applies to the optimization context as well. When we say we heat the system, we encourage random perturbations. So, when we start an optimization with a high temperature, the algorithm explores the space and comes up with an initial structure of the problem. And as we reduce the temperature, the structure is refined to arrive at a final solution. This technique helps us avoid getting stuck in any local optima. Local optima are extrema in the objective function surface that are better than other values nearby but may not be the absolute best solution possible. The <i>Further reading </i>section contains a resource that explains what local and global optima are in a concise language.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at the algorithm.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_575.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_576.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_577.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="26" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_578.png"/></span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 86%;text-align: justify;">Here, is a set of candidates (base forecasts), is the objective we want to minimize, is the maximum number of iterations we want to run the optimization for, is the maximum temperature, and <span class="s199">α </span>is the temperature decay. The algorithm for simulated annealing is as follows:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="31" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_579.png"/></span></p><ol id="l88"><li><p style="padding-top: 9pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Initialize a starting solution, , as the candidate. This can be done by picking a candidate at random or choosing the best-performing model.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="32" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_580.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_581.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="34" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_582.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="33" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_583.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_584.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Set the value of the objective function for , , as , and remove from .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_585.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Set the current temperature, <i>t</i>, as .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_586.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Repeat this for iterations:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_587.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="32" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_588.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="50" height="32" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_589.png"/></span></p><ol id="l89"><li><p style="padding-top: 8pt;padding-left: 79pt;text-indent: -24pt;text-align: left;">Draw a random sample from , add it to , and store it as .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="35" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_590.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_591.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 79pt;text-indent: -24pt;text-align: left;">Evaluate on the objective function, , and store it as .</p><p style="text-indent: 0pt;text-align: left;"><span><img width="31" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_592.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_593.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_594.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="30" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_595.png"/></span></p></li><li><p style="padding-top: 2pt;padding-left: 64pt;text-indent: -8pt;line-height: 146%;text-align: left;">If  <span><img width="38" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_596.png"/></span><span class="s139"> </span><span class="s135">&gt; </span>, then do the following: i. <span class="s130">= ∪</span><span class="s111"> </span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_597.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="31" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_598.png"/></span></p><p style="padding-left: 64pt;text-indent: 0pt;line-height: 11pt;text-align: left;">ii. <span class="s130">= </span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_599.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_600.png"/></span></p><p style="padding-top: 3pt;padding-left: 64pt;text-indent: 0pt;text-align: left;">iii. Remove from .</p></li><li><p style="padding-top: 8pt;padding-left: 79pt;text-indent: -24pt;text-align: left;">Otherwise, do the following:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_601.png"/></span></p><ol id="l90"><li><p style="padding-top: 5pt;padding-left: 81pt;text-indent: -17pt;text-align: left;">Calculate the acceptance probability,  <span class="s111">= </span><span><img width="68" height="18" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_602.png"/></span>.</p></li><li><p style="padding-top: 3pt;padding-left: 81pt;text-indent: -17pt;text-align: left;">Draw a random sample between 0 and 1, as <i>p</i>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_603.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="5" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_604.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_605.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="25" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_606.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="31" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_607.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 81pt;text-indent: -17pt;line-height: 119%;text-align: left;">If <span class="s61">&lt; </span>, then do the following: i. <span class="s191">= ∪</span><span class="s68"> </span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_608.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="33" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_609.png"/></span></p><p style="padding-left: 81pt;text-indent: 0pt;text-align: left;">ii. <span class="s59">= </span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="24" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_610.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_611.png"/></span></p><p style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">iii. Remove from .</p></li></ol></li><li><p style="padding-top: 6pt;padding-left: 79pt;text-indent: -24pt;text-align: left;"><i>t </i>= <i>t </i>- <span class="s200">α</span><span class="s107"> </span>(for linear decay) and <i>t</i>=<i>t</i>/<span class="s201">α</span><span class="s58"> </span>(for geometric decay).</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_612.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 79pt;text-indent: -24pt;text-align: left;">Exit when is empty.</p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="28" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_613.png"/></span></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">At the end of the run, we have , which is the best combination of forecasts we got through simulated annealing. We have provided an implementation of this in <span class="s20">src.forecasting.ensembling. py </span>under the <span class="s20">simulated_annealing </span>function. Setting the temperature to the right value is key for the algorithm to work well and is typically the hardest hyperparameter to set. More intuitively, we can think of temperature in terms of the probability of accepting a worse solution in the beginning. In the implementation, we have also made it possible to input the starting and ending probability of accepting a worse solution.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_614.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The research paper by D.S. Johnson, titled <i>Optimization by Simulated Annealing: An Experiment Evaluation; Part I, Graph Partitioning</i>, is cited in the <i>References </i>section as reference <i>1</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="text-indent: 0pt;text-align: left;">al</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In 1989, D.S. Johnson et al. proposed a procedure for estimating the temperature range from the given probability range. This has been implemented in <span class="s20">initialize_temperature_range</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">The parameters for the <span class="s20">simulated_annealing </span>function are as follows:</p><ul id="l91"><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">objective<span class="p">: This is a callable that takes in a list of strings as the candidates and returns a</span></p><p class="s20" style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">float <span class="p">objective value.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">candidates<span class="p">: This is a list of candidates to be included in the optimization.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">n_iterations<span class="p">: The number of iterations to run simulated annealing for. This is a mandatory parameter.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">p_range<span class="p">: The starting and ending probabilities as a tuple. This is the probability with which a worse solution is accepted in simulated annealing. The temperature range (</span>t_range<span class="p">) is inferred from </span>p_range <span class="p">during optimization.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">t_range<span class="p">: We can use this if we want to directly set the temperature range as a tuple (start, end). If this is set, </span>p_range <span class="p">is ignored.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">init<span class="p">: This determines the strategy that’s used for the initial solution. This can be </span>random</p><p style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: left;">or <span class="s20">best</span>.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">temperature_decay<span class="p">: This specifies how to decay the temperature. It can be </span>linear <span class="p">or</span></p><p class="s20" style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: left;">geometric<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">verbose<span class="p">: A flag that specifies whether progress is printed or not.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">random_state<span class="p">: The seed for getting repeatable results.</span></p></li></ul><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">The function returns a tuple of the best solution as a list of strings and the best score that was obtained through optimization.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark359">This can be used in a very similar fashion to the other ways of combining forecasts. We will show just the part that is different here. The full code is available in the notebook:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">from src.forecasting.ensembling import simulated_annealing</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># ensemble forecasts is the list of candidates</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">solution, best_score = simulated_annealing( objective,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">ensemble_forecasts, p_range=(0.5, 0.0001), n_iterations=50, init=&quot;best&quot;,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">temperature_decay=&quot;geometric&quot;, random_state=42,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Once we run this code, we will have a combination forecast called <span class="s20">simulated_annealing_ ensemble </span>in our prediction DataFrame. The candidates that are part of the optimal solution are <span class="s20">LightGBM</span>, <span class="s20">Lasso Regression_auto_stat</span>, <span class="s20">LightGBM_auto_stat</span>, and <span class="s20">XGB Random Forest</span>. Let’s evaluate the results and look at the aggregated metrics:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 84pt;text-indent: 0pt;text-align: left;"><span><img width="380" height="189" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_615.jpg"/></span></p><p class="s37" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 9.6 – Aggregate metrics for a simulated annealing-based ensemble</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Simulated annealing seems to be doing better than stochastic hill climbing. We discussed three disadvantages of simple hill climbing earlier – runtime considerations, short-sightedness, and forward-only. Simulated annealing solves the runtime consideration because we are not evaluating all the combinations and selecting the best. Instead, we are randomly evaluating the combinations and adding them to the ensemble as soon as we see a solution that performs better. It also solves the short-sightedness problem because, by using temperature, we are also accepting solutions that are slightly worse toward the beginning of the optimization. However, it is still a forward-only procedure.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="17" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_616.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="17" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_617.png"/></span></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><a name="bookmark360">So far, we have looked at combinatorial optimization because we said </a><span class="s131">ϵ [0,1]</span>. But if we can relax <span class="s138">this constraint and make </span><span class="s131">ϵ ℝ </span><span class="s138">(real numbers), the combinatorial optimization problem can be </span>relaxed to a general mathematical optimization problem. Let’s see how we can do that.<a name="bookmark347">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Optimal weighted ensemble</p><p style="text-indent: 0pt;text-align: left;"><span><img width="79" height="41" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_618.png"/></span></p><p class="s111" style="text-indent: 0pt;line-height: 10pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"/><p class="s111" style="text-indent: 0pt;line-height: 10pt;text-align: left;">∑</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_619.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="14" height="37" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_620.png"/></span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Previously, we defined the optimization problem we are trying to solve as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s202" style="padding-top: 3pt;padding-bottom: 2pt;text-indent: 0pt;text-align: right;"><span><img width="7" height="11" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_621.png"/></span> <span class="s111">= arg min ℒ</span></p><p style="padding-left: 191pt;text-indent: 0pt;line-height: 3pt;text-align: left;"><span><img width="7" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_622.png"/></span></p><p class="s111" style="padding-top: 3pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">× <span class="s203">i</span>,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_623.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_624.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="17" height="10" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_625.png"/></span></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Here, <span class="s136">ℒ</span><span class="s111"> </span>is the loss or metric that we are trying to minimize. In our case, we chose that to be the MAE. <span><img width="12" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_626.png"/></span><span class="s139"> </span>is the set of <i>N </i>base forecasts while is the real observed values of the time series. Instead of defining <span class="s103">ϵ</span><span class="s68"> </span><span class="s103">[0,1]</span>, let’s make <span class="s59">ϵ</span><span class="s131"> ℝ</span>, the continuous weights of each of the base forecasts. With this new relaxation, the combination becomes a weighted average between the different base forecasts. Now, we are looking at a soft mixing of the different forecasts as opposed to the hard-choice-based combinatorial optimization (which was what we had been using up until this point).</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is an optimization problem that can be solved using off-the-shelf algorithms from <span class="s20">scipy</span>. Let’s see how we can use <span class="s20">scipy.optimize </span>to solve this problem.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">First, we need to define a loss function that takes in a set of weights as a list and returns the metric we need to optimize:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">def loss_function(weights):</p><p class="s38" style="padding-top: 3pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"># Calculating the weighted average</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 48pt;line-height: 106%;text-align: left;">fc = np.sum(pred_wide[candidates].values * np.array(weights), axis=1)</p><p class="s38" style="padding-top: 2pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"># Using any metric function to calculate the metric</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">return metric_fn(pred_wide[target].values, fc)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, all we need to do is call <span class="s20">scipy.optimize </span>with the necessary parameters. Let’s learn how to do this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from scipy import optimize opt_weights = optimize.minimize(</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 11pt;text-align: left;">loss_function,</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 47pt;line-height: 106%;text-align: left;"># set x0 as initial values, which is a uniform distribution over all the candidates</p><p class="s28" style="padding-top: 2pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">x0=[1 / len(candidates)] * len(candidates),</p><p class="s38" style="padding-top: 3pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"># Set the constraint so that the weights sum to one</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">constraints=({&quot;type&quot;: &quot;eq&quot;, &quot;fun&quot;: lambda w: 1 -</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">sum(w)}),</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 47pt;line-height: 106%;text-align: left;"># Choose the optimization technique. Should be gradient-free and bounded.</p><p class="s28" style="padding-top: 2pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">method=&quot;SLSQP&quot;,</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 47pt;line-height: 106%;text-align: left;"># Set the lower and upper bound as a tuple for each element in the candidate list.</p><p class="s38" style="padding-top: 2pt;padding-left: 57pt;text-indent: 0pt;text-align: left;"># We set the maximum values between 1 and 0</p><p class="s28" style="padding-top: 3pt;padding-left: 56pt;text-indent: 0pt;line-height: 131%;text-align: left;">bounds=[(0.0, 1.0)] * len(candidates), <span class="s38"># Set the tolerance for termination </span>options={&quot;ftol&quot;: 1e-10},</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)[&quot;x&quot;]</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The optimization is usually fast and we will get the weights as a list of floating-point numbers. We have wrapped this in a function in <span class="s20">src.forecasting.ensembling.py </span>called under the <span class="s20">find_optimal_combination </span>function. The parameters for this function are as follows:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">candidates<span class="p">: This is a list of candidates to be included in the optimization. They are returning in the same order in which the returned weights would.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">pred_wide<span class="p">: This is the prediction DataFrame on which we need to learn the weights.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">target<span class="p">: This is the column name of the target.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">metric_fn<span class="p">: This is any callable with a </span>metric(actuals, pred) <span class="p">signature.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The function returns the optimal weights as a list of floating-point numbers. Let’s see what the optimal weights are when we learned them through our validation forecast:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 144pt;text-indent: 0pt;text-align: left;"><span><img width="217" height="229" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_627.gif"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 9.7 – The optimal weights that were learned through optimization</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark361">Here, we can see that the optimization automatically learned to ignore </a><span class="s20">FFT</span>, <span class="s20">Theta</span>, <span class="s20">XGB Random Forest</span>, and <span class="s20">XGB Random Forest_auto_stat </span>because they didn’t add much value to the ensemble. It has also learned some non-zero weights for each of the forecasts. The weights already resemble the selection we made using the techniques we discussed previously. Now, we can use these weights to come up with a weighted average and call it <span class="s20">optimal_combination_ensemble</span>. The aggregated results should be as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 93pt;text-indent: 0pt;text-align: left;"><span><img width="372" height="209" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_628.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 9.8 – Aggregate metrics for the optimal combination-based ensemble</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="198" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_629.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">In all the techniques we discussed, we were using MAE as the objective function. But we can use any metric, a combination of metrics, or even metrics with regularization as the objective function. When we discussed Random Forest, we talked about how decorrelated trees were essential to getting better performance. A very similar principle applies while choosing ensembles as well. Having decorrelated base forecasts adds value to the ensemble. So, we can use any measure of variety to regularize our metric as well. For instance, we can use correlation as a measure and create a regularized metric to be used in these techniques. The <span class="s20">01-Forecast Combinations.ipynb </span>notebook in the <span class="s20">chapter09 </span>folder contains a bonus section that shows how to do that.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, we can see that this soft mixing of the forecasts is doing much better than all of the hard-choice- based ensembles on all three metrics.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">We started by discussing combining forecasts with a mathematical formulation:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="9" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_630.png"/></span></p><p class="s68" style="padding-top: 8pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">= ℱ(<span><img width="10" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_631.png"/></span>, <span><img width="10" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_632.png"/></span>, … , <span><img width="12" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_633.png"/></span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here, <span class="s131">ℱ </span>is the function that combines the <i>N </i>forecasts.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><a name="bookmark362">We did all this while looking at ways to come up with this function as an optimization problem, using something such as a mean or median to combine the metrics. But we have also seen another way to learn this function, </a><span class="s58">ℱ</span>, from data, haven’t we? Let’s see how that can be done.<a name="bookmark348">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Stacking or blending</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We started this chapter by talking about machine learning algorithms, which learn a function from a set of inputs and outputs. While using those machine learning algorithms, we learned about the functions that forecast our time series, which we&#39;ll call base forecasts now. Why not use the same machine learning paradigm to learn this new function, <span class="s68">ℱ</span>, that we are trying to learn as well?</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This is exactly what we do in stacking (often called stacked generalization), where we train another learning algorithm on the predictions of some base learners to combine these predictions. This second-level model is often called a <span class="s5">stacked model </span>or a <span class="s5">meta model</span>. And typically, this meta model performs equal to or better than the base learners.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_634.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The research papers by Leo Breiman (1996) and Mark J. Van der Laan (2007) are cited in th</p><p class="s4" style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">References <span class="p">section as </span>2 <span class="p">and </span>3<span class="p">, respectively.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">e</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although the idea originated with Wolpert in 1992, Leo Breiman formalized this idea in the way it is used now in his 1996 paper titled <i>Stacked Regressions</i>. And in 2007, Mark J. Van der Laan et al. established the theoretical underpinnings of the technique and provided proof that this meta model will perform at least as well or even better than the base learners.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This is a very popular technique in machine learning competitions such as Kaggle and is considered a black art among machine learning practitioners. We also discussed some other techniques, such as bagging and boosting, which combine base learners into something more. But those techniques require the base learner to be a weak learner. This is where stacking differs because stacking tries to combine a <i>diverse </i>set of <i>strong </i>learners.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The intuition behind stacking is that different models or families of functions learn the output function slightly differently, capturing different properties of the problem. For instance, one model may have captured the seasonality very well, whereas the other may have captured any particular interaction with an exogenous variable better. The stacking model will be able to combine these base models into a model that learns to look toward one model for seasonality and the other for interaction. This is done by making the meta model learn the predictions of the base models. But to avoid data leakage and thereby avoid overfitting, the meta model should be trained on out-of-sample predictions. There are two small variations of this technique that are used today – stacking and blending.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark363">Stacking </a><span class="p">is when the meta model is trained on the entire training dataset, but with out-of-sample predictions. The following steps are involved in stacking:</span></p><ol id="l92"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Split the training dataset into <i>k </i>parts.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Iteratively, train the base models on <i>k-1 </i>parts, predict on the <i>kth </i>part, and save the predictions. Once this step is done, we have the out-of-sample predictions for the training dataset from all base models.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Train a meta model on these predictions.</p></li></ol><p class="s5" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Blending <span class="p">is similar to this but slightly different in the way we generate out-of-sample predictions. The following steps are involved in blending:</span></p><ol id="l93"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Split the training dataset into two parts – train and holdout.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Train the base models on the training dataset and predict on the holdout dataset.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Train a meta model on the validation dataset with the predictions of the base model as the features.</p></li></ol><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Intuitively, we can see that stacking can work better because it is using a much larger dataset (usually all the training data) as the out-of-sample prediction, so the meta model may be more generalized. But there is a caveat: we assume that the entire training data is <span class="s5">independent and identically distributed </span>(<span class="s5">iid</span>). This is typically an assumption that is hard to meet in time series since the data generating process can change at any time (either gradually or drastically). If we know that the data distribution has changed significantly over time, blending the holdout period (which is usually the most recent part of the dataset) is better because the meta model is only learning on the latest data, thus paying respect to the temporal changes in the distribution of data.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_635.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_636.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_637.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_638.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_639.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_640.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="12" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_641.png"/></span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">There is no limit to the number of models we can include as base models, but usually, there is a plateau that we reach where additional models do not add much to the stacked ensemble. We can also add multiple levels of stacking. For instance, let’s assume there are four base learners:   <span class="s131">,   ,   , </span>and <span><img width="14" height="13" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_642.png"/></span>. We have also trained two meta models <span class="s204">and </span>, on the base models. Now, we can train a second- level meta model, <span class="s201">ℳ</span>, on the outputs of <span style=" color: #000;">and </span>and use that as the final prediction. We can use the <span class="s20">pystacknet </span><a href="https://github.com/h2oai/pystacknet" class="s140" target="_blank">Python library (</a><span class="s27">https://github.com/h2oai/pystacknet</span>), which is the Python implementation of an older library, called <span class="s20">stacknet</span>, to make the process of creating multi-level (or single-level) stacked ensembles easy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark364">Another key point to keep in mind is the type of models we usually use as meta models. It is assumed that the bulk of the learning has been taken care of by the base models, which are the multi-dimensional data for patterns for prediction. Therefore, the meta models are usually simple models such as linear regression, a decision tree, or even a random forest with much lower depth than the base models. Another way to think about this is in terms of bias and variance. Stacking can overfit the training or holdout set and by including model families with larger flexibility or expressive power, we are enabling this overfitting. The </a><i>Further reading </i>section contains a few links that explain different techniques of stacking from a general machine learning perspective.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s quickly see how we can use this in our dataset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from sklearn.linear_model import LinearRegression stacking_model = LinearRegression()</p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># ensemble_forecasts is the list of candidates</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">stacking_model.fit(</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">pred_wide_val[ensemble_forecasts], pred_wide_val[&quot;energy_ consumption&quot;]</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">pred_wide_test[&quot;linear_reg_blending&quot;] = stacking_model.predict( pred_wide_test[ensemble_forecasts]</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="229" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_643.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Best practice</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">When there are many base models and we want to do implicit base model selection as well, we can opt for one of the regularized linear models, such as ridge or lasso regression. Breiman, in his original paper, <i>Stacked Regressions</i>, proposed to use linear regression with positive coefficients and no intercept as the meta model. He argued that this gives a theoretical guarantee that the stacked model will give at least as good as any best individual model. But in practice, we can relax those assumptions while experimenting. Non-negative regression without intercepts is very close to the optimal weighted ensemble we discussed earlier. Finally, if we are evaluating multiple stacked models to select which one works well, we should resort to either having a separate validation dataset (instead of a <i>train-validation-test </i>split, we can use a <i>train-validation- validation_meta-test </i>split) or use cross-validated estimates. If we just pick the stacked model that performs best on the test dataset, we are overfitting the test dataset.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">This would save the blended prediction for linear regression as <span class="s20">linear_reg_blending</span>. We can use the same code but swap the models to try out other models as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark365">Now, let’s see how the blended models are doing on our test data:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span><img width="394" height="322" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_644.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 9.9 – Aggregate metrics for blending models</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="166" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_645.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional reading</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">There are other more innovative ways to combine base forecasts. This is an active area of research. The <i>Further reading </i>section contains links to two such ideas that are very similar. <span class="s5">Feature-Based Forecast Model Averaging </span>(<span class="s5">FFORMA</span>) extracts a set of statistical features from the time series and uses it to train a machine learning model that predicts the weights in which the base forecast should be combined. Another technique, from Facebook (Meta) Research, trains a classifier to predict which of the base learners does best, given a set of statistical features extracted from the time series.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, we can see that a simple linear regression has learned a meta model that performs much better than any of our average ensemble methods. And the Huber regression (which is a way to optimize the MAE directly) performs much better on the MAE benchmark. However, keep in mind that this is not universal and has to be evaluated for each problem you come across. Choosing the metric to optimize for and the model to use to combine makes a lot of difference. And often, the simple average ensemble is a very formidable benchmark for combining models.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_646.png"/></span></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark366">Summary</a><a name="bookmark351">&zwnj;</a><a name="bookmark350">&zwnj;</a><a name="bookmark349">&zwnj;</a></p><p class="s47" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Summary 223</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Continuing with the streak of practical lessons in the previous chapter, we completed yet another hands-on lesson. In this chapter, we generated forecasts from different machine learning models from the previous chapter. We learned how to combine these different forecasts into a single forecast that performs better than any single model. Then, we explored concepts such as combinatorial optimization and stacking to achieve state-of-the-art results.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the next chapter, we will start talking about global models of forecasting and explore strategies, feature engineering, and so on to enable such modeling.</p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The following references were provided in this chapter:</p><ol id="l94"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">David S. Johnson, Cecilia R. Aragon, Lyle A. McGeoch, and Catherine Schevon (1989), <i>Optimization by Simulated Annealing: An Experimental Evaluation; Part I, Graph Partitioning</i><a href="http://dx.doi.org/10.1287/opre.37.6.865" class="s140" target="_blank">. Operations Research, 1989, vol. 37, issue 6, 865-892 – </a><a href="http://dx.doi.org/10.1287/opre.37.6.865" class="a" target="_blank">http://dx.doi.org/10.1287/ </a><a href="http://dx.doi.org/10.1287/opre.37.6.865" target="_blank">opre.37.6.865</a></p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;line-height: 111%;text-align: justify;">L. Breiman <span class="p">(1996), </span>Stacked regressions<a href="https://doi.org/10.1007/BF00117832" class="s140" target="_blank">. Mach Learn 24, 49–64 – </a><a href="https://doi.org/10.1007/BF00117832" class="a" target="_blank">https://doi.org/10.1007/ </a><a href="https://doi.org/10.1007/BF00117832" target="_blank">BF00117832</a></p></li><li><p style="padding-top: 2pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Mark J. van der Laan; Eric C.Polley; and Alan E.Hubbard (2007), <i>Super Learner</i><a href="https://biostats.bepress.com/ucbbiostat/paper222" class="s140" target="_blank">. U.C. Berkeley Division of Biostatistics Working Paper Series. Working Paper 222: </a><a href="https://biostats.bepress.com/ucbbiostat/paper222" class="a" target="_blank">https://biostats.</a><a href="https://biostats.bepress.com/ucbbiostat/paper222" target="_blank"> bepress.com/ucbbiostat/paper222</a></p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">To learn more about the topics that were covered in this chapter, take a look at the following resources:</p></li><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">A Kaggler’s Guide to Model Stacking in Practice<a href="https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/" class="s140" target="_blank">, by Ha Nguyen: </a><a href="https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/" class="a" target="_blank">https://datasciblog. github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in- </a><a href="https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/" target="_blank">practice/</a></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Kai Ming Ting and Ian H. Witten (1997), <i>Stacked Generalization: when does it work?</i>:</p><p style="padding-top: 1pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><a href="https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf">https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf</a></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Pablo Montero-Manso, George Athanasopoulos, Rob J. Hyndman, Thiyanga S. Talagala (2020), <i>FFORMA: Feature-based forecast model averaging</i><a href="https://robjhyndman.com/papers/fforma.pdf" class="s140" target="_blank">. International Journal of Forecasting, Volume 36, Issue 1: </a><a href="https://robjhyndman.com/papers/fforma.pdf" target="_blank">https://robjhyndman.com/papers/fforma.pdf</a></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Peiyi Zhang, et al. (2021), <i>Self-supervised learning for fast and scalable time-series hyper-parameter tuning</i><a href="https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf" class="s140" target="_blank">: </a><a href="https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf" target="_blank">https://www.ijcai.org/Proceedings/97-2/Papers/011.pdf</a></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;"><a href="https://www.mathworks.com/help/optim/ug/local-vs-global-optima.html" class="s140" target="_blank">Local versus Global Optima: </a><a href="https://www.mathworks.com/help/optim/ug/local-vs-global-optima.html" class="a" target="_blank">https://www.mathworks.com/help/optim/ug/ </a><a href="https://www.mathworks.com/help/optim/ug/local-vs-global-optima.html" target="_blank">local-vs-global-optima.html</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark367">10</a><a name="bookmark368">&zwnj;</a><a name="bookmark370">&zwnj;</a><a name="bookmark369">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 88pt;text-indent: 0pt;text-align: center;">Global Forecasting Models</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In previous chapters, we saw how we can use modern machine learning models on time series forecasting problems, essentially replacing traditional models such as ARIMA or exponential smoothing. However, before now, we were looking at the different time series in any dataset (such as households in the <i>London Smart Meters </i>dataset) in isolation, just as the traditional models did.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">However, we will now explore a different paradigm of modeling where we use a single machine learning model to forecast a bunch of time series together. As we will learn in the chapter, this paradigm brings many benefits with it, from the perspective of both computation and accuracy.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Why Global Forecasting Models (GFMs)?</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Creating GFMs</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Strategies to improve GFMs</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Bonus – interpretability</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up an Anaconda environment, following the instructions in the <i>Preface </i>of the book, to get a working environment with all the packages and datasets required for the code in this book.</p><p class="s27" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter10" class="s140" target="_blank">The associated code for the chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter10" class="a" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter10<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark386">You need to run the following notebooks for this chapter:</a><a name="bookmark371">&zwnj;</a></p><ul id="l95"><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">02-Preprocessing London Smart Meter Dataset.ipynb <span class="p">in </span>Chapter02</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">01-Setting up Experiment Harness.ipynb <span class="p">in </span>Chapter04</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">From the <span class="s20">Chapter06 </span>and <span class="s20">Chapter07 </span>folders:</p><ul id="l96"><li><p class="s20" style="padding-top: 10pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">01-Feature Engineering.ipynb</p></li><li><p class="s20" style="padding-top: 7pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">02-Dealing with Non-Stationarity.ipynb</p></li><li><p class="s20" style="padding-top: 7pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">02a-Dealing with Non-Stationarity-Train+Val.ipynb</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">From the <span class="s20">Chapter08 </span>folder:</p><ul id="l97"><li><p class="s20" style="padding-top: 10pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">00-Single Step Backtesting Baselines.ipynb</p></li><li><p class="s20" style="padding-top: 7pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">01-Forecasting with ML.ipynb</p></li><li><p class="s20" style="padding-top: 7pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">01a-Forecasting with ML for Test Dataset.ipynb</p></li><li><p class="s20" style="padding-top: 7pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">02-Forecasting with Target Transformation.ipynb</p></li><li><p class="s20" style="padding-top: 7pt;padding-left: 74pt;text-indent: -11pt;text-align: left;">02a-Forecasting with Target Transformation(Test).ipynb</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Why Global Forecasting Models (GFMs)?</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">We talked about global models briefly in </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, where we mentioned related datasets. We can think of many scenarios where we would encounter related time series. We may need to forecast the sales for all the products of a retailer, the number of rides requested for a cab service across different areas of a city, or the energy consumption of all the households in a particular area (which is what the London Smart Meters dataset does). We call these related time series because all the different time series in the dataset can have a lot of factors in common with each other. For instance, the yearly seasonality that might occur in retail products might be present for a large section of products, or the way an external factor such as temperature affects energy consumption may be similar for a large number of households. Therefore, one way or the other, the different time series in a related time series dataset share attributes between them.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Traditionally, we used to consider each time series an independent time series; in other words, each time series was assumed to be generated using a different data generating process. Classical models such as ARIMA and exponential smoothing are trained for each time series. However, we can also consider all the time series in the dataset as being generated from a single data generating process, and the subsequent modeling approach would be to train a single model to forecast all the time series in the dataset. The latter is what we refer to as GFMs and, in contrast, the traditional approach is referred to as <span class="s5">Local Forecasting Models </span>(<span class="s5">LFMs</span>).</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">Although we briefly talked about the drawbacks of LFMs in </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, let’s summarize them in a more concrete fashion and see why GFMs help us tide over a lot of those drawbacks.</span></p><p class="s37" style="padding-top: 4pt;padding-left: 274pt;text-indent: 0pt;text-align: left;"><a name="bookmark387">Why Global Forecasting Models (GFMs)? 227</a><a name="bookmark373">&zwnj;</a><a name="bookmark372">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_647.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Sample size</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In most real-world applications (especially in business forecasting), the time series we have to forecast is not very long. Adopting a completely data-driven approach to modeling such a small time series is problematic. Training a highly flexible model with a handful of data points will lead to the model memorizing the training data, resulting in an overfit.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Traditionally, this has been overcome by placing strong priors or inductive bias into the models we use for forecasting. Inductive bias loosely refers to a set of assumptions or restrictions that are built into a model that should help the model predict feature combinations it has not encountered while training. For instance, double exponential smoothing has strong assumptions about seasonality and trend. The model does not allow any other more complicated patterns to be learned from the data. Therefore, using these strong assumptions, we are restricting the model search to a small section of the hypothesis space. While this helps in low-data regimes, the flip side is that these assumptions may limit accuracy.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Recent developments in the field of machine learning have shown us without a doubt that using a data- driven approach (with much fewer assumptions or priors) on large training sets will lead to us training better models. However, conventional statistical wisdom tells us that the number of data points needs to be at least 10 to 100 times the number of parameters that we are trying to learn from those data points.</p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="p">So, if we stick to LFMs, scenarios in which we can adopt a completely data-driven approach will be very rare. This is where GFMs shine. A GFM is able to use the history of </span>all <a href="#bookmark186" class="s140">the time series in a dataset to train the model and learn a single set of parameters that work for all the time series in the dataset. Borrowing the terminology introduced in </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, we increase the </span>width <span class="p">of the dataset, keeping the </span>length <span class="p">the same (refer back to </span>Figure 5.2<span class="p">). This explosion of historical information available to a single model lets us use completely data-driven techniques on time series datasets.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Cross-learning</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">GFMs, by design, promote cross-learning across different time series in a dataset. Imagine we have a time series that is quite new and does not have a history rich enough for teaching the model – for instance, the sales of a newly introduced retail product or the electricity consumption of a new household in a region. If we consider these time series in isolation, it will be a while before we start to get reasonable forecasts from the models we train on them, but GFMs make that process easier by enabling cross-learning. GFMs have an implicit sense of similarity between different time series and they will be able to use patterns they have seen in similar time series with a rich history to come up with a forecast on the new time series.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Another way cross-learning helps is by acting like a regularizer while estimating common parameters such as seasonality. For instance, the seasonality exhibited by similar products in a retail scenario is best estimated at an aggregate level, because each individual time series will have some sort of noise that can creep into the seasonality extraction. By enforcing common seasonality across multiple products, we are essentially regularizing the seasonality estimation and, in the process, making the seasonality estimate more robust. The good thing about GFMs is that they take a data-driven approach to define</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark388">the seasonality of which products should be estimated together and which ones have different patterns. If you have different seasonality patterns in different products, a GFM may struggle to model them together. However, when provided with enough information on how to distinguish between different products, the GFM will be able to learn that difference too.</a><a name="bookmark375">&zwnj;</a><a name="bookmark374">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Multi-task learning</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">GFMs can be considered multi-task learning paradigms where a single model is trained to learn multiple tasks (as forecasting each time series is a separate task). Multi-task learning is an active area of research, and there are many benefits to using multi-task models:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">When the model is learning from noisy, high-dimensional data, it becomes harder for the model to distinguish between useful and non-useful features. When we train the model on a multi-task paradigm, the model can understand useful features by looking at features that are useful for other tasks as well, thus providing the model with an additional perspective for discerning useful features.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Sometimes, features such as seasonality might be hard to learn from a particularly noisy time series. However, under a multi-task framework, the model can learn the difficult feature using other time series in the dataset.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Finally, multi-task learning introduces a kind of regularization that forces the model to find a model that works well on all tasks, thus reducing the risk of overfitting.</p></li></ul></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Engineering complexity</p><p class="s4" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark286" class="s140">LFMs pose a challenge from the engineering side as well for large datasets. If we have thousands or millions of time series to forecast, it becomes increasingly difficult to both train and manage the life cycle of these LFMs. In </a>Chapter 8<span class="p">, </span>Forecasting Time Series with Machine Learning Models<span class="p">, we trained LFMs for just a subset of households in the dataset. It took almost 20 to 30 minutes to train a machine learning model for all 150 households and we ran them with the default hyperparameters. In a normal machine learning workflow, we train multiple machine learning models and do hyperparameter tuning to find the best configuration of the model. However, carrying out all these steps for thousands of time series in a dataset becomes increasingly complex and time-consuming.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Equally, then there is the issue of managing the life cycle of these models. All these individual models need to be deployed to production, the performance monitored to check for model and data drift, and retrained at a set frequency. This becomes increasingly complex, as we have more and more time series to forecast.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">However, by shifting to a GFM paradigm, we drastically reduce the time and effort required to train and manage a machine learning model throughout its life cycle. As we will see in this chapter, training a GFM on these 150 households takes only a fraction of the time it takes to train LFMs.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark389">Creating GFMs 229</a><a name="bookmark376">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_648.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Despite all the advantages of GFMs, they are not without some drawbacks. The main drawback is that we are assuming that all the time series in a dataset are generated by a single <span class="s5">Data Generating Process </span>(<span class="s5">DGP</span>). This might not be a valid assumption and this can lead to the GFM underfitting some specific types of time series patterns that are underrepresented in the dataset.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Another open issue is whether a GFM is good for use with unrelated tasks or time series. The jury is out on this one, but Montero-Manso et al. proved that there are also gains in modeling unrelated time series with a GFM. The same finding has been put forward, although from another perspective, by Oreshkin et al., who trained a global model on the M4 dataset (a set of unrelated datasets) and obtained state-of-the-art performance. They attributed it to the meta-learning capabilities of the model. That being said, relatedness does help the GFM, as the learning task becomes easier this way. We will see a practical application of this in upcoming sections of this chapter as well.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the larger scheme of things, the benefits we derive from a GFM paradigm far outweigh the drawbacks. On most tasks, the GFMs either perform on par with or better than local models. It has been proven theoretically as well, by Montero-Manso et al., that a GFM, in a worst-case scenario, learns the same function as a local model. We will see this clearly in the models we are going to train in the upcoming sections. Finally, the training time and engineering complexity drop drastically as you move to a GFM paradigm.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now that we have motivated you as far as why a GFM is a worthwhile paradigm to adopt, let’s see how we can train one.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Creating GFMs</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_649.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code, use the notebook named <span class="s20">01-Global Forecasting Models-ML.ipynb </span>in the <span class="s20">chapter10 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark286" class="s140">Training a GFM is very straightforward. While we were training LFMs in </a>Chapter 8<span class="p">, </span>Forecasting Time Series with Machine Learning Models<span class="p">, we were looping over different households in the London Smart Meters dataset and training a model for each household. However, if we just take all the households into a single dataframe (our dataset is already that way) and train a single model on it, we get a GFM. One thing we want to keep in mind is to make sure that all the time series in the dataset have the same frequency. In other words, if we mix daily time series with weekly ones while training these models, the performance drop will be noticeable – especially if we are using time-varying features and other time-based information. For a purely autoregressive model, mixing time series in this way is much less of a problem.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark286" class="s140">The standard framework we developed in </a><a href="#bookmark286" class="s21">Chapter </a><i>8</i>, <i>Forecasting Time Series with Machine Learning Models</i>, is general enough to work for GFMs as well. So, as we did in that chapter, we define <span class="s20">FeatureConfig </span>and <span class="s20">MissingValueConfig </span>in the <span class="s20">01-Global Forecasting Models-ML.ipynb </span>notebook. We also slightly tweaked the Python function to train and evaluate the machine learning to make it work for all households. The details and exact functions can be found in the notebook.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Now, instead of looping over different households, we input the entire training dataset into the</p><p class="s20" style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">get_X_y <span class="p">function:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Define the ModelConfig</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from lightgbm import LGBMRegressor model_config = ModelConfig(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">model=LGBMRegressor(random_state=42), name=&quot;Global LightGBM Baseline&quot;,</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># LGBM is not sensitive to normalized data</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">normalize=False,</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># LGBM can handle missing values</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">fill_missing=False,</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Get train and test data</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">train_features, train_target, train_original_target = feat_ config.get_X_y(</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">train_df, categorical=True, exogenous=False</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">test_features, test_target, test_original_target = feat_config. get_X_y(</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">test_df, categorical=True, exogenous=False</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p class="s4" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark286" class="s140">Now that we have the data, we need to train the model. Training the model is also exactly the same as we saw in </a>Chapter 8<span class="p">, </span>Forecasting Time Series with Machine Learning Models<span class="p">. We will just choose LightGBM, which was the best-performing LFM model, and use functions we have defined earlier to train the model and evaluate the results:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 57pt;text-indent: -48pt;line-height: 131%;text-align: left;">y_pred, feat_df = train_model( model_config,</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">_feat_config, missing_value_config, train_features, train_target,</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">test_features,</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">agg_metrics, eval_metrics_df = evaluate_forecast( y_pred, test_target, train_target, model_config</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark390"/><a name="bookmark377">&zwnj;</a></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now in <span class="s20">y_pred</span>, we will have the forecast for all the households and <span class="s20">feat_df </span>will have the feature importance. <span class="s20">agg_metrics </span>will have the aggregated metric for all the selected households.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s look at how well our GFM model did:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 73pt;text-indent: 0pt;text-align: left;"><span><img width="414" height="67" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_650.gif"/></span></p><p class="s37" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.1 – Aggregate metrics with the baseline GFM</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We are not doing better than the best LFM (in the first row) in terms of the metrics. However, one thing we should note is the time taken to train the model – ~30 seconds. The LFM for all the selected households was taking ~30 minutes. This huge reduction in time taken gives us a lot of flexibility to iterate faster with different features and techniques.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">With that said, let’s now look at a few techniques with which we can improve the accuracy of the GFMs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Strategies to improve GFMs</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="74" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_651.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The Montero-Manso and Hyndman (2020) research paper is cited in <i>References </i>under reference number 1.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">GFMs have been in use in many forecasting competitions in Kaggle and outside of it. They have been battle-tested empirically, although very little work has gone into examining why they work so well from a theoretical point of view. Montero-Manso and Hyndman (2020) have a working paper titled <i>Principles and Algorithms for Forecasting Groups of Time Series: Locality and Globality</i>, which is an in-depth investigation, both theoretical and empirical, of GFMs and the many techniques that have been developed by the data science community collectively. In this section, we will try to include strategies to improve GFMs and, wherever possible, try to give theoretical justifications for why they would work.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark391">In the paper, Montero-Manso and Hyndman use a basic result in machine learning about generalization error to carry out the theoretical analysis and it is worth spending a bit of time to understand that, at last on a high level. </a><span class="s5">Generalization error</span>, as we know, is the difference between out-of-sample error and in-sample error. Yaser S Abu-Mostafa has a free, online <span class="s5">Massive Open Online Course </span>(<span class="s5">MOOC</span>) and an associated book (both of which are linked in the <i>Further reading </i>section). It is a short course on machine learning and is a course that I would recommend to anyone in the machine learning field for developing a stronger theoretical and conceptual basis for what we do. One of the important concepts the course and book put forward is the use of Hoeffding’s inequality from probability theory to derive bounds on a learning problem. Let’s quickly look at the result to develop our understanding:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s143" style="padding-top: 9pt;text-indent: 0pt;text-align: right;">𝐸<span class="s90">𝐸</span><span class="s127">𝑜𝑜𝑜𝑜𝑜𝑜  </span>&lt;<span class="s90"> 𝐸𝐸</span><span class="s127">𝑖𝑖𝑖𝑖  </span>+</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">It has a probability of at least <span class="s41">1 − 𝛿𝛿</span>.</p><p class="s155" style="padding-top: 14pt;text-indent: 0pt;text-align: left;">√<span class="s90">𝑙𝑙𝑙𝑙𝑙𝑙(|ℋ|) + 𝑙𝑙𝑙𝑙𝑙𝑙</span></p><p class="s90" style="padding-left: 45pt;text-indent: 0pt;text-align: left;">2𝑁𝑁</p><p class="s165" style="padding-top: 8pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"> <span class="s205">2</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="118" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_652.png"/></span></p><p class="s90" style="text-indent: 0pt;line-height: 6pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"><span><img width="118" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_653.png"/></span></p><p class="s90" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝛿𝛿</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 92%;text-align: justify;"><span class="s41">𝐸𝐸𝑖𝑖𝑖𝑖 </span>is the in-sample average error and <span class="s41">𝐸𝐸</span><span class="s42">𝑜𝑜𝑜𝑜𝑜𝑜 </span>is the expected out-of-sample error. <i>N </i>is the total number of samples in the dataset from which we are learning and <span class="s137">ℋ </span>is the hypothesis class of models. It is a finite set of functions that can potentially fit the data. The size of <span class="s137">ℋ</span>, denoted by <span class="s109">|</span><span class="s146">ℋ</span><span class="s109">|</span>, is the complexity of <span class="s137">ℋ</span>. Although the formula of the bound looks intimidating, let’s simplify the way we look at it to develop the necessary understanding.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 85%;text-align: justify;">We want <span class="s41">𝐸𝐸</span><span class="s42">𝑜𝑜𝑜𝑜𝑜𝑜 </span>to be as close to <span class="s41">𝐸𝐸</span><span class="s88">𝑖𝑖𝑖𝑖 </span>as possible and for that, we need the terms in the square root to be as small as possible. There are two terms under the square root that are in our <i>control</i>, so to speak</p><ul id="l98"><li><p style="padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">N and <span class="s74">|ℋ|</span>. Therefore, to make the generalization error (<span class="s137">𝐸𝐸𝑖𝑖𝑖𝑖 − 𝐸𝐸𝑜𝑜𝑜𝑜𝑜𝑜</span>) as small as possible, we either need to increase <i>N </i>(have more data) or decrease <span class="s109">|</span><span class="s146">ℋ</span><span class="s109">|</span><span class="s146"> </span>(have a less complex model). This is a result that is applicable to all machine learning but Montero-Manso and Hyndman, with a few assumptions, made this applicable to time series models as well. It is this result that they used to give theoretical backing to the arguments put forward in their working paper.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Montero-Manso and Hyndman have taken Hoeffding’s inequality and applied it to LFMs and GFMs to compare them. We can see the result here (for full mathematical and statistical understanding, refer to the original paper under <i>References</i>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 222pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="158" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_654.png"/></span></p><p class="s41" style="padding-top: 5pt;text-indent: 0pt;line-height: 3pt;text-align: right;">𝑙𝑙𝑙𝑙𝑙𝑙(∏𝐾𝐾</p><p class="s41" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">|ℋ |) + 𝑙𝑙𝑙𝑙𝑙𝑙<u> 2</u></p><p class="s207" style="padding-top: 3pt;padding-left: 126pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝐸𝐸<span class="s43">𝐿𝐿𝑜𝑜𝐿𝐿𝐿𝐿𝐿𝐿 </span>&lt; <span class="s41">𝐸𝐸</span><span class="s43">𝐿𝐿𝑜𝑜𝐿𝐿𝐿𝐿𝐿𝐿 </span>+ <span class="s150">√</span></p><p class="s43" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">𝑖𝑖=1</p><p class="s43" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">𝒾𝒾</p><p class="s41" style="padding-left: 38pt;text-indent: 0pt;line-height: 8pt;text-align: left;">( )</p><p class="s41" style="padding-left: 44pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝛿𝛿</p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: right;">𝑜𝑜𝑜𝑜𝑜𝑜</p><p class="s43" style="text-indent: 0pt;line-height: 9pt;text-align: right;">𝑖𝑖𝑖𝑖</p><p style="padding-left: 34pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="158" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_655.png"/></span></p><p class="s41" style="padding-left: 83pt;text-indent: 0pt;text-align: left;">2𝑁𝑁𝑁𝑁</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 54pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="130" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_656.png"/></span></p><p class="s80" style="padding-left: 54pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑙𝑙𝑙𝑙𝑙𝑙(|𝒥𝒥|) + 𝑙𝑙𝑙𝑙𝑙𝑙<u> 2</u></p><p class="s209" style="padding-top: 3pt;padding-left: 128pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝐸𝐸<span class="s65">𝐺𝐺𝐺𝐺𝑜𝑜𝐺𝐺𝐺𝐺𝐺𝐺 </span>&lt; <span class="s80">𝐸𝐸</span><span class="s65">𝐺𝐺𝐺𝐺𝑜𝑜𝐺𝐺𝐺𝐺𝐺𝐺 </span>+ <span class="s210">√</span></p><p class="s80" style="padding-left: 139pt;text-indent: 0pt;line-height: 8pt;text-align: center;">(  )</p><p class="s80" style="padding-left: 138pt;text-indent: 0pt;line-height: 9pt;text-align: center;">𝛿𝛿</p><p class="s65" style="text-indent: 0pt;line-height: 9pt;text-align: right;">𝑜𝑜𝑜𝑜𝑜𝑜</p><p class="s65" style="text-indent: 0pt;line-height: 9pt;text-align: right;">𝑖𝑖𝑖𝑖</p><p style="padding-left: 43pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="133" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_657.png"/></span></p><p class="s80" style="padding-left: 79pt;text-indent: 0pt;text-align: left;">2𝑁𝑁𝑁𝑁</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s54" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝑜𝑜𝑜𝑜𝑜𝑜</p><p style="text-indent: 0pt;text-align: left;"/><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝑜𝑜𝑜𝑜𝑜𝑜</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 27pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a name="bookmark392"><span class="s50">𝐸𝐸𝐿𝐿𝑜𝑜𝐿𝐿𝐿𝐿𝐿𝐿 </span></a>and <span class="s74">𝐸𝐸𝐺𝐺𝐺𝐺𝑜𝑜𝐺𝐺𝐺𝐺𝐺𝐺 </span>are the average in-sample errors across all the time series using the local and<a name="bookmark378">&zwnj;</a></p><p class="s64" style="text-indent: 0pt;line-height: 6pt;text-align: left;">𝑜𝑜𝑜𝑜𝑜𝑜</p><p style="text-indent: 0pt;text-align: left;"/><p class="s54" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝑖𝑖𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">global approaches, respectively. <span class="s65">𝐸𝐸𝐺𝐺𝐺𝐺𝑜𝑜𝐺𝐺𝐺𝐺𝐺𝐺 </span>and <span class="s50">𝐸𝐸𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺 </span>are the out-of-sample expectations under the local</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 91%;text-align: justify;">and global approaches, respectively. <span class="s137">ℋ</span><span class="s42">𝒾𝒾</span><span class="s43">  </span>is the hypothesis class for the <i>i</i>-th time series and <span class="s41">𝒥𝒥 </span>is the hypothesis class for the global approach (the global approach only fits a single function and hence, has just a single hypothesis class).</p><p class="s211" style="text-indent: 0pt;line-height: 3pt;text-align: left;">𝐾𝐾</p><p style="text-indent: 0pt;text-align: left;"/><p class="s211" style="text-indent: 0pt;line-height: 3pt;text-align: left;">𝑖𝑖=1</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">One of the most interesting results that comes out of this is that the complexity term for LFMs (<span class="s212">𝑙𝑙𝑙𝑙𝑙𝑙</span><span class="s213"> (∏|ℋ</span><span class="s211">𝒾𝒾</span><span class="s212">|</span><span class="s213">)</span>) grows the size of the dataset. The greater the number of time series we have in the dataset, the more the complexity and the worse the generalization error, whereas with GFMs, the complexity term (<span class="s65">𝑙𝑙𝑙𝑙𝑙𝑙</span><span class="s214">(</span><span class="s65">|𝒥𝒥</span><span class="s214">|</span><span class="s65">)</span>) stays constant. Therefore, for a dataset of moderate size, the overall complexity of LFMs (such as exponential smoothing) can be much higher than a single GFM, no matter how complex the GFM is. As a corollary, we can also think that with the available dataset (<i>NK</i>), we can afford to train a model with much higher complexity than a model for LFMs. There are many ways to increase the complexity of the model, which we will see in the following section.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s return to the GFMs we were training. We saw that the performance of the GFM we trained was not up to the mark when we compared it with the best LFM (LightGBM), but it is better than the baseline and other models we tried, so right off the bat, we know the GFM we trained is not terrible. Now, let’s look at a few ways to improve the performance of the model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Increasing memory</p><p class="s4" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">As we discussed in </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, the machine learning models that we discuss in this book are finite memory models or Markov models. A model such as exponential smoothing takes into account the entire history of a time series while forecasting, but models such as any of the machine learning models we discussed only take in a finite memory to make their predictions. In a finite memory model, the amount of memory we allow the model to access is called the size of the memory (</span>M<span class="p">) or order of autoregression (from econometrics).</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Providing a greater amount of memory to the model increases the complexity of the model. Therefore, one of the ways to increase the performance of the GFM is to increase the amount of memory the model has access to. There are many ways to increase the amount of memory.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Adding more lag features</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">If you have prior exposure to ARIMA models, you will know that the number of <span class="s5">Autoregressive </span>(<span class="s5">AR</span>) terms are sparingly used. We usually see AR models with single-digit lags. However, when we are moving to GFMs, we can afford to have much larger lags. Montero-Manso and Hyndman empirically showed the benefits of adding more lags to GFMs. For highly seasonal time series, a peculiar phenomenon was observed. The accuracy improves with an increase in lags but it then saturates and suddenly worsens when the lag becomes equal to the seasonal cycle. On further increasing the lags beyond the seasonal cycle, the accuracy shows huge gains. This may be because of the overfitting that happens because of seasonality. It becomes very easy for the model to favor the seasonal lag because it works very well in a sample, so it’s better to add a few more lags on the plus side of the seasonal cycle.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark393">Adding rolling features</a><a name="bookmark379">&zwnj;</a></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another way to increase the memory of the model is to include rolling averages as features. Rolling averages take information from extended windows on memory and encode that information by way of descriptive statistics (such as the mean or max). This is an efficient way of including the memory because we can take very large windows for memory and include the information as a single feature in the model.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Adding EWMA features</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">An <span class="s5">Exponentially Weighted Moving Average </span>(<span class="s5">EWMA</span>) is a way to include infinite memory into a finite memory model. The EWMA essentially takes the average of the entire history but is weighted according to the that we set. Therefore, with different values of , we get different kinds of memory, again encoded as a single feature. Including different EWMA features has also empirically proved beneficial.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have already included these kinds of features in our feature engineering and they are part of the baseline GFM we trained, so let’s move on to the next strategy for improving the accuracy of GFMs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Using time series meta-features</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The baseline GFM we trained earlier in the <i>Creating Global Forecasting Models (GFMs) </i>section had lag features, rolling features, and EWMA features, but we have given no feature that helps the model distinguish between different time series in the dataset. The baseline GFM model learned a generalized function that generates a forecast provided the features. This might work well enough for homogenous datasets where all the time series are very similar in nature, but for heterogenous datasets, the information with which the model can distinguish each time series comes in handy.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">So, information about the time series itself is what we call meta-features. In a retail context, it can be the product ID, the category of products, the store number, and so on. In our dataset, we have features such as <span class="s20">stdorToU</span>, <span class="s20">Acorn</span>, <span class="s20">Acorn_grouped</span>, and <span class="s20">LCLid</span>, which give some information about the time series itself. Including these meta-features in the GFM will improve the performance of the model.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">However, there is just one problem – more often than not, these meta-features are categorical in nature. A feature is categorical when the values in the feature can only take discrete values. For instance, <span class="s20">Acorn_ grouped </span>can only have one of three values – <span class="s20">Affluent</span>, <span class="s20">Comfortable</span>, or <span class="s20">Adversity</span>. Most machine learning models do not work well with categorical features. All the models in <span class="s20">scikit-learn</span>, the most popular machine learning library in the Python ecosystem, do not allow categorical features at all. To include categorical features in machine learning models, we need to encode them into numerical form, and there are many ways to encode categorical columns. Let’s review a few popular options.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Ordinal encoding and one-hot encoding</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The most popular ways of encoding categorical features are ordinal encoding and one-hot encoding, but they are not always the best choices. Let’s quickly review what these techniques are and when they are suitable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark394">Ordinal encoding is the simplest of them all. We simply assign a numerical code to the unique values of a category and then replace the categorical value with the numerical code. To encode the </a><span class="s20">Acorn_grouped </span>feature from our dataset, all we need to do is assign codes, say <span class="s20">1 </span>for <span class="s20">Affluent</span>, <span class="s20">2 </span>for <span class="s20">Comfortable</span>, and <span class="s20">3 </span>for <span class="s20">Adversity</span>, and replace all instances of the categorical values with the code we assigned. While this is really easy, this kind of encoding introduces meanings to the categorical values that we may or may not intend. When we assign numerical codes, we are implicitly saying that the categorical value that gets assigned <span class="s20">2 </span>as a code is better than the categorical value with <span class="s20">1 </span>as the code. This kind of encoding only works for ordinal features (features whose categorical values have an intrinsic sense of rank in their meaning) and should be sparingly used. Another way we can think about the problem is in terms of distance. When we do ordinal encoding, the distance between <span class="s20">Comfortable </span>and <span class="s20">Affluent </span>can be higher than the distance between <span class="s20">Comfortable </span>and <span class="s20">Adversity</span>, depending on the way we encode.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">One-hot encoding is a better way of representing categorical features with no ordinal meaning. It essentially encodes the categorical features in a higher dimension, placing the categorical values equally distant in that space. The size of the dimension it requires to encode the categorical values is equal to the cardinality of the categorical variable. <span class="s5">Cardinality </span>is the number of unique values in the categorical feature. Let’s see how sample data would be encoded in a one-hot encoding scheme:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 73pt;text-indent: 0pt;text-align: left;"><span><img width="406" height="147" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_658.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.2 – One-hot encoding of categorical features</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can see that the resulting encoding will have a column for each unique value in the categorical feature and the value is indicated by <span class="s20">1 </span>in the column. For instance, the first row is <span class="s20">Comfortable</span>, and therefore, every other column except the <span class="s20">Comfortable </span>column will have <span class="s20">0 </span>and the <span class="s20">Comfortable </span>column will have <span class="s20">1</span>. If we calculate the Euclidean distance between any two categorical values, we can see that they are the same.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">However, there are three main issues with this encoding, all of which become a problem with high cardinality categorical variables:</p><ul id="l99"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">The embedding is inherently sparse and many machine learning models (for instance, tree- based models and neural networks) do not really work well with sparse data (sparse data is when a majority of values in the data are zeros). When the cardinality is just 5 or 10, the sparsity introduced may not be that much of a problem, but when we consider a cardinality of 100 or 500, the encoding becomes really sparse.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l100"><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a name="bookmark395">Another issue is the explosion of dimensions of the problem. When we increase the total number of features of a problem due to the large number of new features that are created through one-hot encoding, we make the problem harder to solve. This can be explained by the </a><span class="s5">curse of dimensionality</span>. The <i>Further reading </i>section has a link with more information on the curse of dimensionality.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">The last problem is related to practical concerns. For a large dataset, if we one-hot encode a categorical value with hundreds or thousands of unique values, the resulting dataframe is not going to be easy to work with because it will not fit in the computer memory.</p></li></ul></li></ul></li></ul><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There is a slightly different way of one-hot encoding where we drop one of these dimensions, called <span class="s5">dummy variable encoding</span>. This has the added benefit of making the encoding linearly independent, which, in turn, has some advantages, especially for vanilla linear regression. The <i>Further reading </i>section has a link if you want to know more.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Since the categorical columns that we must encode have high cardinality (at least a few of them), we will not be doing this encoding. Instead, let’s look at a few encoding techniques that can handle high cardinality categorical variables better.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Frequency encoding</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Frequency encoding is an encoding schema that does not increase the dimensions of the problem. It takes a single categorical array and returns a single numeric array. The logic is very simple – it replaces the categorical values with the number of times the value occurs in the training dataset. Although it’s not perfect, this works pretty well, as it lets the model distinguish between different categories based on how frequently they occur.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There is a popular library, <span class="s20">category_encoders</span>, that implements a lot of different encoding schemes in a standard <span class="s20">scikit-learn </span><a href="#bookmark286" class="s140">style estimator, and we will be using that in our experiments as well. The standard framework we developed in </a><i>Chapter 8</i>, <i>Forecasting Time Series with Machine Learning Models</i>, also had a couple of functionalities that we didn’t use – <span class="s20">encode_categorical </span>and <span class="s20">categorical_encoder</span>. So, let’s use these and train our model now:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from category_encoders import CountEncoder from lightgbm import LGBMRegressor</p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">#Define which columns names are categorical features</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">cat_encoder = CountEncoder(cols=cat_features)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">model_config = ModelConfig( model=LGBMRegressor(random_state=42),</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">name=&quot;Global LightGBM with Meta Features (CountEncoder)&quot;,</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># LGBM is not sensitive to normalized data</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">normalize=False,</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># LGBM can handle missing values</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">fill_missing=False,</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># Turn on categorical encoding</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">encode_categorical=True,</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># Pass the categorical encoder to be used</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">categorical_encoder=cat_encoder</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark396"/></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">The rest of the process is the same as what we saw in the <i>Creating Global Forecasting Models (GFMs)</i></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">section and we get the forecast using the encoded meta-features:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><img width="523" height="96" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_659.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.3 – Aggregate metrics with the GFM with meta-features (frequency encoding)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Right away, we can see that there is a reduction in error, although it is minimal. We can also see that the training time has almost doubled. This may be because now we have an additional step of encoding the categorical features in addition to training the machine learning model.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The main issue with frequency encoding is that it doesn’t work with features that are uniformly distributed in the dataset. For instance, the <span class="s20">LCLid </span>feature, which is just a unique code for each household, is uniformly distributed in the dataset and when we use frequency encoding, all the <span class="s20">LCLid </span>features will come to almost the same frequency, and hence the machine learning model considers them almost the same.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s now look at a slightly different approach.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Target mean encoding</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Target mean encoding, in its most vanilla form, is a very simple concept. It is a <i>supervised </i>approach that uses the target in the training dataset to encode the categorical columns. Let’s look at an example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 49pt;text-indent: 0pt;text-align: left;"><span><img width="487" height="190" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_660.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.4 – Target mean encoding</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The vanilla target mean encoding has a few limitations. It increases the chance of overfitting the training data because we are using the mean targets directly and thereby leaking the target into the model in a way. Another problem with the approach is that when the categorical values are unevenly distributed, there may be a few categorical values with very small sample sizes, and therefore, the mean estimate becomes noisy. Extending this problem to the extreme, we get another case where an unseen categorical value comes up in test data. This is also not supported in the vanilla version. Therefore, in practice, this simple version is almost never used, but slightly more sophisticated versions of this concept are widely used and are an effective strategy while encoding categorical features.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In <span class="s20">category_encoders</span>, there are many variations of this concept, but let’s look at two popular and effective ones here.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In 2001, Daniele Micci-Barreca proposed a variant of mean encoding. If we consider the target as a binary variable, say 1 and 0, the mean (which is the number of 1s or number of samples) is also the probability of having 1. Using this interpretation of the means, Daniele proposed to blend two probabilities – prior and posterior probabilities – as the final encoding for the categorical features.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_661.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Daniele Micci-Barreca is cited in <i>References </i>under reference number 2.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">The prior probability is defined as follows:</p><p class="s80" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;line-height: 12pt;text-align: center;">𝑛𝑛<span class="s82">𝑦𝑦</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_662.png"/></span></p><p class="s85" style="text-indent: 0pt;text-align: right;">𝑃𝑃<span class="s50">𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 </span>= <span class="s215">𝑛𝑛</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s50" style="text-indent: 0pt;text-align: left;">𝑇𝑇𝑇𝑇</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here, <span class="s41">𝑛𝑛𝑦𝑦 </span>is the number of cases such that <span class="s74">𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 = 1</span>, and <span class="s150">𝑛𝑛</span><span class="s41">𝑇𝑇𝑇𝑇 </span>is the number of samples in the training data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The posterior probability is defined for category <i>i </i>as follows:</p><p class="s80" style="padding-top: 3pt;padding-left: 171pt;text-indent: 0pt;line-height: 31%;text-align: center;"><span class="s216">𝑃</span>𝑃<span class="s82">𝑝𝑝</span><span class="s50">                     </span><span class="s216">=</span> <span class="s85">𝑛</span>𝑛<span class="s50">𝑝𝑝𝑖𝑖</span></p><p style="padding-left: 249pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="22" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_663.png"/></span></p><p class="s50" style="text-indent: 0pt;line-height: 8pt;text-align: right;">𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝</p><p class="s80" style="padding-left: 17pt;text-indent: 0pt;line-height: 16pt;text-align: left;">𝑛𝑛<span class="s82">𝑝𝑝</span></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 84%;text-align: justify;">Here, <span class="s41">𝑛𝑛</span><span class="s88">𝑖𝑖𝑖𝑖 </span>is the number of samples in the dataset where <span class="s50">category = 𝑖𝑖 </span>and <span class="s65">𝑌𝑌 = 1</span>, and <span class="s41">𝑛𝑛𝑖𝑖 </span>is the number of samples in the dataset where <span class="s146">𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 = 𝑖𝑖</span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, the final encoding for category <i>i </i>is as follows:</p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑖𝑖𝑝𝑝𝑝𝑝</p><p style="text-indent: 0pt;text-align: left;"/><p class="s41" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑆𝑆<span class="s42">𝑖𝑖   </span>= 𝜆𝜆(𝑛𝑛<span class="s42">𝑖𝑖 </span>) × 𝑃𝑃𝑖𝑖                  + (1 − 𝜆𝜆(𝑛𝑛<span class="s42">𝑖𝑖 </span>)) × 𝑃𝑃<span class="s42">𝑝𝑝𝑝𝑝𝑖𝑖𝑝𝑝𝑝𝑝</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 90%;text-align: justify;">Here, <span class="s137">𝜆𝜆 </span>is the weighting factor, which is a monotonically increasing function on <span class="s41">𝑛𝑛</span><span class="s88">𝑖𝑖 </span>that is bounded between 0 and 1. So, this function gives a larger weight to the posterior probability as the number of samples increases.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Adapting this to the regression setting, the probabilities change to expected values so that the formula becomes the following:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝑆𝑆</p><p class="s217" style="padding-top: 4pt;padding-left: 3pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s74">= λ(𝑛𝑛 ) × </span>∑<span class="s218">𝑘</span><span class="s75">𝑘ϵ𝑇𝑇𝑅𝑅</span><span class="s219">𝑖𝑖</span><span class="s220">  </span>𝑌𝑌<span class="s218">𝑘𝑘</span><span class="s75">  </span><span class="s74">+ (1 − λ(𝑛𝑛 )) × </span>∑<span class="s221">𝑘</span><span class="s75">𝑘ϵ𝐿𝐿𝑇𝑇𝑅𝑅 </span><span class="s222">𝑌𝑌</span><span class="s221">𝑘𝑘</span></p><p class="s223" style="padding-left: 47pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="51" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_664.png"/></span>	<span><img width="53" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_665.png"/></span></p><p class="s75" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝑖𝑖</p><p class="s75" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝑖𝑖</p><p class="s74" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝑛𝑛<span class="s79">𝑖𝑖</span></p><p class="s75" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝑖𝑖</p><p class="s224" style="padding-left: 30pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝑛𝑛<span class="s75">𝑇𝑇𝑅𝑅</span></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Here, <span class="s74">𝑇𝑇𝑅𝑅 </span>is all the rows where <span class="s146">𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 = 1 </span>and <span class="s225">∑ </span><span class="s220">𝑌𝑌</span><span class="s226">𝑘𝑘 </span>is the sum of <i>Y </i>for <span class="s50">𝑇𝑇𝑅𝑅 </span>. <span class="s90">∑𝑌𝑌</span></p><p class="s90" style="padding-top: 3pt;text-indent: 0pt;line-height: 6pt;text-align: right;">𝑌𝑌</p><p style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">is the sum</p><p class="s127" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑖𝑖</p><p class="s227" style="padding-top: 1pt;text-indent: 0pt;line-height: 5pt;text-align: right;">𝑘𝑘𝑘𝑘𝑘𝑘𝑅𝑅<span class="s228">𝑖𝑖</span></p><p class="s54" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑖𝑖</p><p class="s127" style="padding-left: 14pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑘𝑘𝑘𝑘𝑘𝑘𝑘𝑘</p><p class="s127" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑘𝑘</p><p style="padding-left: 27pt;text-indent: 0pt;text-align: justify;">of <i>Y </i>for all the rows in the training dataset. As with the binary variable, we are mixing the expected value of <i>Y</i>, given <span class="s50">𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 = 𝑖𝑖  </span>(<span class="s65">𝐸𝐸[𝑌𝑌 | 𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 = 𝑖𝑖]</span>) and the expected value of <i>Y </i>(<span class="s146">𝐸𝐸[𝑌𝑌]</span>) for the final categorical encoding.</p><p style="padding-top: 6pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">There are many functions that we can use for <span class="s137">𝜆𝜆</span>. Daniele mentions a very common functional form (sigmoid):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s137" style="text-indent: 0pt;text-align: right;">𝜆𝜆(𝑛𝑛<span class="s42">𝑖𝑖</span><span class="s43"> </span>) =</p><p class="s137" style="padding-top: 7pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="67" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_666.png"/></span></p><p class="s43" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 7pt;text-align: left;">−</p><p class="s137" style="text-indent: 0pt;line-height: 10pt;text-align: left;">1 + 𝑒𝑒</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s40" style="text-indent: 0pt;line-height: 10pt;text-align: center;"><span class="s39"> </span>𝑛𝑛<span class="s229">𝑖𝑖</span>−𝑘𝑘</p><p class="s43" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: center;">𝑓𝑓</p><p style="padding-top: 4pt;padding-left: 27pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Here, <span class="s157">𝑛𝑛</span><span class="s88">𝑖𝑖 </span>is the number of samples in the dataset where, <span class="s146">𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 = 𝑖𝑖 </span>and <i>k </i>and <i>f </i>are tunable hyperparameters. <i>k </i>determines half of the minimal sample size for which we completely trust the estimate. If <span class="s20">k = 1</span>, what we are saying is that we trust the posterior estimate from a category that has only two samples. <i>f </i>determines how fast the sigmoid transitions between the two extremes. As <i>f </i>tends to infinity, the transition becomes a hard threshold between prior and posterior probabilities. <span class="s20">TargetEncoder </span>from <span class="s20">category_encoders </span>has implemented this <span class="s137">𝜆𝜆</span>. The <i>k </i>parameteris called <span class="s20">min_samples_leaf </span>with a default value of 1, and the <i>f </i>parameteris called <span class="s20">smoothing </span>with a default value of 1. Let’s see how this encoding works on our problem. Using a different encoder in the framework we are working on is as simple as passing a different <span class="s20">cat_encoder</span>(the initialized categorical encoder) to <span class="s20">ModelConfig</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from category_encoders import TargetEncoder cat_encoder = TargetEncoder(cols=cat_features)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark397">The rest of the code is exactly the same. We can find the full code in the corresponding notebook. Let’s see how well the new encoding has done:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><span><img width="521" height="128" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_667.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.5 – Aggregate metrics with the GFM with meta-features (target encoding)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">It’s not doing that well, is it? As with machine learning models, the <span class="s5">No Free Lunch Theorem </span>(<span class="s5">NFLT</span>) applies to categorical encoding as well. There is no one encoding scheme that works well all the time. Although not directly related to the topic, if you want to know more about the NFLT, head to <i>the Further reading section</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="122" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_668.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">With all these <i>supervised </i>categorical encoding techniques, such as target mean encoding, we have to be really careful not to induce data leakage. The encoder should be fit using training data and not using the validation or test data. Another very popular technique is to generate categorical encoding using cross-validation and use the out-of-sample encodings to absolutely avoid data leakage or overfitting.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">There are many more encoding schemes, such as <span class="s20">MEstimateEncoder </span>(which uses additive smoothing as the <span class="s137">𝜆𝜆</span>), <span class="s20">HashingEncoder</span>, and so on, in <span class="s20">category_encoders</span>. Another very effective way of encoding categorical features is using embedding from deep learning. The <i>Further reading </i>section has a link to a tutorial for doing this.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Before now, all this categorical encoding was a separate step before the modeling. Now, let’s look at a technique that considers categorical features natively for model training.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">LightGBM’s native handling of categorical features</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">A few machine learning model implementations handle categorical features natively, especially gradient-boosting models. CatBoost and LightGBM, two of the most popular GBM implementations, handle categorical features out of the box. CatBoost has a unique way of encoding categorical features into numerical ones internally using something similar to additive smoothing. The <i>Further reading </i>section has links to further information on how this encoding is done. <span class="s20">category_encoders </span>has implemented this logic as <span class="s20">CatBoostEncoder </span>so that we can use this type of encoding for any machine learning model as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">While CatBoost handles this internal conversion into numerical features, LightGBM takes a more native approach to dealing with categorical features. LightGBM considers the categorical features as is while growing and splitting the trees. For a categorical feature with <i>k </i>unique values (cardinality of <i>k</i>), there are <span class="s90">2𝑘𝑘−1 − 1 </span>possible partitions. This soon becomes intractable, but for regression trees, Walter D. Fisher proposed a technique back in 1958 that makes the complexity of finding an optimal split much lesser. The essence of the method is to use average target statistics for each categorical value to order them and then find the optimal split in the ordered categorical values.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_669.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Fisher is cited in <i>References </i>under reference number 3.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">LightGBM’s <span class="s20">scikit-learn </span>API supports this feature by taking an argument, <span class="s20">categorical_ feature</span>, which has a list of categorical feature names, during <span class="s20">fit</span>. We can use the <span class="s20">fit_kwargs </span>argument in the fit of our <span class="s20">MLModel </span><a href="#bookmark286" class="s140">that we defined in </a><a href="#bookmark286" class="s21">Chapter </a><i>8</i>, <i>Forecasting Time Series with Machine Learning Models</i>, to pass in this parameter. Let’s see how we can do this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from lightgbm import LGBMRegressor model_config = ModelConfig(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">model=LGBMRegressor(random_state=42),</p><p class="s28" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;line-height: 131%;text-align: left;">name=&quot;Global LightGBM with Meta Features (NativeLGBM)&quot;, <span class="s38"># LGBM is not sensitive to normalized data </span>normalize=False,</p><p class="s38" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># LGBM can handle missing values</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">fill_missing=False,</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># We are using inbuilt categorical feature handling</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">encode_categorical=False,</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Training the model and passing in fit_kwargs</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">y_pred, feat_df = train_model( model_config,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">_feat_config, missing_value_config, train_features, train_target, test_features,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">fit_kwargs=dict(categorical_feature=cat_features),</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark398">y_pred </a><span class="p">has the forecasts, which we evaluate as usual. Let’s also see the results:</span><a name="bookmark380">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="153" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_670.gif"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.6 – Aggregate metrics with the GFM with meta-features (native LightGBM)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can observe a good reduction in <span class="s5">MAE </span>as well as <span class="s5">meanMASE </span>with the native handling of categorical features. We can also see a reduction in the total training time because we don’t have a separate step for encoding the categorical feature. Empirically, the native handling of categorical features works better most of the time.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have encoded the categorical features, let’s look at another way to improve accuracy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Tuning hyperparameters</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Although hyperparameter tuning is common practice in machine learning, we haven’t been able to do so because of the sheer number of models we had under the LFM paradigm. Now that we have a GFM that finishes training in 30 seconds, hyperparameter tuning becomes feasible. From a theoretical perspective, we also saw that GFMs can afford a larger complexity and can therefore evaluate a greater number of functions to pick the best without overfitting.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Wikipedia defines mathematical optimization as <i>“the selection of a best element, with regard to some criterion, from some set of available alternatives.” </i>In most cases, this involves finding the maximum or minimum value of some function (an <span class="s5">objective function</span>) from a set of alternatives (the <span class="s5">search space</span>) subject to some conditions (<span class="s5">constraints</span>). The search space can be discrete variables, continuous variables, or a mixture of both, and the objective function can be differentiable or non-differentiable. There is a large body of research that tackles these variations.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">You may be wondering why we are talking about mathematical optimization now, right? Hyperparameter tuning is a mathematical optimization problem. The objective function here is non-differentiable and returns the metric for which we are optimizing – for instance, the <span class="s5">Mean Absolute Error </span>(<span class="s5">MAE</span>). The search space comprises the different hyperparameters we are tuning – say, the number of trees or depth of the trees. It could be a mixture of continuous and discrete variables and the constraints would be any restriction on the search space we impose – for instance, a particular hyperparameter cannot be negative, or a particular combination of hyperparameters cannot occur. Therefore, being aware of the terms used in mathematical optimization will help us in our discussion.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark399">Even though hyperparameter tuning is a standard machine learning concept, we will quickly review three main techniques (besides manual trial and error) for doing hyperparameter tuning.</a></p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Grid search</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Grid search can be thought of as a brute-force method where we define a discrete grid over the search space, check the objective function at each point in the grid, and pick the best point in that grid. The grid is defined as a set of discrete points for each of the hyperparameters we choose to tune. Once the grid is defined, all the intersections of the grid are evaluated to search for the best objective value. If we are tuning 5 hyperparameters and the grid has 20 discrete values for each parameter, the total number of trials for a grid search would be 3,200,000 (<span class="s74">205</span>). This means training a model 3.2 million times and evaluating it. This becomes quite limiting because most modern machine learning models have many hyperparameters. For instance, LightGBM has more than 100 and out of that, at least 20 are highly impactful parameters when tuned. So, using a brute force approach such as grid search forces us to make the search space quite small so that it becomes feasible to carry out the tuning in a reasonable amount of time.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">For our case, we have defined a very small grid of just 27 trials by limiting ourselves to a really small search space. Let’s see how we do that:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import ParameterGrid</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">grid_params = {</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">&quot;num_leaves&quot;: [16, 31, 63],</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">&quot;objective&quot;: [&quot;regression&quot;, &quot;regression_l1&quot;, &quot;huber&quot;], &quot;random_state&quot;: [42],</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&quot;colsample_bytree&quot;: [0.5, 0.8, 1.0],</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">}</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">parameter_space = list(ParameterGrid(grid_params))</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">We just tune three hyperparameters (<span class="s20">num_leaves</span>, <span class="s20">objective</span>, and <span class="s20">colsample_bytree</span>), and with just three options for each parameter. Performing the grid search after this is just about looping over the parameter space and evaluating the model at each combination of hyperparameters:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">scores = []</p><p class="s28" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">for p in tqdm(parameter_space, desc=&quot;Performing Grid Search&quot;):</p><p class="s28" style="padding-top: 3pt;padding-left: 56pt;text-indent: -24pt;line-height: 131%;text-align: left;">_model_config = ModelConfig( model=LGBMRegressor(**p, verbose=-1), name=&quot;Global Meta LightGBM Tuning&quot;,</p><p class="s38" style="padding-left: 56pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># LGBM is not sensitive to normalized data</p><p class="s28" style="padding-top: 3pt;padding-left: 56pt;text-indent: 0pt;text-align: left;">normalize=False,</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"># LGBM can handle missing values</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">fill_missing=False,</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">y_pred, feat_df = train_model(</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">_model_config,</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">_feat_config, missing_value_config, train_features, train_target, test_features,</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 11pt;text-align: left;">fit_kwargs=dict(categorical_feature=cat_features),</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">scores.append(ts_utils.mae(</p><p class="s28" style="padding-top: 3pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">test_target[&#39;energy_consumption&#39;], y_pred</p><p class="s28" style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">))</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark400"/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This takes about 15 minutes to complete and gives us the best MAE of <span class="s20">0.73454</span>, which is already a great improvement from our untuned GFM.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">However, this makes us wonder whether there is an even better solution that we haven’t covered in the grid we defined. One option is to expand the grid and run the grid search again. This increases the number of trials exponentially and soon becomes infeasible.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s look at a different method where we can explore a larger search space with the same number of trials.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Random search</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Random search takes a slightly different route. In random search, we also define the search space, but instead of discretely defining specific points in the space, we define probability distributions over the range we want to explore. These probability distributions can be anything from a uniform distribution (which says any point in the range is equally likely) to Gaussian distribution (which has the familiar peak in the middle) or any other esoteric distributions such as gamma or beta distributions. As long as we can sample from the distribution, we can use it for random search. Once we define the search space, we can sample points from the distribution and evaluate each of the points to find the best hyperparameter.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">While the number of trials is a function of the defined search space for grid search, it is a user input for random search, so we get to decide how much time or computational budget we need to use for hyperparameter tuning and, because of that, we can also search over a larger search space.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark401">With this new flexibility, let’s define a larger search space for our problem and use random search:</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="444" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_671.png"/></span></p><p class="s28" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">import scipy</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">from sklearn.model_selection import ParameterSampler</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">random_search_params = {</p><p class="s38" style="padding-top: 3pt;padding-left: 37pt;text-indent: 24pt;line-height: 106%;text-align: left;"># A uniform distribution between 10 and 100, but only integers</p><p class="s28" style="padding-top: 2pt;padding-left: 61pt;text-indent: 0pt;text-align: left;">&quot;num_leaves&quot;: scipy.stats.randint(10,100),</p><p class="s38" style="padding-top: 3pt;padding-left: 60pt;text-indent: 0pt;text-align: left;"># A list of categorical string values</p><p class="s28" style="padding-top: 3pt;padding-left: 61pt;text-indent: 0pt;line-height: 131%;text-align: left;">&quot;objective&quot;: [&quot;regression&quot;, &quot;regression_l1&quot;, &quot;huber&quot;], &quot;random_state&quot;: [42],</p><p class="s38" style="padding-left: 37pt;text-indent: 24pt;line-height: 106%;text-align: left;"># List of floating point numbers between 0.3 and 1.0 with a resolution of 0.05</p><p class="s28" style="padding-top: 2pt;padding-left: 61pt;text-indent: 0pt;text-align: left;">&quot;colsample_bytree&quot;: np.arange(0.3,1.0,0.05),</p><p class="s38" style="padding-top: 3pt;padding-left: 37pt;text-indent: 24pt;line-height: 106%;text-align: left;"># List of floating point numbers between 0 and 10 with a resolution of 0.1</p><p class="s28" style="padding-top: 2pt;padding-left: 61pt;text-indent: 0pt;text-align: left;">&quot;lambda_l1&quot;:np.arange(0,10,0.1),</p><p class="s38" style="padding-top: 3pt;padding-left: 37pt;text-indent: 24pt;line-height: 106%;text-align: left;"># List of floating point numbers between 0 and 10 with a resolution of 0.1</p><p class="s28" style="padding-top: 2pt;padding-left: 61pt;text-indent: 0pt;text-align: left;">&quot;lambda_l2&quot;:np.arange(0,10,0.1)</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">}</p><p class="s38" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"># Sampling from the search space number of iterations times</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 106%;text-align: left;">parameter_space = list(ParameterSampler(random_search_params, n_iter=27, random_state=42))</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This also runs for about 15 minutes, but we have explored a larger search space. However, the best MAE reported was just <span class="s20">0.73752</span>, which is lower than with grid search. Maybe if we run the search for a greater number of iterations, we will get a better score, but that is just a shot in the dark. Ironically, that is pretty much what random search also does. It closes its eyes and throws the dart at random places on the dartboard and hopes it hits the bull’s eye.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are two terms in mathematical optimization called exploration and exploitation. Exploration ensures the optimization algorithm reaches different regions of the search space, whereas exploitation makes sure we search more in regions that are giving us better results. Random search is purely explorative and is unaware of what is happening as it evaluates different trials.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s look at one last technique that tries to balance between exploration and exploitation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark402">Bayesian optimization</a></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Bayesian optimization has a lot of similarities with random search. Both define their search space as probability distributions, and in both techniques, the user decides how many trials it needs to evaluate, but where they differ is the key advantage of Bayesian optimization. While random search is randomly sampling from the search space, Bayesian optimization is doing it intelligently. Bayesian optimization is aware of its past trials and the objective values that came out of those trials so that it can adapt future trials to exploit the regions where better objective values were seen. At a high level, it does this by building a probability model of the objective function and using it to focus trials on promising areas. The details of the algorithm are worth knowing and we have linked to a couple of resources in <i>Further reading </i>to help you along the way.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s use a popular library, <span class="s20">optuna</span>, to implement Bayesian optimization for hyperparameter tuning on the GFM we have been training.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The process is quite simple. We need to define a function that takes in a parameter called <span class="s20">trial</span>. Inside the function, we sample the different parameters we want to tune from the <span class="s20">trial </span>object, train the model, evaluate the forecast, and return the metric we want to optimize (the MAE). Let’s quickly do that:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="412" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_672.png"/></span></p><p class="s28" style="padding-left: 70pt;text-indent: -24pt;line-height: 131%;text-align: left;">def objective(trial): params = {</p><p class="s38" style="padding-left: 93pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># Sample an integer between 10 and 100</p><p class="s28" style="padding-top: 3pt;padding-left: 93pt;text-indent: 0pt;line-height: 131%;text-align: left;">&quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 10, 100), <span class="s38"># Sample a categorical value from the list provided </span>&quot;objective&quot;: trial.suggest_categorical(</p><p class="s28" style="padding-left: 118pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&quot;objective&quot;, [&quot;regression&quot;, &quot;regression_l1&quot;,</p><p class="s28" style="padding-left: 46pt;text-indent: 0pt;text-align: left;">&quot;huber&quot;]</p><p class="s28" style="padding-top: 3pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">),</p><p class="s28" style="padding-top: 3pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">&quot;random_state&quot;: [42],</p><p class="s38" style="padding-top: 3pt;padding-left: 93pt;text-indent: 0pt;text-align: left;"># Sample from a uniform distribution between 0.3 and</p><p class="s38" style="padding-left: 46pt;text-indent: 0pt;text-align: left;">1.0</p><p class="s28" style="padding-top: 3pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">&quot;colsample_bytree&quot;: trial.suggest_uniform(&quot;colsample_</p><p class="s28" style="padding-left: 46pt;text-indent: 0pt;text-align: left;">bytree&quot;, 0.3, 1.0),</p><p class="s38" style="padding-top: 3pt;padding-left: 93pt;text-indent: 0pt;line-height: 131%;text-align: left;"># Sample from a uniform distribution between 0 and 10 <span class="s28">&quot;lambda_l1&quot;: trial.suggest_uniform(&quot;lambda_l1&quot;, 0, 10), </span># Sample from a uniform distribution between 0 and 10 <span class="s28">&quot;lambda_l2&quot;: trial.suggest_uniform(&quot;lambda_l2&quot;, 0, 10),</span></p><p class="s28" style="padding-left: 70pt;text-indent: 0pt;text-align: left;">}</p><p class="s28" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">_model_config = ModelConfig(</p><p class="s38" style="padding-top: 3pt;padding-left: 93pt;text-indent: 0pt;text-align: left;"># Use the sampled params to initialize the model</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">model=LGBMRegressor(**params, verbose=-1), name=&quot;Global Meta LightGBM Tuning&quot;,</p><p class="s38" style="padding-left: 56pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># LGBM is not sensitive to normalized data</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">normalize=False,</p><p class="s38" style="padding-top: 3pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"># LGBM can handle missing values</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">fill_missing=False,</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">y_pred, feat_df = train_model(</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">_model_config,</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">_feat_config, missing_value_config, train_features, train_target, test_features,</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 11pt;text-align: left;">fit_kwargs=dict(categorical_feature=cat_features),</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">)</p><p class="s38" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># Return the MAE metric as the value</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">return ts_utils.mae(test_target[&quot;energy_consumption&quot;], y_ pred)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark403"/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Once we have defined the objective function, we need to initialize a sampler. <span class="s20">optuna </span>has many samplers such as <span class="s20">GridSampler</span>, <span class="s20">RandomSampler</span>, and <span class="s20">TPESampler</span>. For all standard use cases, <span class="s20">TPESampler </span>is the one to use. <span class="s20">GridSampler </span>does grid search and <span class="s20">RandomSampler </span>does random search. When defining a <span class="s5">Tree Parzen Estimator </span>(<span class="s5">TPE</span>) sampler, there are two parameters that we should pay attention to:</p><ul id="l101"><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">seed <span class="p">– This sets the seed for the random sampling. This makes the process reproducible.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">n_startup_trials <span class="p">– This is the number of trials that are purely exploratory. This is done to understand the search space before the exploitation kicks in. The default value is </span>10<span class="p">. We can reduce or increase this depending on how large our sample space is and how many trials we are planning to do.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The rest of the parameters are best left untouched for the most common use cases.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, we create a study, which is the object that runs the trials and stores all the details about the trials:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Create a study</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">study = optuna.create_study(direction=&quot;minimize&quot;, sampler=sampler)</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Start the optimization run</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">study.optimize(objective, n_trials=27, show_progress_bar=True)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><a name="bookmark404">Here, we define the direction of optimization, and we pass in the sampler we initialized earlier. Once the study is defined, we need to call the </a><span class="s20">optimize </span>method and pass the objective function we defined, the number of trials we need to run, and some other parameters. A full list of parameters for the <span class="s20">optimize </span><a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize" class="s140" target="_blank">method is available here –</a><a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize" class="a" target="_blank">https://optuna.readthedocs.io/en/ stable/reference/generated/optuna.study.Study.html#optuna.study. </a><span class="s27">Study.optimize</span>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This runs slightly longer, maybe because of the additional computation required to generate new trials, but still only takes about 20 minutes for the 27 trials. As expected, this has come up with another combination of hyperparameters for which the objective value is <span class="s20">0.72838 </span>(the lowest before now).</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">To fully illustrate the difference between the three, let’s compare how the three techniques spent their computational budget:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 47pt;text-indent: 0pt;text-align: left;"><span><img width="490" height="264" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_673.gif"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 48pt;text-indent: 0pt;text-align: left;">Figure 10.7 – Distribution of computational effort (grid versus random versus Bayesian optimization)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see that the Bayesian optimization has a fat tail on the lower side, indicating that it spent most of its computational budget evaluating and exploiting the optimal regions in the search space.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 147%;text-align: justify;">Let’s look at how the different trials with these techniques fared as the optimization procedure progressed. The notebook has a more detailed comparison and commentary on the three techniques.</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The bottom line is that if we have unlimited computation, grid search with a well-defined and fine- grained grid is the best option, but if we value the efficiency of our computational effort, we should go for Bayesian optimization.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see how the new parameters worked out for us:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 70pt;text-indent: 0pt;text-align: left;"><a name="bookmark405"><span><img width="421" height="150" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_674.gif"/></span></a><a name="bookmark381">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.8 – Aggregate metrics with the tuned GFM with meta-features</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We have had huge improvements in <span class="s5">MAE </span>and <span class="s5">meanMASE</span>, mostly because we were optimizing for the MAE when hyperparameter tuning. The MAE and MSE have slightly different priorities and we will spend more time on that in <i>Part 4</i>, <i>Mechanics of Forecasting</i>. The runtime also increased because the new parameters build more leaves for a tree than before and are more complex than the default parameters.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at another strategy for improving the performance of a GFM.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Partitioning</p><p class="s211" style="text-indent: 0pt;line-height: 4pt;text-align: left;">𝑃𝑃</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Out of all the strategies we have discussed so far, this is the most counter-intuitive, especially if you are coming from a standard machine learning or statistics background. Normally, we would expect the model to do well with more data, but partitioning or splitting the dataset into multiple, almost equal parts has been shown (empirically) to improve the accuracy of the model. While this has been seen empirically, why this happens is something that is still not quite clear. One explanation is that the GFMs have a slightly simpler job of learning when trained on a subset of similar entities and hence, can learn specific functions to subsets of similar entities. Another explanation for the phenomenon has been put forward by Montero-Manso and Hyndman (number 1 in <i>References</i>). They put forward that partitioning the data is another form of increasing the complexity because instead of having <span class="s146">𝑙𝑙𝑙𝑙𝑙𝑙(|𝒥𝒥|) </span>as the complexity term, we have <span class="s230">log</span><span class="s231"> (∏</span><span class="s230">|ℋ</span><span class="s211">𝒾𝒾</span><span class="s230">|)</span>, where <i>P </i>is the number of partitions. With this rationale, the LFMs are special cases where <span class="s20">P </span>is e<span class="s232">𝑖</span><span class="s211">𝑖</span>q<span class="s232">=</span><span class="s211">1</span>ual to the number of time series in the dataset.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are many ways we can partition the data, each with varying degrees of complexity.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Random partition</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The simplest method is to randomly split the dataset into <i>P</i>-equal partitions and train separate models for each partition. This method faithfully follows the explanation that Montero-Manso and Hyndman provide because we are splitting the dataset randomly, with no concern for the similarity of the different households. Let’s see how we can do that:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Define a function which splits a list into n partitions</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">def partition (list_in, n):</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">random.shuffle(list_in)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">return [list_in[i::n] for i in range(n)]</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># split the unique LCLids into partitions</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">partitions = partition(train_df.LCLid.cat.categories.tolist(), 3)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark406"/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Then, we just loop over these partitions and train separate models for each partition. The exact code can be found in the notebook. Let’s see how well the random partition does:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="523" height="196" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_675.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 53pt;text-indent: 0pt;text-align: left;">Figure 10.9 – Aggregate metrics with the tuned GFM with meta-features and random partitioning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see a decrease in <span class="s5">MAE </span>and <span class="s5">meanMASE </span>even with a random partition. There is even a decrease in runtime because the individual models are working on less data and hence, train faster.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s see another way of partitioning, keeping the similarity of different time series in mind.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Judgmental partitioning</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Judgmental partitioning is when we use some attribute of the time series to split the dataset, and this is called judgmental because usually, this depends on the judgment of the person who is working on the model. There are many ways of doing this. We can use some meta-feature, or we can use some characteristics of the time series (such as volume, variability, intermittency, or a combination of these) to partition the dataset.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s use a meta-feature called <span class="s20">Acorn_grouped </span>to partition the dataset. Again, we will just loop over the unique values in <span class="s20">Acorn_grouped </span>and train a model for each value. We will also not use <span class="s20">Acorn_grouped </span>as a feature. The exact code is in the notebook. Let’s see how well this partitioning is doing:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><a name="bookmark407"><span><img width="519" height="228" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_676.gif"/></span></a></p><p class="s37" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Figure 10.10 – Aggregate metrics with the tuned GFM with meta-features and Acorn_grouped partitioning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">This is doing even better than random partitioning. We can assume each of the partitions (<span class="s20">Affluent</span>, <span class="s20">Comfortable</span>, and <span class="s20">Adversity</span>) have some kind of similarity, which makes the learning easier, and hence, we get better accuracy.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at another way to partition the dataset, again, using similarity.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Algorithmic partitioning</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In judgmental partitioning, we were picking some meta-features or time series characteristics for partitioning the dataset. We are picking a handful of dimensions to partition the dataset because we are doing it in our minds and our mental faculties cannot handle more than two or three dimensions well, but we can see this partitioning as an unsupervised clustering approach and this approach is called algorithmic partitioning.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are two ways we can cluster time series:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Extracting features for each time series and using those features to form clusters</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Using time series clustering techniques using the <span class="s5">Dynamic Time Warping </span>(<span class="s5">DTW</span>) distance</p></li></ul><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s20">tslearn </span>is an open source Python library that has implemented a few time series clustering approaches based on the distances between time series. There is a link in <i>Further reading </i>for more information on the library and how it can be used for time series clustering.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In our example, we are going to use the first method, where we derive a few time series characteristics and use them for clustering. There are many features from statistical and temporal literature, such as autocorrelation, mean, variance, entropy, and the peak-to-peak distance, that we can extract from the time series. We can use another open source Python library called the <span class="s5">Time Series Feature Extraction Library </span>(<span class="s20">tsfel</span>) to make the process easier.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The library has many classes of features – statistical, temporal, and spectral domains – that we can choose, and the rest is handled by the library. Let’s see how we can generate these features and create a dataframe to perform clustering:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">import tsfel</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">cfg = tsfel.get_features_by_domain(&quot;statistical&quot;)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 262%;text-align: left;">cfg = {**cfg, **tsfel.get_features_by_domain(&quot;temporal&quot;)} uniq_ids = train_df.LCLid.cat.categories</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">stat_df = []</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">for id_ in tqdm(uniq_ids, desc=&quot;Calculating features for all households&quot;):</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">ts = train_df.loc[train_df.LCLid==id_, &quot;energy_ consumption&quot;]</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">res = tsfel.time_series_features_extractor(cfg, ts, verbose=False)</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">res[&#39;LCLid&#39;] = id_ stat_df.append(res)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">stat_df = pd.concat(stat_df).set_index(&quot;LCLid&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">The dataframe looks something like this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><span><img width="531" height="190" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_677.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.11 – Features extracted from different time series</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark408">Now that we have the dataframe with each row representing a time series with different features, we can ideally apply any clustering method, such as k-means, k-medoids, or HDBSCAN, and find clusters. However, in high dimensions, a lot of the distance metrics (including Euclidean) do not work as well as they are supposed to. There is a seminal paper on the topic by Charu C. Agarwal et al. from 2001 that explores the topic. When we increase the dimensionality of the space, our common sense (which conceptualizes three dimensions) does not work as well and, as a consequence, common distance metrics such as Euclidean distance do not work very well with high dimensions. We have linked to a blog summarizing the paper (in </a><i>Further reading</i>) and the paper itself (in <i>References </i>under number 5), which make the concept clearer. So, a common way of handling high dimensional clustering is by performing dimensionality reduction first and then using normal clustering.</p><p class="s5" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Principal Component Analysis <span class="p">(</span>PCA<span class="p">) had been the go-to tool in the field, but since PCA only captures and retails linear relationships while reducing the dimensions, nowadays, another class of techniques is starting to become more popular – manifold learning.</span></p><ol id="l102"><ol id="l103"><li><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s5">istributed Stochastic Neighbor Embeddings </span>(<span class="s5">t-SNE</span>) is a popular technique from this category, which is really popular for high-dimensional visualization. It is a really clever technique where we project the points from a high-dimensional space to a lower dimension, keeping the distribution of distance in the original space as close as possible to the one in lower dimensions. There is a lot to learn here, which is beyond the scope of this book. There are links in the <i>Further reading </i>section that can help you get started.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">To cut a long story short, we will be using t-SNE to reduce the dimensions of the dataset we have and then cluster the dataset with the reduced dimensions. Let’s see how we do that:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">from src.utils.data_utils import replace_array_in_dataframe</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">from sklearn.manifold import TSNE <span class="s38">#T-Distributed Stochastic Neighbor Embedding</span></p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Standardizing to make distance calculation fair</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">X_std = replace_array_in_dataframe(stat_df, StandardScaler(). fit_transform(stat_df))</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#Non-Linear Dimensionality Reduction</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">tsne = TSNE(n_components=2, perplexity=50, learning_ rate=&quot;auto&quot;, init=&quot;pca&quot;, random_state=42, metric=&quot;cosine&quot;, square_distances=True)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">X_tsne = tsne.fit_transform(X_std.values)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Clustering reduced dimensions into 3 clusters</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">kmeans = KMeans(n_clusters=3, random_state=42).fit(X_tsne) cluster_df = pd.Series(kmeans.labels_, index=X_std.index)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Since we reduced the dimensions to two, we can also visualize the clusters formed:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 45pt;text-indent: 0pt;text-align: left;"><span><img width="497" height="271" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_678.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.12 – Clustered time series after t-SNE dimensionality reduction</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have three well-defined clusters formed and now we are just going to use these clusters to train a model for each cluster. As usual, we loop over the three clusters and train the models. Let’s see how we did so:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span><img width="518" height="236" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_679.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.13 – Aggregate metrics with the tuned GFM with meta-features and clustered partitioning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark409">It looks as though this is the best MAE we have seen in all our experiments, but the three partition techniques have very similar MAEs. We can’t see whether any one is better than the other just by looking at a single hold-out set. For good measure, we can run these forecasts with a test dataset using the </a><span class="s20">01a-Global Forecasting Models-ML-test.ipynb </span>notebook in the <span class="s20">chapter08 </span>folder. Let’s see how the aggregate metrics are on the test dataset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><img width="534" height="269" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_680.jpg"/></span></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 10.14 – Aggregate metrics on test data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">As expected, the clustered partition is still the methodology that performs the best in this case.</p><p class="s4" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark286" class="s140">In </a>Chapter 8<span class="p">, </span>Forecasting Time Series with Machine Learning Models<span class="p">, it took us 8 minutes and 20 seconds to train an LFM for all the households in our dataset. Now, with the GFM paradigm, we finished training a model in 57 seconds (in the worst-case scenario). That’s 777% less training time and this comes with an 8.78% decrease in the MAE.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We chose to do these experiments with LightGBM. This does not mean that LightGBM or any other gradient-boosting model is the only choice for GFMs, but they are a pretty good default. A well-tuned gradient-boosted trees model is a very difficult baseline to beat, but as always in machine learning, we should check what works best using well-defined experiments.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although there are no hard and fast rules or cutoffs for when a GFM makes more sense than an LFM, as the number of time series in a dataset increases, the GFM becomes more favorable, both from the perspective of accuracy and computation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark410">Bonus – interpretability</a><a name="bookmark384">&zwnj;</a><a name="bookmark383">&zwnj;</a><a name="bookmark382">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Interpretability can be defined as the degree to which a human can understand the cause of a decision. In machine learning and artificial intelligence, that translates to the degree to which someone can understand the how and why of an algorithm and its predictions. There are two ways to look at interpretability – <span class="s5">transparency </span>and <span class="s5">post hoc interpretation</span>.</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Transparency <span class="p">is when the model is inherently simple and can be simulated or thought about using human cognition. A human should be able to fully understand the inputs and the process a model takes to convert these inputs to outputs. This is a very stringent condition that almost none of the model machine learning or deep learning models satisfy.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is where <i>post hoc interpretation </i>techniques shine. There are a wide variety of techniques that use the inputs and outputs of a model to understand why a model has made the predictions it has.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are many popular techniques such as permutation feature importance, Shapley values, and LIME. All of these are general-purpose interpretation techniques that can be used on any machine learning model and that includes the GFMs we were discussing.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">For more extensive coverage of such techniques, I have included a few links in <i>Further reading</i>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Congratulations on finishing the second part of the book! It has been quite an intensive part where we went over quite a bit of theory and practical lessons, and we hope you are now comfortable with using machine learning for time series forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">To round up the second part of the book nicely, we explored GFMs in detail and saw why they are important and why they are an exciting new direction in time series forecasting. We saw how we can use a GFM using machine learning models and also reviewed many techniques to make GFMs perform better, most of which are quite frequently used in competitions and industry use cases alike. Now that we have wrapped up the machine learning section of the book, we will move on to a specific type of machine learning that has become well-known over the past few years – <span class="s5">deep learning </span>– in the next chapter.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The following are sources that we have referenced throughout the chapter:</p><ol id="l104"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Montero-Manso, P., Hyndman, R.J. (2020), <i>Principles and algorithms for forecasting groups of time series: Locality and globality</i><a href="https://arxiv.org/abs/2008.00444" class="s140" target="_blank">. arXiv:2008.00444[cs.LG]: </a><a href="https://arxiv.org/abs/2008.00444" class="a" target="_blank">https://arxiv.org/ </a><span class="s27">abs/2008.00444</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Micci-Barreca, D. (2001), <i>A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems</i>. <i>SIGKDD Explor. Newsl</i><a href="https://doi.org/10.1145/507533.507538" class="s140" target="_blank">. 3, 1 (July 2001), 27–32: </a><a href="https://doi.org/10.1145/507533.507538" class="a" target="_blank">https:// </a><span class="s27">doi.org/10.1145/507533.507538</span>.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark411">Further reading 257</a><a name="bookmark385">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_681.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 55pt;text-indent: -18pt;text-align: justify;">Fisher, W. D. (1958). <i>On Grouping for Maximum Homogeneity</i>. <i>Journal of the American Statistical Association</i><a href="https://doi.org/10.2307/2281952" class="s140" target="_blank">, 53(284), 789–798: </a><span class="s27">https://doi.org/10.2307/2281952</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Fisher, W.D. (1958), <i>A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems</i>. <i>SIGKDD Explor. Newsl. </i>3, 1 (July 2001), 27–32.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Aggarwal, C. C., Hinneburg, A., and Keim, D. A. (2001). <i>On the Surprising Behavior of Distance Metrics in High Dimensional Spaces. </i>In <i>Proceedings of the 8th International Conference on Database Theory </i><a href="https://dl.acm.org/doi/10.5555/645504.656414" class="s140" target="_blank">(ICDT ‘01). Springer-Verlag, Berlin, Heidelberg, 420–434: </a><a href="https://dl.acm.org/doi/10.5555/645504.656414" class="a" target="_blank">https://dl.acm.org/ </a><span class="s27">doi/10.5555/645504.656414</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Oreshkin, B. N., Carpov D., Chapados N., and Bengio Y. (2020). <i>N-BEATS: Neural basis expansion analysis for interpretable time series forecasting</i>. <i>8th International Conference on Learning Representations, ICLR 2020</i><a href="https://openreview.net/forum?id=r1ecqn4YwB" class="s140" target="_blank">: </a><span class="s27">https://openreview.net/forum?id=r1ecqn4YwB</span>.</p></li></ol></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The following are a few resources that you can explore for a detailed study:</p><ul id="l105"><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Learning From Data <a href="https://work.caltech.edu/lectures.html" class="s140" target="_blank">by Yaser Abu-Mostafa: </a><a href="https://work.caltech.edu/lectures.html" class="a" target="_blank">https://work.caltech.edu/lectures. </a><span class="s27">html</span><span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Curse of Dimensionality <a href="https://www.youtube.com/watch?v=OyPcbeiwps8" class="s140" target="_blank">– Georgia Tech: </a><a href="https://www.youtube.com/watch?v=OyPcbeiwps8" class="a" target="_blank">https://www.youtube.com/ </a><span class="s27">watch?v=OyPcbeiwps8</span><span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Dummy Variable Trap<a href="https://www.learndatasci.com/glossary/dummy-variable-trap/" class="s140" target="_blank">: </a><a href="https://www.learndatasci.com/glossary/dummy-variable-trap/" class="a" target="_blank">https://www.learndatasci.com/glossary/dummy- </a><span class="s27">variable-trap/</span><span class="p">.</span></p></li><li><p class="s27" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a href="https://pytorch-tabular.readthedocs.io/en/latest/tutorials/03-Extracting%20and%20Using%20Learned%20Embeddings/" class="s140" target="_blank">Using deep learning to learn categorical embeddings: </a><a href="https://pytorch-tabular.readthedocs.io/en/latest/tutorials/03-Extracting%20and%20Using%20Learned%20Embeddings/" class="a" target="_blank">https://pytorch-tabular. readthedocs.io/en/latest/tutorials/03-Extracting%20and%20Using%20 </a>Learned%20Embeddings/<span class="p">.</span></p></li><li><p class="s27" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a href="https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic" class="s140" target="_blank">Handling categorical features – CatBoost: </a><a href="https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic" class="a" target="_blank">https://catboost.ai/en/docs/concepts/ </a>algorithm-main-stages_cat-to-numberic<span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Exploring Bayesian Optimization <a href="https://distill.pub/2020/bayesian-optimization/" class="s140" target="_blank">– from Distil.pub: </a><a href="https://distill.pub/2020/bayesian-optimization/" class="a" target="_blank">https://distill.pub/2020/ </a><span class="s27">bayesian-optimization/</span><span class="p">.</span></p></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -13pt;text-align: justify;">Frazier, P.I. (2018). <i>A Tutorial on Bayesian Optimization</i><a href="https://arxiv.org/abs/1807.02811" class="s140" target="_blank">. arXiv:1807.02811 [stat.ML]: </a><a href="https://arxiv.org/abs/1807.02811" class="a" target="_blank">https:// </a><span class="s27">arxiv.org/abs/1807.02811</span>.</p></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -13pt;text-align: justify;">Time series clustering using <span class="s20">tslearn</span><a href="https://tslearn.readthedocs.io/en/stable/user_guide/clustering.html" class="s140" target="_blank">: </a><a href="https://tslearn.readthedocs.io/en/stable/user_guide/clustering.html" class="a" target="_blank">https://tslearn.readthedocs.io/en/ </a><span class="s27">stable/user_guide/clustering.html</span>.</p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 54pt;text-indent: -13pt;text-align: justify;">The Surprising Behaviour of Distance Metrics in High Dimensions<a href="https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6" class="s140" target="_blank">: </a><a href="https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6" class="a" target="_blank">https:// towardsdatascience.com/the-surprising-behaviour-of-distance- </a><span class="s27">metrics-in-high-dimensions-c2cb72779ea6</span><span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l106"><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">An illustrated introduction to the t-SNE algorithm<a href="https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/" class="s140" target="_blank">: </a><a href="https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/" class="a" target="_blank">https://www.oreilly.com/content/ </a><span class="s27">an-illustrated-introduction-to-the-t-sne-algorithm/</span><span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">How to Use t-SNE Effectively <a href="https://distill.pub/2016/misread-tsne/" class="s140" target="_blank">– from Distil.pub: </a><a href="https://distill.pub/2016/misread-tsne/" class="a" target="_blank">https://distill.pub/2016/misread- </a><span class="s27">tsne/</span><span class="p">.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">The NFLT: <span class="s27">https://en.wikipedia.org/wiki/No_free_lunch_in_search_ and_optimization</span>.</p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Interpretability: Cracking open the black box <a href="https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-i/" class="s140" target="_blank">– parts I, II, and III by Manu Joseph: </a><a href="https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-i/" class="a" target="_blank">https:// deep-and-shallow.com/2019/11/13/interpretability-cracking-open- </a><span class="s27">the-black-box-part-i/</span><span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Interpretable Machine Learning: A Guide for Making Black Box Models Explainable <a href="https://christophm.github.io/interpretable-ml-book/" class="s140" target="_blank">by Christoph Molnar: </a><span class="s27">https://christophm.github.io/interpretable-ml-book/</span><span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s31" style="padding-top: 13pt;padding-left: 50pt;text-indent: 279pt;line-height: 114%;text-align: left;"><a name="bookmark412">Part 3 – Deep Learning for Time Series</a><a name="bookmark413">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s32" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this part, we focus on the exciting field of deep learning to tackle time series problems. This part starts with a good introduction of the necessary concepts and slowly builds up to different specialized architectures that are suited to handle time series data. It also talks about global models in deep learning and some strategies to make them work better.</p><p class="s32" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This part comprises the following chapters:</p></li></ul></li><li><p class="s34" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 11<span class="s32">, </span>Introduction to Deep Learning</p></li><li><p class="s34" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><a href="#bookmark454" class="s33">Chapter </a>12<span class="s32">, </span>Building Blocks of Deep Learning for Time Series</p></li><li><p class="s34" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 13<span class="s32">, </span>Common Modeling Patterns for Time Series</p></li><li><p class="s34" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 14<span class="s32">, </span>Attention and Transformers for Time Series</p></li><li><p class="s34" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><a href="#bookmark561" class="s33">Chapter </a>15<span class="s32">, </span>Strategies for Global Deep Learning Forecasting Models</p></li><li><p class="s34" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 16<span class="s32">, </span>Specialized Deep Learning Architectures for Forecasting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark414">11</a><a name="bookmark415">&zwnj;</a><a name="bookmark418">&zwnj;</a><a name="bookmark417">&zwnj;</a><a name="bookmark416">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">Introduction to Deep Learning</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous chapter, we understood how to use modern machine learning models to tackle time series forecasting. Now, let’s focus our attention on a subfield of machine learning that has shown a lot of promise in the last few years – <span class="s5">deep learning</span>. We will be trying to demystify deep learning and go into why it is popular nowadays. We will also break down deep learning into major components and learn about the workhorse behind deep learning – gradient descent.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">What is deep learning and why now?</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Components of a deep learning system</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Representation learning</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Linear layers and activation functions</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Gradient descent</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p class="s27" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter11" class="s140" target="_blank">The associated code for the chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter11" class="a" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter11<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">What is deep learning and why now?</p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">In </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, we talked about machine learning and borrowed a definition from Arthur Samuel: “</span>Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed<span class="p">.” And we further saw how we can learn useful functions from data using machine learning. Deep learning is a subfield of this same field of study. The objective of deep learning is also to learn useful functions from data, but with a few specifications on how it does that.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark432">Before we talk about what is special about deep learning, let’s answer another question first. Why are we talking about this subfield of machine learning as a separate topic? The answer to that lies in the unreasonable effectiveness of deep learning methods in countless applications. Deep learning has taken the world of machine learning by storm, overthrowing state-of-the-art systems across types of data such as images, videos, text, and so on. If you remember the speech recognition systems on phones a decade ago, they were more meme-worthy than really useful. But today, you can say </a><i>Hey Google, play Pink Floyd </i>and <i>Comfortably Numb </i>will start playing on your phone or speakers. Multiple deep learning systems made this process possible in a smooth way. The voice assistant in your phone, self-driving cars, web search, language translation… the list of applications of deep learning in our day-to-day lives just keeps on going.<a name="bookmark419">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">By now, you might be wondering what this new technology called deep learning is all about, right? Deep learning is not a new technology. The origins of deep learning can be traced way back to the late 1940s and early 1950s. It only appears to be new because of the recent surge in popularity of the field.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s quickly see why deep learning is suddenly popular.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Why now?</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are two main reasons why deep learning has gained a lot of ground in the last two decades:</p><ul id="l107"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Increase in compute availability</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Increase in data availability</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s discuss the preceding points in detail in the following sections.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Increase in compute availability</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Back in 1960, Frank Rosenblatt wrote a paper about a three-layer neural network and stated that it went a long way in demonstrating the ability of neural networks as a pattern-recognizing device. But in the same paper, he noted the burden on a digital computer (of the 1960s) was too great as we increase the number of connections. However, in the decades that followed, computer hardware showed close to 50,000 times more improvement, which provided a good boost to neural networks and deep learning. Although it was still not enough as neural networks were still not considered to be good enough for <i>large-scale applications</i>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is when a particular type of hardware, which was initially developed for gaming, came to the rescue – GPUs. It’s not entirely clear who started using GPUs for deep learning. Kyoung-Su Oh and Keechul Jung published a paper titled <i>GPU implementation of neural networks </i>back in 2004, which seems to be the first to show massive speed-ups in using GPUs for deep learning. One of the earliest and more popular research papers on the topic came from Rajat Raina, Anand Madhavan, and Andrew Ng, who published a paper titled <i>Large-scale deep unsupervised learning using graphics processors </i>back in 2009. It showed the effectiveness of GPUs for deep learning.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="101" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_682.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The research papers <i>GPU implementation of neural networks</i>, <i>Large-scale deep unsupervise learning using graphics processors</i>, and <i>ImageNet Classification with Deep Convolutional Neur Networks </i>are cited in the <i>References </i>section under 1, 2, and 3, respectively.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="text-indent: 0pt;line-height: 89%;text-align: left;">d al</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark433">Although many groups led by LeCun, Schmidhuber, Bengio, and so on were playing around with using GPUs, the turning point really came when Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton used a GPU-based deep learning system that outperformed all the other competing technologies in an image recognition contest called the </a><i>ImageNet Large Scale Visual Recognition Challenge 2012</i>. The introduction of GPUs provided the much-needed boost to the widespread use of deep learning and accelerated the progress in the field.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Increase in data availability</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In addition to the skyrocketing compute capability, the other main factor that helped deep learning is the sheer increase in data. As the world became more and more digitized, the amount of data that we generate increased drastically. Tables that had hundreds and thousands of rows now exploded into millions and billions of rows, and the ever-decreasing cost of storage helped this explosion of data collection.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And why would an increase in data availability help deep learning? This lies in the way deep learning works. Deep learning is quite data-hungry and needs large amounts of data to learn good models. Therefore, if we keep increasing the data that we provide to a deep learning model, the model will be able to learn better and better functions. But the same can’t be said for the traditional machine learning models. Let’s cement this learning with a chart that Andrew Ng, a world-renowned ML educator and an adjunct professor at Stanford, popularized in his famous machine learning course – <i>Machine Learning by Stanford University </i>in Coursera (<i>Figure 11.1</i>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 108pt;text-indent: 0pt;text-align: left;"><span><img width="317" height="208" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_683.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 11.1 – Deep learning versus traditional machine learning as we increase the data size</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark434">In </a><i>Figure 11.1</i>, which was popularized by Andrew Ng, we can see that as we increase the data size, traditional machine learning hits a plateau and won’t improve anymore.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">It has been proven empirically that there are significant benefits to the overparameterization of a deep learning model. <span class="s5">Overparameterization </span>means that there are more parameters in the model than the number of data points available to train. In classical statistics, this is a big no-no because, under this scenario, the model invariably overfits. But deep learning seems to flaunt this rule with ease. One of the examples of overparameterization is the current state-of-the-art image recognition system, <span class="s5">NoisyStudent</span>. It has 480 million parameters, but it was trained on <i>ImageNet </i>with 1.2 million data points.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">It has been argued that the way deep learning models are trained (stochastic gradient descent, which we will be explaining soon) is the key because it has a regularizing effect. In a research paper titled <i>The Computational Limits of Deep Learning</i>, Niel C. Thompson and others tried to illustrate this using a simple experiment. They set up a dataset with 1,000 features, but only 10 of them had any signal in them. Then they tried to learn four models based on the dataset using varying dataset sizes:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Oracle model <span class="p">– A model that uses the exact 10 parameters that have any signal in them.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Expert model <span class="p">– A model that uses 9 out of 10 significant parameters.</span></p></li><li><p class="s5" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Flexible model <span class="p">– A model that uses all 1,000 parameters.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s5">Regularized model </span>– A model that uses all 1,000 parameters, but is now a regularized (<span class="s5">lasso</span><a href="#bookmark286" class="s140">) model. (We covered regularization back in </a><a href="#bookmark286" class="s21">Chapter </a><i>8</i>, <i>Forecasting Time Series with Machine Learning Models</i>.)</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see <i>Figure 11.2 </i>from the research paper with the study:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 118pt;text-indent: 0pt;text-align: left;"><span><img width="312" height="261" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_684.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 11.2 – The chart shows how different models perform under different sizes of data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark435">The chart has a number of data points used on the </a><i>x </i>axis, and the performance (<i>-log(Mean Squared Error)</i>) on the <i>y </i>axis. The different colored lines show the different types of models. The regularized model (which is a proxy for deep learning models) keeps improving as we give the model more and more data, whereas the expert model (a proxy for machine learning model) plateaus. This strengthens the concept Andrew Ng popularized once more – with more data, deep learning starts to outperform traditional machine learning.<a name="bookmark420">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">A lot more factors, apart from compute and data availability, have contributed to the success of deep learning. Sara Hooker, in her essay <i>The Hardware Lottery </i>(#9 in <i>References</i>), talks about how an idea wins not necessarily because it is superior to other ideas, but because it is suited to the software and hardware available at the time. And once a research direction gets the lottery, it snowballs because more funding and big research organizations get behind that idea and it eventually becomes the most prominent idea in the space of ideas.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We have talked about deep learning for some time but have still not understood what it is. Let’s do that now.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">What is deep learning?</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There is no single definition of deep learning because it means slightly different things to different people. However, a large majority of people agree on one thing: a model is called deep learning when it involves automatic feature learning from raw data. As Yoshua Bengio (a Turing Award winner and one of the <i>godfathers </i>of AI) explains it in his 2021 paper titled <i>Deep Learning of Representations for Unsupervised and Transfer Learning</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 65pt;text-indent: 0pt;line-height: 89%;text-align: center;">“Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features.”</p><p style="padding-top: 4pt;text-indent: 0pt;text-align: center;">Peter Norvig, the Director of Research at Google, has a similar but simpler definition:</p><p class="s4" style="padding-top: 9pt;padding-left: 131pt;text-indent: -59pt;line-height: 89%;text-align: justify;">“A kind of learning where the representation you form have (sic) several levels of abstraction, rather than a direct input to output.”</p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Another key feature of deep learning a lot of people agree upon is compositionality. Yann LeCun, a Turing Award winner and another one of the <i>godfathers </i>of AI, has a slightly more complex, but more exact definition of deep learning:</p><p class="s4" style="padding-top: 9pt;padding-left: 67pt;text-indent: 0pt;line-height: 89%;text-align: center;">“DL is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods.”</p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The key points we would like to highlight here are as follows:</p></li></ul></li><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Assembling parametrized modules <span class="p">– This refers to the compositionality of deep learning. Deep learning systems, as we will shortly see, are composed of a few submodules with a few parameters (some without) assembled into a graph-like structure.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l108"><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a name="bookmark436">Optimizing it with gradient-based methods </a><span class="p">– Although having gradient-based learning as a sufficient criterion for deep learning is not widely accepted, we can still see empirically that, today, most successful deep learning systems are trained using gradient-based methods. (If you are not aware of what a gradient-based optimization method is, don’t worry. We will be covering it soon in this chapter.)</span><a name="bookmark421">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">If you have read anything about deep learning before, you may have seen neural networks and deep learning used together, or interchangeably. But we haven’t talked about neural networks till now. Before we do that, let’s look at a fundamental unit of any neural network.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Perceptron – the first neural network</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">A lot of what we call deep learning and neural networks are deeply influenced by the human brain and its inner workings. The human desire to create intelligent beings like themselves was manifested as early as back in Greek mythology (Galatea and Pandora). And owing to this desire, humans have studied and looked for inspiration from human anatomy for years. One of the organs of the human body that has been studied intensely is the brain because it is the center of intelligence, creativity, and everything else that makes a human.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Even though we still don’t know a lot about the brain, we do know a bit about it, and we use that little information to design artificial systems. The fundamental unit of a human brain is something we call a <span class="s5">neuron</span>, shown here:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="524" height="267" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_685.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 11.3 – A biological neuron</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Many of you might have come across this in biology or in the context of machine learning as well. But let’s refresh this anyway. The biological neuron has the following parts:</p></li></ul></li><li><p class="s5" style="padding-top: 9pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Dendrites <span class="p">are branched extensions of the nerve cell that collect inputs from surrounding cells or other neurons.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Soma<span class="p">, or the cell body, collects these inputs, joins them, and is passed on.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Axon hillock <span class="p">connects the soma to the axon, and it controls the firing of the neuron. If the strength of a signal exceeds a threshold, the axon hillock fires an electrical signal through the axon.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Axon <span class="p">is the fiber that connects the soma to the nerve endings. It is the axon’s duty to pass on the electrical signal to the endpoints.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Synapses <span class="p">are the end points of the nerve cell and transmit the signal to other nerve cells.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">McCulloch and Pitts (1943) were the first to design a mathematical model for the biological neuron. But the McCulloch-Pitts model had a few limitations:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">It only accepted binary variables.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">It considered all input variables equally important.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">There was only one parameter, a threshold, which was not learnable.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_686.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The original research paper for Frank Rosenblatt’s <i>Perceptron </i>is cited in <i>References </i>und reference number 5.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">er</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">In 1957, Frank Rosenblatt generalized the McCulloch-Pitts model and made it a full model whose parameters could be learned.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s understand the Perceptron in detail because it is the fundamental building block of all neural networks:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 42pt;text-indent: 0pt;text-align: left;"><span><img width="512" height="351" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_687.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">Figure 11.4 – Perceptron</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;">As we see from <i>Figure 11.4</i>, the Perceptron has the following components:</p><ul id="l109"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Inputs <span class="p">– These are the real-valued inputs that are fed to a Perceptron. This is like the dendrites in neurons that collect the input.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Weighted sum <span class="p">– Each input is multiplied by a corresponding weight and summed up. The weights determine the importance of each input in determining the outcome.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Non-linearity <span class="p">– The weighted sum goes through a non-linear function. For the original Perceptron, it was a step function with a threshold activation. The output would be positive or negative based on the weighted sum and the threshold of the unit. Modern-day Perceptrons and neural networks use different kinds of activation functions, but we will see that later on.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">We can write the Perceptron in the mathematical form as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 108pt;text-indent: 0pt;text-align: left;"><a name="bookmark437"><span><img width="308" height="263" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_688.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 11.5 – Perceptron – a math perspective</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">As shown in <i>Figure 11.5</i>, the Perceptron output is defined by the weighted sum of inputs, which is passed in through a non-linear function. Now, we can think of this using linear algebra as well. This is an important perspective for two reasons:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">The linear algebra perspective will help you understand neural networks faster.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">It will also make the whole thing feasible because matrix multiplications are something that our modern-day computers and GPUs are really good at. Without linear algebra, multiplying these inputs with corresponding weights would require us to loop through the inputs, and it quickly becomes infeasible.</p></li></ul><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Linear algebra intuition recap</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s take a look at a couple of concepts as a refresher.</p><p class="s233" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Vectors and vector spaces</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">At the superficial level, a <span class="s5">vector </span>is an array of numbers. But in linear algebra, a vector is an entity that has both magnitude and direction. Let’s take an example to elucidate:</p><p class="s41" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 8pt;text-align: center;">5</p><p class="s41" style="text-indent: 0pt;line-height: 9pt;text-align: right;">𝐴𝐴 = [ ]</p><p class="s41" style="text-indent: 0pt;line-height: 11pt;text-align: right;">0</p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can see that this is an array of numbers. But if we plot this point in the two-dimensional coordinate space, we get a point. And if we draw a line from the origin to this point, we will get an entity with direction and magnitude. This is a vector.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark438">The two-dimensional coordinate space is called a </a><span class="s5">vector space</span>. A two-dimensional vector space, informally, is all the possible vectors with two entries. And extending it to <i>n</i>-dimensions, an <i>n</i>-dimensional vector space is all the possible vectors with <i>n </i>entries.<a name="bookmark422">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The final intuition I want to leave with you is this: <i>a vector is a point in the n-dimensional vector space</i>.</p><p class="s233" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Matrices and transformations</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Again, at the superficial level, a <span class="s5">matrix </span>is a rectangular arrangement of numbers that looks like this:</p><p class="s146" style="padding-top: 7pt;padding-left: 173pt;text-indent: 0pt;line-height: 7pt;text-align: center;">1    0</p><p class="s146" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑀𝑀 = [ ]</p><p class="s146" style="text-indent: 0pt;line-height: 9pt;text-align: right;">0 0</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Matrices have many uses but the one intuition that is most relevant for us is that a matrix specifies a linear transformation of the vector space it resides in. When we multiply a vector with a matrix, we are essentially transforming the vector, and the values and dimensions of the matrix define the kind of transformation that happens. Depending on the content of the matrix, it does <i>rotation, reflection, scaling, shearing</i>, and so on.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have included a notebook in the <span class="s20">chapter11 </span>folder titled <span class="s20">01-Linear Algebra Intuition. ipynb</span>, which explores matrix multiplication as a transformation. We also apply these transformation matrices to vector spaces to develop intuition on how matrix multiplication can rotate and warp the vector spaces.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">I highly suggest heading over to the <i>Further reading </i>section where we have given a few resources to get started and solidify necessary intuition.</p><p class="s234" style="text-indent: 0pt;line-height: 4pt;text-align: left;">𝑚𝑚</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">If we consider the inputs as vectors in the feature space (vector space with <i>m</i>-dimensions), the term</p><p class="s234" style="text-indent: 0pt;line-height: 4pt;text-align: left;">𝑖𝑖=1</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s235">∑</span><span class="s220"> 𝑥𝑥</span><span class="s234">𝑖𝑖 </span><span class="s235">𝑤</span><span class="s220">𝑤</span><span class="s234">𝑖𝑖   </span>is nothing but a linear combination of input vectors. We can convert this to vector dot products by <span class="s75">XTW</span>. We can include the bias also in there by adding an additional dummy input with a fixed value of 1 and adding <span class="s162">𝑤</span><span class="s90">𝑤</span><span class="s236">0</span><span class="s127"> </span>to the <span class="s50">𝑊𝑊 </span>vector. This is what is shown in <i>Figure 11.5 </i>as the vector representation.</p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have had an introduction to deep learning, let us recall one of the aspects of deep learning we discussed earlier – compositionality – and explore it a bit more deeply in the next section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Components of a deep learning system</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let us recall Yann LeCun’s definition of deep learning:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 83pt;text-indent: 0pt;line-height: 89%;text-align: center;">“Deep learning is a methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods.”</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark439">The core idea here is that deep learning is an extremely modular system. Deep learning is not just one model, but rather a language to express any model in terms of a few parametrized modules with these specific properties:</a></p><ul id="l110"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">It should be able to produce an output from a given input through a series of computations.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">If the desired output is given, they should be able to pass on information to its inputs on how to change, to arrive at the desired output. For instance, if the output is lower than what is desired, the module should be able to tell its inputs to change in some direction so that the output becomes closer to the desired one.</p></li></ul><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The more mathematically inclined may have figured out the connection to the second point of differentiation. And you would be correct. To optimize these kinds of systems, we predominantly use gradient-based optimization methods. Therefore, condensing the two properties into one, we can say that these parameterized modules should be <i>differentiable functions</i>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s take the help of a visual to aid further discussion:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 31pt;text-indent: 0pt;text-align: left;"><span><img width="514" height="251" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_689.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 11.6 – A deep learning system</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">As shown in <i>Figure 11.6</i>, deep learning can be thought of as a system that takes in raw input data through a series of linear and non-linear transforms to provide us with an output. It also can adjust its internal parameters to make the output as close as possible to the desired output through learning. To make the diagram simpler, we have chosen a paradigm that fits most of the popular deep learning systems. It all starts with raw input data. The raw input data goes through <i>N </i>blocks of linear and non-linear functions that do representation learning. Let’s explore this block in some detail.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark440">Representation learning</a><a name="bookmark423">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s5">Representation learning</span>, informally, learns the best features by which we can make the problem linearly separable. Linearly separable means when we can separate the different classes (in a classification problem) with a straight line (<i>Figure 11.7</i>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="526" height="236" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_690.jpg"/></span></p><p class="s37" style="padding-top: 6pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">Figure 11.7 – Transforming non-linearly separable data into linearly separable using a function, <span class="s90">𝚽𝚽</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The <i>representation learning </i>block in <i>Figure 11.6 </i>may have multiple linear and non-linear functions stacked on top of each other and the overall function of the block is to learn a function, <span class="s137">𝚽𝚽</span>, which transforms the raw input into good features that make the problem linearly separable.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another way to look at this is through the lens of linear algebra. As we explored earlier in the chapter, matrix multiplication can be thought of as a linear transformation of vectors. And if we extend that intuition to the vector spaces, we can see that matrix multiplication warps the vector space in some way or another. And when we stack multiple linear and non-linear transformations on top of each other, we are essentially warping, twisting, and squeezing the input vector space (with the features) into another space. When we are asking a parameterized system to warp the input space (pixels of images) in such a way as to perform a particular task (such as the classification of dogs versus cats), the representation learning block learns the right transformations, which makes the task (separating cats from dogs) easier.</p><p class="s27" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://www.youtube.com/watch?v=5xYEa9PPDTE" class="s140" target="_blank">I have created a video illustrating this because nothing establishes an intuition better than a video of what is happening. I’ve taken a sample dataset that is not linearly separable, trained a neural network on the problem to classify, and then visualized how the input space was transformed by the model into a linearly separable representation. You can find the video here: </a><a href="https://www.youtube.com/watch?v=5xYEa9PPDTE" class="a" target="_blank">https://www.youtube.</a> com/watch?v=5xYEa9PPDTE<span class="p">.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look inside the representation learning block. We can see there is a linear transformation and a non-linear activation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark441">Linear transformation</a><a name="bookmark425">&zwnj;</a><a name="bookmark424">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Linear transformations are just transformations that are applied to the vector space. When we say linear transformation in a neural network context, we actually mean affine transformations.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">A linear transformation fixes the origin while applying the transformation, but an affine transformation doesn’t. Rotation, reflection, scaling, and so on are purely linear transformations because the origin won’t change while we do this. But something like a translation, which moves the vector space, is an affine transformation. Therefore <span class="s146">A ⋅ 𝑋𝑋𝑇𝑇 </span>is a linear transformation, but <span class="s146">A ⋅ 𝑋𝑋𝑇𝑇 + b </span>is an affine transformation.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">So, linear transformations are simply matrix multiplications that transform the input vector space, and this is at the heart of any neural network or deep learning system today.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">What happens if we stack linear transformations on top of each other? For instance, we first multiply the input, <i>X</i>, with a transformation matrix, <i>A</i>, and then multiply the results with another transformation matrix, <i>B</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑇𝑇  =  𝐵𝐵 ⋅ (𝐴𝐴 ⋅ 𝑋𝑋)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">By the associative property (which is applicable for linear algebra as well), we can rewrite this equation as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s146" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑇𝑇  =  (𝐵𝐵 ⋅ 𝐴𝐴) ⋅ 𝑋𝑋</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Generalizing this to a stack of <i>N </i>transformation matrices, we can see that it all works out to be a single linear transformation. This kind of defeats the purpose of stacking <i>N </i>layers, doesn’t it?</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This is where the non-linearity becomes essential and we introduce non-linearities by using a non-linear function, which we call activation functions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Activation functions</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s5">Activation functions </span>are non-linear differentiable functions. In a biological neuron, the axon hillock decides whether to fire a signal based on the inputs. The activation functions serve a similar function and are key to the neural network’s ability to model non-linear data. Or in other words, activation functions are key in neural networks’ ability to transform input vector space (which is linearly inseparable) to a linearly separable vector space, informally. To <i>unwarp </i>a space such that linearly inseparable points become linearly separable, we need to have non-linear transformations.</p><p class="s27" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://www.youtube.com/watch?v=z-nV8oBpH2w" class="s140" target="_blank">We repeated the same experiment we did in the last section, where we visualized the trained transformation of a neural network on the input vector space, but this time without any non-linearities. The resulting video can be found here: </a>https://www.youtube.com/watch?v=z-nV8oBpH2w<span class="p">. The best transformation that the model learned is just not sufficient and the points are still linearly inseparable.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark442">Theoretically, an activation function can be any non-linear differentiable (differentiable almost everywhere, to be exact) function. But over the course of time, there are a few non-linear functions that are popularly used as activation functions. Let’s look at a few of them.</a></p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Sigmoid</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Sigmoid is one of the most common activation functions around, and probably one of the oldest. It is also known as the logistic function. When we discussed Perceptron, we mentioned a step (also called <i>heavyside </i>in literature) function as the activation function. The step function is not a continuous function and hence <i>is not </i>differentiable everywhere. A very close substitute is the sigmoid function.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">It is defined as follows:</p><p class="s137" style="padding-top: 3pt;padding-left: 43pt;text-indent: 0pt;line-height: 10pt;text-align: center;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="48" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_691.png"/></span></p><p class="s137" style="padding-left: 173pt;text-indent: 0pt;line-height: 81%;text-align: center;">𝑔𝑔(𝑥𝑥) = <span class="s237">1 + </span>𝑒𝑒<span class="s238">−𝑥𝑥</span></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Sigmoid is a continuous function and therefore <i>is </i>differentiable everywhere. The derivative is also computationally simpler to calculate. Because of these properties of the sigmoid, it was adopted widely in the early days of deep learning as a standard activation function.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see what a sigmoid function looks like and how it transforms a vector space:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="525" height="166" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_692.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 37pt;text-indent: 1pt;text-align: justify;">Figure 11.8 – Sigmoid activation function (left) and original, and activated vector space (middle and right)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The sigmoid function squashes the input between 0 and 1 as seen in <i>Figure 11.8 (left)</i>. We can observe the same phenomenon in the vector space. One of the drawbacks of the sigmoid function is that the gradients tend to zero on the flat portions of the sigmoid. When a neuron approaches this area in the function, the gradients that it receives and propagates become negligible and the unit stops learning. We call this <i>saturating of the activation</i>. Because of this, nowadays, <i>sigmoid </i>is not typically used in deep learning, except in the output layer (we will be talking about this usage soon).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark443">Hyperbolic tangent (tanh)</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Hyperbolic tangents are another popular activation. They can be easily defined as follows:</p><p class="s137" style="padding-top: 9pt;padding-left: 173pt;text-indent: 0pt;line-height: 11pt;text-align: center;">𝑠𝑠𝑠𝑠𝑡𝑡ℎ(𝑥𝑥)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="50" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_693.png"/></span></p><p class="s137" style="padding-left: 12pt;text-indent: 0pt;line-height: 81%;text-align: center;">𝑡𝑡𝑡𝑡𝑡𝑡ℎ(𝑥𝑥)  = <span class="s237">𝑐𝑐𝑐𝑐𝑠𝑠ℎ(𝑥𝑥)</span></p><p style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">It is very similar to sigmoid. In fact, we can express <i>tanh </i>as a function of sigmoid. Let’s see what the activation function looks like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 34pt;text-indent: 0pt;text-align: left;"><span><img width="516" height="163" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_694.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 35pt;text-indent: 0pt;text-align: left;">Figure 11.9 – TanH activation function (left) and original, and activated vector space (middle and right)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can see that the shape is similar to sigmoid, although a bit sharper. But the key difference is that the <i>tanh </i>function outputs a value between -1 and 1. And because of the sharpness, we can also see the vector space getting pushed out to the edges as well. The fact that the function outputs a value that is symmetrical around the origin (0) works well with the optimization of the network and hence <i>tanh </i>was preferred over <i>sigmoid</i>. But since the <i>tanh </i>function is also a saturating function, the same problem of very small gradients hampering the flow of gradients and, in turn, learning plagues <i>tanh </i>activations as well.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Rectified linear unit and variants</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">As neuroscience gained more information about the human brain, researchers found out that only one to four percent of neurons in the brain are activated at any time. But with all the activation functions such as <i>sigmoid </i>or <i>tanh</i>, almost half of the neurons in a network are activated. In 2010, Vinod Nair and Geoffrey Hinton proposed <span class="s5">rectified linear units </span>(<span class="s5">ReLUs</span>) in the seminal paper <i>Rectified Linear Units Improve Restricted Boltzmann Machines</i>. And ever since, ReLUs have taken over as the de facto activation functions for deep neural networks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s233" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark444">ReLU</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">A ReLU is defined as follows:</p><p class="s239" style="padding-top: 8pt;padding-left: 42pt;text-indent: 0pt;text-align: center;">𝑔𝑔(𝑥𝑥)  =  𝑚𝑚𝑚𝑚𝑥𝑥(𝑥𝑥, 0)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">It is just a linear function, but with a kink at zero. Any value greater than zero is retained as is, but all values below zero are squashed to zero. The range of the output goes from 0 to . Let’s see how it looks visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;"><span><img width="514" height="163" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_695.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 37pt;text-indent: 5pt;text-align: justify;">Figure 11.10 – ReLU activation function (left) and original, and activated vector space (middle and right)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="69" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_696.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper that proposed ReLU is cited in <i>References </i>under reference number 7.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see that the points in the left and bottom quadrants are all pushed into the axes&#39; lines. This squashing is what gives the non-linearity to the activation function. And because of the way the activation sharply becomes zero and does not tend to zero like the sigmoid or tanh, ReLUs are non-saturating.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">There are a lot of advantages to using ReLUs:</p><ul id="l111"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">The computations of the activation function as well as its gradients are really cheap.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Training converges much faster than those with saturating activation functions.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">ReLU helps bring sparsity in the network (by having the activation as zero, a large majority of neurons in the network can be turned off) and resembles how biological neurons work.</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">But ReLUs are not without problems:</p></li><li><p style="padding-top: 7pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">When <span class="s41">𝑥𝑥 &lt; 0</span>, the gradients become zero. This means a neuron that has an output &lt; 0 will have zero gradients and therefore, the unit will not learn anymore. These are called dead ReLUs.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Another disadvantage is that the average output of a ReLU unit is positive and when we stack multiple layers, this might lead to a positive bias in the output.</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark445">Let’s see a few variants that tried to resolve the problems we discussed for ReLU.</a></p><p class="s233" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Leaky ReLU and parametrized ReLU</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Leaky ReLU is a variant of standard ReLU that resolves the <i>dead ReLU </i>problem. It was proposed by Maas and others in 2013. A Leaky ReLU can be defined as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s90" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑔𝑔(𝑥𝑥) = 𝑥𝑥,   𝑖𝑖𝑖𝑖 𝑥𝑥 ≥ 0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 173pt;text-indent: 0pt;text-align: center;">𝛼𝛼  𝑥𝑥,   𝑖𝑖𝑖𝑖 𝑥𝑥  &lt; 0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-bottom: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, is the slope parameter (typically set to a very small value such as 0.001) and is considered a hyperparameter. This makes sure the gradients are not zero when <i>x&lt;0 </i>and thereby ensures there are no <i>dead </i>ReLUs. But the sparsity that ReLU provides is lost here because there is no zero output that turns off a unit completely. Let’s visualize this activation function:</p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="525" height="166" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_697.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 141pt;text-indent: -33pt;line-height: 127%;text-align: left;">Figure 10.11 – Leaky ReLU activation function (left) and original, and activated vector space (middle and right)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_698.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The research paper that proposed leaky ReLU is cited in <i>References </i>under reference number 8 and parametrized ReLU is cited under reference number 9.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;line-height: 13pt;text-align: left;">,</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In 2015, He and others proposed another minor modification to Leaky ReLU called <span class="s5">parametrized ReLU</span>. In parametrized ReLU, instead of considering <span class="s41">α </span>as a hyperparameter, they considered it as a learnable parameter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are many other activation functions that are less popularly used but still have enough use cases to be included in <i>PyTorch</i><a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity" class="s140" target="_blank">. You can find a list of them here: </a><a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity" class="a" target="_blank">https://pytorch.org/docs/ </a><span class="s27">stable/nn.html#non-linear-activations-weighted-sum-nonlinearity</span>. We encourage you to use the notebook titled <span class="s20">02-Activation Functions.ipynb </span>in the <span class="s20">Chapter 11 </span>folder to try out different activation functions and see how they warp the vector space.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark446">And with that, we now have an idea of the components of the first block in </a><i>Figure 11.6</i>, representation learning. The next block in there is the linear classifier, which has a linear transformation and an output activation. We already know what a linear transformation is, but what is an output activation?<a name="bookmark426">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Output activation functions</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="133" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_699.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Additional reading</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">These functions have a deeper connection with <span class="s5">maximum likelihood estimation </span>(<span class="s5">MLE</span>) an the chosen loss function, but we will not be getting into that because it is out of the scope this book. We have linked to the book <i>Deep Learning </i>by Ian Goodfellow, Yoshua Bengio, an Aaron Courville in the <i>Further reading </i>section. If you are interested in a deeper understandin of deep learning, we suggest you use the book to that effect.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 89%;text-align: justify;">d of d g</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Output activation functions are functions that enforce a few desirable properties to the output of the network.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">If we want the neural network to predict a continuous number in the case of regression, we just use a linear activation function (which is like saying there is no activation function). The raw output from the network is considered the prediction and fed into the loss function.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">But in the case of classification, the desired output is a class out of all possible classes. If there are only two classes, we can use our old friend, the <i>sigmoid </i>function, which has an output between 0 and</p><ol id="l112"><li><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can also use <i>tanh </i>because its output is going to be between -1 and 1. The <i>sigmoid </i>function is preferred because of the intuitive probabilistic interpretation that comes along with it. The closer the value is to one, the more confident the network is about that prediction.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, <i>sigmoid </i>works for binary classification. What about multiclass classification where the possible classes are more than two?</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Softmax</p><p class="s4" style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Softmax <span class="p">is a function that converts a vector of </span>K <span class="p">real values into another </span>K<span class="p">-positive real value, which sums up to one. </span>Softmax <span class="p">is defined as follows:</span></p><p class="s207" style="padding-top: 8pt;padding-left: 118pt;text-indent: 0pt;line-height: 11pt;text-align: center;">𝑒𝑒<span class="s43">𝑥𝑥</span><span class="s240">𝑖𝑖</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="53" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_700.png"/></span></p><p class="s41" style="text-indent: 0pt;line-height: 11pt;text-align: left;">∑</p><p style="text-indent: 0pt;text-align: left;"/><p class="s41" style="padding-left: 175pt;text-indent: 0pt;line-height: 82%;text-align: left;">𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆(𝑆𝑆<span class="s42">𝑖𝑖 </span>) = <span class="s241">𝐾𝐾</span></p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: right;">𝑗𝑗=1</p><p class="s207" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">𝑒𝑒<span class="s43">𝑥𝑥</span><span class="s240">𝑗𝑗</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This function converts the raw output from a network into something that resembles a probability across <i>K </i>classes. This has a strong relation with <i>sigmoid </i>– <i>sigmoid </i>is a special case of <i>softmax </i>when <i>K=2</i>. In the following figure, let’s see how a random vector of size 3 is converted into probabilities that add up to one:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: left;"><a name="bookmark447"><span><img width="529" height="210" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_701.png"/></span></a><a name="bookmark427">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 11.12 – Raw output versus softmax output</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">If we look closely, we can see that in addition to converting the real values into something that resembles probability, it also increases the relative gap between the maximum and the rest of the values. This activation is a standard output activation for multiclass classification problems.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, there is only one major component left in the diagram (<i>Figure 11.6</i>) – the loss function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Loss function</p><p class="s4" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">The loss function we touched upon in </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, translates nicely to deep learning. In deep learning also, the loss function is a way to tell how good the predictions of the model are. If the predictions are way off the target, the loss function would be higher and as we get closer to the truth, it becomes smaller. In the deep learning paradigm, we just have one more additional requirement from the loss function – it should be differentiable.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Common loss functions from classical machine learning, such as <span class="s5">mean squared error </span>or <span class="s5">mean absolute error, </span>are valid in deep learning as well. In fact, in regression tasks, they are the default choices that practitioners adopt. For classification tasks, we adopt a concept borrowed from information theory called <span class="s5">cross-entropy loss</span><a href="https://pytorch.org/docs/stable/nn.html#loss-functions" class="s140" target="_blank">. But since deep learning is a very flexible framework, we can use any loss function as long as it is differentiable. There are a lot of loss functions people have already tried and found working in many situations. A lot of them are part of PyTorch’s API as well. You can find them here: </a><span class="s27">https://pytorch.org/docs/stable/nn.html#loss-functions</span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now that we have covered all the components of a deep learning system, let’s also briefly look at how we train the whole system.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark448">Forward and backward propagation</a><a name="bookmark428">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In <i>Figure 11.6</i>, we can see two sets of arrows, one going toward the desired output from input, marked as <i>Forward Computation</i>, and another going backward to the input from the desired output, marked <i>Backward Computation</i>. These two steps are at the core of learning a deep learning system. In the <i>Forward Computation</i>, popularly known as <span class="s5">Forward Propagation</span>, we use the series of computations that are defined in the layers and propagate the input all the way through the network to get the output. And now that we have the output, we would use the loss function to assess how close or far we are from the desired output. This information is now used in the <i>Backward Computation</i>, popularly known as <span class="s5">Backward Propagation</span>, to calculate the gradient with respect to all the parameters.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, what is a gradient and why do we need it? In high school math, we might have come across gradients or derivatives in another form called <span class="s5">slope</span>. It is the rate of change of a quantity when we change a variable by unit measure. Derivatives inform us of the local slope of a scalar function. While derivatives are always with respect to a single variable, gradients are a generalization of derivatives to multivariate functions. Intuitively, both gradient and derivatives inform us of the local slope of the function. And with the gradient of the loss function, we can use one of the techniques from mathematical optimization called <span class="s5">gradient descent</span>, to optimize our loss function.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see this with an example.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Gradient descent</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;line-height: 87%;text-align: justify;">Any machine learning or deep learning model can be thought of as a function that converts an input, <i>x</i>, to an output, <span class="s74">𝑦𝑦̂ </span>using a few parameters, <span class="s242">θ</span>. Here, <span class="s243">𝜃𝜃 </span>can be the collection of all the matrix transformations that we do to the input throughout the network. But to simplify the example, let’s</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="87" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_702.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 25pt;text-indent: -6pt;line-height: 89%;text-align: left;">o follow along with the complete code, use the notebook named <span class="s20">03-Gradient Descen ynb </span>in the <span class="s20">chapter11 </span>folder and the code in the <span class="s20">src </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 9pt;text-indent: 0pt;text-align: left;">t.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 13pt;text-indent: 0pt;text-align: left;">T</p><p class="s20" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">ip</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">assume there are only two parameters, <i>a </i>and <i>b</i>. And if we think about the whole process of learning a bit, we will see that by keeping the input and expected output the same, the way to change your loss would be by changing the parameters of the model. Therefore, we can postulate the loss function to be parameterized by the parameters, in this case, <i>a </i>and <i>b</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s assume the loss function takes the following form:</p><p class="s41" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">ℒ(𝑎𝑎, 𝑏𝑏) = (𝑎𝑎 − 8)2 + (𝑏𝑏 − 2)2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark449">Let’s see what the function looks like. We can use a three-dimensional plot to visualize a function with two parameters, as seen in </a><i>Figure 11.13</i>. Two dimensions will be used to denote the two parameters and at each point in that two-dimensional mesh, we can plot the loss value in the third dimension. This kind of plot of the loss function is also called a loss curve (in univariate settings), or loss surface (in multivariate settings).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 73pt;text-indent: 0pt;text-align: left;"><span><img width="403" height="411" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_703.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 11.13 – Loss surface plot</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The lighter portion of the 3D shape is where the loss function is less and as we move away from there, it increases.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In machine learning, our aim is to minimize the loss function, or in other words, find the parameters that make our predicted output as close as possible to the ground truth. This falls under the realm of mathematical optimization and a particular technique lends itself suitable for this approach – <span class="s5">gradient descent</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark450">Gradient descent is a mathematical optimization algorithm used to minimize a cost function by iteratively moving in the direction of the steepest descent. In a univariate function, the derivative (or the slope) gives us the direction (and magnitude) of the steepest ascent. For instance, if we know that the slope of a function is 1, we know if we move to the right, we are climbing up the slope, and moving to the left, we will be climbing down. Similarly, in the multivariate setting, the gradient of a function at any point will give us the direction (and magnitude) of the steepest ascent. And since we are concerned with minimizing a loss function, we will be using the negative gradient, which will point us in the direction of the steepest descent.</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">So, let’s define the gradient for our loss function. We are using high school calculus, but even if you are not comfortable, you don’t need to worry:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s146" style="padding-top: 3pt;padding-bottom: 2pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">ⅆ𝛻𝛻</p><p style="padding-left: 223pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="15" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_704.png"/></span></p><p class="s146" style="padding-left: 12pt;text-indent: 0pt;line-height: 11pt;text-align: center;">𝛻𝛻𝛻𝛻(𝑎𝑎, 𝑏𝑏) = [ <span class="s244">ⅆ</span>𝑎𝑎 ] = [ <span class="s245">2(𝑎𝑎</span> − 8<span class="s245">)</span> ]</p><p class="s146" style="padding-left: 223pt;text-indent: 0pt;line-height: 10pt;text-align: left;">ⅆ𝛻𝛻</p><p class="s146" style="padding-left: 223pt;text-indent: 0pt;text-align: left;">̅<span class="s246">𝑑𝑑</span>̅̅<span class="s246">𝑏𝑏</span>̅</p><p class="s146" style="padding-left: 22pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2(𝑏𝑏 − 2)</p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, how does the algorithm work? Very simply, as follows:</p><ol id="l113"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Initialize the parameters to random values.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Compute the gradient at that point.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Make a step in the direction opposite to the gradient.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Repeat steps 2 and 3 until it converges, or we reach maximum iterations.</p></li></ol></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There is just one more aspect that needs more clarity: how much of a step do we take in each iteration?</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Ideally, the magnitude of the gradient tells you how fast the function is changing in that direction, and we should just take the step equal to the gradient. But there is a property of the gradient that makes that a bad idea. The gradient only defines the direction and magnitude of the steepest ascent in the infinitesimally small locality of the current point and is blind to what happens beyond it. Therefore, we use a hyperparameter, commonly called the <span class="s5">learning rate</span>, to temper the steps we take in each iteration. Therefore, instead of taking a step equal to the gradient, we take a step equal to the learning rate multiplied by the gradient.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Mathematically, if <span class="s41">𝜃𝜃 </span>is the vector of parameters, at each iteration, we update the parameters using the following formula:</p><p class="s157" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝜃𝜃  =  𝜃𝜃 − 𝜂𝜂 × Δ𝑓𝑓(𝑎𝑎, 𝑏𝑏)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here, <span class="s146">𝜂𝜂 </span>is the learning rate and <span class="s50">Δ𝑓𝑓 </span>is the gradient at the point.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s see a very simple implementation of gradient descent. First, let’s define a function that returns us the gradient at any point:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">def gradient(a, b):</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">return 2*(a-8), 2*(b-2)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now we define a few initial parameters such as the maximum iterations, learning rate, and initial value of <i>a </i>and <i>b</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># maximum number of iterations that can be done</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">maximum_iterations = 500 <span class="s38"># current iteration </span>current_iteration = 0</p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;"># Learning Rate <span class="s28">learning_rate = 0.01 </span>#Initial value of a, b <span class="s28">current_a_value = 28</span></p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">current_b_value = 27</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now all that is left is the actual process of gradient descent:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">while current_iteration &lt; maximum_iterations: previous_a_value = current_a_value previous_b_value = current_b_value</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># Calculating the gradients at current values</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">gradient_a, gradient_b = gradient(previous_a_value, previous_b_value)</p><p class="s38" style="padding-top: 2pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># Adjusting the parameters using the gradients</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">current_a_value = current_a_value - learning_rate * gradient_a * (previous_a_value)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">current_b_value = current_b_value - learning_rate * gradient_b * (previous_b_value)</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">current_iteration = current_iteration + 1</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We know the minimum for this function will be at <i>a=8 </i>and <i>b=2 </i>because that would make the loss function zero. And gradient descent finds a solution that is pretty accurate – <i>a = 8.000000000000005 </i>and <i>b = 2.000000002230101</i>. We can also visualize the path it took to reach the minimum, as seen in <i>Figure 11.14</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><span><img width="525" height="547" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_705.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 11.14 – Gradient descent optimization on the loss surface</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark451">We can see that even though we initialized the parameters far from the actual origin, the optimization algorithm makes a direct path to the optimal point. At each point, the algorithm looks at the gradient of the point and moves in the opposite direction, and eventually converges on the optimum.</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">When gradient descent is adopted in a learning task, there are a few kinks to be noted. Let’s say we have a dataset of <i>N </i>samples. There are three popular variants of gradient descent that are used in learning and each of them has its pros and cons.</p><p class="s233" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Batch gradient descent</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">We run <i>all N </i>samples through the network and average the losses across <i>all N </i>instances. Now, we use this loss to calculate the gradient and make a step in the right direction and repeat.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The pros are as follows:</p><ul id="l114"><li><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 13pt;line-height: 162%;text-align: left;">The optimization path is direct, and it has guaranteed convergence. The cons are as follows:</p></li><li><p style="padding-left: 55pt;text-indent: -13pt;text-align: left;">The entire dataset needs to be evaluated for a single step and that is computationally expensive. The computation per optimization step becomes prohibitively high for huge datasets.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The time taken per optimization step is high and hence the convergence will also be slow.</p><p class="s233" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Stochastic gradient descent (SGD)</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">In SGD, we randomly sample <i>one </i>instance from <i>N </i>samples, calculate the loss and gradients, and then make an update to the parameters.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The pros are as follows:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Since we only use a single instance to make the optimization step, computation per optimization step is very low.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Time taken per optimization step is also faster.</p></li><li><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 13pt;line-height: 162%;text-align: left;">Stochastic sampling also acts as regularization and helps to avoid overfitting. The cons are as follows:</p></li><li><p style="padding-left: 55pt;text-indent: -13pt;text-align: left;">The gradient estimates are noisy because we are making the step based on just one instance. Therefore, the path toward optimum will be choppy and noisy.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Just because the time taken per optimization is low, it need not mean convergence is faster. We may not be taking the right step many times because of noisy gradient estimates.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s233" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark452">Mini-batch gradient descent</a><a name="bookmark430">&zwnj;</a><a name="bookmark429">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Mini-batch gradient descent is a technique that falls somewhere between the spectrum of batch gradient descent and SGD. In this variant, we have another quality called mini-batch size (or simply batch size), <i>b</i>. And in each optimization step, we randomly pick <i>b </i>instances from <i>N </i>samples and calculate gradients on the average loss of all <i>b </i>instances. With <i>b = N</i>, we have <i>batch gradient descent</i>, and with <i>b = 1</i>, we have <i>stochastic gradient descent</i>. This is the most popular way neural networks are trained today. By varying the batch size, we can travel between the two variants and manage the pros and cons of each option.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Nothing develops intuition better than a visual playground where we can see the effects of the different components we discussed. <i>Tensorflow Playground </i>is an excellent resource (see the link in the <i>Further reading </i>section) to do just that. I strongly urge you to head over there and play with the tool, train a few neural networks right in the browser, and see in real time how the learning happens.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We kicked off a new section of the book with an introduction to deep learning. We started with a bit of history to understand why deep learning is so popular today and we also explored its humble beginnings in Perceptron. We understood the composability of deep learning and understood and dissected the different components of deep learning such as the representation learning block, linear layers, activation functions, and so on. Finally, we rounded off the discussion by looking at how a deep learning system uses gradient descent to learn from data. With that understanding, we are now ready to move on to the next chapter, where we will drive the narrative toward time series models.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Following is the list of the reference used throughout this chapter:</p><ol id="l115"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Kyoung-Su Oh and Keechul Jung. (2004), <i>GPU implementation of neural networks</i><a href="https://doi.org/10.1016/j.patcog.2004.01.013" class="s140" target="_blank">. Pattern Recognition, Volume 37, Issue 6, 2004: </a><a href="https://doi.org/10.1016/j.patcog.2004.01.013" class="a" target="_blank">https://doi.org/10.1016/j. </a><span class="s27">patcog.2004.01.013.</span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Rajat Raina, Anand Madhavan, and Andrew Y. Ng. (2009), <i>Large-scale deep unsupervised learning using graphics processors</i><a href="https://doi.org/10.1145/1553374.1553486" class="s140" target="_blank">. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML ‘09): </a><span class="s27">https://doi.org/10.1145/1553374.1553486.</span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. (2012), <i>ImageNet Classification with Deep Convolutional Neural Networks</i><a href="https://doi.org/10.1145/3065386" class="s140" target="_blank">. Commun. ACM 60, 6 (June 2017), 84–90: </a><a href="https://doi.org/10.1145/3065386" class="a" target="_blank">https:// </a><span class="s27">doi.org/10.1145/3065386.</span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. (2020). <i>The Computational Limits of Deep Learning</i><a href="https://arxiv.org/abs/2007.05558v1" class="s140" target="_blank">. arXiv:2007.05558v1 [cs.LG]: </a><a href="https://arxiv.org/abs/2007.05558v1" class="a" target="_blank">https://arxiv. </a><span class="s27">org/abs/2007.05558v1.</span></p><p class="s47" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark453">Further reading 287</a><a name="bookmark431">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_706.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 55pt;text-indent: -18pt;text-align: justify;">Frank Rosenblatt. (1957), <i>The perceptron – A perceiving and recognizing automaton</i>, Technical Report 85-460-1, Cornell Aeronautical Laboratory.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Keim. (2001). <i>On the Surprising Behavior of Distance Metrics in High Dimensional Spaces. </i><a href="https://dl.acm.org/doi/10.5555/645504.656414" class="s140" target="_blank">In Proceedings of the 8th International Conference on Database Theory (ICDT ‘01). Springer-Verlag, Berlin, Heidelberg, 420–434: </a><span class="s27">https://dl.acm.org/doi/10.5555/645504.656414.</span></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Nair, V., and Hinton, G.E. (2010). <i>Rectified Linear Units Improve Restricted Boltzmann Machines</i><a href="https://icml.cc/Conferences/2010/papers/432.pdf" class="s140" target="_blank">. ICML: </a><span class="s27">https://icml.cc/Conferences/2010/papers/432.pdf.</span></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Andrew L. Maas and Awni Y. Hannun and Andrew Y. Ng. (2013). <i>Rectifier nonlinearities improve neural network acoustic models</i><a href="https://ai.stanford.edu/%7Eamaas/papers/relu_hybrid_icml2013_final.pdf" class="s140" target="_blank">. ICML Workshop on Deep Learning for Audio, Speech, and Language Processing: </a><a href="https://ai.stanford.edu/%7Eamaas/papers/relu_hybrid_icml2013_final.pdf" class="a" target="_blank">https://ai.stanford.edu/~amaas/papers/ </a><span class="s27">relu_hybrid_icml2013_final.pdf.</span></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">He, K., Zhang, X., Ren, S., and Sun, J. (2015). <i>Delving Deep into Rectifiers: Surpassing Human- Level Performance on ImageNet Classification</i><a href="https://ieeexplore.ieee.org/document/7410480" class="s140" target="_blank">. 2015 IEEE International Conference on Computer Vision (ICCV), 1026-1034: </a><a href="https://ieeexplore.ieee.org/document/7410480" target="_blank">https://ieeexplore.ieee.org/document/741048.0</a></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;line-height: 111%;text-align: justify;">Sara Hooker. (2021). <i>The hardware lottery</i><a href="https://doi.org/10.1145/3467017" class="s140" target="_blank">. Commun. ACM, Volume 64: </a><a href="https://doi.org/10.1145/3467017" class="a" target="_blank">https://doi. </a><span class="s27">org/10.1145/3467017.</span></p></li></ol><p class="s3" style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">You can check out the following sources if you want to read more about a few topics covered in this chapter:</p></li><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Linear Algebra <a href="https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/" class="s140" target="_blank">course from Gilbert Strang: </a><a href="https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/" class="a" target="_blank">https://ocw.mit.edu/resources/ </a><a href="https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/" target="_blank">res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Essence of Linear Algebra <span class="p">from </span><span class="s20">3Blue1Brown</span><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" class="s140" target="_blank">: </a><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" class="a" target="_blank">https://www.youtube.com/ </a><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank">playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">Neural Networks – A Linear Algebra Perspective <a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" class="s140" target="_blank">by Manu Joseph: </a><a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" class="a" target="_blank">https://deep- and-shallow.com/2022/01/15/neural-networks-a-linear-algebra- </a><a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" target="_blank">perspective/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">Deep Learning <a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" class="s140" target="_blank">– Ian Goodfellow, Yoshua Bengio, Aaron Courville: </a><a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" class="a" target="_blank">https://deep- and-shallow.com/2022/01/15/neural-networks-a-linear-algebra- </a><a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" target="_blank">perspective/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Tensorflow Playground<a href="https://playground.tensorflow.org/" class="s140" target="_blank">: </a><a href="https://playground.tensorflow.org/" target="_blank">https://playground.tensorflow.org/</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark454">12</a><a name="bookmark455">&zwnj;</a><a name="bookmark457">&zwnj;</a><a name="bookmark456">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 121pt;text-indent: 5pt;line-height: 114%;text-align: left;">Building Blocks of Deep Learning for Time Series</h4><p style="padding-top: 24pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">While we laid the foundations of deep learning in the previous chapter, it was very general. Deep learning is a vast field with applications in all possible domains, but the focus of this book is time series forecasting.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">So, in this chapter, let’s strengthen the foundation by looking at a few building blocks of deep learning that are commonly used in time series forecasting. Even though the global machine learning models perform well in time series problems, some deep learning approaches have also shown good promise. They are a good addition to your toolset due to the flexibility they allow when modeling.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will cover the following topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Understanding the encoder-decoder paradigm</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Feed-forward networks</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Recurrent neural networks</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Long short-term memory (LSTM) networks</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Gated recurrent unit (GRU)</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Convolution networks</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p class="s27" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter12" class="s140" target="_blank">The associated code for this chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter12" class="a" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter12<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark473">Understanding the encoder-decoder paradigm</a><a name="bookmark458">&zwnj;</a></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark186" class="s140">In </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">, we saw that machine learning is all about learning a function that maps our inputs to the desired output:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s146" style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑦𝑦 = ℎ(𝑥𝑥), 𝑤𝑤ℎ𝑒𝑒𝑒𝑒𝑒𝑒 𝑥𝑥 𝑖𝑖𝑠𝑠 𝑡𝑡ℎ𝑒𝑒 𝑖𝑖𝑛𝑛𝑛𝑛𝑛𝑛𝑡𝑡 𝑎𝑎𝑛𝑛𝑎𝑎 𝑦𝑦 𝑖𝑖𝑠𝑠 𝑜𝑜𝑛𝑛𝑒𝑒 𝑎𝑎𝑒𝑒𝑠𝑠𝑖𝑖𝑒𝑒𝑒𝑒𝑎𝑎 𝑜𝑜𝑛𝑛𝑡𝑡𝑛𝑛𝑛𝑛𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Adapting this to time series forecasting (considering univariate time series forecasting to keep it simple), we can rewrite it as follows:</p><p class="s242" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑦𝑦<span class="s247">𝑡𝑡</span><span class="s146">  </span>= ℎ(𝑦𝑦<span class="s247">𝑡</span><span class="s146">𝑡−1</span>, 𝑦𝑦<span class="s247">𝑡</span><span class="s146">𝑡−2</span>, ⋯ , 𝑦𝑦<span class="s247">𝑡</span><span class="s146">𝑡−𝑁𝑁</span>)</p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <i>t </i>is the current timestep and <i>N </i>is the total amount of history available at time <i>t</i>.</p><p class="s4" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark414" class="s140">Deep learning, like any other machine learning approach, is tasked with learning this function, which maps history to the future. In </a>Chapter 11<span class="p">, </span>Introduction to Deep Learning<span class="p">, we saw how deep learning learns good features using representation learning and then uses the learned features to carry out the task at hand. This understanding can be further refined to the time series perspective by using the encoder-decoder paradigm.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_707.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The research papers by Ramon Neco et al., Nal Kalchbrenner et al., Cho et al., and Ilya Sutskeve et al. are cited in the <i>References </i>section as <i>1</i>, <i>2</i>, <i>3</i>, and <i>4</i>, respectively.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">r</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Like everything in research, it is not entirely clear when and who proposed this idea of the encoder- decoder architecture. In 1997, Ramon Neco and Mikel Forcada proposed an architecture for machine translation that had ideas reminiscent of the encoder-decoder paradigm. In 2013, Nal Kalchbrenner and Phil Blunsom proposed an encoder-decoder model for machine translation, although they did not call it that. But it is when Ilya Sutskever et al. (2014) and Cho et al. (2014) proposed two new models for machine translation, which worked independently, that this idea took off. Cho et al. called it the encoder-decoder architecture, while Sutskever et al. called it the Seq2Seq architecture. The key innovation it drove was the ability to model variable-length inputs and outputs in an end-to-end fashion.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The idea is very straightforward, but before we get into that, we need to have a high-level understanding of latent spaces and feature/input spaces.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The <span class="s5">feature space</span>, or the <span class="s5">input space</span>, is the vector space where your data resides. If the data has 10 dimensions, then the input space is the 10-dimensional vector space. Latent space is an abstract vector space that encodes a meaningful internal representation of the feature space. To understand this, we can think about how we, as humans, recognize a tiger. We do not remember every minute detail of a tiger; we just have a general idea of what a tiger looks like and its prominent features, such as its stripes. It is a compressed understanding of this concept that helps our brain process and recognize a tiger faster.</p><p class="s47" style="padding-top: 4pt;padding-left: 228pt;text-indent: 0pt;text-align: left;"><a name="bookmark474">Understanding the encoder-decoder paradigm 291</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_708.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 147%;text-align: left;">Now that we have an idea about latent spaces, let’s see what an encoder-decoder architecture does. An encoder-decoder architecture has two main parts – an encoder and a decoder:</p></li><li><p style="padding-top: 2pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><span class="s5">Encoder</span>: The encoder takes in the input vector, <i>x</i>, and encodes it into a latent space. This encoded representation is called the latent vector, <i>z</i>.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><span class="s5">Decoder</span>: The decoder takes in the latent vector, <i>z</i>, and decodes it into the kind of output we need (<span class="s146">𝑦𝑦̂</span>).</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The following diagram shows the encoder-decoder setup visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="524" height="113" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_709.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 12.1 – The encoder-decoder architecture</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the context of time series forecasting, the encoder consumes the history and retains the information that is required for the decoder to generate the forecast. As we learned previously, time series forecasting can be written as follows:</p><p class="s242" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑦𝑦<span class="s247">𝑡𝑡</span><span class="s146">  </span>= ℎ(𝑦𝑦<span class="s247">𝑡</span><span class="s146">𝑡−1</span>, 𝑦𝑦<span class="s247">𝑡</span><span class="s146">𝑡−2</span>, ⋯ , 𝑦𝑦<span class="s247">𝑡</span><span class="s146">𝑡−𝑁𝑁</span>)</p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, using the encoder-decoder paradigm, we can rewrite it as follows:</p><p class="s248" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑧𝑧<span class="s246">𝑡𝑡</span><span class="s146">  </span>= ℎ(𝑦𝑦<span class="s246">𝑡</span><span class="s146">𝑡−1</span>, 𝑦𝑦<span class="s246">𝑡</span><span class="s146">𝑡−2</span>, ⋯ , 𝑦𝑦<span class="s246">𝑡</span><span class="s146">𝑡−𝑁𝑁</span>)</p><p class="s248" style="padding-top: 9pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑦𝑦<span class="s246">𝑡𝑡</span><span class="s146">  </span>= 𝑔𝑔(𝑧𝑧<span class="s246">𝑡</span><span class="s146">𝑡</span>)</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Here, <i>h </i>is the encoder and <i>g </i>is the decoder.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Each encoder and decoder can be some special architecture suited for time series forecasting. Let’s look at a few common components that are used in the encoder-decoder paradigm.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark475">Feed-forward networks</a><a name="bookmark459">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s5">Feed-forward networks </span>(<span class="s5">FFNs</span>) or <span class="s5">fully connected networks </span><a href="#bookmark414" class="s140">are the most basic architecture a neural network can take. We discussed perceptrons in </a><i>Chapter 11</i>, <i>Introduction to Deep Learning</i>. If we stack multiple perceptrons (both linear units and non-linear activations) and create a network of such units, we get what we call an FFN. The following diagram will help us understand this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="520" height="369" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_710.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 12.2 – Feed-forward network</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">An FFN takes a fixed-size input vector and passes it through a series of computational layers leading up to the desired output. This architecture is called feed-forward because the information is fed forward through the network. This is also called a <span class="s5">fully connected network </span>because every unit in a layer is connected to every unit in the previous layer and every unit in the next layer.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The first layer is called the input layer, and this is equal to the dimension of the input. The last layer is called the output layer, which is defined as per our desired output. If we need a single output, we will need one unit, while if we need 10 outputs, we will need 10 units. All the layers in between are called <span class="s5">hidden layers</span>. Two hyperparameters define the structure of the network – the number of hidden layers and the number of units in each layer. For instance, in <i>Figure 12.2</i>, we have a network with two hidden layers and eight units per layer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="149" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_711.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional reading</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">We are going to be using PyTorch throughout this book to work with deep learning. If you are not comfortable with PyTorch, don’t worry – I’ll try and explain the concepts when necessary. To get a head start, you can go through the <span class="s20">01-PyTorch Basics.ipynb </span>notebook in <span class="s20">Chapter12</span>, where we have explored the basic functionalities of tensors and trained a very small neural network from scratch using <span class="s20">PyTorch</span>. I also suggest heading over to the <i>Further reading </i>section at the end of this chapter, where you’ll find a few resources to learn PyTorch.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s4" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140" name="bookmark476">In the time series forecasting context, an FFN can be used as an encoder as well as a decoder. As an encoder, we can use an FFN just like we used machine learning models in </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<span class="p">. We embed time and convert a time series problem into a regression problem before feeding it into the FFN. As a decoder, we use it on the latent vector (the output from the encoder) to get to the output (this is the most common usage of an FFN in time series forecasting).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="87" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_712.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code, use the <span class="s20">02-Building Blocks.ipynb </span>notebook in the <span class="s20">Chapter12 </span>folder and the code in the <span class="s20">src </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s put on our practical hats and see some of these in action. PyTorch is an open source deep learning framework developed primarily by the <span class="s5">Facebook AI Research Lab </span>(<span class="s5">FAIR</span>). Although it is a library that can manipulate <span class="s5">tensors </span>(which are <i>n</i>-dimensional matrices) and accelerate such manipulations with a GPU, a large part of the use case for such a library is in building and training deep learning systems. Because of that, PyTorch provides a lot of ready-to-use components that we can use to build a deep learning system. Let’s see how we can use PyTorch for an FFN.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">As we learned earlier in the section, an FFN is a network of linear and non-linear units arranged in a network. A linear operation consists of multiplying the input vector, <span class="s74">𝑋𝑋</span>, with a weight matrix, <span class="s249">𝑊𝑊</span>, and adding a bias term, <i>b</i>. This operation, <span class="s43">𝑊𝑊𝑊𝑊 + 𝑏𝑏</span>, is encapsulated in a <span class="s20">Linear </span>class in the <span class="s20">nn </span>module of the <span class="s20">PyTorch </span>library. We can import this from the library using <span class="s20">torch.nn import Linear</span>. But usually, we must import the <span class="s20">nn </span>module as a whole because we would be using a lot of components from that module. For non-linearity, let’s use <span class="s20">ReLU </span><a href="#bookmark414" class="s140">(as introduced in </a><i>Chapter 11</i>, <i>Introduction to Deep Learning</i>), which is also a class in the <span class="s20">nn </span>module.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Before moving on, let’s create a random walk time series whose length is <span class="s20">20</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">N = 20</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">df = pd.DataFrame({</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">&quot;date&quot;: pd.date_range(periods=N, start=&quot;2021-04-12&quot;, freq=&quot;D&quot;),</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">&quot;ts&quot;: np.random.randn(N)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">})</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark477">We can use this tensor directly in the FFN, but usually, we use a sliding window technique to split the tensor and train the networks. We do this for multiple reasons:</a></p><ul id="l116"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">We can see this as a data augmentation technique that is creating a greater number of samples as opposed to using the entire sequence just once.</p></li><li><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 13pt;line-height: 162%;text-align: left;">It helps us reduce and restrict computation by limiting the calculation to a fixed window. Let’s do that now:</p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">ts = torch.from_numpy(df.ts.values).float() window = 15</p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># Creating windows of 15 over the dataset</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ts_dataset = ts.unfold(0, size=window, step=1)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, we have a tensor, <span class="s20">ts_dataset</span>, whose size is <i>6x15 </i>(this can create 6 samples of 15 input features each when we move the sliding window across the length of the series). For a standard FFN, the input shape is specified as <i>batch size x input features</i>. So, 6 becomes our batch size and 15 becomes the input feature size.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s define the layers in the FFN. For this exercise, let’s assume the network’s structure is as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span><img width="513" height="287" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_713.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 12.3 – FFNs – a matrix multiplication perspective</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark478">The input data (6x15) will be passed through these layers one by one. Here, we can see how the tensor dimensions are changing as it flows through the network. Each of the linear layers is essentially a matrix multiplication that converts the input into the output of a specified dimension. After each linear transformation, we stack a non-linear activation function in there. These alternative linear and non-linear modules are what give the neural network the expressive power it has. The linear layers are an affine transformation of the vector space (rotation, translation, and so on), and the non-linearity </a><i>squashes </i>the vector space. Together, they can morph the input space so that it’s useful for the task at hand. Now, let’s see how we can code this in PyTorch. We are going to use a handy module from PyTorch called <span class="s20">Sequential</span>, which allows us to stack different sub-components together and use them with ease:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># The FFN we define would have this architecture</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># window(windowed input) &gt;&gt; 64 (hidden layer 1) &gt;&gt; 32 (hidden layer 2) &gt;&gt; 32 (hidden layer 2) &gt;&gt; 1 (output)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ffn = nn.Sequential(</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">nn.Linear(in_features=window,out_features=64), <span class="s38"># (batch- size x window) --&gt; (batch-size x 64)</span></p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">nn.ReLU(),</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">nn.Linear(in_features=64,out_features=32), <span class="s38"># (batch-size x</span></p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">64) --&gt; (batch-size x 32)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">nn.ReLU(),</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">nn.Linear(in_features=32,out_features=32), <span class="s38"># (batch-size x</span></p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">32) --&gt; (batch-size x 32)</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">nn.ReLU(),</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">nn.Linear(in_features=32,out_features=1), <span class="s38"># (batch-size x</span></p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">32) --&gt; (batch-size x 1)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now that we have defined the FFN, let’s see how we can use it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ffn(ts_dataset)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># or more explicitly</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ffn.forward(ts_dataset)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This will return a tensor whose shape is based on <i>batch size x output units</i>. We can have any number of output units, not just one. Therefore, when using an encoder, we can have an arbitrary dimension for the latent vector. Then, when we are using it as a decoder, we can have the output units equal the number of time steps we are forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="149" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_714.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Sneak peek</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">We have not seen multi-step forecasting until now because it will be covered in more deta in <i>Part 4</i>, <i>Mechanics of Forecasting</i>. But for now, just understand that there are cases where w will need to forecast multiple time steps into the future. The classical statistical models do thi out of the box. But for machine learning and deep learning, we need to design systems th can do that. Fortunately, there are a few different techniques to do so which will be covere in <i>Part 4 </i>of the book.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 89%;text-align: justify;">il e s at d</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark479">FFNs are designed for non-temporal data. We can use FFNs by embedding our data temporally and then passing that to the network. Also, the computational cost in an FFN is directly proportional to the memory we use in the embedding (the number of previous time steps we include as features). We will also not be able to handle variable-length sequences in this setting.</a><a name="bookmark460">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at another common architecture that is specifically designed for temporal data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Recurrent neural networks</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s5">Recurrent neural networks </span>(<span class="s5">RNNs</span>) are a family of neural networks specifically designed to handle sequential data. They were first proposed by <i>Rumelhart et al. </i>(1986) in their seminal work, <i>Learning Representations by Back-Propagating Errors</i>. The work borrows ideas such as parameter sharing and recurrence from previous work in statistics and machine learning to come up with a neural network architecture that helps overcome many of the disadvantages FFNs have when processing sequential data.</p><p class="s5" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Parameter sharing <span class="p">is when we use the same set of parameters for different parts of the model. Apart from a regularization effect (restricting the model to using the same set of weights for multiple tasks, which regularizes the model by constraining the search space while optimizing the model), parameter sharing enables us to extend and apply the model to examples of different forms. RNNs can scale to much longer sequences because of this. In an FFN, each timestep (each feature) has a fixed weight and even if the motif we are looking for shifts by one timestep, the network may not capture it correctly. In an RNN enabled by parameter sharing, they are captured in a much better way.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In sentences (which are also sequences), we want the model to recognize that “<i>Tomorrow I will go to the bank</i>” and “<i>I will go to the bank tomorrow</i>” are the same thing. An FFN can’t do this, but an RNN will be able to because it uses the same parameters at all positions and will be able to identify the motif “<i>I will go to the bank</i>” wherever it occurs. Intuitively, we can think of RNNs as applying the same FFN at each time window but enhanced with some kind of memory to store relevant information for the task at hand.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s visualize how an RNN processes inputs:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 36pt;text-indent: 0pt;text-align: left;"><span><img width="505" height="236" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_715.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 12.4 – How an RNN processes input sequences</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 86%;text-align: justify;">Let’s assume we are talking about a sequence with four elements in it, <span class="s250">𝑥</span><span class="s74">𝑥</span><span class="s251">1</span><span class="s75">  </span><span class="s250">t</span><span class="s74">o 𝑥𝑥</span><span class="s251">4</span><span class="s75"> </span>. Any RNN block (let’s consider it as a black box for now) consumes input and a hidden state (memory) and produces an output. In the beginning, there is no memory, so we start with an initial memory (<span class="s250">𝐻</span><span class="s74">𝐻</span><span class="s251">0</span>), which is typically an array filled with zeroes. Now, the RNN block takes in the first input (<span class="s137">𝑥𝑥</span><span class="s42">1</span>) along with the initial hidden state (<span class="s162">𝐻</span><span class="s90">𝐻</span><span class="s236">0</span>) and produces an output (<span class="s137">o</span><span class="s128">1</span>) and a hidden state (<span class="s74">𝐻𝐻</span><span class="s79">1</span>).</p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 92%;text-align: justify;">To process the second element in the sequence, <i>the same RNN </i>block takes in the hidden state from the previous timestep (<span class="s146">𝐻𝐻</span><span class="s126">1</span>) and the input at the current timestep (<span class="s94">𝑥</span><span class="s74">𝑥2</span>) to produce the output at the second timestep (<span class="s90">o2</span>) and a new hidden state (<span class="s146">𝐻𝐻</span><span class="s79">2</span>). This process continues until we reach the end of the sequence. After processing the entire sequence, we will have all the outputs at each timestep <span class="s138">(</span><span class="s146">𝑜𝑜</span><span class="s79">1</span><span class="s75">  </span><span class="s146">through o</span><span class="s79">4</span><span class="s138">)</span> and the final hidden state (<span class="s146">𝐻𝐻4</span><span class="s138">).</span></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">These outputs and the hidden state will have encoded the information contained in the sequence and can be used for further processing, such as to predict the next step using a decoder. The RNN block can also be used as a decoder that takes in the encoded representation and produces the outputs. Because of this flexibility, the RNN blocks can be arranged to suit a wide variety of input and output combinations, such as the following:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Many-to-one, where we have many inputs and a single output – for instance, single-step forecasting or time series classification</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Many-to-many, where we have many inputs and many outputs – for instance, multi-step forecasting</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now let’s look at what happens inside an RNN.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark480">Let the input to the RNN at time </a><span class="s157">𝑡𝑡 </span>be <span class="s41">𝑥𝑥𝑡𝑡</span>, and the hidden state from the previous timestep be <span class="s74">𝐻𝐻𝑡𝑡−1</span>. The updated equations are as follows:</p><p class="s146" style="padding-top: 9pt;padding-left: 41pt;text-indent: 0pt;text-align: center;">𝐴𝐴<span class="s126">𝑡𝑡</span><span class="s54">  </span>= 𝑊𝑊 ⋅ 𝐻𝐻<span class="s126">𝑡𝑡</span><span class="s54">−1  </span>+ 𝑈𝑈 ⋅ 𝑥𝑥<span class="s126">𝑡𝑡</span><span class="s54">  </span>+ 𝑏𝑏<span class="s126">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s90" style="padding-left: 173pt;text-indent: 0pt;text-align: center;">𝐻𝐻<span class="s128">𝑡𝑡</span><span class="s127">  </span>= 𝑡𝑡𝑡𝑡𝑡𝑡ℎ(𝐴𝐴<span class="s128">𝑡</span><span class="s127">𝑡</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s90" style="padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑜𝑜<span class="s128">𝑡𝑡</span><span class="s127">  </span>= 𝑉𝑉 ⋅ 𝐻𝐻<span class="s128">𝑡𝑡</span><span class="s127">  </span>+ 𝑏𝑏<span class="s128">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <i>U</i>, <i>V</i>, and <i>W </i>are learnable weight matrices and <span class="s157">𝑏𝑏1 </span>and <span class="s252">𝑏</span><span class="s137">𝑏</span><span class="s253">2</span><span class="s43"> </span>are two learnable bias vectors. <i>U</i>, <i>V</i>, and <i>W </i>can be easily remembered as <i>input-to-hidden</i>, <i>hidden-to-output</i>, and <i>hidden-to-hidden </i>matrices based on the kind of transformation they perform, respectively. Intuitively, we can think of the operation that the RNN is doing as a kind of learning and forgetting the information as it sees fit. The <i>tanh </i><a href="#bookmark414" class="s140">activation, as we saw in </a><a href="#bookmark414" class="s21">Chapter 1</a><i>1</i>, <i>Introduction to Deep Learning</i>, produces a value between -1 and 1, which acts analogous to forgetting and remembering. So, the RNN transforms the input into a latent dimension, uses the <i>tanh </i>activation to decide what information from the current timestep and previous memory to keep and forget, and uses this new memory to generate an output.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In standard backpropagation, we backpropagate the gradients from one unit to another. But in recurrent nets, we have a special situation where we have to backpropagate the gradients within a single unit, but through time or the different time steps. A special case of backpropagation, called <span class="s5">Back Propagation Through Time </span>(<span class="s5">BPTT</span>), has been developed for RNNs. Thankfully, all the major deep learning frameworks are capable of doing this without any problems. For a more detailed understanding and mathematical foundations of BPTT, please to the <i>Further reading </i>section.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">PyTorch has made RNNs available as ready-to-use modules – all you need to do is import one of the modules from the library and start using it. But before we do that, we need to understand a few more concepts.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The first concept we will look at is the possibility of <i>stacking multiple layers </i>of RNNs on top of each other so that the outputs at each timestep become the input to the RNN in the next layer. Each layer will have a hidden state or memory. This enables hierarchical feature learning, which is one of the bedrocks of the successes of deep learning today.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another concept is <i>bidirectional </i>RNNs, introduced by Schuster and Paliwal in 1997. Bidirectional RNNs are very similar to RNNs. In a vanilla RNN, we process the inputs sequentially from start to end (forward). However, a bidirectional RNN uses one set of input-to-hidden and hidden-to-hidden weights to process the inputs from start to end and another set to process the inputs in reverse (end to start) and concatenate the hidden states from both directions. It is on this concatenated hidden state that we apply the output equation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_716.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The research papers by Rumelhart et al and Schuster and Paliwal are cited in the <i>References</i></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">section as <i>5 </i>and <i>6</i>, respectively.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark481">The RNN layer in PyTorch</a><a name="bookmark461">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s understand the PyTorch implementation for RNN. As with the <span class="s20">Linear </span>module, the <span class="s20">RNN </span>module is also available from <span class="s20">torch.nn</span>. Let’s look at the different parameters the implementation provides while initializing:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">input_size<span class="p">: The number of expected features in the input. If we are using just the history of the time series, then this would be 1. However, when we use history along with some other features, then this will be &gt;1.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">hidden_size<span class="p">: The dimension of the hidden state. This defines the size of the input-to-hidden and hidden-to-hidden matrices.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">num_layers<span class="p">: This is the number of RNNs that will be stacked on top of each other. The default is </span>1<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">nonlinearity<span class="p">: The non-linearity to use. Although tanh is the originally proposed non-linearity, PyTorch also allows us to use ReLU (</span>relu<span class="p">). The default is </span>‘tanh’<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">bias <span class="p">This parameter decides whether or not to add bias to the update equations we discussed earlier. If the parameter is </span>False<span class="p">, there will be no bias. The default is </span>True<span class="p">.</span></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s20">batch_first</span>: There are two input data configurations that the RNN cell can use – we can have the input as <i>(batch size, sequence length, number of features) </i>or <i>(sequence length, batch size, number of features)</i>. <span class="s20">batch_first = True </span>selects the former as the expected input dimensions. The default is <span class="s20">False</span>.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s20">dropout</span>: This parameter, if non-zero, uses a dropout layer on the outputs of each RNN layer except the last. Dropout is a popular regularization technique where randomly selected neurons are ignored during training (the <i>Further reading </i>section contains a link to the paper that proposed this). The dropout probability will be equal to <span class="s20">dropout</span>. The default is <span class="s20">0</span>.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">bidirectional<span class="p">: This parameter enables a bidirectional RNN. If </span>True<span class="p">, a bidirectional RNN is used. The default is </span>False<span class="p">.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">To continue applying the model to the same synthetic data we generated earlier in this chapter, let’s initialize the RNN model, as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">rnn = nn.RNN(</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">input_size=1, hidden_size=32,</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">num_layers=1, batch_first=True, dropout=0, bidirectional=False,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark482"/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at the inputs and outputs that are expected from an RNN cell.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">As opposed to the <span class="s20">Linear </span>layer we saw earlier, the <span class="s20">RNN </span>cell takes in <i>two inputs </i>– the input sequence and the hidden state vector. The input sequence can be either (<i>batch size, sequence length, number of features</i>) or (<i>sequence length, batch size, number of features</i>), depending on whether we have set <span class="s20">batch_first=True</span>. The hidden state is a tensor whose size is (<i>D*number of layers, batch size, hidden size</i>), where <i>D = 1 </i>for <span class="s20">bidirectional=False </span>and <i>D = 2 </i>for <span class="s20">bidirectional=True</span>. The hidden state is an optional input and will default to zero tensors if left blank.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are two outputs of the RNN cell: an output and a hidden state. The output can be either (<i>batch size, sequence length, D*hidden size</i>) or (<i>sequence length, batch size, D*hidden size</i>), depending on <span class="s20">batch_first</span>. The hidden state has the dimension of (<i>D*number of layers, batch size, hidden size</i>). Here, <i>D = 1 </i>or <i>2 </i>is based on the <span class="s20">bidirectional </span>parameter.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="167" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_717.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Although we saw that the RNN cell contains the output as well as the hidden state, we also know that the output is just an affine transformation of the hidden state. Therefore, to provide flexibility to the users, PyTorch only implements the update equations regarding the hidden states in the module. There are cases where we have no use for the outputs at each timestep (such as in a many-to-one scenario) and we can save computation if we do not do the output update at each step. Therefore, <span class="s20">output </span>from the PyTorch RNN is just the hidden states at each timestep and <span class="s20">hidden_states </span>is the latest hidden state.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">So, let’s run our sequence through an RNN and look at the inputs and outputs (for more detailed steps, refer to the accompanying notebook):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#input dim: torch.Size([6, 15, 1])</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># batch size = 6, sequence length = 15 and number of features = 1, batch_first = True</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">output, hidden_states = rnn(rnn_input)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># output.shape -&gt; torch.Size([6, 15, 32])</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># hidden_states.shape -&gt; torch.Size([1, 6, 32]))</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark483">We can verify this by checking whether the hidden state tensor is equal to the last output tensor:</a></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  torch.equal(hidden_states[0], output[:,-1]) </span><span class="s23" style=" background-color: #F3F2F1;"># -&gt; True           </span></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">To make this clearer, let’s look at it visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: left;"><span><img width="462" height="320" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_718.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 12.5 – PyTorch implementation of stacked RNNs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The hidden states at each timestep are used as input for the subsequent layer of RNNs and the hidden states of the last layer of RNNs are collected as the output. But each layer has a hidden state (that’s not shared with the others) and the PyTorch RNN collects the last hidden state from each layer and gives us that as well. Now, it is up to us to decide how to use these outputs. For instance, in a one-step-ahead forecast, we can use the output hidden states and stack a few linear layers on top of it to get the next timestep prediction. Alternatively, we can use the hidden states to transfer memory into another RNN as a decoder and generate predictions for multiple time steps. There are many more ways we can use this output and PyTorch gives us that flexibility.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">RNNs, while very effective in modeling sequences, have one big flaw. Because of BPTT, the number of units through which you need to backpropagate increases drastically with the length of the sequence to be used for training. When we have to backpropagate through such a long computational graph, we will encounter <span class="s5">vanishing </span>or <span class="s5">exploding gradients</span>. This is when the gradient, as it is backpropagated through the network, either shrinks to zero or explodes to a very high number. The former makes the network stop learning, while the latter makes the learning unstable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_719.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The references for Hochreiter (1991) and Bengio et al. (1993, 1994) are cited in the <i>Reference</i></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">section as <i>7</i>, <i>8</i>, and <i>9</i>, respectively.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">s</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark484">We can think of what’s happening as akin to what happens when we multiply a scalar number repeatedly by itself. If the number is less than one, with every subsequent multiplication, the number becomes smaller and smaller until it is practically zero. If the number is greater than one, then the number becomes larger and larger at an exponential scale. This was discovered, independently, by Hochreiter in his diploma thesis (1991) and Yoshua Bengio et al. in two papers published in 1993 and 1994. Over the years, many tweaks to the model and training process have been proposed to tackle this disadvantage. Nowadays, vanilla RNNs are hardly used in practice and have been replaced almost completely by their newer cousins.</a><a name="bookmark462">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at two key improvements that have been made to the RNN architecture that have shown good performance and gained popularity in the machine learning community.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Long short-term memory (LSTM) networks</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Hochreiter and Schmidhuber proposed a modification of the classical RNNs in 1997 – LSTM networks. It aimed to resolve the vanishing and exploding gradients in vanilla RNNs. The design of the LSTM was inspired by the logic gates of a computer. It introduces a new component, called a <span class="s5">memory cell</span>, which serves as long-term memory and is used in addition to the hidden state memory of classical RNNs. In an LSTM, multiple gates are tasked with reading, adding, and forgetting information from these memory cells. This memory cell acts as a <i>gradient highway</i>, allowing the gateways to pass relatively unhindered through the network. This is the key innovation that avoided vanishing gradients in RNNs.</p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 15pt;text-align: justify;">Let the input to the LSTM at time <span class="s157">𝑡𝑡 </span>be <span class="s41">𝑥𝑥𝑡𝑡</span>, and the hidden state from the previous timestep be <span class="s41">𝐻𝐻𝑡𝑡−1</span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">. Now, there are three gates that process information. Each gate is nothing but two learnable weight matrices (one for the input and one for the hidden state from the last step) and a bias term that is multiplied/added to the input and hidden state and finally passed through a sigmoid activation. The output of these gates will be a real number between 0 and 1. Let’s look at each of these gates in detail:</p><ul id="l117"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Input gate<span class="p">: The function of this gate is to decide how much information to read from the current input and previous hidden state. The update equation for this is:</span></p><p class="s248" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐼𝐼<span class="s82">𝑡𝑡</span><span class="s50">  </span>= σ(𝑊𝑊<span class="s82">𝑥𝑥𝑥</span><span class="s50">𝑥  </span>⋅ 𝑥𝑥<span class="s82">𝑥𝑥</span><span class="s50">  </span>+ 𝑊𝑊<span class="s82">ℎ</span><span class="s50">𝑥𝑥  </span>⋅ 𝐻𝐻<span class="s82">𝑡</span><span class="s50">𝑡−1  </span>+ 𝑏𝑏<span class="s82">𝑥𝑥</span><span class="s50"> </span>)</p></li><li><p class="s5" style="padding-top: 10pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Forget gate<span class="p">: The forget gate decides how much information to forget from long-term memory. The updated equation for this is:</span></p><p class="s137" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐹𝐹<span class="s42">𝑡𝑡</span><span class="s43">  </span>= σ(𝑊𝑊<span class="s42">𝑥𝑥𝑥</span><span class="s43">𝑥  </span>⋅ 𝑥𝑥<span class="s42">𝑖𝑖</span><span class="s43">  </span>+ 𝑊𝑊<span class="s42">ℎ</span><span class="s43">𝑥𝑥  </span>⋅ 𝐻𝐻<span class="s42">𝑡</span><span class="s43">𝑡−1  </span>+ 𝑏𝑏<span class="s42">𝑥𝑥</span><span class="s43"> </span>)</p><p class="s47" style="padding-top: 4pt;padding-left: 247pt;text-indent: 0pt;text-align: left;"><a name="bookmark485">Long short-term memory (LSTM) networks 303</a><a name="bookmark463">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_720.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p class="s5" style="padding-left: 55pt;text-indent: -13pt;text-align: left;">Output gate<span class="p">: The output gate decides how much of the current cell state should be used to create the current hidden state, which is the output of the cell. The update equation for this is:</span></p><p class="s80" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑂𝑂<span class="s82">𝑡𝑡</span><span class="s50">  </span>= σ(𝑊𝑊<span class="s82">𝑥𝑥𝑥</span><span class="s50">𝑥  </span>⋅ 𝑥𝑥<span class="s82">𝑖𝑖</span><span class="s50">  </span>+ 𝑊𝑊<span class="s82">ℎ</span><span class="s50">𝑥𝑥  </span>⋅ 𝐻𝐻<span class="s82">𝑡𝑡</span><span class="s50">−1  </span>+ 𝑏𝑏<span class="s82">𝑥</span><span class="s50">𝑥</span>)</p><p class="s137" style="padding-top: 13pt;padding-left: 28pt;text-indent: 0pt;line-height: 81%;text-align: justify;"><span class="s138">Here, </span>Wxi, Wxf, Wxo, Whi, Whf, and Who <span class="s138">are learnable weight parameters and </span>𝑏𝑏<span class="s42">𝑖𝑖 </span>, 𝑏𝑏<span class="s42">𝑓𝑓 </span>, and 𝑏𝑏<span class="s42">𝑜𝑜 </span><span class="s138">are </span><span class="p">learnable bias parameters.</span></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 74%;text-align: justify;"><span class="s138">N</span>ow, we can introduce a new long-term memory (cell state), <span class="s157">𝐶𝐶</span><span class="s88">𝑡</span><span class="s65">𝑡</span><span class="s138">.</span> The three gates mentioned previously serve to update and forget from this memory. If the cell state from the previous timestep is <span class="s41">𝐶𝐶</span><span class="s42">𝑡𝑡</span><span class="s43">−1</span>, then the LSTM cell calculates a candidate cell state, <span class="s157">𝐶𝐶</span><span class="s254">̃</span><span class="s88">𝑡</span><span class="s65">𝑡</span>, using another gate, but this time with <i>tanh </i>activation:</p><p class="s137" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐶𝐶<span class="s255">̃</span><span class="s128">𝑖𝑖</span><span class="s127">  </span>= 𝑡𝑡𝑡𝑡𝑡𝑡ℎ(𝑊𝑊<span class="s128">𝑥𝑥𝑥</span><span class="s127">𝑥  </span>⋅ 𝑥𝑥<span class="s128">𝑡𝑡</span><span class="s127">  </span>+ 𝑊𝑊<span class="s128">ℎ</span><span class="s127">𝑥𝑥  </span>⋅ 𝐻𝐻<span class="s128">𝑡</span><span class="s127">𝑡−1  </span>+ 𝑏𝑏<span class="s128">𝑥𝑥</span><span class="s127"> </span>)</p><p style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Here, <span class="s86">𝑊𝑊</span><span class="s238">𝑥𝑥𝑥</span><span class="s43">𝑥 </span><span class="s86">,</span><span class="s137"> and 𝑊𝑊</span><span class="s238">𝑥</span><span class="s43">𝑥ℎ </span>are learnable weight parameters and <span class="s157">𝑏𝑏</span><span class="s88">𝑐𝑐</span><span class="s65"> </span>is the learnable bias parameter.</p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, let’s look at the key update equation, which updates the cell state or long-term memory of the cell:</p><p class="s248" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐶𝐶<span class="s246">𝑡𝑡</span><span class="s146">  </span>= 𝐹𝐹<span class="s246">𝑡𝑡</span><span class="s146">  </span>⊙ 𝐶𝐶<span class="s246">𝑡𝑡</span><span class="s146">−1  </span>+ 𝐼𝐼<span class="s246">𝑡𝑡</span><span class="s146">  </span>⊙ 𝐶𝐶<span class="s256">̃</span><span class="s246">𝑡𝑡</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Here, <span class="s80">⊙ </span>is elementwise multiplication. Here, we use the forget gate to decide how much information from the previous timestep to carry forward, and the input gate to decide how much of the current candidate cell state will be written into long-term memory.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Last but not least, we use the newly created current cell state and the output gate to decide how much information to pass on to the predictor through the current hidden state:</p><p class="s248" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐻𝐻<span class="s246">𝑡𝑡</span><span class="s146">  </span>= 𝑂𝑂<span class="s246">𝑡𝑡</span><span class="s146">  </span>⊙ 𝑡𝑡𝑡𝑡𝑡𝑡ℎ(𝐶𝐶<span class="s246">𝑡</span><span class="s146">𝑡</span>)</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">A visual representation of this process can be seen in <i>Figure 12.6</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">The LSTM layer in PyTorch</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM" class="s140" target="_blank">Now, let’s understand the PyTorch implementation of LSTM. It is very similar to the RNN implementation we saw earlier, but it has one key difference: the parameters to initialize the class are pretty much the same. The API for this can be found at </a><a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM" class="a" target="_blank">https://pytorch.org/docs/stable/generated/ </a><span class="s27">torch.nn.LSTM.html#torch.nn.LSTM</span>. The key difference here is how the hidden states are used. While the RNN has a single tensor as a hidden state, the LSTM expects a <span class="s20">tuple </span>of tensors of the same dimensions: <span class="s20">(hidden state, cell state)</span>. LSTMs, just like RNNs, have stacked and bidirectional variants, and PyTorch handles them in the same way.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark486">Now, let’s initialize some LSTM modules and use the synthetic data we have been using to see them in action:</a><a name="bookmark464">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">lstm = nn.LSTM( input_size=1, hidden_size=32, num_layers=5, batch_first=True, dropout=0,</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># bidirectional=True,</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">output, (hidden_states, cell_states) = lstm(rnn_input) output.shape <span class="s38"># -&gt; [6, 15, 32]</span></p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">hidden_states.shape <span class="s38"># -&gt; [5, 6, 32]</span></p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">cell_states.shape <span class="s38"># -&gt; [5, 6, 32]</span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at another modification that’s been made to vanilla RNNs that has resolved the vanishing and exploding gradient problems.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Gated recurrent unit (GRU)</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In 2014, <i>Cho et al. </i>proposed another variant of the RNN that has a much simpler structure than an LSTM, called a <span class="s5">gated recurrent unit </span>(<span class="s5">GRU</span>). The intuition behind this is similar to when we use a bunch of gates to regulate the information that flows through the cell, but a GRU eliminates the long- term memory component and uses just the hidden state to propagate information. So, instead of the memory cell becoming the <i>gradient highway</i>, the hidden state itself becomes the “gradient highway.” In keeping with the same notation convention we used in the previous section, let’s look at the updated equations for a GRU.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">While we had three gates in an LSTM, we only have two in a GRU:</p><ul id="l118"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Reset gate<span class="p">: This gate decides how much of the previous hidden state will be considered as the candidate&#39;s hidden state of the current timestep. The equation for this is:</span></p><p class="s248" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑅𝑅<span class="s247">𝑡𝑡</span><span class="s146">  </span>= σ(𝑊𝑊<span class="s247">𝑥𝑥𝑥</span><span class="s146">𝑥  </span>⋅ 𝑥𝑥<span class="s247">𝑡𝑡</span><span class="s146">  </span>+ 𝑊𝑊<span class="s247">ℎ</span><span class="s146">𝑥𝑥  </span>⋅ 𝐻𝐻<span class="s247">𝑡𝑡</span><span class="s146">−1  </span>+ 𝑏𝑏<span class="s247">𝑥</span><span class="s146">𝑥</span>)</p><p class="s47" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark487">Gated recurrent unit (GRU) 305</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_721.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p class="s5" style="padding-left: 55pt;text-indent: -13pt;text-align: justify;">Update gate<span class="p">: The update gate decides how much of the previous hidden state should be carried forward and how much of the current candidate’s hidden state will be written into the hidden state. The equation for this is:</span></p><p class="s157" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑈𝑈<span class="s88">𝑡𝑡</span><span class="s65">  </span>= σ(𝑊𝑊<span class="s88">𝑥𝑥𝑥</span><span class="s65">𝑥  </span>⋅ 𝑥𝑥<span class="s88">𝑡𝑡</span><span class="s65">  </span>+ 𝑊𝑊<span class="s88">ℎ</span><span class="s65">𝑥𝑥  </span>⋅ 𝐻𝐻<span class="s88">𝑡</span><span class="s65">𝑡−1  </span>+ 𝑏𝑏<span class="s88">𝑥</span><span class="s65">𝑥</span>)</p><p class="s43" style="padding-top: 12pt;padding-left: 28pt;text-indent: 0pt;line-height: 80%;text-align: justify;"><span class="p">Here </span><span class="s86">𝑊𝑊</span><span class="s241">𝑥𝑥𝑥</span>𝑥<span class="s86">,</span><span class="s137"> 𝑊𝑊</span><span class="s241">𝑥𝑥𝑥</span>𝑥<span class="s86">,</span><span class="s137"> 𝑊𝑊</span><span class="s241">ℎ</span>𝑥𝑥<span class="s86">,</span><span class="s137"> and 𝑊𝑊</span><span class="s241">ℎ</span>𝑥𝑥 <span class="p">are learnable weight parameters and </span><span class="s137">𝑏𝑏</span><span class="s42">𝑟𝑟</span>  <span class="s137">and \𝑏𝑏</span><span class="s42">𝑢𝑢</span> <span class="p">are learnable bias parameters.</span></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, we can calculate the candidate’s hidden state (<span class="s137">𝐻𝐻</span><span class="s255">̃</span><span class="s42">𝑡</span><span class="s43">𝑡</span>) as follows:</p><p class="s80" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐻𝐻<span class="s85">̃</span><span class="s82">𝑡𝑡</span><span class="s50">  </span>= 𝑡𝑡𝑡𝑡𝑡𝑡ℎ(𝑊𝑊<span class="s82">𝑥</span><span class="s50">𝑥ℎ  </span>⋅ 𝑥𝑥<span class="s82">𝑡𝑡</span><span class="s50">  </span>+ 𝑊𝑊<span class="s82">ℎ</span><span class="s50">ℎ  </span>⋅ 𝑅𝑅<span class="s82">𝑡𝑡</span><span class="s50">  </span>⊙ 𝐻𝐻<span class="s82">𝑡</span><span class="s50">𝑡−1  </span>+ 𝑏𝑏<span class="s82">ℎ</span>)</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 88%;text-align: justify;">Here, <span class="s137">𝑊𝑊</span><span class="s42">𝑥</span><span class="s43">𝑥ℎ  </span><span class="s137">and 𝑊𝑊</span><span class="s42">ℎ</span><span class="s43">ℎ </span>are learnable weight parameters and <span class="s41">𝑏𝑏ℎ </span>is the learnable bias parameter. Here, we use the reset gate to throttle the information flow from the previous hidden state to the current candidate’s hidden state.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Finally, the current hidden state (the output that goes to a predictor) is computed using the following equation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s248" style="padding-top: 2pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐻𝐻<span class="s82">𝑡𝑡</span><span class="s50">  </span>= 𝑈𝑈<span class="s82">𝑡𝑡</span><span class="s50">  </span>⊙ 𝐻𝐻<span class="s82">𝑡</span><span class="s50">𝑡−1  </span>+ (1 − 𝑈𝑈<span class="s82">𝑡</span><span class="s50">𝑡</span>) ⊙ 𝐻𝐻<span class="s256">̃</span><span class="s82">𝑡𝑡</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_722.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The research papers for LSTM and GRUs are cited in the <i>References </i>section as <i>10 </i>and <i>1</i></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">respectively.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="text-indent: 0pt;text-align: left;">1<span class="p">,</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">We use the update gate to decide how much from the previous hidden state and how much from the current candidate will be passed to the next timestep or predictor.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 27pt;text-indent: 0pt;text-align: left;">A visual representation of this process can be found in <i>Figure 12.6</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark488"><span><img width="530" height="306" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_723.jpg"/></span></a><a name="bookmark465">&zwnj;</a></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 12.6 – A gating diagram of LSTM versus GRU</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The GRU layer in PyTorch</p><p class="s27" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU" class="s140" target="_blank">Now, let’s understand the PyTorch implementation of the GRU. The APIs, inputs, and outputs are the same as with an RNN. The API for this can be referenced here: </a><a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU" class="a" target="_blank">https://pytorch.org/ </a>docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU<span class="p">. The key difference is the internal workings of the modules, where the GRU update equations are used instead of the standard RNN ones.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s initialize a GRU module and use the synthetic data we have been using to see it in action:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">Gru = nn.GRU(</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">input_size=1, hidden_size=32, num_layers=5, batch_first=True, dropout=0,</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># bidirectional=True,</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">output, hidden_states = gru(rnn_input) output.shape <span class="s38"># -&gt; [6, 15, 32]</span></p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">hidden_states.shape <span class="s38"># -&gt; [5, 6, 32]</span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark489"/><a name="bookmark467">&zwnj;</a><a name="bookmark466">&zwnj;</a></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at another major component that can be used for sequential data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Convolution networks</p><p class="s5" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Convolution networks<span class="p">, also called </span>convolutional neural networks <span class="p">(</span>CNNs<span class="p">), are like neural networks for processing data in the form of a grid. This grid can be 2D, such as an image, 1D, such as a time series, 3D, such as data from LIDAR sensors, and so on. The basic idea behind CNNs is inspired by how human vision works. In 1979, Fukushima proposed Neocognitron. It was a one-of-a-kind architecture that was directly inspired by how human vision works. But CNNs came into existence as we know them today in 1989 when Yann LeCun used backpropagation to learn such a network and proved it by getting state-of-the-art results in handwritten digit recognition. In 2012, when AlexNet (a CNN architecture for image recognition) won the annual challenge of image recognition called ImageNet, that too by a large margin between it and competing non-deep learning approaches, the interest and research in CNNs peaked. People soon figured out that, apart from images, CNNs are effective with sequences, such as language and time series data.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Convolution</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">At the heart of CNNs is a mathematical operation called <span class="s5">convolution</span>. The mathematical interpretation of a convolution operation is beyond the scope of this book, but there are a couple of links in the <i>Further reading </i>section if you want to learn more. For our purposes, we’ll develop an intuitive understanding of the convolution operation. Since CNNs rose to popularity on image data, let’s start by discussing the image domain and then transition to the sequence domain.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Any image (for simplicity, let’s assume it’s grayscale) can be considered as a grid of pixel values, each value denoting how bright that point is, with 1 being pure white and 0 being pure black. Before we start talking about convolution, let’s understand what a <span class="s5">kernel </span>is. For now, let’s think of a kernel as a 2D matrix with some values in it. Typically, the kernel’s size is smaller than the image’s size we are using. Since the kernel is smaller than the image, we can “fit” the kernel inside the image. Let’s start with the kernel aligned on the left top edge. With the kernel at the current position, there is a set of values in the image that this kernel is superpositioned over. We can perform element-wise multiplication between this subset of the image and the kernel, and then sum up all the elements into a single scalar. Now, we can repeat this process by “sliding” the kernel into all positions in the image. For instance, the following shows a sample image input whose size is 4x4 and how a convolution operation is carried out on that using a kernel whose size is 2x2:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 110pt;text-indent: 0pt;text-align: left;"><a name="bookmark490"><span><img width="332" height="231" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_724.jpg"/></span></a><a name="bookmark468">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 12.7 – A convolution operation on 2D and 1D inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">So, if we place the 2x2 kernel at the top left position and perform the element-wise multiplication and the summation, we get the top left item in the 3x3 output. If we slide the kernel by one position to the right, we get the next element in the top row of the output, and so on. Similarly, if we slide the kernel by one position down, we get the second element in the first column in the output.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">While this is interesting, we want to understand convolutions from a time series perspective. To do so, let’s shift our paradigm to 1D convolutions – convolutions performed on 1-dimensional data such as a sequence. In the preceding diagram, we can also see an example of a 1D convolution where we take the 1D kernel and slide it across the sequence to get an output of 1x3.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_725.png"/></span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 85%;text-align: justify;">Although we have set the kernel weights so that they’re convenient to understand and compute, in practice, these weights are learned by the network from data. If we set the kernel size as <span class="s41">𝑛𝑛 </span>and all the kernel weights as <span class="s257">1</span><a href="#bookmark211" class="s140">, what would such a convolution give us? This is something we covered in </a><a href="#bookmark211" class="s21">Chapter </a><i>6</i>, <i>Feature Enginee</i><span class="s258">𝑛𝑛</span><i>ring for Time Series Forecasting</i>. Yes, they result in the rolling means with a window of <span class="s41">𝑛𝑛</span>. Remember, we learned this as a feature engineering technique for machine learning models.</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">So, 1D convolutions can be thought of as a more powerful feature generator, where the features are</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">learned from data. With different weights on the kernels, we will be extracting different features. It is this intuition that we should hold on to while learning CNNs for time series data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Padding, stride, and dilations</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have understood what a convolution operation is, we need to understand a few more terms, such as <span class="s5">padding</span>, <span class="s5">stride</span>, and <span class="s5">dilations</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 79%;text-align: justify;">Before we start talking about these terms, let’s look at an equation that gives the output dimensions (<span class="s74">𝑂𝑂</span>) of a convolutional layer, given the input dimensions (<span class="s259">𝐿</span><span class="s157">𝐿</span>), kernel size (<span class="s210">𝑘</span><span class="s80">𝑘</span>), padding size (<span class="s260">𝑝</span><span class="s41">𝑝</span><span class="s65">𝑙𝑙 </span>for left padding and <span class="s260">𝑝</span><span class="s41">𝑝</span><span class="s43">𝑟𝑟 </span>for right padding), stride (  ), and dilation (<span class="s261">𝑑</span><span class="s157">𝑑</span>):</p><p class="s90" style="padding-top: 2pt;padding-left: 12pt;text-indent: 0pt;line-height: 10pt;text-align: center;">𝐿𝐿 + 𝑝𝑝<span class="s128">𝑙𝑙  </span>+ 𝑝𝑝<span class="s128">𝑟𝑟  </span>− 𝑑𝑑  × (𝑘𝑘 − 1) − 1</p><p class="s90" style="text-indent: 0pt;line-height: 10pt;text-align: right;">𝑂𝑂 =</p><p class="s90" style="padding-left: 132pt;text-indent: 0pt;line-height: 8pt;text-align: left;">+ 1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="173" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_726.png"/></span></p><p class="s90" style="padding-left: 63pt;text-indent: 0pt;line-height: 10pt;text-align: left;">𝑠𝑠</p><p class="s137" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 91%;text-align: justify;"><span class="p">The default values (padding, strides, and dilations are special cases of a convolution process) of these terms are </span>𝑝𝑝<span class="s42">𝑟𝑟</span>, 𝑝𝑝<span class="s42">𝑙𝑙 </span>= 0, 𝑠𝑠 = 1, 𝑑𝑑 = 1<span class="p">. Don’t worry if you don’t understand the formula or the terms in it – just keep the default values in mind so that when we understand each term, we can negate the others.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;">In <i>Figure 12.7</i>, we noticed that the convolution operation always reduces the size of the input. So, in the default case, the formula becomes <span class="s137">𝑂𝑂 = 𝐿𝐿 – (𝑘𝑘 − 1)</span>. This is because the earliest position we can place the kernel in the sequence is from <span class="s137">𝑡𝑡 = 0 </span>to <span class="s41">𝑡𝑡 = 𝑘𝑘</span>. Then, by convolving through the sequence, we get <span class="s137">𝐿𝐿 − (𝑘𝑘 − 1) </span>terms in the output. Padding is when we add some values to the beginning or the end of the sequence. The value we use for padding is dependent on the problem. Typically, we choose zero as a padding value. So, padding a sequence essentially increases the size of the input. So, in the preceding formula, we can think of <span class="s137">𝐿𝐿 + 𝑝𝑝</span><span class="s42">𝑙𝑙 </span><span class="s137">+ 𝑝𝑝</span><span class="s42">𝑟𝑟 </span>as the effective length of the sequence after padding.</p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The next two terms (stride and dilation) are closely related to the <span class="s5">receptive field </span>of the convolutional layer. The receptive field of a convolutional layer is the region in the input space that influences the feature that’s generated by the convolutional layer. Or, in other words, it is the size of the window of input over which we have performed the convolution operation. For a single convolutional layer (with default settings), this is pretty much the kernel size. For multi-layered CNNs, this calculation becomes a bit more complicated because of the hierarchical structure (the <i>Further reading </i>section contains a link to a paper by Arujo et al. who derived a formula to calculate the receptive field of a CNN). But generally, increasing the receptive field of a CNN is associated with an increase in the accuracy of the CNN. For computer vision, Araujo et al. noted the following:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 67pt;text-indent: 0pt;line-height: 89%;text-align: center;">“We observe a logarithmic relationship between classification accuracy and receptive field size, which suggests that large receptive fields are necessary for high- level recognition tasks, but with diminishing rewards.”</p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In time series, this is important because if the receptive field of a CNN is smaller than the long-term dependency, such as the seasonality, that we want to capture, then the network will fail to do so. Making the CNN deeper by stacking more convolutional layers on top of the others is one way to increase the receptive field of a network. But there are a few ways to increase the receptive field of a single convolutional layer. Strides and dilations are two such ways:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s5">Stride</span>: Earlier, when we talked about <i>sliding </i>the kernel over the sequence, we mentioned that we move the kernel by one position at a time. This is called the stride of the convolutional layer and there is no necessity that the stride should be 1. If we set the stride to 2, the convolution operation would be performed by skipping a position in between, as shown in <i>Figure 12.8</i>. This can make each layer in the convolutional network look at a larger slice of history, thereby increasing the receptive field.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l119"><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s5">Dilation</span>: Another way we can tweak the basic convolutional layer is by dilating the input connections. In the standard convolutional layer with a kernel size of 3, we apply the kernel to three consecutive elements in the input with a dilation of 1. If we increase the dilation to 2, then the kernel will be dilated spatially and will be applied. Instead of being applied to three consecutive elements, an element in between will be skipped. <i>Figure 12.8 </i>shows how this works. As we can see, this can also increase the receptive field of the network.</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Both these techniques are similar but different and are compatible with each other. The following diagram shows what happens when we apply strides and dilations together (although this doesn’t happen frequently):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="389" height="404" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_727.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 12.8 – Strides and dilations in convolutions</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, what if we want to make the output dimensions the same as the input dimensions? By using some basic algebra and rearranging the previous formula, we get the following:</p><p class="s157" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑃𝑃<span class="s88">𝑙𝑙    </span>+ 𝑝𝑝<span class="s88">𝑟𝑟   </span>= 𝑑𝑑(𝑘𝑘 − 1) + 𝐿𝐿(𝑠𝑠 − 1) − (𝑠𝑠 − 1)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark491">And in time series, we typically pad on the left rather than the right because of the strong autocorrelation that is typically present. Padding the latest few entries with zeros or some other values will make the learning of the prediction function very hard because the latest hidden states are directly influenced by the padded values. The </a><i>Further reading </i>section contains a link to an article by Kilian Batzner about autoregressive convolutions. It is a must-read if you wish to really understand the concepts we have discussed here and also understand a few limitations. The <i>Further reading </i>section also contains a link to a GitHub repository that contains animations of convolutions for 2D inputs, which will give you a good intuition of what is happening.<a name="bookmark469">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There is just one more term that you may hear often in convolutions, especially in time series – <span class="s5">causal convolutions</span>. But all you have to keep in mind is that causal convolutions are not special types of convolutions. So long as we ensure that we won’t be using future time steps to predict the current timestep while training, we are performing causal operations. This is typically done by offsetting the target and padding the inputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The convolution layer in PyTorch</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s understand the PyTorch implementation of the CNN (a one-dimensional CNN, which is typically used for sequences such as time series). Let’s look at the different parameters the implementation provides while initializing. We have just discussed the following terms, so they should be familiar to you by now:</p></li></ul></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">in_channels<span class="p">: The number of expected features in the input. If we are using just the history of the time series, then this would be 1. But when we use history along with some other features, then this will be &gt;1. For subsequent layers, </span>out_channels <span class="p">you have used in the previous layer become your </span>in_channels <span class="p">in the current layer.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">out_channels<span class="p">: The number of kernels or filters applied to the input. Each kernel/filter produces a convolution operation with its own weights.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">kernel_size<span class="p">: This is the size of the kernel we use for convolving.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">stride<span class="p">: The stride of the convolution. The default is </span>1<span class="p">.</span></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s20">padding</span>: This is the padding that is added to <i>both </i>sides. If we set the value as 2, the sequence that we pass to the layer will have a padded position on both the left and right. We can also give <span class="s20">valid </span>or <span class="s20">same </span>as input. These are easy ways of mentioning the kind of padding we need to add. <span class="s20">padding=’valid’ </span>is the same as no padding. <span class="s20">padding=’same’ </span>pads the input so that the output has the shape as the input. However, this mode doesn’t support any stride values other than 1. The default is <span class="s20">0</span>.</p></li><li><p style="padding-top: 5pt;padding-left: 54pt;text-indent: -13pt;text-align: justify;"><span class="s20">padding_mode</span>: This defines how the padded positions should be filled with values. The most common and default option is <i>zeros</i>, where all the padded tokens are filled with zeros. Another useful mode that is relevant for time series is <span class="s20">replicate</span>, which behaves like forward and backward fill in pandas. The other two options – <span class="s20">reflect </span>and <span class="s20">circular </span>– are more esoteric and are only used for specific use cases. The default is <span class="s20">zeros</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l120"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a name="bookmark492">dilation</a><span class="p">: The dilation of the convolution. The default is </span>1<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">groups<span class="p">: This parameter lets you control the way input channels are connected to output channels. The number specified in </span>groups <span class="p">specifies how many groups will be formed so that the convolutions happen within a group and not across. For instance, </span>group=2 <span class="p">means that half the input channels will be convolved by one set of kernels and that the other half will be convolved by a separate set of kernels. This is equivalent to running two convolution layers side by side. Check the documentation for more information on this parameter. Again, this is for an esoteric use case. The default is </span>1<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">bias<span class="p">: This parameter adds a learnable bias to the convolutions. The default is </span>True<span class="p">.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s apply a CNN model to the same synthetic data we generated earlier in this chapter with a kernel size of 3:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=k)  </span></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at the inputs and outputs that are expected from a CNN.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s20">Conv1d </span>expects the inputs to have three dimensions – <i>(batch size, number of channels, sequence length)</i>. For the initial input layer, the number of channels is the number of features you are feeding into the network; for intermediate layers, it is the number of kernels we have used in the previous layer. The output from <span class="s20">Conv1d </span>is in the form of <i>(batch size, number of channels (output), sequence length (output))</i>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">So, let’s run our sequence through <span class="s20">Conv1d </span>and look at the inputs and outputs (for more detailed steps, refer to the <span class="s20">02-Building Blocks.ipynb </span>notebook):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">#input dim: torch.Size([6, 1, 15])</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># batch size = 6, number of features = 1 and sequence length = 15</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">output = conv(cnn_input)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Output should be in_dim - k + 1</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">assert output.size(-1)==cnn_input.size(-1)-k+1 output.shape <span class="s38">#-&gt; torch.Size([6, 1, 13])</span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The notebook provides a slightly more detailed analysis of <span class="s20">Conv1d</span>, with tables showing the impact that the hyperparameters have on the shape of the output, what kind of padding is used to make the input and output dimensions the same, and how a convolution with equal weights is just like a rolling mean. I highly suggest that you check it out and play around with the different options to get a feel of what the layer does for you.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="138" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_728.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s29" style="padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional information</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The inbuilt padding in <span class="s20">Conv1d </span>has its roots in image processing, so the padding techniqu defaults to adding adding to both sides. But for sequences, it is preferable to use padding o the left and because of that, it is also preferable to handle how the input sequences are padde separately and not use the inbuilt mechanism. <span class="s20">torch.nn.functional </span>has a handy metho called <span class="s20">pad </span>that can be used to this effect.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 89%;text-align: justify;">e n d d</p><p style="text-indent: 0pt;text-align: left;"/><p class="s47" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark493">Summary 313</a><a name="bookmark471">&zwnj;</a><a name="bookmark470">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark522" class="s140">Other building blocks are used in time series forecasting because the architecture of a deep neural network is only limited by creativity. But the point of this chapter was to introduce you to the common ones that appear in many different architectures. We also intentionally left out one of the most popular architectures used nowadays: the transformer. This is because we have devoted another chapter (</a>Chapter 14<span class="p">, </span>Attention and Transformers for Time Series<span class="p">) to understanding attention before we look at transformers. Another major block that is slowly gaining popularity is graph neural networks, which can be thought of as specialized CNNS that operate on graph-based data rather than grids. However, this is outside the scope of this book since it is an area of active research.</span></p><p class="s3" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">After introducing deep learning in the previous chapter, in this chapter, we gained a deeper understanding of the common architectural blocks that are used for time series forecasting. The encoder-decoder paradigm was explained as a fundamental way we can structure a deep neural network for forecasting. Then, we learned about FFNs, RNNS, and CNNs and explored how they are used to process time series. We also saw how we can use all these major blocks in PyTorch by using the associated notebook and got our hands dirty with some PyTorch code.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the next chapter, we’ll learn about a few major patterns we can use to arrange these blocks to perform time series forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The following references were used in this chapter:</p><ol id="l121"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Neco, R. P., and Forcada, M. L. (1997), <i>Asynchronous translations with recurrent neural nets</i><a href="https://ieeexplore.ieee.org/document/614693" class="s140" target="_blank">. Neural Networks, 1997., International Conference on (Vol. 4, pp. 2535–2540). IEEE: </a><a href="https://ieeexplore.ieee.org/document/614693" class="a" target="_blank">https:// </a><span class="s27">ieeexplore.ieee.org/document/614693</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Kalchbrenner, N., and Blunsom, P. (2013), <i>Recurrent Continuous Translation Models</i><a href="https://aclanthology.org/D13-1176/" class="s140" target="_blank">. EMNLP (Vol. 3, No. 39, p. 413): </a><span class="s27">https://aclanthology.org/D13-1176/</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. (2014), <i>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</i><a href="https://aclanthology.org/D14-1179/" class="s140" target="_blank">. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar. Association for Computational Linguistics: </a><span class="s27">https://aclanthology.org/D14-1179/</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;"><a name="bookmark494">Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. (2014), </a><i>Sequence to sequence learning with neural networks. </i><a href="https://dl.acm.org/doi/10.5555/2969033.2969173" class="s140" target="_blank">Proceedings of the 27th International Conference on Neural Information Processing Systems – Volume 2: </a><span class="s27">https://dl.acm.org/doi/10.5555/2969033.2969173</span>.<a name="bookmark472">&zwnj;</a></p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Rumelhart, D., Hinton, G., and Williams, R (1986). <i>Learning representations by back-propagating errors</i><a href="https://doi.org/10.1038/323533a0" class="s140" target="_blank">. Nature 323, 533–536: </a><span class="s27">https://doi.org/10.1038/323533a0</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Schuster, M., and Paliwal, K. K. (1997). <i>Bidirectional recurrent neural networks</i><a href="https://doi.org/10.1109/78.650093" class="s140" target="_blank">. IEEE Transactions on Signal Processing, 45(11), 2673–2681: </a><span class="s27">https://doi.org/10.1109/78.650093</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Sepp Hochreiter (1991) <i>Untersuchungen zu dynamischen neuronalen Netzen</i><a href="https://people.idsia.ch/%7Ejuergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" class="s140" target="_blank">. Diploma thesis, TU Munich: </a><a href="https://people.idsia.ch/%7Ejuergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" class="a" target="_blank">https://people.idsia.ch/~juergen/ </a><span class="s27">SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Y. Bengio, P. Frasconi, and P. Simard (1993), <i>The problem of learning long-term dependencies in recurrent networks</i><a href="http://10.1109/ICNN.1993.298725" class="s140" target="_blank">. IEEE International Conference on Neural Networks, pp. 1183-1188 vol.3: </a><span class="s27">10.1109/ICNN.1993.298725</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Y. Bengio, P. Simard, and P. Frasconi (1994) <i>Learning long-term dependencies with gradient descent is difficult </i><a href="http://10.1109/72.279181" class="s140" target="_blank">in IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157–166, March 1994: </a><span class="s27">10.1109/72.279181</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Hochreiter, S., and Schmidhuber, J. (1997). <i>Long short-term memory</i><a href="https://doi.org/10.1162/neco.1997.9.8.1735" class="s140" target="_blank">. Neural computation, 9(8), 1735–1780: </a><span class="s27">https://doi.org/10.1162/neco.1997.9.8.1735</span>.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Cho, K., Merrienboer, B.V., Gülçehre, Ç., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio,</p><p style="padding-left: 64pt;text-indent: 0pt;text-align: justify;">Y. (2014). <i>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. </i><a href="https://www.aclweb.org/anthology/D14-1179.pdf" class="s140" target="_blank">EMNLP: </a><span class="s27">https://www.aclweb.org/anthology/D14-1179.pdf</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Fukushima, K. <i>Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</i><a href="https://doi.org/10.1007/BF00344251" class="s140" target="_blank">. Biol. Cybernetics 36, 193–202 (1980): </a><a href="https://doi.org/10.1007/BF00344251" class="a" target="_blank">https:// </a><span class="s27">doi.org/10.1007/BF00344251</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Y. Le Cun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D. Jackel, and D. Henderson. 1990. <i>Handwritten digit recognition with a back-propagation network</i><a href="https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf" class="s140" target="_blank">. Advances in neural information processing systems 2. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 396–404: </a><a href="https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf" class="a" target="_blank">https://proceedings.neurips.cc/paper/1989/file/53c3bce6 </a><span class="s27">6e43be4f209556518c2fcb54-Paper.pdf</span>.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Take a look at the following resources to learn more about the topics that were covered in this chapter:</p><ul id="l122"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;line-height: 111%;text-align: justify;"><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" class="s140" target="_blank">Official PyTorch Tutorials: </a><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" class="a" target="_blank">https://pytorch.org/tutorials/beginner/basics/ </a><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank">intro.html</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 64pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Essence of linear algebra<a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" class="s140" target="_blank">, by3Blue1Brown: </a><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" class="a" target="_blank">https://www.youtube.com/ </a><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank">playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab</a></p></li></ul><p class="s47" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Further reading 315</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_729.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p class="s4" style="padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">Neural Networks – A Linear Algebra Perspective<a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" class="s140" target="_blank">, by Manu Joseph: </a><a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" class="a" target="_blank">https://deep- and-shallow.com/2022/01/15/neural-networks-a-linear-algebra- </a><a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" target="_blank">perspective/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">Deep Learning<a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" class="s140" target="_blank">, by Ian Goodfellow, Yoshua Bengio,and Aaron Courville: </a><a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" class="a" target="_blank">https://deep- and-shallow.com/2022/01/15/neural-networks-a-linear-algebra- </a><a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/" target="_blank">perspective/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Understanding LSTMs<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="s140" target="_blank">, by Christopher Olah: </a><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="a" target="_blank">http://colah.github.io/posts/2015- </a><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">08-Understanding-LSTMs/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Intuitive Guide to Convolution<a href="https://betterexplained.com/articles/intuitive-convolution/" class="s140" target="_blank">: </a><a href="https://betterexplained.com/articles/intuitive-convolution/" class="a" target="_blank">https://betterexplained.com/articles/ </a><a href="https://betterexplained.com/articles/intuitive-convolution/" target="_blank">intuitive-convolution/</a></p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Computing Receptive Fields of Convolutional Neural Networks<a href="https://distill.pub/2019/computing-receptive-fields/" class="s140" target="_blank">, by Andre Araujo, Wade Norris, and Jack Sim: </a><a href="https://distill.pub/2019/computing-receptive-fields/" target="_blank">https://distill.pub/2019/computing-receptive-fields/</a></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Convolutions in Autoregressive Neural Networks<a href="https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/" class="s140" target="_blank">, by Kilian Batzner: </a><a href="https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/" class="a" target="_blank">https://theblog. </a><a href="https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/" target="_blank">github.io/post/convolution-in-autoregressive-neural-networks/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Convolution Arithmetic<a href="https://github.com/vdumoulin/conv_arithmetic" class="s140" target="_blank">, by Vincent Dumoulin and Francesco Visin: </a><a href="https://github.com/vdumoulin/conv_arithmetic" class="a" target="_blank">https://github. </a><a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">com/vdumoulin/conv_arithmetic</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Dropout: A Simple Way to Prevent Neural Networks from Overfitting<span class="p">, by Nitish Srivastava et al:</span></p><p style="padding-top: 1pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><a href="https://jmlr.org/papers/v15/srivastava14a.html">https://jmlr.org/papers/v15/srivastava14a.html</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark495">13</a><a name="bookmark496">&zwnj;</a><a name="bookmark498">&zwnj;</a><a name="bookmark497">&zwnj;</a></h2><h4 style="padding-top: 2pt;text-indent: 0pt;text-align: right;">Common Modeling Patterns for</h4><h4 style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Time Series</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We reviewed a few major and common building blocks of a <span class="s5">deep learning </span>(<span class="s5">DL</span>) system, specifically suited for time series, in the last chapter. Now that we know what those blocks are, it’s time for a more practical lesson. Let’s see how we can put these common blocks together in various common ways in which time series forecasting is modeled using the dataset we have been working with all through this book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Tabular regression</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Single-step-ahead recurrent neural networks</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Sequence-to-sequence models</p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all packages and datasets required for the code in this book.</p><p class="s27" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter13" class="s140" target="_blank">The associated code for the chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter13" class="a" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter13<span class="p">.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You need to run the following notebooks for this chapter:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">02-Preprocessing London Smart Meter Dataset.ipynb <span class="p">in </span>Chapter02</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">01-Setting up Experiment Harness.ipynb <span class="p">in </span>Chapter04</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">01-Feature Engineering.ipynb <span class="p">in </span>Chapter06</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l123"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a name="bookmark507">00-Single Step Backtesting Baselines.ipynb, 01-Forecasting with ML.ipynb</a><span class="p">, and </span>02-Forecasting with Target Transformation.ipynb in Chapter08<a name="bookmark499">&zwnj;</a></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">01-Global Forecasting Models-ML.ipynb <span class="p">in </span>Chapter10</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Tabular regression</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark186" class="s140">In </a>Chapter 5<span class="p">, </span>Time Series Forecasting as Regression<a href="#bookmark211" class="s140">, we saw how we can convert a time series problem into a standard regression problem by temporal embedding and time delay embedding. In </a>Chapter 6<span class="p">, </span>Feature Engineering for Time Series Forecasting<a href="#bookmark286" class="s140">, we have already created the necessary features for the household energy consumption dataset we have been working on, and in </a>Chapter 8<span class="p">, </span>Forecasting Time Series with Machine Learning Models<a href="#bookmark337" class="s140">, </a>Chapter 9<span class="p">, </span>Ensembling and Stacking<a href="#bookmark367" class="s140">, and </a>Chapter 10<span class="p">, </span>Global Forecasting Models<span class="p">, we used traditional </span><span class="s5">machine learning </span><span class="p">(</span><span class="s5">ML</span><span class="p">) models to create a forecast.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Just as we used standard ML models for forecasting, we can also use DL models built for tabular data using the feature-engineered dataset we have created. One of the advantages of using a DL model in this setting, over the ML models, is the flexibility DL offers us. All through <i>Chapters 8</i>, <i>9</i>, and <i>10</i>, we only saw how we can create single-step-ahead forecasting using ML models. We have a separate section on multi-step forecasting in <i>Part 3</i>, <i>Deep Learning for Time Series, </i>where we go into detail on different strategies with which we can generate multi-step forecasts, and we address one of the limitations of standard ML models in multi-step forecasting. But right now, let’s just understand that standard ML models are designed to have a single output and, because of that fact, getting multi-step forecasts is not straightforward. But with tabular DL models, we have the flexibility to train the model to predict multiple targets, and this enables us to generate multi-step forecasts easily.</p><p class="s4" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/manujosephv/pytorch_tabular" class="s140" target="_blank">PyTorch Tabular is an open source library (</a><a href="https://github.com/manujosephv/pytorch_tabular" class="a" target="_blank">https://github.com/manujosephv/pytorch_ </a><span class="s27">tabular</span><a href="#bookmark211" class="s140">) that makes it easy to work with DL models in the tabular data domain, and it also has ready-to-use implementations of many state-of-the-art DL models. We are going to use PyTorch Tabular to generate forecasts using the feature-engineered datasets we created in </a>Chapter 6<span class="p">, </span>Feature Engineering for Time Series Forecasting<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_730.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code, use the notebook named <span class="s20">01-Tabular Regression. ipynb </span>in the <span class="s20">Chapter13 </span>folder and the code in the <span class="s20">src </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://pytorch-tabular.readthedocs.io/en/latest/" class="s140" target="_blank">PyTorch Tabular has very detailed documentation and tutorials to get you started here: </a><a href="https://pytorch-tabular.readthedocs.io/en/latest/" class="a" target="_blank">https:// </a><span class="s27">pytorch-tabular.readthedocs.io/en/latest/</span>. Although we won’t be going into detail on all the intricacies of the library, we will look at how we can use a bare-bones version to generate a forecast on the dataset we are working on using a <span class="s20">FTTransformer </span>model. <span class="s20">FTTransformer </span>is one of the state-of-the-art DL models for tabular data. DL for tabular data is a whole different kind of model, and I’ve linked a blog post in the <i>Further reading </i>section as a primer to the field of study. For our purposes, we can treat them as any standard ML model in scikit-learn.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark508">We start off, pretty much like before, by loading the libraries and necessary datasets. Just one additional thing we are doing here is that instead of taking the same selection of blocks we worked with in </a><i>Part 2</i>, <i>Machine Learning for Time Series</i>, we take smaller-sized data by selecting half the number of blocks as before. This is done to make the <span class="s5">neural network </span>(<span class="s5">NN</span>) training smoother and faster and for it to fit into GPU memory (if any). I’d like to stress here that this is done purely for hardware reasons, and provided we have sufficiently powerful hardware, we need not have smaller datasets for DP. On the contrary—DL loves larger datasets. But since we want to keep the focus on the modeling side, the engineering constraints and techniques in working with larger datasets have been kept outside the scope of this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">uniq_blocks = train_df.file.unique().tolist()</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">sel_blocks = sorted(uniq_blocks, key=lambda x: int(x. replace(&quot;block_&quot;,&quot;&quot;)))[:len(uniq_blocks)//2]</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">train_df = train_df.loc[train_df.file.isin(sel_blocks)] test_df = test_df.loc[test_df.file.isin(sel_blocks)] sel_lclids = train_df.LCLid.unique().tolist()</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">After handling the missing values, we are ready to start using PyTorch Tabular. We first import the necessary classes from the library, like so:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">from pytorch_tabular.config import DataConfig, OptimizerConfig,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">TrainerConfig</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from pytorch_tabular.models import FTTransformerConfig from pytorch_tabular import TabularModel</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">PyTorch Tabular uses a set of config files to define the parameters required for running the model, and these configs include everything from how the dataframe is configured to what kind of preprocessing needs to be applied, what kind of training we need to do, what model we need to use, what the hyperparameters of the model are, and so on. Let’s see how we can define a bare-bones configuration (because PyTorch Tabular makes use of intelligent defaults wherever possible to make the usage easier for the practitioner):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">data_config = DataConfig(</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">target=[target], <span class="s38">#target should always be a list</span></p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: -24pt;line-height: 131%;text-align: left;">continuous_cols=[ &quot;visibility&quot;, &quot;windBearing&quot;,</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">… &quot;timestamp_Is_month_start&quot;,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">],</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">categorical_cols=[</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">&quot;holidays&quot;,</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">… &quot;LCLid&quot;</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">],</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">normalize_continuous_features=True</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">trainer_config = TrainerConfig(</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">auto_lr_find=True, <span class="s38"># Runs the LRFinder to automatically derive a learning rate</span></p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">batch_size=1024, max_epochs=1000, auto_select_gpus=True, gpus=-1</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">optimizer_config = OptimizerConfig()</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark509"/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We use a very high <span class="s20">max_epochs </span>parameter in <span class="s20">TrainerConfig </span>because by default, PyTorch Tabular employs a technique called <span class="s5">early stopping, </span>where we continuously keep track of the performance on a validation set and stop the training when the validation loss starts to increase.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Selecting which model to use from the implemented models in PyTorch Tabular is as simple as choosing the right configuration. Each model is associated with a configuration that defines the hyperparameters of the model. So, just by using that specific configuration, PyTorch Tabular understands which model the user wants to use. Let’s choose the <span class="s20">FTTransformerConfig </span>model and define a few hyperparameters:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">model_config = FTTransformerConfig( task=&quot;regression&quot;, num_attn_blocks=3,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">num_heads=4, transformer_head_dim=64, attn_dropout=0.2, ff_dropout=0.1, out_ff_layers=&quot;32&quot;, metrics=[&quot;mean_squared_error&quot;]</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The main and only mandatory parameter here is <span class="s20">task</span>, which tells PyTorch Tabular whether it is a</p><p class="s4" style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">regression <span class="p">or </span>classification <span class="p">task.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="107" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_731.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Although PyTorch Tabular provides the best defaults, we only set these parameters to make the training faster and fit into the memory of the GPU we are running on. If you are not running the notebook on a machine with a GPU, choosing a smaller and faster model such as <span class="s20">CategoryEmbeddingConfig </span>would be better.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, all that is left to do is put all these configs together in a class called <span class="s20">TabularModel</span>, which is the workhorse of the library, and as with any scikit-learn model, call <span class="s20">fit </span>on the object. But, unlike a scikit-learn model, you don’t need to split <span class="s20">X </span>and <span class="s20">y</span>; we just need to provide the dataframe, as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">tabular_model.fit(train=train_df)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Once the training completes, you can save the model by just running the following code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">tabular_model.save_model(&quot;notebooks/Chapter13/ft_transformer_ global&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">If for any reason you have to close your notebook instance after training, you can always load the model back by using the following code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">tabular_model = TabularModel.load_from_checkpoint(&quot;notebooks/ Chapter13/ft_transformer_global&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">This way, you don’t need to spend a lot of time training the model again, but instead, use it for prediction.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, all that is left is to predict on the unseen data and evaluate the performance. Here’s how we can do this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">forecast_df = tabular_model.predict(test_df) agg_metrics, eval_metrics_df = evaluate_forecast(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">y_pred=forecast_df[f&quot;{target}_prediction&quot;], test_target=forecast_df[&quot;energy_consumption&quot;], train_target=train_df[&quot;energy_consumption&quot;], model_name=model_config._model_name,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p class="s4" style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140">We have used the untuned global forecasting model with metadata that we trained in </a>Chapter 10<span class="p">, </span>Global Forecasting Models<span class="p">, as the baseline against which we can do a cursory check on how well the DL model is doing, as illustrated in the following screenshot:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: left;"><a name="bookmark510"><span><img width="430" height="80" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_732.jpg"/></span></a><a name="bookmark500">&zwnj;</a></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.1 – Evaluation of the DL-based tabular regression</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see that the <span class="s20">FTTransformer </span>model is competitive with the <span class="s20">LightGBM </span><a href="#bookmark367" class="s140">model we trained in </a><i>Chapter 10</i>. Maybe, with the right amount of tuning and partitioning, the <span class="s20">FTTransformer </span>model can do as well or better than the <span class="s20">LightGBM </span>one. Training a competitive DL model in the same way as <span class="s20">LightGBM </span>is useful in many ways. First, this brings us flexibility and trains the model to predict multiple timesteps at once. Second, this can also be combined with the <span class="s20">LightGBM </span>model in an ensemble, and because of the variety the DL model brings to the mix, this can make the ensemble performance better.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="96" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_733.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Things to try</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">Use PyTorch Tabular’s documentation and play around with other models or change the parameters to see how the performances changes.</p><p style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Select a few households and plot them to see how well the forecast matches up to the targets.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at how we can use RNN for single-step-ahead forecasting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Single-step-ahead recurrent neural networks</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140">Although we took a little detour to check out how DL regression models can be used to train the same global models we learned about in </a>Chapter 10<span class="p">, </span>Global Forecasting Models<a href="#bookmark561" class="s140">, now we are back to looking at DL models and architectures specifically built for time series. And as always, we will look at simple one-step-ahead and local models first before moving on to more complex modeling paradigms. In fact, we have another chapter (</a>Chapter 15<span class="p">, </span>Strategies for Global Deep Learning Forecasting Models<span class="p">) entirely devoted to techniques we can use to train global DL models.</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s bring our attention back to one-step-ahead local models. We saw <span class="s5">recurrent neural networks </span>(<span class="s5">RNNs</span>) (vanilla RNN, <span class="s5">long short-term memory </span>(<span class="s5">LSTM</span>), and <span class="s5">gated recurrent unit </span>(<span class="s5">GRU</span>)) as a few blocks we can use for sequences such as time series. Now, let’s see how we can use them in an <span class="s5">end-to-end </span>(<span class="s5">E2E</span>) model on the dataset we have been working on (the <i>London smart meters </i>dataset).</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Although we will be looking at a few libraries (such as <span class="s20">darts</span>) that make the process of training DL models for time series forecasting easier, in this chapter, we will be looking at how to develop such models from scratch. Understanding how a DL model for time series forecasting is put together from the ground up will give you a good grasp of the concepts that are needed to use and tweak the libraries that we will be looking at later.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark454" class="s140" name="bookmark511">We will be using PyTorch, and if you are not comfortable, I suggest you head to </a>Chapter 12<span class="p">, </span>Building Blocks of Deep Learning for Time Series<span class="p">, and the associated notebooks for a quick refresher. On top of that, we are also going to use PyTorch Lightning, which is another library built on top of PyTorch to make training models using PyTorch easy, among other benefits.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We talked about <span class="s5">time delay embedding </span><a href="#bookmark186" class="s140">in </a><i>Chapter 5</i>, <i>Time Series Forecasting as Regression</i>, where we talked about using a window in time to embed the time series into a format more suitable for regression. When training NNs for time series forecasting also, we need such windows. Suppose we are training on a single time series. We can give this super long time series to an RNN as is, but then it only becomes one sample in the dataset. And with just one sample in the dataset, it’s close to impossible to train any ML or DL models. So, it’s advisable to sample multiple windows from the time series to convert the time series into a number of data samples in a process that is very much similar to time delay embedding. This window also sets the memory of the DL model.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The first step we need to take is to create a PyTorch dataset that takes the raw time series and prepares these samples’ windows. A dataset is like an iterator over the data that gives us samples corresponding to a provided index. Defining a custom dataset for PyTorch is as simple as defining a class that takes in a few arguments (data being one of them) and defining two mandatory methods in the class, as follows:</p></li></ul></li><li><p class="s166" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">     <span class="s20">len (self)</span><span class="p">—This sets the maximum number of samples in the dataset.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s166">     </span>get_item (self, idx)<span class="p">—This picks the </span>idx<span class="p">th sample from the dataset.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We have defined a dataset in <span class="s20">src/dl/dataloaders.py </span>with the name <span class="s20">TimeSeriesDataset</span>, which takes in the following parameters:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">data<span class="p">—This argument can either be a pandas dataframe or a NumPy array with the time series. This is the entire time series, including train, validation, and test, and the splits occur inside the class.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">window<span class="p">—This sets the length of each sample.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">horizon<span class="p">—This sets the number of future timesteps we want to get as the target.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">n_val<span class="p">—This parameter can either be a </span>float <span class="p">or an </span>int <span class="p">data type. If </span>int<span class="p">, it represents the number of timesteps to be reserved as validation data. And if </span>float<span class="p">, this represents the percent of total data to be reserved as validation data.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">n_test<span class="p">—This parameter is similar to </span>n_val<span class="p">, but does the same for test data.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l124"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">normalize<span class="p">—This parameter defines how we want to normalize the data. This takes in three options: </span>none <span class="p">means no normalizing; </span>global <span class="p">means we calculate the mean and standard deviation of the train data and use it to standardize the entire series using this equation:</span></p><p class="s80" style="padding-bottom: 2pt;padding-left: 40pt;text-indent: 0pt;line-height: 13pt;text-align: center;">𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠  − 𝑚𝑚𝑠𝑠𝑚𝑚𝑚𝑚</p><p style="padding-left: 195pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="107" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_734.png"/></span></p><p class="s80" style="padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑠𝑠𝑠𝑠𝑠𝑠</p><p class="s20" style="padding-top: 1pt;padding-left: 64pt;text-indent: 0pt;text-align: justify;">local <span class="p">means we use the window mean and standard deviation to standardize the series.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s20">normalize_params</span>—This parameter takes in a tuple of mean and standard deviations. If provided, this is be used to standardize in <i>global </i>standardization. This is typically used to use the train mean and standard deviation on validation and test data as well.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">mode<span class="p">—This parameter sets which dataset we want to make. It takes in one of three values:</span></p><p class="s20" style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">train<span class="p">, </span>val<span class="p">, or </span>test<span class="p">.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Each sample from this dataset returns to you two tensors—the window (<i>X</i>) and the corresponding target (<i>Y</i>) (see <i>Figure 13.2</i>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 87pt;text-indent: 0pt;text-align: left;"><span><img width="391" height="407" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_735.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.2 – Sampling the time series using a dataset and dataloader</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark512">Now that we have the dataset defined, we need another PyTorch artifact called a dataloader. A dataloader uses the dataset to pick samples into a batch of samples, among other things. In the PyTorch Lightning ecosystem, we have another concept called a datamodule, which is a standard way of generating dataloaders. We need train dataloaders, validation dataloaders, and test dataloaders. Datamodules provide a good abstraction to encapsulate the whole data part of the pipeline. We have defined a datamodule in </a><span class="s20">src/dl/dataloaders.py </span>called <span class="s20">TimeSeriesDataModule </span>that takes in the data along with the batch size and prepares the datasets and dataloaders necessary for training. The parameters are exactly the same as <span class="s20">TimeSeriesDataset</span>, with <span class="s20">batch_size </span>as the only additional parameter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_736.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code, use the notebook named <span class="s20">02-One-Step RNN. ipynb </span>in the <span class="s20">Chapter13 </span>folder and the code in the <span class="s20">src </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">We will not be going into each and every step in the notebook but will be just stressing the key points. The code in the notebook is well commented, and we urge you to follow the code along with the book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">We have already sampled a household from the data, and now, let’s see how we can define a datamodule:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 57pt;text-indent: -48pt;line-height: 131%;text-align: left;">datamodule = TimeSeriesDataModule(data = sample_df[[target]], n_val = sample_val_df.shape[0],</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 11pt;text-align: left;">n_test = sample_test_df.shape[0],</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 48pt;line-height: 106%;text-align: left;">window = 48, <span class="s38"># giving enough memory to capture daily seasonality</span></p><p class="s28" style="padding-top: 2pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">horizon = 1, <span class="s38"># single step</span></p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">normalize = &quot;global&quot;, <span class="s38"># normalizing the data</span></p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">batch_size = 32,</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 48pt;line-height: 131%;text-align: left;">num_workers = 0) datamodule.setup()</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p class="s20" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">datamodule.setup() <span class="p">is the method that calculates and sets up the dataloaders. Now, we can access the train dataloader by simply calling </span>datamodule.train_dataloader()<span class="p">, and similarly, validation and test by </span>val_dataloader <span class="p">and </span>test_dataloader <span class="p">methods, respectively. And we can access the samples, as follows:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Getting a batch from the train_dataloader</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">for batch in datamodule.train_dataloader(): x, y = batch</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">break</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">print(&quot;Shape of x: &quot;,x.shape) <span class="s38">#-&gt; torch.Size([32, 48, 1])</span></p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">print(&quot;Shape of y: &quot;,y.shape) <span class="s38">#-&gt; torch.Size([32, 1, 1])</span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark513"/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see that each sample has two tensors—<span class="s20">x </span>and <span class="s20">y</span>. There are three dimensions for the tensors, and they correspond to <i>batch size, sequence length, features</i>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html" class="s140" target="_blank">Now that we have the data pipeline ready, we need to build out the modeling and training pipelines. PyTorch Lightning has a standard way of defining these so that they can be plugged into the training engine they provide (which makes our life so much easier). The PyTorch Lightning documentation (</a><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html" class="a" target="_blank">https:// </a><span class="s27">pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html</span>) has good resources to get started with and go into depth on as well. We have also linked to a video in the <i>Further reading </i>section that makes the transition from pure PyTorch to PyTorch Lightning easy. I strongly urge you to take some time to familiarize yourself with it.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">If you are familiar with standard PyTorch, you’ll know that a standard method called <span class="s20">forward </span>is the only mandatory method you have to define, apart from<u> </u><span class="s20">init</span><span class="s262"> </span>. This is because the training loop is something that we will have to write on our own. In the <span class="s20">01-PyTorch Basics.ipynb </span><a href="#bookmark454" class="s140">notebook for </a><i>Chapter 12</i>, <i>Building Blocks of Deep Learning for Time Series</i>, we saw how we can write a PyTorch model and a training loop to train a simple classifier. But now that we are delegating the training loop to PyTorch Lightning, we have to include a few additional methods as well:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">training_step<span class="p">—This method takes in a batch and uses the model to get the outputs, calculate the loss/metrics, and return the loss.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">validation_step <span class="p">and </span>test_step<span class="p">—These methods take in the batch and use the model to get the outputs and calculate the loss/metrics.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">predict_step<span class="p">—This method is used to define the step to be taken while inferencing. If there is anything special we have to do for inferencing, we can define this method. If this is not defined, it uses </span>test_step <span class="p">for the prediction use case as well.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">configure_optimizers<span class="p">—This method defines the optimizer to be used; for instance,</span></p><p class="s20" style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Adam<span class="p">, </span>RMSProp<span class="p">, and so on.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have defined a <span class="s20">BaseModel </span>class in <span class="s20">src/dl/models.py </span>that implements all the common functions, such as loss and metric calculation, logging of results, and so on, as a framework to implement new models. And using this <span class="s20">BaseModel </span>class, we have defined a <span class="s20">SingleStepRNNModel </span>class that takes in a standard config (<span class="s20">SingleStepRNNConfig</span>) and initializes an RNN, LSTM, or GRU model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Before we look at how the model is defined, let’s see what the different config (<span class="s20">SingleStepRNNConfig</span>) parameters are:</p></li></ul></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">rnn_type<span class="p">—This parameter takes in one of three strings as an input: </span>RNN<span class="p">, </span>GRU<span class="p">, or </span>LSTM<span class="p">. This defines what kind of model we will initialize.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">input_size<span class="p">—This parameter defines the number of features the RNN is expecting.</span></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s20">hidden_size</span>, <span class="s20">num_layers</span>, <span class="s20">bidirectional</span><a href="#bookmark454" class="s140">—These parameters are the same as the ones we saw in the RNN cell in </a><a href="#bookmark454" class="s21">Chapter </a><i>12</i>, <i>Building Blocks of Deep Learning for Time Series</i>.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">learning_rate<span class="p">—This defines the learning rate of the optimization procedure.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">optimizer_params<span class="p">, </span>lr_scheduler<span class="p">, </span>lr_scheduler_params<span class="p">—These are parameters that let us tweak the optimization procedure. Let’s not worry about these for now because all of them have been set to intelligent defaults.</span></p></li></ul><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">With this setup, defining a new model is as simple as this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">rnn_config = SingleStepRNNConfig( rnn_type=&quot;RNN&quot;,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">input_size=1, hidden_size=128, num_layers=3, bidirectional=True, learning_rate=1e-3, seed=42,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">model = SingleStepRNNModel(rnn_config)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s take a peek at the <span class="s20">forward </span><a href="#bookmark454" class="s140">method, which is the heart of the model. We want our model to do one-step-ahead prediction, and from </a><i>Chapter 12</i>, <i>Building Blocks of Deep Learning for Time Series</i>, we know what a typical RNN output is and how PyTorch RNNs just output the hidden state at each timestep. Let’s see what we want to do visually and then see how we can code it up:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 131pt;text-indent: 0pt;text-align: left;"><span><img width="278" height="264" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_737.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">Figure 13.3 – A single-step RNN</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Suppose we are using the same example we saw in the dataloader—a time series with the following entries :, <span class="s137">𝑥𝑥1, 𝑥𝑥2, 𝑥𝑥3, … 𝑥𝑥7 </span>and a window of three. So, one of the samples the dataloader gives will have</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s137">𝑥𝑥1, 𝑥𝑥2, 𝑥𝑥3 </span>as the input (<i>x</i>) and <span class="s137">𝑥𝑥4 </span>as the target. One way we can use this is by passing the sequence through the RNN, ignoring all the outputs except the last one, and using that to predict the target,</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><span class="s137">𝑥𝑥4</span>. But that is not an efficient use of the samples we have, right? We also know that the output from the first timestep (using <span class="s41">𝑥𝑥1</span>) should output <span class="s41">𝑥𝑥2</span>, the second timestep should output <span class="s41">𝑥𝑥3</span>, and so on. Therefore, we can formulate the RNN in such a way that we maximize the usage of the data and, while training, use these additional points in time to also give a better signal to our model. Now, let’s break down the <span class="s20">forward </span>method.</p><p class="s20" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">forward <span class="p">takes in a single argument called </span>batch<span class="p">, which is a tuple of input and output. So, we unpack </span>batch <span class="p">into two variables, </span>x <span class="p">and </span>y<span class="p">, like so:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  x, y = batch                                                    </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s20">x </span>will have the shape <span class="s263"></span><span class="s139"> </span><i>(batch size, window length, features) </i>and <span class="s20">y </span>will have the shape <span class="s263"></span><span class="s139"> </span><i>(batch size, target length, features)</i>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now we need to pass the input sequence (<span class="s20">x</span>) through the RNN (RNN, LSTM, or GRU), like so:</p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  x, _ = self.rnn(x)                                              </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark454" class="s140">As we saw in </a><a href="#bookmark454" class="s21">Chapter </a><i>12</i>, <i>Building Blocks of Deep Learning for Time Series</i>, the PyTorch RNNs process the input and return two outputs—hidden states for each timestep and output (which is the hidden state of the last timestep). Here, we need the hidden states from all the timesteps, and therefore we capture that in the <span class="s20">x </span>variable. <span class="s20">x </span>will now have the dimension <span class="s263"></span><span class="s139"> </span><i>(batch size, window length, hidden size of RNN)</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark514">We have the hidden states, but to get the output, we need to apply a fully connected layer over the hidden states, and this fully connected layer should be shared across timesteps. An easy way to do this is to just define a fully connected layer with an input size equal to the hidden size of the RNN and then do the following:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">x = self.fc(x)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s20">x </span>is a three-dimensional tensor, and when we use a fully connected layer on a three-dimensional tensor, PyTorch automatically applies the fully connected layer to each of the timesteps. Now, this final output is captured in <span class="s20">x</span>, and its dimensions would be a <i>(batch size, window length, 1)</i>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, we have got the output of the network, but we also must do a bit of rearrangement to prepare the targets. Currently, <span class="s20">y </span>has just the one timestep beyond the window, but if we skip the first timestep from <span class="s20">x </span>and concatenate it with <span class="s20">y</span>, we would get the target, as we have in <i>Figure 13.3</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">y = torch.cat([x[:, 1:, :], y], dim=1)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">By using array indexing, we select everything except the first timestep from <span class="s20">x </span>and concatenate it with</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><span class="s20">y </span>on the first dimension (which is the <i>window length</i>).</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And with that, we have the <span class="s20">x </span>and <span class="s20">y </span>variables, which we can return, and the <span class="s20">BaseModel </span>class will calculate loss and handle the rest of the training. For the entire class, along with the <span class="s20">forward </span>method, you can refer to <span class="s20">src/dl/models.py</span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s test the model we have initialized by passing the batch from the dataloader:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">y_hat, y = model(batch)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">print(&quot;Shape of y_hat: &quot;,y_hat.shape) <span class="s38">#-&gt; ([32, 48, 1])</span></p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">print(&quot;Shape of y: &quot;,y.shape) <span class="s38">#-&gt; ([32, 48, 1])</span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Now that the model is working as expected, without errors, let’s start training the model. For that, we can leverage <span class="s20">Trainer </span>from PyTorch Lightning. There are so many options in the <span class="s20">Trainer </span><a href="https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer" class="s140" target="_blank">class, and a full list of all parameters to tweak the training can be found here: </a><a href="https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer" class="a" target="_blank">https://pytorch- lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer. </a><span class="s27">trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">But here, we are just going to use the bare minimum. Let’s go over the parameters we will be using here one by one:</p><ul id="l125"><li><p class="s20" style="padding-top: 8pt;padding-left: 54pt;text-indent: -13pt;text-align: justify;">auto_select_gpus <span class="p">and </span>gpus<span class="p">—Together, these parameters let us select GPUs for training if present. If we set </span>auto_select_gpus <span class="p">to </span>True <span class="p">and </span>gpus <span class="p">to </span>-1<span class="p">, the </span>Trainer <span class="p">class will choose all GPUs present in the machine, and if there are no GPUs, it falls back to CPU-based training.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l126"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a name="bookmark515">callbacks</a><span class="p">—PyTorch Lightning has a lot of useful callbacks that can be used during training such as </span>EarlyStopping<span class="p">, </span>ModelCheckpoint<span class="p">, and so on. Most useful callbacks are automatically added even if we don’t explicitly set them, but </span>EarlyStopping <span class="p">is one useful callback that needs to be set explicitly. </span>EarlyStopping <span class="p">is a callback that lets us monitor the validation loss or metrics while training and stop the training when this starts to become worse. This is a form of regularization and helps us keep our model from overfitting to the train data. </span>EarlyStopping <a href="https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.EarlyStopping.html" class="s140" target="_blank">has the following major parameters (a full list of parameters can be found here: </a><a href="https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.EarlyStopping.html" class="a" target="_blank">https://pytorch-lightning.readthedocs.io/en/stable/api/ </a><span style=" color: #1D1D1B;">pytorch_lightning.callbacks.EarlyStopping.html</span><span class="p">):</span></p><ul id="l127"><li><p class="s20" style="padding-top: 8pt;padding-left: 74pt;text-indent: -11pt;text-align: justify;">monitor<span class="p">—This parameter takes a string input that specifies the exact name of the metric that we want to monitor for early stopping.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 74pt;text-indent: -11pt;text-align: justify;">patience<span class="p">—This specifies the number of epochs with no improvement in the monitored metric before the callback stops the training. For instance, if we set patience as </span>10<span class="p">, the callback will wait for 10 epochs of the degrading metric before stopping the training. There are finer points on these, which are explained in the documentation.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 74pt;text-indent: -11pt;text-align: justify;">mode<span class="p">—This is a string input and takes one of </span>min <span class="p">or </span>max<span class="p">. This sets the direction of improvement. In </span>min <span class="p">mode, training will stop when the quantity monitored has stopped decreasing, and in </span>max <span class="p">mode, it will stop when the quantity monitored has stopped increasing.</span></p></li></ul></li><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">min_epochs <span class="p">and </span>max_epochs<span class="p">—These parameters help us set </span>min <span class="p">and </span>max <span class="p">limits to the number of epochs the training should run. If we are using </span>EarlyStopping<span class="p">, </span>min_epochs <span class="p">decides the minimum number of epochs that will be run regardless of the validation loss/metrics, and </span>max_epochs <span class="p">sets the upper limit on the number of epochs. So, even if the validation loss is still decreasing when we reach </span>max_epochs<span class="p">, training will stop.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="172" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_738.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Glossary</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Here are a few terms you should know to fully digest NN training:</p><p class="s5" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">Training step<span class="p">—This denotes a single gradient update to the parameter. In batched </span>stochastic gradient descent <span class="p">(</span>SGD<span class="p">), the gradient update after each batch is considered a step.</span></p><p class="s5" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">Batch<span class="p">—A batch is the number of data samples we run through the model and average the gradients over for the update in a training step.</span></p><p class="s5" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">Epoch<span class="p">—An epoch is when the model has seen all the samples in a dataset, or all the batches in the dataset have been used for a gradient update.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">So, let’s initialize a bare-bones <span class="s20">Trainer </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">trainer = pl.Trainer( auto_select_gpus=True, gpus=-1,</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">min_epochs=5, max_epochs=100,</p><p class="s28" style="padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">callbacks=[pl.callbacks.EarlyStopping(monitor=&quot;valid_loss&quot;, patience=3)],</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, all that is left is to trigger the training by passing in the <span class="s20">model </span>and <span class="s20">datamodule </span>to a method called <span class="s20">fit</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">trainer.fit(model, datamodule)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">It will run for a while and, depending on when the validation loss starts to increase, it will stop the training. Once the model is trained, we can still use the <span class="s20">Trainer </span>class to predict on new data. The prediction uses the <span class="s20">predict_step </span>method that we defined in the <span class="s20">BaseModel </span>class, which in turn uses the <span class="s20">predict </span>method that we defined in the <span class="s20">SingleStepRNN </span>model. It’s a very simple method that calls the <span class="s20">forward </span>method, takes in the model outputs, and just picks the last timestep from the output (which is the true output that we are projecting into the future). You can see an illustration of this here:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">def predict(self, batch):</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">y_hat, _ = self.forward(batch) return y_hat[:, -1, :]</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">So, let’s see how we can use the <span class="s20">Trainer </span>class to predict on new data (or new dataloaders, to be exact):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">pred = trainer.predict(model, datamodule.test_dataloader())</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We just need to provide the trained model and the dataloader (here, we use the test dataloader that we have already set up and defined).</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now the output, <span class="s20">pred</span>, is a list of tensors, one for each batch in the dataloader. We just need to concatenate them, squeeze out any redundant dimensions, detach them from the computational graph, and convert them to a NumPy array. Here’s how we can do this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">pred = torch.cat(pred).squeeze().detach().numpy()</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, <span class="s20">pred </span>is a NumPy array of predictions for all the items in the test dataframe (which was used to define <span class="s20">test_dataloader</span>, but remember we had applied a transformation to the raw time series to standardize it. Now, we need to reverse the transformation. The mean and standard deviation we used for the initial transformation are still stored in the train dataset. We merely retrieve them and inverse the transformation we did earlier, like so:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">pred = pred * datamodule.train.std + datamodule.train.mean</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, we can do all kinds of actions on them, such as evaluate against actuals, visualize the predictions, and so on. Let’s see how well the model has done. To get context, we have included the single-step ML models we did back in <i>Chapter 8</i>, <i>Forecasting Time Series with Machine Learning Models, </i>as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 107pt;text-indent: 0pt;text-align: left;"><span><img width="339" height="129" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_739.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.4 – Metrics of the vanilla single-step-ahead RNN on MAC000193 household</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">It looks like the RNN model did pretty badly. Let’s also look at the predictions visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="520" height="275" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_740.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.5 – Single-step-ahead RNN predictions for MAC000193 household</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see that the model has failed to learn the scale of the peaks and the nuances of the patterns. Maybe this is because of the problem that we discussed in terms of RNNs because the seasonality pattern here is spread over 48 timesteps; remember that the pattern requires the RNN to have long-term memory. Let’s quickly swap out the model with LSTM and GRU and see how they are doing. The only thing we need to change is the <span class="s20">rnn_type </span>parameter in <span class="s20">SingleStepRNNConfig</span>. The notebook has the code to train LSTM and GRU as well. But let’s look at the metrics with LSTM and GRU:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 116pt;text-indent: 0pt;text-align: left;"><a name="bookmark516"><span><img width="294" height="159" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_741.jpg"/></span></a></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.6 – Metrics for single-step-ahead LSTM and GRU on MAC000193 household</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, it looks competitive. LightGBM is still the best model, but now the LSTM and GRU models are competitive and not entirely lacking, like the vanilla RNN model. If we look at the predictions, we can see that the LSTM and GRU models have managed to capture the pattern much better as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span><img width="520" height="275" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_742.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.7 – Single-step-ahead LSTM and GRU predictions for MAC000193 household</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="74" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_743.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Things to try</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">Try changing the parameters of the models and see how it works. How does a bidirectional LSTM perform? Can increasing the window increase performance?</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now that we have seen how a standard RNN can be used for single-step-ahead predictions, let’s look at another modeling pattern that is more flexible than the one we just saw.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark517">Sequence-to-sequence (Seq2Seq) models</a><a name="bookmark502">&zwnj;</a><a name="bookmark501">&zwnj;</a></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark454" class="s140">We talked in detail about the sequence-to-sequence (Seq2Seq) architecture and the encoder-decoder paradigm in </a>Chapter 12<span class="p">, </span>Building Blocks of Deep Learning for Time Series<span class="p">. Just to refresh your memory, the Seq2Seq model is a kind of an encoder-decoder model by which an encoder encodes the sequence into a latent representation, and then the decoder steps in to carry out the task at hand using this latent representation. This setup is inherently more flexible because of the separation between the encoder (which does the representation learning) and the decoder, which uses the representation for predictions. One of the biggest advantages of this approach, from a time series forecasting perspective, is that the restriction of single step ahead is taken out. Under this modeling pattern, we can extend the forecast to any forecast horizon we want.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In this section, let’s put together a few encoder-decoder models and test out our single-step-ahead forecasts, just like we have been doing with the <i>single-step-ahead RNNs</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_744.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">To follow along with the complete code, use the notebook named <span class="s20">03-Seq2Seq RNN.ipynb</span></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">in the <span class="s20">Chapter13 </span>folder and the code in the <span class="s20">src </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can use the same mechanism we developed in the last section such as <span class="s20">TimeSeriesDataModule</span>, the <span class="s20">BaseModel </span>class, and the corresponding code for our Seq2Seq modeling pattern as well. Let’s define a new PyTorch model called <span class="s20">Seq2SeqModel</span>, inheriting the <span class="s20">BaseModel </span>class. While we are at it, let’s also define a new config file, called <span class="s20">Seq2SeqConfig</span>, to set the hyperparameters of the model. The final version of both can be found in <span class="s20">src/dl/models.py</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Before we explain the different parameters in the model and the config, let’s talk about the different ways we can set this Seq2Seq model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">RNN-to-fully connected network</p><p class="s4" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark454" class="s140">For our convenience, let’s restrict the encoder to be from the RNN family—it can be a vanilla RNN, LSTM, or GRU. Now, we saw in </a>Chapter 12<span class="p">, </span>Building Blocks of Deep Learning for Time Series, <span class="p">that in PyTorch, all the models in the RNN family have two outputs—</span>output <span class="p">and </span>hidden states<span class="p">, and we also saw that output is nothing but all the hidden states (final hidden states in stacked RNNs) at all timesteps. The hidden state that we get has the latest hidden states (and cell states, in the case of LSTM) of all layers in the stacked RNN setup. The encoder can be initialized just like we initialized the RNN family of models in the previous section, like so:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">self.encoder = nn.LSTM(</p><p class="s28" style="padding-top: 3pt;padding-left: 105pt;text-indent: 0pt;line-height: 131%;text-align: left;">**encoder_params, batch_first=True,</p><p class="s28" style="padding-left: 81pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And in the <span class="s20">forward </span>method, we can just do the following to encode the time series:</p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  o, h = self.encoder(x)                                          </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, there are a few different ways we can decode the information. The first one we will discuss is using a fully connected layer. Either the fully connected layer can take the latest hidden state from the encoder and predict the desired output or we can flatten all the hidden states into a long vector and use that to predict the output. The latter provides more information to the decoder, but there can be more noise as well. Both are shown in <i>Figure 13.8</i>, using the same example we have been using in the last section as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 56pt;text-indent: 0pt;text-align: left;"><span><img width="455" height="479" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_745.jpg"/></span></p><p class="s37" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.8 – RNN as the encoder and a fully connected layer as the decoder</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark518">Let’s also see how we can put together this in code. The decoder in the first case, where we are using just the last hidden state of the encoder, will look like this:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">self.decoder = nn.Linear(</p><p class="s28" style="padding-top: 3pt;padding-left: 129pt;text-indent: 0pt;text-align: left;">hidden_size*bi_directional_multiplier,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">horizon</p><p class="s28" style="padding-top: 3pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <span class="s20">bi_directional_multiplier </span>is <span class="s20">2 </span>if the encoder was bidirectional and <span class="s20">1 </span>otherwise. This is done because if the encoder is bidirectional, there will be two hidden states concatenated together for each timestep. <span class="s20">horizon </span>is the number of timesteps ahead we want to forecast.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In the second case, where we are using the hidden states from all the timesteps, we need to make the decoder, as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">self.decoder = nn.Linear(</p><p class="s28" style="padding-top: 3pt;padding-left: 129pt;text-indent: 0pt;text-align: left;">hidden_size * bi_directional_multiplier *</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">window_size, horizon</p><p class="s28" style="padding-top: 3pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here, the input vector will be the flattened vector of all the hidden states from all the timesteps, and hence the input dimension would be <span class="s20">hidden_size * window_size</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">And in the <span class="s20">forward </span>method, we can do the following for case 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">y_hat = self.decoder(o[:,-1,:]).unsqueeze(-1)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here, we are just taking the hidden state from the latest timestep and unsqueezing to maintain three dimensions as the target, <span class="s20">y</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">For case 2, we can do the following:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">y_hat = self.decoder(o.reshape(o.size(0), -1)).unsqueeze(-1)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, we first reshape the entire hidden state to flatten it and then pass it through the decoder to get the predictions. We unsqueeze to insert the dimension we collapsed so that the output and target, <span class="s20">y</span>, have the same dimensions.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Even though, in theory, we can use the fully connected decoder to predict as much into the future as possible, practically, there are limitations. When we have a large number of steps to forecast, the model will have to learn that big of a matrix to generate those outputs, and that becomes harder as the matrix becomes bigger. Another point worth noting is that each of these predictions happens independently with the information encoded in the latent representation. For instance, the prediction of five timesteps ahead is only dependent on the latent representation from the encoder and not predictions of timesteps 1 to 4. Let’s look at another type of Seq2Seq, which makes the decoding more flexible and aware of the temporal aspect of the problem.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark519">RNN-to-RNN</a><a name="bookmark503">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Instead of using a fully connected layer as the decoder, we can use another RNN for decoding as well— so, one model from the RNN family takes care of the encoding and another model from the RNN family takes care of the decoding. Initializing the decoder in the model is also similar to initializing the encoder. If we want an LSTM model as the decoder, we can do the following:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">self.decoder = nn.LSTM(</p><p class="s28" style="padding-top: 3pt;padding-left: 105pt;text-indent: 0pt;line-height: 131%;text-align: left;">**decoder_params, batch_first=True,</p><p class="s28" style="padding-left: 81pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s develop our understanding of how this is done through a visual representation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 45pt;text-indent: 0pt;text-align: left;"><span><img width="485" height="238" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_746.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.9 – RNN as the encoder and decoder</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 75%;text-align: justify;">The encoder part remains the same: it takes in the input window <span class="s41">𝑥𝑥1 </span>to <span class="s41">𝑥𝑥</span><span class="s42">3</span><span class="s43"> </span>and produces outputs, <span class="s80">𝑜𝑜</span><span class="s82">1 </span>to <span class="s210">𝑜</span><span class="s80">𝑜</span><span class="s249">3</span>, and the last hidden state, <span class="s80">ℎ</span><span class="s88">3</span>. Now, we have another decoder (a model from the RNN family) that takes in <span class="s157">ℎ3</span>, as the initial hidden state, and the latest input from the window to produce the next</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">output. And now, this output is fed back into the RNN as the input and we produce the next output, and this cycle continues until we have got the required number of timesteps in our prediction.</p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Some of you may be wondering why we don’t use the target window (<span class="s252">𝑥</span><span class="s137">𝑥</span><span class="s253">4</span><span class="s43"> </span>to <span class="s41">𝑥𝑥6</span>) during decoding as well. In fact, this is a valid way of training the model and is called <span class="s5">teacher forcing </span>in the literature. This has strong connections to maximum likelihood and is explained well in the <i>Deep Learning </i>book by Goodfellow et al. (see the <i>Further reading </i>section). So, instead of feeding in the output of the model from the previous timestep as the input to the RNN at the current timestep, we feed in the real observation, thereby eliminating the error that might have crept in in the previous timestep.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">While this sounds like the most straightforward thing to do, it does come with a few disadvantages as well. The main one is that the kinds of inputs that the decoder sees during training may not the same as the ones it will see during actual prediction. During prediction, we will still be feeding the output of the model in the previous step to the decoder. This is because in the inference mode, we do not have access to real observations in the future. This can cause problems in some cases. One way to mitigate this problem is to randomly choose between the model’s output at the previous timestep and real observation while training (Bengio et al., 2015).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_747.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Bengio et al., which proposed teacher forcing, is cited in the <i>References </i>section.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s see how we can code the <span class="s20">forward </span>method for both these cases using a parameter called <span class="s20">teacher_forcing_ratio</span>, which is a decimal from 0 to 1. This decides how frequently teacher forcing is implemented. For instance, if <span class="s20">teacher_forcing_ratio </span>= 0, then teacher forcing is never done, and if <span class="s20">teacher_forcing_ratio </span>= 1, then teacher forcing is always done.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The following code block has all the code necessary for decoding, and it comes with line numbers so that we can go line by line and explain what we are doing:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:37pt" cellspacing="0"><tr style="height:32pt"><td style="width:396pt" colspan="3" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 1pt;padding-left: 9pt;padding-right: 97pt;text-indent: 0pt;line-height: 15pt;text-align: left;">01 y_hat = torch.zeros_like(y, device=y.device) 02 dec_input = x[:, -1:, :]</p></td></tr><tr style="height:15pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">03</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">for</p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">i in range(y.size(1)):</p></td></tr><tr style="height:15pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">04</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">out, h = self.decoder(dec_input, h)</p></td></tr><tr style="height:15pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">05</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">out = self.fc(out)</p></td></tr><tr style="height:15pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">06</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">y_hat[:, i, :] = out.squeeze(1)</p></td></tr><tr style="height:15pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">07</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s265" style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">#decide if we are going to use teacher forcing or not</p></td></tr><tr style="height:15pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">08</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">teacher_force = random.random() &lt; teacher_forcing_ratio</p></td></tr><tr style="height:15pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">09</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">if teacher_force:</p></td></tr><tr style="height:15pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">10</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 27pt;text-indent: 0pt;text-align: left;">dec_input = y[:, i, :].unsqueeze(1)</p></td></tr><tr style="height:15pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">11</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">else:</p></td></tr><tr style="height:19pt"><td style="width:27pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">12</p></td><td style="width:27pt" bgcolor="#F3F2F1"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:342pt" bgcolor="#F3F2F1"><p class="s264" style="padding-top: 2pt;padding-left: 27pt;text-indent: 0pt;text-align: left;">dec_input = out</p></td></tr></table><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The first thing we need to do is declare a placeholder to store the desired output during decoding. In <i>line number 1</i>, we do that by using <span class="s20">zeros_like</span>, which generates a tensor with all zeros with the same dimension as <span class="s20">y</span>, and in <i>line number 2</i>, we set the initial input to the decoder as the last timestep in the input window. Now, we are all set to start the decoding process, and for that, in <i>line number 3</i>, we start a loop to run <span class="s20">y.size(1) </span>times. If you remember the dimensions of <span class="s20">y</span>, the second dimension was the sequence length, so we need to run the decoding process that many times.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In <i>line number 4</i>, we pass in the last input from the input window and the hidden state from the encoder to the decoder, and it returns the current output and the hidden state. We capture the current hidden state in the same variable, overwriting the old one. If you remember, the output from the RNN is the hidden state, and we will need to pass it through a fully connected layer for the prediction. So, in <i>line number 5</i>, we do just that. In <i>line number 6</i>, we store the output from the fully connected layer to the <i>i</i>-th timestep in <span class="s20">y_hat</span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, we just have one more thing to do—decide whether to use teacher forcing or not and move on to decoding the next timestep. This we can do by generating a random number between <i>0 </i>and <i>1 </i>and checking whether that number is less than the <span class="s20">teacher_forcing_ratio </span>parameter or not. <span class="s20">random.random() </span>samples a number from a uniform distribution between <i>0 </i>and <i>1</i>. If the <span class="s20">teacher_forcing_ratio </span>parameter is <i>0.5</i>, checking whether <span class="s20">random.random()</span>&lt;<span class="s20">teacher_ forcing_ratio </span>automatically ensures we only do teacher forcing 50% of the time. So, in <i>line number 8</i>, we do this check and get a Boolean output, <span class="s20">teacher_force</span>, which tells us whether we need to do teacher forcing in the next timestep or not. For teacher forcing, we store the current timestep from <span class="s20">y </span>as <span class="s20">dec_input </span>(<i>line number 10</i>). Otherwise, we store the current output as <span class="s20">dec_input </span>(<i>line number 12</i>), and this <span class="s20">dec_input </span>parameter is used as the input to the RNN in the next timestep.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, all of this (both the fully connected decoder and the RNN decoder) has been put together into a single class called <span class="s20">Seq2SeqModel </span>in <span class="s20">src/dl/models.py</span>, and a config class (<span class="s20">Seq2SeqConfig</span>) has also been defined that has all the options and hyperparameters of the models. Let’s take a look at the different parameters in the config:</p></li></ul></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">encoder_type<span class="p">—A string parameter that takes in one of three values: </span>RNN<span class="p">, </span>LSTM<span class="p">, or </span>GRU<span class="p">. This decides the sequence model we need to use as the encoder.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">decoder_type<span class="p">—A string parameter that takes in one of four values: </span>RNN<span class="p">, </span>LSTM<span class="p">, </span>GRU<span class="p">, or</span></p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><span class="s20">FC </span>(for <i>fully connected</i>). This decides the sequence model we need to use as the decoder.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">encoder_params <span class="p">and </span>decoder_params<span class="p">—These parameters take a dictionary of key-value pairs as the input. These are the hyperparameters of the encoder and the decoder, respectively. For the RNN family of models, there is another config class, </span>RNNConfig<span class="p">, which sets standard hyperparameters such as </span>hidden_size<span class="p">, </span>num_layers<span class="p">, and so on. And for the </span>FC <span class="p">decoder, we need to give two parameters: </span>window_size <span class="p">as the number of timesteps included in the input window, and </span>horizon <span class="p">as the number of timesteps ahead we want to be forecasting.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">decoder_use_all_hidden<span class="p">—We discussed two ways we can use the fully connected decoder. This parameter is a flag that switches between the two. If </span>True<span class="p">, the fully connected decoder will flatten the hidden states of all timesteps and use them for the prediction, and if</span><span class="s35"> </span>False<span class="p">, it will just use the last hidden state.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l128"><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s20">teacher_forcing_ratio</span>—We discussed teacher forcing earlier, and this parameter decided the strength of teacher forcing while training. If <i>0</i>, there will be no teacher forcing, and if <i>1</i>, every timestep will be teacher-forced.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">optimizer_params<span class="p">, </span>lr_scheduler<span class="p">, </span>lr_scheduler_params<span class="p">—These are parameters that let us tweak the optimization procedure. Let’s not worry about these for now because all of them have been set to intelligent defaults.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, with this config and the model, let’s run a few experiments. These work exactly the same as the set of experiments we ran in the previous section. The exact code for the experiments is available in the accompanying notebook. So, we ran the following experiments:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">LSTM_FC_last_hidden<span class="p">—Encoder = LSTM/Decoder = Fully Connected, using just the last hidden state</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">LSTM_FC_all_hidden<span class="p">—Encoder = LSTM/Decoder = Fully Connected, using all the hidden states</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">LSTM_LSTM<span class="p">—Encoder = LSTM/Decoder = LSTM</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see how they performed on the metrics we have been tracking:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 99pt;text-indent: 0pt;text-align: left;"><span><img width="359" height="276" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_748.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.10 – Metrics for Seq2Seq models on MAC000193 household</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The <span class="s20">Seq2Seq </span>models seem to be performing better on the metrics and the <i>LSTM_LSTM </i>model is even better than the Random Forest model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are visualizations of each of these forecasts in the notebook. I urge you to look at those visualizations, zoom in, look at different places in the horizon, and so on. The astute observers among you must have figured out something weird with the forecast. Let’s look at a zoomed-in version (on 1 day) of the forecasts we generated to make that point clear:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="525" height="270" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_749.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.11 – Single-step-ahead Seq2Seq predictions for MAC000193 household (1 day)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">What do you see now? Focus on the peaks in the time series. Are they aligned or do they seem at an offset? This phenomenon that you are seeing now is when a model learns to mimic the last seen timestep (like the naïve forecast) rather than learn the true pattern in the data. We will be getting good metrics and we might be happy with the forecast, but upon investigation, we can see that this is not the forecast we want. This is especially true in the case of single-step-ahead models where we are just optimizing to predict the next timestep. Therefore, the model has no real incentive to learn long-term patterns, such as seasonality and so on, and ends up learning a model like the naïve forecast.</p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark678" class="s140">Models that are trained to predict longer horizons overcome this problem because, in this scenario, the model is forced to learn the longer-term patterns in the model. Although multi-step forecasting is a topic that will be covered in detail in </a>Chapter 17, Multi-Step Forecasting<span class="p">, let’s do a little bit of a sneak peek now. In the notebook, we also train multi-step models using the Seq2Seq models.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The only changes we need to make are these:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The horizon we define in the datamodule and the models should change.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The way we evaluate the models should also have a small change.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s see how we can define a datamodule for multi-step forecasting. We have chosen to forecast a complete day, which is 48 timesteps. And as an input window, we are giving <span class="s266">2 × 48 </span>timesteps:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">HORIZON = 48 WINDOW = 48*2</p><p class="s28" style="padding-left: 57pt;text-indent: -48pt;line-height: 131%;text-align: left;">datamodule = TimeSeriesDataModule(data = sample_df[[target]], n_val = sample_val_df.shape[0],</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">n_test = sample_test_df.shape[0], window = WINDOW,</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 11pt;text-align: left;">horizon = HORIZON,</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">normalize = &quot;global&quot;, <span class="s38"># normalizing the data</span></p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">batch_size = 32,</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">num_workers = 0)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have the datamodule, we can initialize the models just like before and train them. The only change we have to make now is while predicting.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In the single-step setting, at each timestep, we were predicting the next one. But now, we are predicting the next 48 timesteps, at each step. There are multiple ways to look at this and measure the metrics, which we will cover in detail in <i>Part 3</i>. For now, let’s choose a heuristic and say that we are considering that we are running this model only once a day, and each such prediction has 48 timesteps. But the test dataloader still increments by one—in other words, the test dataloader still gives us the next 48 timesteps, for each timestep. So, executing the following code, we will get a prediction array with dimensions—<i>(timesteps, horizon)</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">pred = trainer.predict(model, datamodule.test_dataloader())</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># pred is a list of outputs, one for each batch</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">pred = torch.cat(pred).squeeze().detach().numpy()</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The predictions start at <span class="s20">2014, Jan 1 00:00:00</span>. So, if we select the 48 timesteps, every 48 timesteps apart, it’ll be like considering only predictions that are made at the beginning of the day. Using a bit of fancy indexing <span class="s20">numpy </span>provides us, it is easy to do just that:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">pred = pred[0::48].ravel()</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We start at index 0, which is the first prediction of 48 timesteps, and pick every 48 indices (which are timesteps) and just flatten the array. We will get an array of predictions with the desired shape, and then the standard procedure of inverse transformation and metric calculation, and so on, proceeds.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The notebook has the code to do the following experiments:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">MultiStep LSTM_FC_last_hidden<span class="p">—Encoder = LSTM/Decoder = Fully Connected Layer, using only the last hidden state</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">MultiStep LSTM_FC_all_hidden<span class="p">—Encoder = LSTM/Decoder = Fully Connected Layer, using all the hidden states</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">MultiStep LSTM_LSTM_teacher_forcing_0.0<span class="p">—Encoder = LSTM/ Decoder = LSTM, using no teacher forcing</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">MultiStep LSTM_LSTM_teacher_forcing_0.5<span class="p">—Encoder = LSTM/ Decoder = LSTM, using stochastic teacher forcing (randomly, 50% of the time teacher forcing is enabled)</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">MultiStep LSTM_LSTM_teacher_forcing_1.0<span class="p">—Encoder = LSTM/ Decoder = LSTM, using complete teacher forcing</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s look at the metrics of these experiments:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 61pt;text-indent: 0pt;text-align: left;"><span><img width="436" height="392" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_750.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.12 – Metrics for multi-step Seq2Seq models on MAC000193 household</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark520">Although we cannot compare single-step-ahead accuracy to multi-step ones, for the time being, let’s suspend that concern and use the single-step metrics as in the best-case scenario. So, we can see that our model that predicts 1 day ahead (48 timesteps) is not such a bad model after all, and if we visualize the predictions, the problem of imitating naïve forecasts is also not present because now the model is forced to learn long-term models and forecasts:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span><img width="521" height="267" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_751.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 13.13 – Multi-step-ahead Seq2Seq predictions for MAC000193 household (1 day)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see that the model has tried to learn the daily patterns because it is forced to predict the next 48 timesteps. With some tuning and other training tricks, we might get a better model as well. But running a separate model for all <span class="s20">LCLid </span><a href="#bookmark561" class="s140">(consumer ID) instances in the dataset may not be the best option, both from an engineering and a modeling perspective. We will tackle strategies for global modeling in </a><i>Chapter 15</i>, <i>Strategies for Global Deep Learning Forecasting Models.</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="74" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_752.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Things to try</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">Can you train a better model? Tweak the hyperparameters and try to get better performance. Use GRUs or combine a GRU with an LSTM—the possibilities are endless.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Congratulations on getting through yet another hands-on and practical chapter. If this is the first time you are training NNs, I hope this lesson has made you confident enough to try more: trying and experimenting with these techniques is the best way to learn.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark521">Summary 345</a><a name="bookmark506">&zwnj;</a><a name="bookmark505">&zwnj;</a><a name="bookmark504">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_753.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although we learned about the basic blocks of DL in the previous chapter, we put all of that into action while we used those blocks in common modeling patterns using PyTorch.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We saw how standard sequence models such as RNN, LSTM, and GRU can be used for time series prediction, and then we moved on to another paradigm of models, called Seq2Seq models. Here, we talked about how we can mix and match encoders and decoders to get the model we want. Encoders and decoders can be arbitrarily complex. Although we looked at simple encoders and decoders, it is very much possible to have something like a combination of a convolution block and an LSTM block working together for the encoder. Last but not least, we talked about teacher forcing and how it can help models train and converge faster and also with some performance boost.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the next chapter, we will be tackling a subject that has captured a lot of attention (pun intended) in the past few years: attention and transformers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Reference</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer (2015). <i>Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</i>. <i>Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 </i>(<i>NIPS’15</i><a href="https://proceedings.neurips.cc/paper/2015/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf" class="s140" target="_blank">): </a><a href="https://proceedings.neurips.cc/paper/2015/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf" class="a" target="_blank">https://proceedings.neurips.cc/ </a><span class="s27">paper/2015/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You can check out the following sources for further reading:</p></li><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">From PyTorch to PyTorch Lightning <a href="https://www.youtube.com/watch?v=DbESHcCoWbM" class="s140" target="_blank">by Alfredo Canziani and William Falcon: </a><a href="https://www.youtube.com/watch?v=DbESHcCoWbM" class="a" target="_blank">https://www. </a><a href="https://www.youtube.com/watch?v=DbESHcCoWbM" target="_blank">youtube.com/watch?v=DbESHcCoWbM</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Deep Learning<a href="https://www.deeplearningbook.org/contents/rnn.html" class="s140" target="_blank">—Ian Goodfellow, Yoshua Bengio, and Aaron Courville (pages 376-377): </a><a href="https://www.deeplearningbook.org/contents/rnn.html" class="a" target="_blank">https:// </a><a href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank">www.deeplearningbook.org/contents/rnn.html</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">A Short Chronology Of Deep Learning For Tabular Data <a href="https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html" class="s140" target="_blank">by Sebastian Raschka: </a><a href="https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html" class="a" target="_blank">https:// sebastianraschka.com/blog/2022/deep-learning-for-tabular-data. </a><a href="https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html" target="_blank">html</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark522">14</a><a name="bookmark523">&zwnj;</a><a name="bookmark525">&zwnj;</a><a name="bookmark524">&zwnj;</a></h2><h4 style="padding-top: 2pt;text-indent: 0pt;text-align: right;">Attention and Transformers</h4><h4 style="padding-top: 4pt;text-indent: 0pt;text-align: right;">for Time Series</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous chapter, we rolled up our sleeves and implemented a few <span class="s5">deep learning </span>(<span class="s5">DL</span><a href="#bookmark454" class="s140">) systems for time series forecasting. We used the common building blocks we discussed in </a><i>Chapter 12</i>, <i>Building Blocks of Deep Learning for Time Series</i>, put them together in an encoder-decoder architecture, and trained them to produce the forecast we desired.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s talk about another key concept in DL that has taken the field by storm over the past few years—<span class="s5">attention</span>. Attention has a long-standing history, which has culminated in it being one of the most sought-after tools in the DL toolkit. This chapter takes you on a journey to understand attention and transformer models from the ground up from a theoretical perspective and solidify that understanding with practical examples.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">What is attention?</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Generalized attention model</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Forecasting with sequence-to-sequence models and attention</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Transformers – Attention is all you need</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Forecasting with Transformers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">If you have not set up the Anaconda environment following the instructions in the <i>Preface</i>, please do that in order to get a working environment with all the packages and datasets required for the code in this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark538">You need to run the following notebooks for this chapter:</a><a name="bookmark526">&zwnj;</a></p><ul id="l129"><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">02 - Preprocessing London Smart Meter Dataset.ipynb <span class="p">in </span>Chapter02</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">01-Setting up Experiment Harness.ipynb <span class="p">in </span>Chapter04</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">01-Feature Engineering.ipynb <span class="p">in </span>Chapter06</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">02-One-Step RNN.ipynb <span class="p">and </span>03-Seq2Seq RNN.ipynb <span class="p">in </span>Chapter13 <span class="p">(for benchmarking)</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">00-Single Step Backtesting Baselines.ipynb <span class="p">and </span>01-Forecasting with ML.ipynb <span class="p">in </span>Chapter08</p><p class="s27" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter14" class="s140" target="_blank">The associated code for the chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter14" class="a" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter14<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">What is attention?</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The idea of attention was inspired by human cognitive function. At any moment, the optic nerves in our eyes, the olfactory nerves in our noses, and the auditory nerves in our ears send a massive amount of sensory input to the brain. This is way too much information, definitely more than the brain can handle. But our brains have developed a mechanism that helps us to pay <i>attention </i>to only the stimuli that matter—such as a sound or a smell that doesn’t belong. Years of evolution have <i>trained </i>our brains to pick out anomalous sounds or smells because that was key for us surviving in the wild where predators roamed free.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Apart from this kind of instinctive attention, we are also able to control our attention by what we call <i>focusing </i>on something. You are doing it right now by choosing to ignore all the other stimuli that you are getting and focusing your attention on the contents of this book. While you are reading, your mobile phone pings you, and the screen lights up. And your brain decides to focus its attention on the mobile screen, even though the book is still open in front of you. This feature of the human cognitive function has been the inspiration behind the attention mechanism in DL. Giving learning machines the ability to acquire this kind of attention has led to big breakthroughs in all fields of AI today.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark495" class="s140">The idea was first applied to DL in Seq2Seq models, which we learned about in </a><i>Chapter 13</i>, <i>Common Modeling Patterns for Time Series</i>. In the chapter, we saw how the handshake between the encoder and decoder was done. For the <span class="s5">recurrent neural network </span>(<span class="s5">RNN</span>) family of models, we use the hidden states from the encoder at the end of the sequence as the initial hidden states in the decoder. Let’s call this handshake—the <span class="s5">context</span>. The assumption here is that all the information required for the decoding task is encoded in the context. This creates a kind of information bottleneck (<i>Figure 14.1</i>). There may be information in previous hidden states that can be useful for the decoding task. In 2015, Bahdanau et al. proposed the first known attention model in the context of DL. They proposed to learn attention weights, <span class="s41">𝛼𝛼</span>, for each hidden state corresponding to the input sequence and combine them into a single context vector while decoding. And these attention weights are re-calculated for each decoding step using the similarity between the hidden states during decoding and all the hidden states in the input sequence (<i>Figure 14.2</i>):</p><p class="s47" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">What is attention? 349</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_754.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="380" height="320" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_755.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.1 – Traditional (top) versus attention model (bottom) in Seq2Seq models</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s90" style="padding-left: 28pt;text-indent: 0pt;line-height: 87%;text-align: justify;"><span class="s138">T</span><span class="p">o make things clearer, let’s adopt a formal way of describing the mechanism. Let </span>H = ℎ<span class="s128">𝑖𝑖</span><span class="s127"> </span>, iϵ{1,2, … , 𝑇𝑇<span class="s128">𝑠</span><span class="s127">𝑠</span>} <span class="p">be the hidden states generated during the encoding process and </span>S = 𝑠𝑠<span class="s128">𝑗𝑗</span><span class="s127"> </span>, jϵ{1,2, … , 𝑇𝑇<span class="s128">𝑡𝑡</span><span class="s127"> </span>} <span class="p">be the hidden states generated during decoding. The context vector will be </span>𝑐𝑐<span class="s128">𝑗</span><span class="s127">𝑗</span><span class="p">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 68pt;text-indent: 0pt;text-align: left;"><span><img width="419" height="249" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_756.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.2 – Decoding using attention</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><a name="bookmark539">So, now we have the hidden states from the encoding stage (</a><span class="s162">H</span>), and we need to have a way to use this information at each step of decoding. The key here is that at each step of the decoding process, information from different hidden states might be relevant. And this is exactly what attention weights do. So, for decoding step <span class="s41">𝑗𝑗</span>, we use <span class="s90">𝑠𝑠𝑗𝑗−1 </span>and calculate attention weights (we’ll look at how attention weights are learned in detail soon) <span class="s137">α𝑖𝑖,𝑗𝑗</span>, using the similarity between <span class="s90">𝑠𝑠𝑗𝑗−1 </span>and each hidden state in <span class="s162">𝐻𝐻</span>. Now, we calculate the context vector, which combines the information in <span class="s41">H </span>in the right way:<a name="bookmark527">&zwnj;</a></p><p class="s50" style="padding-top: 5pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑇𝑇<span class="s79">𝑠𝑠</span></p><p class="s80" style="padding-top: 5pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">𝑐𝑐<span class="s82">𝑗𝑗    </span>= ∑ α<span class="s82">𝑖𝑖,𝑗𝑗 </span>ℎ<span class="s82">𝑖𝑖</span></p><p class="s50" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑖𝑖=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are two main ways we can use this context vector, which we will go into in detail later in the chapter. This breaks the information bottleneck that was present in the traditional Seq2Seq model and allows the models to access a larger pool of information and decide which information is relevant at each step of the decoding process.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s see how these attention weights, , are calculated.</p><p class="s3" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The generalized attention model</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Over the course of years, researchers have come up with different ways of calculating attention weights and using attention in DL models. Sneha Choudhari et al. published a survey paper on attention models that proposes a generalized attention model that tries to incorporate all the variations in a single framework. Let’s structure our discussion around this generalized framework.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can think of an attention model as learning an attention distribution ( ) for a set of keys, <i>K</i>, using a set of queries, <i>q</i>. In the example we discussed in the last section, the query would be <span class="s90">𝑠𝑠𝑗𝑗−1</span>—the hidden state from the last timestep during decoding—and the keys would be <span class="s137">𝐻𝐻</span>—all the hidden states generated using the input sequence. In some cases, the generated attention distribution is applied to another set of inputs called values, <i>V</i>. In many cases, <i>K </i>and <i>V </i>are the same, but to maintain the general form of the framework, we consider these separately. Using these terminologies, we can define an attention model as a function of <i>q</i>, <i>K</i>, and <i>V</i>:</p><p class="s90" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝒜𝒜(𝑞𝑞, 𝐾𝐾, 𝑉𝑉) = ∑ 𝑝𝑝(𝑎𝑎(𝑘𝑘<span class="s128">𝑖𝑖</span><span class="s127"> </span>, 𝑞𝑞)) × 𝑣𝑣<span class="s128">𝑖𝑖</span></p><p class="s127" style="padding-top: 2pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="69" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_757.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research papers by Sneha Choudhari et al. are cited in the <i>References </i>section under <i>8</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, is an <span class="s5">alignment function </span>that calculates a similarity or a notion of similarity between the queries and keys. In the example we discussed in the previous section, this alignment function calculates how relevant an encoder hidden state is to a decoder hidden state, and <span class="s90">𝑝𝑝 </span>is a <span class="s5">distribution function </span>that converts this score into attention weights that sum up to 1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Since we have the generalized attention model now, let’s also see how we can implement this in PyTorch. The full implementation can be found in the <span class="s20">Attention </span>class in <span class="s20">src/dl/attention.py</span>, but we will cover the key parts of it here.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The only information we require beforehand to initialize such a module is the hidden dimension of the queries and keys (encoder and decoder). So, the class definition and the<u> </u><span class="s20">init </span>function of the class look like this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">class Attention(nn.Module, metaclass=ABCMeta):</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: -24pt;line-height: 131%;text-align: left;">def<u> </u>init<u> </u>(self, encoder_dim: int, decoder_dim: int): super().<u> </u>init<u> </u>()</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">self.encoder_dim = encoder_dim self.decoder_dim = decoder_dim</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, we need to define a <span class="s20">forward </span>function, which takes in two inputs:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><span class="s20">query</span>: The query vector of size (<i>batch size, decoder dimension</i>), which we are going to use to find the attention weights with which to combine the keys. This is the <span class="s90">𝑞𝑞 </span>in  <span class="s146">𝒜𝒜</span><span class="s109">(</span><span class="s146">𝑞𝑞, 𝐾𝐾, 𝑉𝑉</span><span class="s109">)</span><span class="s146"> </span>.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><span class="s20">key</span>: The key vector of size (<i>batch size, sequence length, encoder dimension</i>), which is the sequence of hidden states across which we will be calculating the attention weights. This is the</p><p class="s90" style="padding-left: 54pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝐾𝐾 <span class="p">in </span>𝒜𝒜(𝑞𝑞, 𝐾𝐾, 𝑉𝑉)<span class="p">.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">We are assuming keys and values are the same because in most cases, they are. So, from the generalized attention model, we know that there are a few steps we need to perform:</p><ol id="l130"><li><p class="s90" style="padding-top: 7pt;padding-left: 55pt;text-indent: -18pt;text-align: left;"><span class="p">Calculate an alignment score—</span>𝑎𝑎<span class="s144">(</span>𝑘𝑘𝑖𝑖, 𝑞𝑞<span class="s144">)</span><span class="p">—for each query and key combination.</span></p></li><li><p class="s94" style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;"><span class="p">Convert the scores to weights by applying a function— </span>𝑝<span class="s74">𝑝(𝑎𝑎</span>(𝑘𝑘<span class="s74">𝑖𝑖, 𝑞𝑞</span>))<span class="p">.</span></p></li><li><p class="s90" style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;"><span class="p">Use the learned weights to combine the values—</span>∑ 𝑝𝑝(𝑎𝑎<span class="s144">(</span>𝑘𝑘𝑖𝑖, 𝑞𝑞<span class="s144">)</span>) × 𝑣𝑣𝑖𝑖<span class="p">.</span></p><p class="s127" style="padding-top: 2pt;padding-left: 173pt;text-indent: 0pt;line-height: 7pt;text-align: center;">𝑖𝑖</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;">So, let’s see those steps in code in the <span class="s20">forward </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">def forward(</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">self,</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">query: torch.Tensor, <span class="s38"># [batch_size, decoder_dim]</span></p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 48pt;line-height: 106%;text-align: left;">values: torch.Tensor, <span class="s38"># [batch_size, seq_length, encoder_dim]</span></p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">):</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 48pt;line-height: 106%;text-align: left;">scores = self._get_scores(query, values) <span class="s38"># [batch_ size, seq_length]</span></p><p class="s28" style="padding-top: 2pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">weights = torch.nn.functional.softmax(scores, dim=-1)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 48pt;line-height: 106%;text-align: left;">return (values*weights.unsqueeze(-1)).sum(dim=1) <span class="s38"># [batch_size, encoder_dim]</span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark540">The three lines of code in the </a><span class="s20">forward </span>method correspond to the three steps we discussed earlier. The first step, which is calculating the scores, is the key step that has led to many different types of attention, and therefore we have generalized that into a <span class="s20">_get_scores </span>abstract method that must be implemented by any class inheriting the <span class="s20">Attention </span>class. For the second line, we have used the <span class="s20">softmax </span>function for converting the scores to weights, and in the last line, we are doing an element- wise multiplication (<span class="s20">*</span>) between weights and values and summing across the sequence length to get the weighted value.<a name="bookmark528">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now let’s turn our attention toward alignment functions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Alignment functions</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are many variations of the alignment function that have come up over the years. Let’s review a few popular ones that are used today.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Dot product</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is probably the simplest alignment function of all. Luong et al. proposed this form of attention in 2015. From linear algebra intuition, we know that a dot product between two vectors tells us what amount of one vector goes in the direction of another. It measures some kind of similarity between the two vectors, and this similarity considers both the magnitude of the vectors and the angle between them in the vector space. Therefore, when we take a dot product between our query and key vectors, we get a notion of similarity between them. One thing to note here is that the hidden dimensions of the query and key should be the same for dot product attention to be applied. Formally, the similarity function can be defined as follows:</p><p class="s242" style="padding-top: 5pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">a(𝑘𝑘<span class="s247">𝑖𝑖</span><span class="s146"> </span>, 𝑞𝑞) = q𝑇𝑇𝑘𝑘<span class="s247">𝑖𝑖</span></p><p style="padding-top: 12pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We need to calculate this score for each of the elements in the key, <span class="s137">𝐾𝐾</span>, and instead of running a loop over each element in <i>K</i>, we can use a clever matrix multiplication trick to calculate the scores for all the keys in K in one shot. Let’s see how we can define the <span class="s20">_get_scores </span>function for dot product attention.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We know from the previous section that the query and values (which are the same as keys in our case) are of (<i>batch size, decoder dimension</i>) and (<i>batch size, sequence length, encoder dimension</i>) dimensions respectively, and will be called <span class="s20">q </span>and <span class="s20">v </span>in the <span class="s20">_get_scores </span>function. And in this particular case, the decoder dimension and the encoder dimension are the same, so the scores can be calculated as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  scores = (q @ v.transpose(1,2))                                 </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <span class="s20">@ </span>is shorthand for <span class="s20">torch.matmul</span>, which does matrix multiplication. The entire implementation is named <span class="s20">DotProductAttention </span>and can be found in <span class="s20">src/dl/attention.py</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark541">Scaled dot product attention</a></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In 2017, Vaswani et al. proposed this type of attention in the seminal paper, <i>Attention is All you Need</i>. We will delve into that paper later in this chapter, but now, let’s understand one key modification they suggested to the dot product attention. The modification is motivated by the concern that when the input is large, the <i>softmax function </i>we use to convert scores to weights may have very small gradients and hence makes efficient learning difficult.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This is because the <i>softmax </i>function is not scale-invariant. The exponential function in the <i>softmax </i>function is the reason for this behavior. So, the higher we scale the inputs to the function, the more the largest input dominates the output, and this throttles the gradient flow in the network. If we assume</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><span class="s90">𝑞𝑞 </span>and <span class="s90">𝑣𝑣 </span>are <span class="s90">𝑑𝑑</span><span class="s128">𝑘𝑘</span><span class="s127"> </span>dimensional vectors with 0 mean and a variance of 1, then their dot product wo<u>uld</u> have a mean of zero and a variance of <span class="s144">𝑑</span><span class="s90">𝑑</span><span class="s145">𝑘</span><span class="s127">𝑘</span>. Therefore, if we scale the output of the dot product by <span class="s90">√𝑑𝑑𝑘𝑘 </span>, then we bring the variance of the dot product back to 1. So, by controlling for the scale of the inputs to the <i>softmax </i>function, we manage a healthy gradient flow through the network. The <i>Further reading </i>section has a link to a blog post that goes into this in more depth. Therefore, the scaled dot product alignment function can be defined as follows:</p><p class="s90" style="padding-top: 2pt;padding-left: 230pt;text-indent: 0pt;line-height: 10pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="26" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_758.png"/></span></p><p class="s90" style="padding-left: 176pt;text-indent: 0pt;line-height: 10pt;text-align: left;">a(𝑘𝑘<span class="s128">𝑖𝑖 </span>, 𝑞𝑞) =</p><p class="s90" style="padding-left: 223pt;text-indent: 0pt;line-height: 11pt;text-align: left;">√𝑑𝑑𝑘𝑘</p><p class="s90" style="padding-top: 10pt;text-indent: 0pt;text-align: left;">× q𝑇𝑇𝑘𝑘<span class="s128">𝑖𝑖</span></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And consequently, the only change we will have to make in the <span class="s20">PyTorch </span>implementation is one additional line:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  scores = scores/math.sqrt(encoder_dim)                          </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: justify;">This has been implemented as a parameter in <span class="s20">DotProductAttention </span>in <span class="s20">src/dl/attention. py</span>. If you pass <span class="s20">scaled=True </span>while initializing the class, it will perform scaled dot product attention. We need to keep in mind that similar to dot product attention, the scaled variant also requires the query and values to have the same dimensions.</p><p class="s24" style="padding-top: 10pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">General attention</p><p style="padding-top: 8pt;padding-left: 27pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">In 2015, Luong et al. also proposed a slight variation of dot product attention by introducing a learnable</p><p class="s90" style="padding-left: 27pt;text-indent: 0pt;text-align: justify;">𝑊𝑊 <span class="p">matrix into the calculation. They called it general attention. We can think of it as an attention mechanism that allows for the query to be projected into a learned plane of the same dimension as the values/keys using the </span>𝑊𝑊 <span class="p">matrix before computing the similarity score using a dot product. The alignment function can be written as follows:</span></p><p class="s90" style="padding-top: 9pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">a(𝑘𝑘<span class="s128">𝑖</span><span class="s127">𝑖</span>, 𝑞𝑞) = q𝑇𝑇W𝑘𝑘<span class="s128">𝑖𝑖</span></p><p style="padding-top: 12pt;padding-left: 27pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">The corresponding <span class="s20">PyTorch </span>implementation can be found under the name <span class="s20">GeneralAttention</span></p><p style="padding-left: 27pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">in <span class="s20">src/dl/attention.py</span>. The key line calculating the attention scores can be written as follows:</p><p style="padding-top: 11pt;text-indent: 0pt;text-align: center;"><span class="s49" style=" background-color: #F3F2F1;">  scores = (q @ self.W) @ v.transpose(1,2)                        </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark542">Here, </a><span class="s20">self.W </span>is a tensor of size (<i>encoder hidden dimension x decoder hidden dimension</i>). General attention can be used in cases where the query and key/value dimensions are different.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Additive/concat attention</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In 2015, Bahdanau et al. proposed additive attention, which was one of the first attempts at introducing attention to DL systems. Instead of using a defined similarity function such as the dot product, Bahdanau et al. proposed that the similarity function can be learned, giving the network more flexibility in deciding what it deems to be similar. They suggested that we can concatenate the query and the key into a single tensor and use a learnable matrix, <span class="s90">𝑊𝑊</span>, to calculate the attention scores. This alignment function can be written as follows:</p><p class="s65" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝑖𝑖𝑖𝑖𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"/><p class="s157" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">a(𝑘𝑘<span class="s88">𝑖</span><span class="s65">𝑖</span>, 𝑞𝑞) = 𝑤𝑤𝑇𝑇    tanh(𝑊𝑊<span class="s88">𝑞</span><span class="s65">𝑞</span>𝑞𝑞𝑇𝑇 + W<span class="s88">k</span>𝑘𝑘<span class="s88">𝑖𝑖</span><span class="s65">  </span>+ 𝑏𝑏)</p><p style="padding-top: 14pt;padding-left: 37pt;text-indent: 0pt;line-height: 92%;text-align: justify;">Here, <span class="s137">𝑣𝑣𝑡𝑡</span>, <span class="s90">𝑊𝑊</span><span class="s128">𝑞𝑞 </span>and <span class="s90">𝑊𝑊𝑘𝑘 </span>are learnable matrices. In cases where the query and key have different hidden dimensions, we can use <span class="s90">𝑊𝑊𝑞𝑞 </span>and <span class="s74">𝑊𝑊</span><span class="s128">𝑘𝑘 </span>to project them into a single dimension and then perform a similarity calculation on them. In the case that the query and key have the same hidden dimension, this is also equivalent to the variant of attention used in Luong et al., which they call <i>concat </i>attention, represented as follows:</p><p class="s146" style="text-indent: 0pt;line-height: 10pt;text-align: left;">𝑖𝑖𝑖𝑖𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"/><p class="s248" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">a(𝑘𝑘<span class="s246">𝑖</span><span class="s146">𝑖</span>, 𝑞𝑞) = 𝑤𝑤𝑇𝑇    tanh(𝑊𝑊[𝑞𝑞𝑇𝑇; 𝑘𝑘<span class="s246">𝑖𝑖</span><span class="s146"> </span>] + 𝑏𝑏)</p><p style="padding-top: 13pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">It is simple linear algebra to see that both are the same and for engineering simplicity. The <i>Further reading </i>section has a link to a Stack Overflow answer that explains the equivalence.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">We have included both implementations in <span class="s20">src/dl/attention.py </span>under <span class="s20">ConcatAttention</span></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">and <span class="s20">AdditiveAttention</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">For <span class="s20">AdditiveAttention</span>, the key lines calculating the score are these:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">q = q.repeat(1, v.size(1), 1) <span class="s38"># [batch_size, seq_length, decoder_dim]</span></p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">scores = self.W_q(q) + self.W_v(v) <span class="s38"># [batch_size, seq_length, decoder_dim]</span></p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">torch.tanh(scores) @ self.v <span class="s38"># [batch_size, seq_length]</span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The first line repeats the query vector to the sequence length. This is just a linear algebra trick to calculate the score for all the encoder hidden states in a single operation rather than looping through them. <i>Line 2 </i>projects both query and value into the same dimension using <span class="s20">self.W_q </span>and <span class="s20">self.W_v</span>, and <i>line 3 </i>applies the <span class="s20">tanh </span>activation function and uses matrix multiplication with <span class="s20">self.v </span>to produce the final scores. <span class="s20">self.W_q</span>, <span class="s20">self.W_v</span>, and <span class="s20">self.v </span>are learnable matrices, defined as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">self.W_q = torch.nn.Linear(self.decoder_dim, self.decoder_dim) self.W_v = torch.nn.Linear(self.encoder_dim, self.decoder_dim) self.v = torch.nn.Parameter(torch.FloatTensor(self.decoder_dim)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark543">The only difference in </a><span class="s20">ConcatAttention </span>is that instead of two separate weights—<span class="s20">self.W_q </span>and <span class="s20">self.W_v</span>—we just have a single weight—<span class="s20">self.W</span>—defined as follows:<a name="bookmark529">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">self.W = torch.nn.Linear(self.decoder_dim + self.encoder_dim, self.decoder_dim)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">And instead of adding the projections (<i>line 2</i>), we use the following line:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">scores = self.W(</p><p class="s28" style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">torch.cat([q, v], dim=-1)</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">) <span class="s38"># [batch_size, seq_length, decoder_dim]</span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_759.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The research papers by Luong et al., Badahnau et al., and Vaswani et al. are cited in the <i>References</i></p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">section under <i>2</i>, <i>1</i>, and <i>5 </i>respectively.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Therefore, we can think of <span class="s20">AdditiveAttention </span>and <span class="s20">ConcatAttention </span>doing the same operation, but <span class="s20">AdditiveAttention </span>is adapted to handle different encoder and decoder dimensions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now that we have learned about a few popular alignment functions, let’s turn our attention toward the distribution function of the attention model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The distribution function</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The primary goal of the distribution function is to convert the learned scores from the alignment function into a set of weights that add up to 1. The <i>softmax </i>function is the most popular choice as a distribution function. It converts the score into a set of weights that sum up to one. This also gives us the freedom to interpret the learned weights as probabilities—the probability that the corresponding element is the most relevant.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although <i>softmax </i>is the most popular choice, it is not without its drawbacks. The <i>softmax </i>weights are typically <i>dense</i>. What that means is that there will be some probability mass (some weight) assigned to every element in the sequence over which we calculated the attention. The weights can be low, but still not 0. There are situations where sparsity in the distribution function is desirable. Maybe we want to make sure we don’t give any weights some implausible options. Maybe we want to make the attention mechanism more interpretable.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are alternate distribution functions such as <i>sparsemax </i>(Martins et al. 2016) and <i>entmax </i>(Peters et al. 2019) that are capable of assigning probability mass to a select few relevant elements and assigning zero to the rest of them. When we know that the output is only dependent on a few timesteps in the encoder, we can use such distribution functions to encode that knowledge into the model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="85" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_760.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The research papers by Martins et al. and Peters et al. are cited in the <i>References </i>section under</p><p class="s4" style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">3 <span class="p">and </span>4 <span class="p">respectively.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark544">Now that we have learned about a few attention mechanisms, it’s time to put them into practice.</a><a name="bookmark530">&zwnj;</a></p><p class="s3" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;line-height: 119%;text-align: left;">Forecasting with sequence-to-sequence models and attention</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="87" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_761.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code, use the notebook named <span class="s20">01-Seq2Seq RNN with Attention.ipynb </span>in the <span class="s20">Chapter14 </span>folder and the code in the <span class="s20">src </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark495" class="s140">Let’s pick up the thread from </a><i>Chapter 13</i>, <i>Common Modeling Patterns for Time Series</i>, where we used Seq2Seq models to forecast a sample household (if you have not read the previous chapter, I strongly suggest you do it now) and modify the <span class="s20">Seq2SeqModel </span>class to also include an attention mechanism.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We are still going to inherit the <span class="s20">BaseModel </span>class we have defined in <span class="s20">src/dl/models.py</span>, and the overall structure is going to be very similar to the <span class="s20">Seq2SeqModel </span>class. The key difference will be that in our new model, with attention, we do not accept a fully connected layer as the decoder. It is not because it is not possible, but for convenience and brevity of the implementation. In fact, implementing a Seq2Seq model with a fully connected decoder can be some homework you can take up to really internalize the concept.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Similar to the <span class="s20">Seq2SeqConfig </span>class, we define a very similar <span class="s20">Seq2SeqwAttnConfig </span>class that has the exact same set of parameters, but with some additional validation checks. One of the validation checks is disallowing a fully connected decoder. Another validation check would be making sure the decoder input size allows for the attention mechanism as well. We will see those requirements in detail shortly.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In addition to <span class="s20">Seq2SeqwAttnConfig</span>, we also define a <span class="s20">Seq2SeqwAttnModel </span>class to enable attention-enabled decoding. The only additional parameter here is <span class="s20">attention_type</span>, which is a string parameter that takes the following values:</p><ul id="l131"><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">dot<span class="p">—Dot product attention</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">scaled dot<span class="p">—Scaled dot product attention</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">general<span class="p">—General attention</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">additive<span class="p">—Additive attention</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">concat<span class="p">—Concat attention</span></p></li></ul></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The entire code is available in <span class="s20">src/dl/models.py</span>. We will be covering only the <span class="s20">forward </span>function in detail in the book because that is the only place where there is a key difference. The rest of the class is about defining the right attention model based on input parameters and so on.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The encoder part is exactly the same as <span class="s20">SeqSeqModel</span>, which we saw in the last chapter. The only difference is in the decoding where we will be using attention.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s talk about how we are going to use the attention output in decoding.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">As I mentioned before, there are two schools of thought on how to use attention while decoding. Using the same terminology we have been using for attention, let’s see the difference.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Luong et al. use the decoder hidden state at step <span class="s74">𝑗𝑗</span>, <span class="s90">𝑠𝑠𝑗𝑗</span>, to calculate the similarity between itself and all the encoder hidden states, <span class="s90">𝐻𝐻</span>, to calculate the context vector, <span class="s90">𝑐𝑐𝑗𝑗</span>. This context vector, <span class="s90">𝑐𝑐𝑗𝑗</span>, is then concatenated with the decoder hidden state, <span class="s90">𝑠𝑠𝑗𝑗</span>, and this combined tensor is used as the input to the linear layer that generates the output.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Bahdanau et al. use attention in another way. They use the decoder hidden state from the previous timestep, <span class="s90">𝑠𝑠𝑗𝑗−1</span>, and calculate the similarity with all the encoder hidden states, <span class="s90">𝐻𝐻</span>, to calculate the context vector, <span class="s90">𝑐𝑐𝑗𝑗</span>. And now, this context vector, <span class="s90">𝑐𝑐𝑗𝑗</span>, is concatenated with the input to decoding step j,</p><p class="s90" style="padding-left: 27pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">𝑥𝑥𝑗𝑗<span class="p">. It is this concatenated input that is used in the decoding step that uses an RNN.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can see the differences visually in <i>Figure 14.3</i>. The <i>Further reading </i>section also has another brilliant animation of attention under <i>Attn: Illustrated Attention</i>. That can also help you understand the mechanism well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span><img width="528" height="289" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_762.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.3 – Attention-based decoding: Bahdanau versus Luong</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In our implementation, we have chosen the Bahdanau way of decoding, where we use the concatenated context vector and input as the input for decoding. And because of that, there is a condition the decoder must satisfy: the <span class="s20">input_size </span>parameter of the decoder should be equal to the sum of the <span class="s20">input_size </span>parameter of the encoder and the <span class="s20">hidden_size </span>parameter of the encoder. This validation is inbuilt into <span class="s20">Seq2SeqwAttnConfig</span>.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="356" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_763.png"/></span></p><p class="s28" style="text-indent: 0pt;text-align: left;">if teacher_force:</p><p class="s28" style="padding-top: 3pt;text-indent: 24pt;line-height: 131%;text-align: left;">dec_input = y[:, i, :].unsqueeze(1) else:</p><p class="s28" style="padding-left: 24pt;text-indent: 0pt;line-height: 11pt;text-align: left;">dec_input = out</p><p style="text-indent: 0pt;text-align: left;"/><p class="s28" style="text-indent: 0pt;text-align: left;">13</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">14</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">15</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">16</p><p style="text-indent: 0pt;text-align: left;"/><p class="s28" style="text-indent: 0pt;line-height: 11pt;text-align: left;">teacher_forcing_ratio</p><p style="text-indent: 0pt;text-align: left;"/><p class="s28" style="text-indent: 0pt;line-height: 131%;text-align: left;">out, h = self.decoder(dec_input, h) out = self.fc(out)</p><p class="s28" style="text-indent: 0pt;line-height: 11pt;text-align: left;">y_hat[:, i, :] = out.squeeze(1)</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">teacher_force = random.random() &lt; self.hparams.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s28" style="text-indent: 0pt;text-align: left;">09</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">10</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">11</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">12</p><p style="text-indent: 0pt;text-align: left;"/><p class="s28" style="text-indent: 0pt;line-height: 11pt;text-align: left;">unsqueeze(1)), dim=-1)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s28" style="text-indent: 0pt;line-height: 131%;text-align: left;">y_hat = torch.zeros_like(y, device=y.device) dec_input = x[:, -1:, :]</p><p class="s28" style="text-indent: 0pt;line-height: 11pt;text-align: left;">for i in range(y.size(1)):</p><p class="s28" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;line-height: 131%;text-align: left;">top_h = self._get_top_layer_hidden_state(h) context = self.attention(</p><p class="s28" style="padding-left: 48pt;text-indent: 0pt;line-height: 11pt;text-align: left;">top_h.unsqueeze(1), o</p><p class="s28" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;line-height: 11pt;text-align: left;">dec_input = torch.cat((dec_input, context.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s28" style="text-indent: 0pt;text-align: left;">01</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">02</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">03</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">04</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">05</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">06</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">07</p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">08</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The following code block has all the code necessary for decoding with attention and has line numbers so that we can go line by line and explain what we are doing:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><i>Lines 1 </i>and <i>2 </i>are the same as in the <span class="s20">Seq2SeqMode</span>l class where we set up the variable to store the prediction and extract the starting input to be passed to the decoder, and <i>line 3 </i>starts the loop for decoding step by step.</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark454" class="s140">Now, in each step, we need to use the hidden state from the previous timestep to calculate the context vector. If you remember the output shapes of an RNN (</a>Chapter 12<span class="p">, </span>Building Blocks of Deep Learning for Time Series<span class="p">), we know that it is (</span>number of layers, batch size, hidden size<span class="p">). But we need our query hidden state to be of the dimension (</span>batch size, hidden size<span class="p">). Luong et al. suggested using the hidden states from the top layer of a stacked RNN model as the query, and we are doing just that here:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  hidden_state[-1, :, :]                                          </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark545">If the RNN is bi-directional, we would need to slightly alter the retrieval because now, the last two rows of the tensor would be the output from the last layer (one forward and one backward). There are many ways we can combine these into a single tensor—we can concatenate them, we can sum them, or we can even mix them using a linear layer. Here, we just concatenate them:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">torch.cat((hidden_state[-1, :, :], hidden_state[-2, :, :]), dim=-1)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And now that we have the hidden state, we use it as the query in the attention layer (<i>line 5</i>). In <i>line 8</i>, we concatenate the context with the input. Lines <i>9 </i>to <i>16 </i>do the rest of the decoding similar to <span class="s20">Seq2SeqModel</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The notebook trains a multi-step Seq2Seq model (the best-performing variant with teacher forcing) with all the different types of attention we covered in the chapter using the same setup we developed in the last chapter. The results are summarized in the following table:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="209" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_764.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.4 – Summary table for Seq2Seq models with attention</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can see that it is showing considerable improvements in <span class="s5">MAE</span>, <span class="s5">MSE</span>, and <span class="s5">MASE </span>by including attention, and out of all the variants of attention, the simple dot product attention performed the best, closely followed by additive attention. At this point, some of you might have a question in your mind—<i>Why didn’t the scaled dot product work better than dot product attention? </i>Scaling was supposed to make the dot product work better, wasn’t it?</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There is a lesson to be learned here (which applies to all <span class="s5">machine learning </span>(<span class="s5">ML</span>)). No matter how much better a particular technique is in theory, you can always find examples in which it performs worse. And here, we saw just one household, and it is not surprising that we saw that scaled dot product attention didn’t work better than the normal dot product attention. But if we had evaluated at scale and realized that this is a pattern across multiple datasets, then it would be concerning.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark546">So, we have seen that attention does make the models better. There was a lot of research done on using attention in various forms to enhance the performance of </a><span class="s5">neural network </span>(<span class="s5">NN</span>) models. Most of that research was carried out in <span class="s5">natural language processing </span>(<span class="s5">NLP</span>), specifically in language translation and language modeling. Soon, researchers stumbled upon a surprising result that changed the course of DL progress drastically.<a name="bookmark532">&zwnj;</a><a name="bookmark531">&zwnj;</a></p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Transformers – Attention is all you need</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">While the introduction of attention was a shot in the arm for RNNs and Seq2Seq models, they still had one problem. The RNNs were recurrent, and that meant it needed to process each word in a sentence in a sequential manner. And for popular Seq2Seq model applications such as language translation, it meant processing long sequences of words became really time-consuming. In short, it was difficult to scale them to a large corpus of data. In 2017, Vaswani et al. authored a seminal paper titled <i>Attention Is All You Need</i>. Just as the title of the paper implies, they explored an architecture that used attention (scaled dot product attention) and threw away recurrent networks altogether. And to the surprise of NLP researchers around the world, these models (which were dubbed Transformers) outperformed the then state-of-the-art Seq2Seq models in language translation.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This spurred a flurry of research activity around this new class of models, and pretty soon, in 2018, Devlin et al. from Google developed a bi-directional version of Transformers and trained the now famous language model, <span class="s5">BERT </span>(which stands for <span class="s5">Bidirectional Encoder Representations from Transformers</span>), and broke many state-of-the-art results across multiple tasks. This is considered to be the moment when Transformers as a model class really <i>arrived</i>. Fast-forward to 2022—Transformer models are ubiquitous. They are used in almost all tasks in NLP, and in many other sequence-based tasks such as time series forecasting, <span class="s5">reinforcement learning </span>(<span class="s5">RL</span>), and so on. They have also been successfully used in <span class="s5">computer vision </span>(<span class="s5">CV</span>) tasks as well.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There have been numerous modifications and adaptations to the vanilla Transformer model to make it more suitable for time series forecasting. But let’s understand the vanilla Transformer architecture that Vaswani et al. proposed in 2017 first.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Attention is all you need</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The model Vaswani et al. proposed (hereby referred to as the vanilla Transformer) is also an encoder- decoder model, but both the encoder and decoder are non-recurrent. They are entirely comprised of attention mechanisms and feed-forward networks. Since the Transformer model was developed first for text sequences, let’s use the same example to understand and then adapt to the time series context.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are a few key components of the model that need to be understood to put the whole thing together. Let’s take them one by one.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark547">Self-attention</a></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We saw how scaled dot product attention works earlier in this chapter (in the <i>Alignment functions </i>section), but there, we were calculating attention between the encoder and decoder hidden states. Self- attention is when we have an input sequence, and we calculate the attention scores between that input sequence itself. Intuitively, we can think of this operation as enhancing the contextual information and enabling the downstream components to use this enhanced information for further processing.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="87" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_765.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code, use the notebook named <span class="s20">02-Self-Attentio and Multi-Headed Attention.ipynb </span>in the <span class="s20">Chapter14 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 7pt;text-indent: 0pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We saw the <span class="s20">PyTorch </span>implementation for encoder-decoder attention earlier, but that implementation was more aligned toward the step-by-step decoding of an RNN. Computing the attention scores for each query-key pair in one shot is something very simple to achieve using standard matrix multiplication and is essential to computing efficiency.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In NLP, it is standard practice to represent each word as a learnable vector called an embedding. This is because text or strings have no place in a mathematical model. For our example’s sake, let’s assume we use an embedding vector of size 512 for each word, and let’s assume that the attention mechanism has an internal dimension of 64. Let’s elucidate the attention mechanism using a sentence with 10 words.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 91%;text-align: justify;">After embedding, the sentence would be a tensor with dimensions (10, 512). We need to have three learnable weight matrices, <span class="s90">𝑊𝑊𝑞𝑞</span>, <span class="s90">𝑊𝑊</span><span class="s128">𝑘𝑘</span>, and <span class="s90">𝑊𝑊𝑣𝑣</span>, to project the input embedding into the attention dimension (64). See <i>Figure 14.5</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 70pt;text-indent: 0pt;text-align: left;"><span><img width="406" height="76" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_766.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.5 – Self-attention layer: input sentence and the learnable weights</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The first operation projects the sentence tensor into a query, key, and value with dimensions equal to (<i>sequence length, attention dim</i>). This is done by using a matrix multiplication between the sentence tensor and learnable matrices. See <i>Figure 14.6</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 145pt;text-indent: 0pt;text-align: left;"><a name="bookmark548"><span><img width="238" height="246" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_767.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.6 – Self-attention layer: query, key, and value projection</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have the query, key, and value, we can calculate the attention weights of every query- key pair using matrix multiplication between the query and the transpose of the keys. The matrix multiplication is nothing but the dot product of each query with each of the values and gives us a square matrix of (<i>sequence length, sequence length</i>). See <i>Figure 14.7</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 107pt;text-indent: 0pt;text-align: left;"><span><img width="338" height="79" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_768.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.7 – Self-attention layer: attention scores between Q and K</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Converting the attention scores to attention weights is just about scaling and applying the <i>softmax</i></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">function, as we discussed in the <i>Scaled dot product attention </i>section.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have the attention weights, we can use them to combine the value. The element-wise multiplication and then summing across the weights can be done efficiently using another matrix multiplication. See <i>Figure 14.8</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span><img width="348" height="78" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_769.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.8 – Self-attention layer: combining V using the learned attention weights</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark549">Now, we have seen how attention is applied to all the query-key pairs in monolithic matrix operations rather than doing the same operation for each query in a sequential way. But </a><i>Attention is All you Need </i>proposed something even better.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Multi-headed attention</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Since Transformers intended to take away the entire recurrent architecture, they needed to beef up the attention mechanism because that was the workhorse of the model. So, instead of using a single attention head, the authors of the paper proposed multiple attention heads acting together in different subspaces. We know that attention helps the model focus on a few elements from the many. <span class="s5">Multi- headed attention </span>(<span class="s5">MHA</span>) does the same thing but focuses on different aspects or different sets of elements, thereby increasing the capacity of the model. If we want to draw an analogy to the human mind, we consider many aspects of a situation before we take a decision.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">For instance, if we decide to step out of the house, we will pay attention to the weather, we will pay attention to the time so that whatever we want to accomplish is still possible, we will pay attention to how punctual that friend you made a plan with has been in the past, and leave our house accordingly. Each of those aspects you can think of as one head of attention. Therefore, MHA enables Transformers to <i>attend </i>to multiple aspects at the same time.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Normally, if there are eight heads, we would assume that we would have to do the computation that we saw in the last section eight times. But thankfully, that is not the case. There are clever ways of accomplishing this MHA using the same kind of matrix multiplication, but now with larger matrices. Let’s see how that is done.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We will continue the same example and see a case where we have eight attention heads. There is one condition that needs to be satisfied to do this efficient calculation of MHA—the attention dimension should be divisible by the number of heads we are using.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The initial steps are exactly the same. We take in the input sentence tensor and project it into the query, key, and value. Now, we split the query, key, and value into separate query, key, and value subspaces for each head by doing some basic tensor re-arrangement. See <i>Figure 14.9</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><img width="523" height="102" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_770.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.9 – Multi-headed attention: reshaping Q, K, and V into subspaces for each head</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, we calculate the attention scores for each head in a single operation and combine them with the value to get the attention output for each head. See <i>Figure 14.10</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 76pt;text-indent: 0pt;text-align: left;"><a name="bookmark550"><span><img width="427" height="232" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_771.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.10 – Multi-headed attention: calculating attention weights and combining the value</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-bottom: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have the attention output of each head in the <span class="s20">attn_output </span>variable. Now, all we need to do is to reshape the array so that we stack the outputs from all the attention heads in a single dimension. See <i>Figure 14.11</i>:</p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="523" height="87" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_772.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.11 – Multi-headed attention: reshaping and stacking all the attention head outputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">In this way, we can do MHA in a fast and efficient manner. Now, let’s look at another key innovation that makes Transformers work.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Positional encoding</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Transformers successfully avoided the recurrence and unlocked a performance bottleneck of sequential operations. But now there is a problem. By processing all the positions in a sequence in parallel, the model also loses the ability to understand the relevance of the position. For the Transformer, each position is independent of the other, and hence one key aspect we would seek from a model that processes sequences is missing. The original authors did propose a way to make sure we do not lose this information—<span class="s5">positional encoding</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There have been many variants of positional encoding that have come up in subsequent years of research, but the most common one is still the variant that is used in the vanilla Transformer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The solution proposed by Vaswani et al. was to add a particular vector, which encodes the position mathematically using sine and cosine functions, to each of the input tokens before processing them through the self-attention layer. If the input<span class="s90">, 𝑋𝑋</span>, is a <span class="s144">d</span><span class="s90">model</span>-dimensional embedding for <span class="s74">𝑛𝑛 </span>tokens in a sequence, positional embeddings, <span class="s137">𝑃𝑃</span>, is a matrix of the same size(<span class="s90">𝑛𝑛 × 𝑑𝑑</span><span class="s128">𝑚𝑚</span><span class="s127">𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚</span>). The element on the</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><span class="s74">𝑝𝑝𝑝𝑝𝑝𝑝𝑡𝑡ℎ</span>row and <span class="s168">2𝑖𝑖𝑡𝑡ℎ </span>or <span class="s90">(2𝑖𝑖 + 1)𝑡𝑡ℎ </span>column is defined as follows:</p><p class="s90" style="padding-top: 2pt;padding-left: 173pt;text-indent: 0pt;line-height: 10pt;text-align: center;">𝑝𝑝𝑝𝑝𝑝𝑝</p><p style="text-indent: 0pt;text-align: left;"><span><img width="87" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_773.png"/></span></p><p class="s143" style="padding-left: 160pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑝<span class="s90">𝑝</span><span class="s127">𝑝𝑝𝑝𝑝𝑝𝑝,2𝑖𝑖  </span>=<span class="s90"> sin (</span></p><p class="s90" style="padding-left: 221pt;text-indent: 0pt;line-height: 9pt;text-align: left;">10000</p><p style="text-indent: 0pt;text-align: left;"><span><img width="87" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_774.png"/></span></p><p class="s143" style="padding-top: 8pt;padding-left: 154pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑝<span class="s90">𝑝</span><span class="s127">𝑝𝑝𝑝𝑝𝑝𝑝,2𝑖𝑖+1  </span>=<span class="s90"> cos (</span></p><p class="s268" style="text-indent: 0pt;line-height: 15pt;text-align: left;">2<span class="s127">𝑖𝑖/𝑑𝑑</span><span class="s64">𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚 </span><span class="s269">)</span></p><p class="s90" style="padding-top: 2pt;text-indent: 0pt;line-height: 10pt;text-align: left;">𝑝𝑝𝑝𝑝𝑝𝑝</p><p class="s268" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2<span class="s127">𝑖𝑖/𝑑𝑑</span><span class="s64">𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚 </span><span class="s269">)</span></p><p class="s90" style="padding-left: 173pt;text-indent: 0pt;line-height: 11pt;text-align: center;">10000</p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although this looks a little complicated and counterintuitive, let’s break this down to understand it better.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">From 20,000 feet, we know that these positional encodings capture the positional information, and we add them to the input embeddings. But why do we add them to the input embeddings? Let’s make that intuition clearer. Let’s assume the embedding dimension is just 2 (this is for ease of visualization and grabbing the concept better), and we have a word, <i>A</i>, represented using this token. For our experiment, let’s assume that we have the same word, <i>A</i>, repeated several times in our sequence. What happens if we add the positional encoding to it?</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;">We know that the sine or cosine functions vary between 0 and 1. So, each of these encodings we add to the word embedding just perturbs the word embedding within a unit circle. As <span class="s144">𝑝𝑝𝑝𝑝𝑝𝑝 </span>increases, we can see the position-encoded word embedding trace a unit circle around the original embedding (<i>Figure 14.12</i>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="321" height="316" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_775.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.12 – Position encoding: intuition</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In <i>Figure 14.12</i>, we have assumed a random embedding for a word, <i>A </i>(represented by the cross marker), and added position embedding assuming <i>A </i>is in different positions. And these position-encoded vectors are represented by star markers with the corresponding positions mentioned in numbers next to them. We can see how each position is a slightly perturbed point of the original vector, and it happens in a cyclical manner in a clockwise direction. We can see position 0 right at the top with 1, 2, 3, and so on in the clockwise direction. By having this representation, the model can figure out the word in different locations, and still retain the overall position in the semantic space.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Now that we know why we are adding the positional encodings to the input embeddings and have seen the intuition of why it works, let’s get down into more details and see how the terms inside the sine and cosine are calculated. <span class="s94">𝑝𝑝𝑝𝑝𝑝𝑝</span><span class="s74"> </span>represents the position of the token in the sequence. If the maximum length of the sequence is 128, <span class="s90">𝑝𝑝𝑝𝑝𝑝𝑝 </span>varies from 0 to 127. <span class="s214">𝑖𝑖</span><span class="s65"> </span>represents the position along the embedding dimension, and because of the way the formula has been defined, for each value of <span class="s44">𝑖</span><span class="s43">𝑖</span>, we have two values—a sine and a cosine. Therefore, <span class="s90">𝑖𝑖 </span>will be half the number of dimensions, <span class="s90">𝑑𝑑</span><span class="s128">𝑚𝑚𝑚</span><span class="s127">𝑚𝑚𝑚𝑚𝑚𝑚𝑚</span>, and will go from 0 to <span class="s168">𝑑𝑑𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚/2</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">With all this information, we know that the term inside the sine and cosine functions approaches 0 as we go toward the end of the embedding dimension. It also increases from 0 as we move along the sequence dimension. For each pair (<span class="s168">2𝑖𝑖 </span>and <span class="s90">2𝑖𝑖 + 1</span>) of positions in the embedding dimension, we have a complementary sine and cosine wave, as <i>Figure 14.13 </i>shows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span><img width="522" height="276" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_776.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.13 – Positional encoding: sine and cosine terms</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see that embedding dimensions <span class="s5">40 </span>and <span class="s5">41 </span>are sine and cosine waves of the same frequency, and embedding dimensions <span class="s5">40 </span>and <span class="s5">42 </span>are sine waves with a slight increase in frequency. By using the combination of sine and cosine waves of varying frequencies, the positional encoding can encode</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-bottom: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark551">rich positional information as a vector. If we plot a heatmap of the whole positional encoding vector (</a><i>Figure 14.14</i>), we can see how the values change and encode the positional information:</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"><span><img width="462" height="322" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_777.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.14 – Positional encoding: heatmap of the entire vector</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Another interesting observation is that the positional encoding quickly shrinks to 0/1 as we move forward in the embedding dimension because the term inside the sine or cosine functions (angle in radians) quickly becomes zero on account of the large denominator. The zoomed plot shows the color differences more clearly.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now for the last component in the Transformer model.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Position-wise feed-forward layer</p><p class="s4" style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark454" class="s140">We have already covered what feed-forward networks are in </a>Chapter 12<span class="p">, </span>Building Blocks of Deep Learning for Time Series<span class="p">. The only thing to be noted here is that the position-wise feed-forward layer is when we apply the same feed-forward layer in each position, independently. If we have 12 positions (or words), we will have a single feed-forward network to process each of these positions.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Vaswani et al. defined this as a two-layer feed-forward network where the transformations were defined in a way that the input dimensions get expanded to four times the input dimension, with a ReLU activation function applied at that stage, and then transformed back to the input dimension again. The exact operation can be written as a mathematical formula:</p><p class="s146" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">FFN(𝑥𝑥) = max(0, 𝑊𝑊<span class="s79">1</span>𝑥𝑥 + 𝑏𝑏<span class="s79">1</span>)𝑊𝑊<span class="s79">2</span><span class="s75">  </span>+ b<span class="s79">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 88%;text-align: justify;"><a name="bookmark552">Here, </a><span class="s144">𝑊𝑊</span><span class="s90">1 </span>is a matrix of dimensions (<i>input size, 4*input size</i>), <span class="s90">𝑊𝑊</span><span class="s128">2</span><span class="s127"> </span>is a matrix of dimensions (<i>4*input size, input size</i>), <span class="s90">𝑏𝑏1 </span>and <span class="s74">𝑏𝑏2 </span>are corresponding biases, and <span class="s90">𝑚𝑚𝑚𝑚𝑚𝑚(0, 𝑚𝑚) </span>is the standard ReLU operator.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There have been studies where researchers have tried replacing ReLU with other activation functions, more specifically <span class="s5">Gated Linear Units </span>(<span class="s5">GLUs</span>), which have shown promise. Noam Shazeer from Google has a paper on the topic, and if you want to know more about these new activation functions, I recommend checking out his paper in the <i>Further reading </i>section.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we know all the necessary components of a Transformer model, let’s see how they are put together.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Encoder</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The vanilla Transformer model is an encoder-decoder model. There are N blocks of encoders, and each block contains an MHA layer and a position-wise feed-forward layer with residual connections in between (<i>Figure 14.15</i>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 132pt;text-indent: 0pt;text-align: left;"><span><img width="275" height="422" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_778.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.15 – Transformer model from Attention is All you Need by Vaswani et al.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark553">For now, let’s focus on the left side of </a><i>Figure 14.15</i>, which is the encoder. The encoder takes in the input embeddings, with the positional encoding vector added to it, as the input. The three-pronged arrow that goes into MHA denotes the query, key, and value split. The output from the MHA goes into a block named <i>Add &amp; Norm</i>. Let’s quickly see what that does.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are two key operations that happen here—<span class="s5">residual connections </span>and <span class="s5">layer normalization</span>.</p><p class="s233" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Residual connections</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Residual connections (or skip connections) are a family of techniques that were introduced to DL to make learning deep networks easier. The primary benefit of the technique is that it makes the gradient flow through the network better and thereby encourages learning in all parts of the network. They incorporate a pass-through memory highway in the network. We have already seen one instance where a skip connection (although not an apparent one) resolved gradient flow issues—<span class="s5">long short-term memory networks </span>(<span class="s5">LSTMs</span>). The cell state in the LSTM serves as this highway to let gradients flow through the network without getting into vanishing gradient issues.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">But nowadays, when we say residual connections, we typically think of <i>ResNets</i>, which made a splash in the history of DL through a <span class="s5">convolutional NN </span>(<span class="s5">CNN</span>) architecture that won major image classification challenges, including ImageNet in 2015. They introduced residual connections to train much deeper architectures than those prevalent at the time. The concept is deceptively simple. Let’s look at it visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: left;"><span><img width="461" height="228" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_779.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.16 – Residual networks</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 90%;text-align: justify;">Let’s assume a DL model with two layers with functions, <span class="s90">𝑀𝑀1 </span>and <span class="s90">𝑀𝑀</span><span class="s128">2</span>. In a regular NN, the input, <span class="s90">𝑥𝑥</span>, passes through the two layers to give us the output, <span class="s90">𝑦𝑦</span>. These two individual functions can be considered as <span class="s138">a</span> single function that converts <span class="s252">𝑥𝑥</span><span class="s137"> </span><span class="s138">t</span>o <span class="s137">𝑦𝑦</span><span class="s138">:</span> <span class="s90">𝑦𝑦 = 𝐹𝐹(𝑥𝑥)</span><span class="s138">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 88%;text-align: justify;"><a name="bookmark554">In residual networks, we change this paradigm into saying that each individual function (or layer) only learns the difference between the input to the function and the expected output. That is where the name residual connections came from. So, if </a><span class="s143">ℎ</span><span class="s127">1 </span>is the desired output and <span class="s137">𝑥𝑥 </span>is the input, then</p><p class="s90" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">𝑀𝑀1<span class="s144">(</span>𝑥𝑥<span class="s144">) </span>= ℎ1 − 𝑥𝑥<span class="p">. Rewriting that, we get </span>ℎ1 = M1<span class="s144">(</span>𝑥𝑥<span class="s144">) </span>+ x<span class="p">. And this is what is most used as residual connections.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Among many benefits such as better gradient flows, residual connections also make the loss surface smooth (Li et al. 2018) and more amenable to gradient-based optimization. For more details and intuition around residual networks, I urge you to check out the blog linked in the <i>Further reading </i>section.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">So, the <i>Add </i>in the <i>Add &amp; Norm </i>block in the Transformer is actually the residual connection.</p><p class="s233" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Layer normalization</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Normalization in <span class="s5">deep NNs </span>(<span class="s5">DNNs</span>) has been an active field of research. Among many benefits, normalization leads to faster training, higher learning rates, and even a bit of regularization. Batch normalization is the most common normalization technique in use, typically in CV, which makes the input have approximately zero mean and unit variance by subtracting the input mean in the current batch and dividing it by the standard deviation.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">But in NLP, researchers prefer layer normalization, where the normalization is happening across each feature. The difference can be seen in <i>Figure 14.17</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 80pt;text-indent: 0pt;text-align: left;"><span><img width="413" height="227" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_780.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.17 – Batch normalization versus layer normalization</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">This preference for layer normalization emerged empirically, but there have been studies and intuitions around the reason for this preference. NLP data usually has a higher variance as opposed to CV data, and this variance causes some problems for batch normalization. Layer normalization, on the other hand, is immune to this because it doesn’t rely on batch-level variance.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark555">Either way, Vaswani et al. decided to use layer normalization in their </a><i>Add &amp; Norm </i>block.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, we know the <i>Add &amp; Norm </i>block is nothing but a residual connection that is then passed through a layer normalization. So, we can see that the position-encoded inputs are first used in the MHA layer, and the output from the MHA is added with the position-encoded inputs again and passed through a layer normalization. And now, this output is passed through the position-wise feed-forward network and another <i>Add &amp; Norm </i>layer, and this becomes one block of the encoder. An important point to keep in mind is that the architecture of all the elements in the encoder is designed in such a way that the dimension of the input at each position is preserved throughout. In other words, if the embedding vector is of dimension 100, the output from the encoder will also have a dimension of 100. This is a convenient way to make it possible to have residual connections and stack as many layers on top of each other as possible. Now, there are multiple such encoder blocks stacked on top of each other to form the encoder of the Transformer.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Decoder</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The decoder block is also very similar to the encoder block, but with one key addition. Instead of a single self-attention layer, the decoder block has a self-attention layer, which operates on the decoder input, and an encoder-decoder attention layer. The encoder-decoder attention layer takes the query from the decoder at each stage and the key and values from the top encoder block.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There is something peculiar to the self-attention that is applied in the decoder block. Let’s see what that is.</p><p class="s233" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Masked self-attention</p><p class="s143" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><span class="p">We talked about how the Transformer can process sequences in parallel and be computationally efficient. But the decoding paradigm poses another challenge. Suppose we have an input sequence, </span>X<span class="s90"> = </span>{<span class="s90"> </span>x<span class="s127">1</span>,<span class="s90">  x</span><span class="s127">2</span>,<span class="s90">  … ,  𝑥𝑥</span><span class="s127">n</span>}<span class="s90"> </span><span class="p">, and the task is to predict the next step. So, in the decoder, if we give the sequence, </span><span class="s137">𝑋𝑋</span><span class="p">, because of the parallel-processing architecture, each sequence is processed at once using self-attention. And we know self-attention is agnostic to sequence order. If left unrestricted, the model will cheat by using information from the future timesteps to predict the current timestep. This is where masked attention becomes important.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We saw earlier in the <i>Self-attention </i>section how calculate a square matrix (if the query and key have the same length) of attention weights, and it is with these weights that we combine the information from the value vector. This self-attention has no concept of temporality, and all the tokens will attend to all other tokens irrespective of their position. Let’s see <i>Figure 14.18 </i>to solidify our intuition:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 62pt;text-indent: 0pt;text-align: left;"><a name="bookmark556"><span><img width="460" height="214" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_781.jpg"/></span></a><a name="bookmark533">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.18 – Masked self-attention</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s90" style="padding-left: 37pt;text-indent: 0pt;line-height: 86%;text-align: justify;"><span class="p">We have the sequence, </span>X = <span class="s144">{</span> x1,  x2,  … ,  𝑥𝑥5<span class="s144">}</span> <span class="p">, and we are still trying to predict one step ahead. So, </span><span class="s138">t</span><span class="p">he expected output from the decoder would be </span><span class="s143">𝑋𝑋</span><span class="s270">̂</span>  <span class="s143">=</span> <span class="s143">{𝑥𝑥</span>̂<span class="s127">2</span><span class="s143">,</span> 𝑥𝑥̂<span class="s127">3</span><span class="s143">,</span> … , 𝑥𝑥̂<span class="s127">6</span><span class="s143">}</span> <span class="s138">.</span><span class="p"> When we use self-attention,      the attention weights that will be learned will be a square matrix of </span><span class="s137">5𝑥𝑥5 </span><span class="p">dimension. But if we look at</span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">the upper triangle of the square matrix (the part that is shaded in <i>Figure 14.18</i>), those combinations of tokens violate the temporal sanctity.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can take care of this simply by adding a pre-generated mask that has zeros in all the white cells and <i>-inf </i>in all the shaded cells to the generated attention energies (the stage before applying softmax). This makes sure the attention weights for the shaded region will be zero, and this in turn ensures that no future information is used while calculating the weighted sum of the value vector.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, to wrap everything up, the output from the decoder is passed to a standard task-specific head to generate the output we desire.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We discussed the Transformer in the context of NLP, but it is a very small leap to adapt it to time series data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Transformers in time series</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Time series have a lot of similarities with NLP because of the fact that both deal with information in sequences. This can be further evidenced by the phenomenon that most of the popular techniques that are used in NLP are promptly adapted to a time series context. Transformers are no exception to that.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Instead of looking at tokens at each position, we have real numbers in each position. And instead of talking about input embeddings, we can talk in terms of input features. The vector of features at each timestep can be considered the equivalent of an embedding vector in NLP. And instead of making</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark603" class="s140" name="bookmark557">causal decoding an optional step (in NLP, that really depends on the task at hand), we have a strict requirement for causal decoding. There, it is trivial to adapt Transformers to time series, although in practice, there are many challenges because in time series we typically encounter sequences that are much longer than the ones in NLP, and this creates a problem because the complexity of self- attention is scaled quadratically with respect to the input sequence length. There have been many alternate proposals for self-attention that make it feasible to use it for long sequences as well, and we will be covering a few of them in </a>Chapter 16<span class="p">, </span>Specialized Deep Learning Architectures for Forecasting<span class="p">.</span><a name="bookmark534">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s try to put everything we have learned about Transformers into practice.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Forecasting with Transformers</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="87" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_782.png"/></span></p><p class="s29" style="padding-top: 10pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">To follow along with the complete code, use the notebook named <span class="s20">03-Transformers ipynb </span>in the <span class="s20">Chapter14 </span>folder and the code in the <span class="s20">src </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 7pt;text-indent: 0pt;text-align: left;">.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">For some continuity, we will continue with the same household we were forecasting with RNNs and RNNs with attention.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although we learned about the vanilla Transformer as a model with an encoder-decoder architecture, it was really designed for language translation tasks. In language translation, the source sequence and target sequence are quite different, and therefore the encoder-decoder architecture made sense. But soon after, researchers figured out that using the decoder part of the Transformer alone does well. It is called a decoder-only Transformer in literature. The naming is a bit confusing because if you think about it, the decoder is different from the encoder in two things—masked self-attention and encoder- decoder attention. So, in a decoder-only Transformer, how do we have the encoder-decoder attention? The short answer is that we don’t. The architecture of the decoder-only Transformer resembles the encoder block more, but we call it decoder-only because we use masked self-attention to make our model respect the temporal sanctity of our sequences.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We are also going to implement a decoder-only Transformer. The first thing we need to do is to define a config class, <span class="s20">TransformerConfig</span>, with the following parameters:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">input_size<span class="p">—This parameter defines the number of features the Transformer is expecting.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">d_model<span class="p">—This parameter defines the hidden dimension of the Transformer or the dimension over which all the attention calculation and subsequent operations happen.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">n_heads<span class="p">—This parameter defines how many heads we have in the MHA mechanism.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">n_layers<span class="p">—This parameter defines how many blocks of encoders we are going to stack on top of each other.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l132"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">ff_multiplier<span class="p">—This parameter defines the scale of expansion within the position-wise feed-forward layers.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">activation<span class="p">—This parameter lets us define which activation we need to use in the position- wise feed-forward layers. It can be either </span>relu <span class="p">or </span>gelu<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">multi_step_horizon<span class="p">—This parameter lets us define how many timesteps into the future we should be forecasting.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">dropout<span class="p">—This parameter lets us define the magnitude of dropout regularization to be applied in the Transformer model.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">learning_rate<span class="p">—This defines the learning rate of the optimization procedure.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">optimizer_params<span class="p">, </span>lr_scheduler<span class="p">, </span>lr_scheduler_params<span class="p">—These are parameters that let us tweak the optimization procedure. Let’s not worry about these for now because all of them have been set to intelligent defaults.</span></p></li></ul></li></ul><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, we are going to inherit the <span class="s20">BaseModel </span>class we defined in <span class="s20">src/dl/models.py </span>and define a <span class="s20">TransformerModel </span>class.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The first method we need to implement is <span class="s20">_build_network</span>. The entire model can be found in</p><p class="s20" style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">src/dl/models.py<span class="p">, but we will be covering the important aspects here as well.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">The first module we need to define is a linear projection layer that takes in the <span class="s20">input_size </span>parameter and projects it into <span class="s20">d_model</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">self.input_projection = nn.Linear(</p><p class="s28" style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">self.hparams.input_size, self.hparams.d_model,</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">bias=False</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is an additional step we have introduced to adapt the Transformers to the time series forecasting paradigm. In the vanilla Transformer, this is not needed because each word is represented by an embedding vector that typically has dimensions such as 200 or 500. But while doing time series forecasting, we might have to do the forecasting with just one feature (which is the history), and this seriously restricts our ability to provide capacity to the model because, without the projection layer, <span class="s20">d_model </span>can only be equal to <span class="s20">input_size</span>. Therefore, we have introduced a linear projection layer that decouples the number of features available and <span class="s20">d_model</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, we need to have a module that adds positional encoding. We have packaged the same code we saw earlier into a <span class="s20">PyTorch </span>module and added it to <span class="s20">src/dl/models.py</span>. We just use that module and define our positional encoding operator, like so:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  self.pos_encoder = PositionalEncoding(self.hparams.d_model)     </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We said earlier that we are going to use a decoder-only approach to building the model, and for that, we are using the <span class="s20">TransformerEncoderLayer </span>and <span class="s20">TransformerEncoder </span>modules defined in <span class="s20">PyTorch</span>. Just keep in mind that when using these layers, we will be using masked self-attention, and that makes it a decoder-only Transformer. The code is presented here:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 1pt;padding-left: 81pt;text-indent: -72pt;line-height: 15pt;text-align: left;">self.encoder_layer = nn.TransformerEncoderLayer( d_model=self.hparams.d_model, nhead=self.hparams.n_heads, dropout=self.hparams.dropout, dim_feedforward=self.hparams.d_model * self.</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">hparams.ff_multiplier,</p><p class="s28" style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;line-height: 131%;text-align: left;">activation=self.hparams.activation, batch_first=True,</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p class="s28" style="padding-left: 81pt;text-indent: -24pt;line-height: 15pt;text-align: left;">self.transformer_encoder = nn.TransformerEncoder( self.encoder_layer, num_layers=self.hparams.n_</p><p class="s28" style="padding-top: 1pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">layers</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The last module we need to define is a linear layer that converts the output from the Transformer into the number of timesteps we are forecasting:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">self.decoder = nn.Sequential(nn.Linear(self.hparams.d_model, 100),</p><p class="s28" style="padding-top: 2pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">nn.ReLU(),</p><p class="s28" style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">nn.Linear(100, self.hparams.multi_step_horizon)</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-bottom: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 147%;text-align: left;">That concludes the definition of the model. Now, let’s define a forward pass in the <span class="s20">forward </span>method. The first step is to generate a mask we need to apply masked self-attention:</p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">mask = self._generate_square_subsequent_mask(x.shape[1]).to(x. device)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We define the mask to have the same length as the input sequence. <span class="s20">_generate_square_subse- quent_mask </span>is a method we have defined that generates a mask. Assuming the sequence length is 5, we can look at the two steps in preparing the mask:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">mask = (torch.triu(torch.ones(5, 5)) == 1).transpose(0, 1)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="107" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_783.png"/></span></p><p class="s25" style="text-indent: 0pt;line-height: 131%;text-align: left;">True, False, False], True, True, False],</p><p class="s25" style="text-indent: 0pt;line-height: 11pt;text-align: left;">True, True, True]])</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 131%;text-align: left;">True, True,</p><p class="s25" style="text-indent: 0pt;line-height: 11pt;text-align: left;">True,</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 11pt;text-align: left;">True, False, False, False],</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;text-align: left;">[ True,</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">[ True,</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">[ True,</p><p class="s25" style="padding-top: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">[ True,</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 11pt;text-align: left;">tensor([[ True, False, False, False, False],</p><p style="text-indent: 0pt;text-align: left;"/><p class="s20" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">torch.ones(sz,sz) <span class="p">creates a square matrix with all ones, and </span>torch.triu(torch. ones(sz,sz)) <span class="p">makes the matrix with a top triangle (including the diagonal) as ones and the rest as zeros. And by using an equality operator with one condition and transposing it, we get a mask that has </span>True <span class="p">in all the bottom triangles, including the diagonal, and </span>False <span class="p">everywhere else. The output of the previous statement will be this:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">We can see that this matrix has <span class="s20">False </span>at all the places where we need to mask attention. Now, all we need to do is to fill all <span class="s20">True </span>instances with <span class="s20">0 </span>and all <span class="s20">False </span>instances with <span class="s20">-inf</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">mask = (</p><p class="s28" style="padding-top: 3pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">mask.float()</p><p class="s28" style="padding-top: 3pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">.masked_fill(mask == 0, float(&quot;-inf&quot;))</p><p class="s28" style="padding-top: 3pt;padding-left: 105pt;text-indent: 0pt;text-align: left;">.masked_fill(mask == 1, float(0.0))</p><p class="s28" style="padding-top: 3pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">These two lines of code are packaged into the <span class="s20">_generate_square_subsequent_mask </span>method, which we can use while training the model.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now that we have created the mask for masked self-attention, let’s start processing the input, <span class="s20">x</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Projecting input dimension to d_model</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">x_ = self.input_projection(x) <span class="s38"># Adding positional encoding </span>x_ = self.pos_encoder(x_)</p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># Encoding the input</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">x_ = self.transformer_encoder(x_, mask)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Decoding the input</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">y_hat = self.decoder(x_)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In these four lines of code, we project the input to <span class="s20">d_model </span>dimensions, add positional encoding, pass it through the Transformer model, and lastly use the linear layer to convert the output to the predictions we want.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now we have <span class="s20">y_hat</span>, which is the prediction from the model. All we need to think of now is how to train this output to be the desired output.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We know that the Transformer model processes all tokens in one shot, and if we have <i>N </i>elements in the sequence, we will have <i>N </i>predictions as well (each prediction corresponding to the next timestep). And if each prediction is of the next H timesteps, the shape of <span class="s20">y_hat </span>would be (<i>B, N, H</i>), where <i>B </i>is the batch size. There are a few ways we can use this output to compare with the target. The most simple and naïve way is to just take the prediction from the last position (which will have <i>H </i>timesteps) and compare it with <span class="s20">y </span>(which also has <i>H </i>timesteps).</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">But this is not the most efficient way of using all the information we have, is it? We are discarding <i>N-1 </i>predictions and not giving any signal to the model on all those <i>N-1 </i>predictions. So, while training, it makes sense to use all these <i>N-1 </i>predictions also so that the model has a much richer signal feeding back while learning.</p><p style="padding-top: 7pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">We can do that by using the original input sequence, <span class="s20">x</span>, but offsetting it by one. When <i>H=1</i>, we can think of this as a simple task where each position’s prediction is compared with the target for the next position (one step ahead). We can easily accomplish this by concatenating <span class="s20">x[:,1:,:] </span>(the input sequence offset by 1) with <span class="s20">y </span>(the original target) and treating this as the target. But when <i>H&gt;1</i>, this becomes slightly complicated, but we can still do it by using a helpful function from <span class="s20">PyTorch </span>called <span class="s20">unfold</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 106%;text-align: left;">y = torch.cat([x[:, 1:, :], y], dim=1).squeeze(-1).unfold(1, y.size(1), 1)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">We first concatenate the input sequence (offset by one) with <span class="s20">y </span>and then use <span class="s20">unfold </span>to create siding windows of <i>size=H</i>. This gives us a target of the same shape, (<i>B,N,H</i>).</p><p style="padding-top: 7pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">But during inference (when we are predicting using a trained model), we do not need the output of all the other positions, and hence we discard them, as seen here:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  y_hat = y_hat[:, -1, :].unsqueeze(1)                            </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: justify;">Our <span class="s20">BaseModel </span>class that we defined also lets us define a slightly different prediction step by using a <span class="s20">predict </span>method. You can look over the complete model in <span class="s20">src/dl/models.py </span>once again to solidify your understanding now.</p><p style="padding-top: 7pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">Now that we have defined the model, we can use the same framework we have been using to train <span class="s20">TransformerModel</span>. The full code is available in the notebook, but we will just look at a summary table with the results:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 66pt;text-indent: 0pt;text-align: left;"><span><img width="420" height="84" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_784.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 14.19 – Metrics for Transformer model on MAC000193 household</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140" name="bookmark558">We can see that the model is not doing as well as its RNN cousins. There can be many reasons for this, but the most probable one is that Transformers are really data-hungry. Transformers have far fewer inductive biases and therefore only shine where there is lots of data available to learn from. When forecasting just one household alone, our model has access to far less data and may not work very well. This is true, to an extent, for all the DL models we have seen so far. In </a>Chapter 10<span class="p">, </span>Global Forecasting Models<a href="#bookmark561" class="s140">, we talked about how we can train a single model for multiple households together, but that discussion was limited to classical ML models. DL is also perfectly capable of global forecasting models and that is exactly what we will be talking about in the next chapter—</a>Chapter 15<span class="p">, </span>Strategies for Global Deep Learning Forecasting Models<span class="p">.</span><a name="bookmark535">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">For now, congratulations on getting through another concept-heavy and information-packed chapter. The concept of attention, which has taken the field by storm, should be clearer in your mind now than when you started the chapter. I urge you to take a second stab at the chapter, read through the <i>Further reading </i>section, and do some of your own googling if it’s not clear because the future chapters assume you have this understanding.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have been storming through the world of DL in the last few chapters. We started off with the basic premise of DL, what it is, and why it became so popular. Then, we saw a few common building blocks that are typically used in time series forecasting and got our hands dirty, learning how we can put what we have learned into practice using PyTorch. Although we talked about RNNs, LSTMs, GRUs, and so on, we purposefully left out attention and Transformers because they deserved a separate chapter.</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark454" class="s140">We started the chapter by learning about the generalized attention model, helping you put a framework around all the different schemes of attention out there, and then went into detail on a few common attention schemes, such as scaled dot product, additive, and general attention. Right after incorporating attention into the Seq2Seq models we were playing with in </a>Chapter 12<span class="p">, </span>Building Blocks of Deep Learning for Time Series<span class="p">, we started with the Transformer. We went into detail on all the building blocks and architecture decisions involved in the original Transformer from the point of view of NLP, and after understanding the architecture, we adapted it to a time-series setting.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">And finally, we capped it off by training a Transformer model for forecasting on a sample household. And now, by finishing this chapter, we have all the basic ingredients to really start using DL for time series forecasting.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In the next chapter, we are going to elevate what we have been doing and move on to the global forecasting model paradigm.</p><p class="s47" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark559">References 379</a><a name="bookmark536">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_785.png"/></span></p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Following is the list of the references used in this chapter:</p><ol id="l133"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio (2015). <i>Neural Machine Translation by Jointly Learning to Align and Translate</i>. In <i>3rd International Conference on Learning Representations</i><a href="https://arxiv.org/pdf/1409.0473.pdf" class="s140" target="_blank">. </a><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">https://arxiv.org/pdf/1409.0473.pdf</a></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Thang Luong, Hieu Pham, and Christopher D. Manning (2015). <i>Effective Approaches to Attention- based Neural Machine Translation</i>. In <i>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</i><a href="https://aclanthology.org/D15-1166/" class="s140" target="_blank">. </a><a href="https://aclanthology.org/D15-1166/" target="_blank">https://aclanthology.org/D15-1166/</a></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">André F. T. Martins, Ramón Fernandez Astudillo (2016). <i>From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification</i>. In <i>Proceedings of the 33rd International Conference on Machine Learning</i><a href="http://proceedings.mlr.press/v48/martins16.html" class="s140" target="_blank">. </a><a href="http://proceedings.mlr.press/v48/martins16.html" target="_blank">http://proceedings.mlr.press/v48/martins16.html</a></p></li><li><p style="padding-top: 3pt;padding-left: 54pt;text-indent: -18pt;text-align: justify;">Ben Peters, Vlad Niculae, André F. T. Martins (2019). <i>Sparse Sequence-to-Sequence Models</i>. In <i>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</i><a href="https://aclanthology.org/P19-1146/" class="s140" target="_blank">. </a><a href="https://aclanthology.org/P19-1146/" target="_blank">https://aclanthology.org/P19-1146/</a></p></li><li><p style="padding-top: 3pt;padding-left: 54pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin (2017). <i>Attention is All you Need</i>. In <i>Advances in Neural Information Processing Systems</i><a href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" class="s140" target="_blank">. </a><a href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" class="a" target="_blank">https://papers.nips.cc/paper/2017/hash/3</a><a href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" target="_blank"> f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a></p></li><li><p style="padding-top: 4pt;padding-left: 54pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova (2019). <i>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</i>. In <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i><a href="https://aclanthology.org/N19-1423/" class="s140" target="_blank">. </a><a href="https://aclanthology.org/N19-1423/" class="a" target="_blank">https://aclanthology. </a><a href="https://aclanthology.org/N19-1423/" target="_blank">org/N19-1423/</a></p></li><li><p style="padding-top: 4pt;padding-left: 54pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein (2018). <i>Visualizing the Loss Landscape of Neural Nets</i>. In <i>Advances in Neural Information Processing Systems</i><a href="https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf" class="s140" target="_blank">. </a><a href="https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf" class="a" target="_blank">https://proceedings.neurips.cc/paper/2018/file/ </a><a href="https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf" target="_blank">a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf</a></p></li><li><p style="padding-top: 3pt;padding-left: 54pt;text-indent: -18pt;text-align: justify;">Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan Ramanath (2021). <i>An Attentive Survey of Attention Models</i>. <i>ACM Trans. Intell. Syst. Technol. 12, 5, Article 53 (October 2021)</i><a href="https://doi.org/10.1145/3465055" class="s140" target="_blank">. </a><a href="https://doi.org/10.1145/3465055" target="_blank">https://doi.org/10.1145/3465055</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark560">Further reading</a><a name="bookmark537">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here are a few resources for further reading:</p><ul id="l134"><li><p class="s4" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;line-height: 111%;text-align: justify;">The Illustrated Transformer <span class="p">by </span>Jay Alammar<a href="https://jalammar.github.io/illustrated-transformer/" class="s140" target="_blank">: </a><a href="https://jalammar.github.io/illustrated-transformer/" class="a" target="_blank">https://jalammar.github.io/ </a><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank">illustrated-transformer/</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">Transformer Networks: A mathematical explanation why scaling the dot products leads to more stable gradients<a href="https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500" class="s140" target="_blank">: </a><a href="https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500" class="a" target="_blank">https://towardsdatascience.com/transformer-networks- a-mathematical-explanation-why-scaling-the-dot-products-leads- </a><a href="https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500" target="_blank">to-more-stable-414f87391500</a></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Why is Bahdanau’s attention sometimes called concat attention?<a href="https://stats.stackexchange.com/a/524729" class="s140" target="_blank">: </a><a href="https://stats.stackexchange.com/a/524729" class="a" target="_blank">https://stats. </a><a href="https://stats.stackexchange.com/a/524729" target="_blank">stackexchange.com/a/524729</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Noam Shazeer <span class="p">(2020). </span>GLU Variants Improve Transformer. <span class="p">arXiv preprint: </span>Arxiv-2002.05202<span class="p">.</span></p><p style="padding-top: 1pt;padding-left: 64pt;text-indent: 0pt;text-align: left;"><a href="https://arxiv.org/abs/2002.05202">https://arxiv.org/abs/2002.05202</a></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 111%;text-align: left;">What is Residual Connection? <span class="p">by </span>Wanshun Wong<a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55" class="s140" target="_blank">: </a><a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55" class="a" target="_blank">https://towardsdatascience. </a><a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55" target="_blank">com/what-is-residual-connection-efb07cab0d55</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 64pt;text-indent: -13pt;line-height: 111%;text-align: left;">Attn: Illustrated Attention <span class="p">by </span>Raimi Karim<a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3" class="s140" target="_blank">: </a><a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3" class="a" target="_blank">https://towardsdatascience.com/ </a><a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3" target="_blank">attn-illustrated-attention-5ec4ad276ee3</a></p></li></ul></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark561">15</a><a name="bookmark562">&zwnj;</a><a name="bookmark564">&zwnj;</a><a name="bookmark563">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 63pt;text-indent: 30pt;line-height: 114%;text-align: left;">Strategies for Global Deep Learning Forecasting Models</h4><p class="s4" style="padding-top: 24pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140">All through the last few chapters, we have been building up deep learning for time series forecasting. We started with the basics of deep learning, saw the different building blocks, practically used some of those building blocks to generate forecasts on a sample household, and finally, talked about attention and transformers. Now, let’s slightly alter our trajectory and take a look at global models for deep learning. In </a>Chapter 10<span class="p">, </span>Global Forecasting Models<span class="p">, we saw why global models make sense and also saw how we can use such a model in the machine learning context. We even got good results in our experiments. In this chapter, we will look at how we can apply similar concepts, but from a deep learning context. We will look at different strategies that we can use to make global deep learning models work better.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p><ul id="l135"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Creating global deep learning forecasting models</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Using time-varying information</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Using static/meta information</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Using the scale of the time series</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Balancing the sampling procedure</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to run these notebooks:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">02 - Preprocessing London Smart Meter Dataset.ipynb <span class="p">in </span>Chapter02</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">01-Setting up Experiment Harness.ipynb <span class="p">in </span>Chapter04</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">01-Feature Engineering.ipynb <span class="p">in </span>Chapter06</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s27" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter15" class="s140" target="_blank" name="bookmark581">The associated code for the chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter15" class="a" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter15<span class="p">.</span><a name="bookmark565">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Creating global deep learning forecasting models</p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140">In </a>Chapter 10<span class="p">, </span>Global Forecasting Models<span class="p">, we talked in detail about why a global model makes sense. We talked about the benefits regarding increased </span>sample size<span class="p">, </span>cross-learning<span class="p">, </span>multi-task learning <span class="p">and the regularization effect that comes with it, and reduced </span>engineering complexity<span class="p">. All of these are relevant for a deep learning model as well. Engineering complexity and sample size become even more important because deep learning models are data-hungry and take quite a bit more engineering effort and training time than other machine learning models. I would go to the extent to say that in the deep learning context, in most practical cases where we have to forecast at scale, global models are the only deep learning paradigm that makes sense.</span></p><p class="s4" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark495" class="s140">So, why did we spend all that time looking at individual models? Well, it’s easier to grasp the concept at that level, and the skills and knowledge we gained at that level are very easily transferred to a global modeling paradigm. In </a>Chapter 13<span class="p">, </span>Common Modeling Patterns for Time Series<span class="p">, we saw how we use a data loader to sample windows from a single time series to train the model. To make the model a global model, all we need to do is to change the data loader so that instead of sampling windows from a single time series, we sample from many time series. The sampling process can be thought of as a two-step process (although in practice, we do it in a single step, it is intuitive to think of it as two) – first, sample the time series we need to pick the window from, and then, sample the window from that time series. And by doing that, we are training a single deep learning model to forecast all the time series together.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">To make our lives easier, we are going to use an open source library called <span class="s20">PyTorch Forecasting </span>to handle the data loading for us. <span class="s20">PyTorch Forecasting </span>aims to make time series forecasting with deep learning easy for both research and real-world cases alike. <span class="s20">PyTorch Forecasting </span><a href="#bookmark603" class="s140">also has implementations for many state-of-the-art forecasting architectures, and we will come back to those in </a><i>Chapter 16</i>, <i>Specialized Deep Learning Architectures for Forecasting</i>. But now, let’s use the high-level API in <span class="s20">PyTorch Forecasting</span>. This will significantly reduce our work in preparing <span class="s20">PyTorch </span>datasets. The <span class="s20">TimeSeriesDataset </span>class in <span class="s20">PyTorch Forecasting </span>takes care of a lot of boilerplate code dealing with different transformations, missing values, padding, and so on. We will be using this framework in this chapter when we look at different strategies to implement global deep learning forecasting models.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="140" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_786.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">To follow along with the complete code, use the notebook named <span class="s20">01-Global Deep Learning Models.ipynb </span>in the <span class="s20">Chapter15 </span>folder. There are two variables in the notebook that act as a switch – <span class="s20">TRAIN_SUBSAMPLE = True </span>makes the notebook run for a subset of 10 households. <span class="s20">train_model = True </span>makes the notebook train different models (warning: training models on the full data takes upward of 3 hours each). <span class="s20">train_model = False </span>loads the trained model weights and predicts on them.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark582">Preprocessing the data</a><a name="bookmark566">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark211" class="s140">We start by loading the necessary libraries and the dataset. We are using the preprocessed and feature- engineered dataset we created in </a><i>Chapter 6</i>, <i>Feature Engineering for Time Series Forecasting</i>. There are different kinds of features in the dataset and to make our feature assignment standardized, we use <span class="s20">namedtuple</span>. <span class="s20">namedtuple() </span>is a factory method in collections that lets you create subclasses of <span class="s20">tuple </span>with named fields. These named fields can be accessed using dot notation. We define <span class="s20">namedtuple </span>like this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">from collections import namedtuple FeatureConfig = namedtuple(</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">&quot;FeatureConfig&quot;, [</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">&quot;target&quot;, &quot;index_cols&quot;, &quot;static_categoricals&quot;, &quot;static_reals&quot;,</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">&quot;time_varying_known_categoricals&quot;, &quot;time_varying_known_reals&quot;, &quot;time_varying_unknown_reals&quot;, &quot;group_ids&quot;</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">],</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s also quickly establish what these names mean:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">target <span class="p">– The column name of what we are trying to forecast.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">index_cols <span class="p">– The columns that we need to make as an index for quick access to data.</span></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s20">static_categoricals </span>– These are columns that are categorical in nature and do not change with time. They are specific to each time series. For instance, the <i>Acorn group </i>in our dataset is <span class="s20">static_categorical </span>because it is categorical in nature and is a value pertaining to a household.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l136"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">static_reals <span class="p">– These are columns that are numeric in nature and do not change with time. They are specific to each time series. For instance, the average energy consumption in our dataset is numeric in nature and pertains to a single household.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">time_varying_known_categoricals <span class="p">– These are columns that are categorical in nature and change with time and we know the future values. They can be seen as quantities that keep varying with time. A prime example would be holidays, which are categorical, vary with time, and we know the future holidays.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">time_varying_known_reals <span class="p">– These are columns that are numeric in nature and change with time and we know the future values. A prime example would be temperature, which is numeric, varies with time, and we know the future values (provided the source we are getting the weather from allows for forecasted weather data as well).</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">time_varying_unknown_reals <span class="p">– These are columns that are numeric in nature and change with time and we don’t know the future values. The target we are trying to forecast is an excellent example.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 37pt;text-indent: 13pt;line-height: 161%;text-align: justify;">group_ids <span class="p">– These columns uniquely identify each time series in the dataframe. Once defined, we can assign different values to each of these names, as follows:</span></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">feat_config = FeatureConfig( target=&quot;energy_consumption&quot;, index_cols=[&quot;LCLid&quot;, &quot;timestamp&quot;], static_categoricals=[</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&quot;LCLid&quot;,</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">&quot;stdorToU&quot;, &quot;Acorn&quot;, &quot;Acorn_grouped&quot;, &quot;file&quot;,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">],</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">static_reals=[], time_varying_known_categoricals=[</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">&quot;holidays&quot;, &quot;timestamp_Dayofweek&quot;,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">],</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">time_varying_known_reals=[&quot;apparentTemperature&quot;], time_varying_unknown_reals=[&quot;energy_consumption&quot;], group_ids=[&quot;LCLid&quot;],</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140" name="bookmark583">We can see that we are not using all the features as we did with machine learning models (</a>Chapter 10<span class="p">, </span>Global Forecasting Models<span class="p">). There are two reasons for that:</span></p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Since we are using sequential deep learning models, a lot of the information we are trying to capture using rolling features and so on is already available to the model.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Unlike robust gradient-boosted decision tree models, deep learning models aren’t that robust to noise. So, irrelevant features would make the model worse.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are a few preprocessing steps that are needed to make the dataset we have compatible with <span class="s20">PyTorch Forecasting</span>. <span class="s20">PyTorch Forecasting </span>needs a continuous time index as a proxy for time. Although we have a <span class="s20">timestamp </span>column, it has datetimes. So, we need to convert it to a new column, <span class="s20">time_idx</span>. The complete code is in the notebook, but the essence of the code is simple. We combine the train and test dataframes and use a formula using the <span class="s20">timestamp </span>column to derive a new <span class="s20">time_idx </span>column. The formula is such that it increments every successive timestamp by one and is consistent between <span class="s20">train </span>and <span class="s20">test</span>. For instance, <span class="s20">time_idx </span>of the last timestep in <span class="s20">train </span>is <span class="s20">256</span>, and <span class="s20">time_idx </span>of the first timesteps in <span class="s20">test </span>would be <span class="s20">257</span>. In addition to that, we also need to convert the categorical columns into <span class="s20">object </span>data types to play nicely with <span class="s20">TimeSeriesDataset </span>from <span class="s20">PyTorch Forecasting</span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">For our experiments, we have chosen to have 2 days (96 timesteps) as the window and predict one single step ahead. And to enable early stopping, we would need a validation set as well. <span class="s5">Early stopping </span>is a way of regularization where we keep monitoring the validation loss and stop training when the validation loss starts to increase. We have selected the last day of training (48 timesteps) as the validation data and 1 whole month as the final test data. But when we prepare these dataframes, we need to take care of something: we have chosen 2 days as our history, and to forecast the first timestep in the validation or test set, we need the last 2 days of history along with it. So, we split our dataframes as shown in the following diagram (the exact code is in the notebook):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 45pt;text-indent: 0pt;text-align: left;"><span><img width="480" height="226" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_787.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.1 – Train-validation-test split</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark584">Now, before using </a><span class="s20">TimeSeriesDataset </span>on our data, let’s try to understand what it does and what the different parameters involved are.<a name="bookmark567">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Understanding TimeSeriesDataset from PyTorch Forecasting</p><p class="s20" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">TimeSeriesDataset <span class="p">automates the following tasks and more:</span></p><ul id="l137"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Scaling numeric features and encoding categorical features:</p><ul id="l138"><li><p style="padding-top: 8pt;padding-left: 74pt;text-indent: -11pt;text-align: justify;">Scaling the numeric features to have the same mean and variance helps gradient descent- based optimization to converge faster and better.</p></li><li><p style="padding-top: 5pt;padding-left: 74pt;text-indent: -11pt;text-align: justify;">Categorical features need to be encoded as numbers so that we can handle them the right way inside the deep learning models.</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Normalizing the target variable:</p><ul id="l139"><li><p style="padding-top: 8pt;padding-left: 74pt;text-indent: -11pt;text-align: justify;">In a global model context, the target variable can have different scales for different time series. For instance, a particular household typically has higher energy consumption, and some other households may be vacant and have little to no energy consumption. Scaling the target variable to a single scale helps the deep learning model to focus on learning the patterns rather than capturing the variance in scale.</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Efficiently converting the dataframe into a dictionary of PyTorch tensors:</p><ul id="l140"><li><p style="padding-top: 8pt;padding-left: 74pt;text-indent: -11pt;text-align: justify;">The dataset also takes in the information about different columns and converts the dataframe into a dictionary of PyTorch tensors, separately handling the static and time-varying information.</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">These are the major parameters of <span class="s20">TimeSeriesDataset</span>:</p></li></ul></li><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">data <span class="p">– This is the pandas DataFrame holding all the data such that each row is uniquely identified with </span>time_idx <span class="p">and </span>group_ids<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">time_idx <span class="p">– This refers to the column name with the continuous time index we created earlier.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s20">target</span>, <span class="s20">group_ids</span>, <span class="s20">static_categoricals</span>, <span class="s20">static_reals</span>, <span class="s20">time_varying_ known_categoricals</span>, <span class="s20">time_varying_known_reals</span>, <span class="s20">time_varying_unknown_ categoricals</span>, and <span class="s20">time_varying_unknown_reals </span>– We already discussed all these parameters in the <i>Preprocessing the data </i>section. These hold the same meaning.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">max_encoder_length <span class="p">– This sets the maximum window length given to the encoder.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">min_decoder_length <span class="p">– This sets the minimum window given in the decoding context.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a name="bookmark585">target_normalizer </a><span class="p">– This takes in a transformer that normalizes the targets. There are a few normalizers built into </span>PyTorch Forecasting <span class="p">– </span>TorchNormalizer<span class="p">,</span><span class="s35"> </span>GroupNormalizer<span class="p">, and </span>EncoderNormalizer<span class="p">. </span>TorchNormalizer <span class="p">does standard and robust scaling of the targets as a whole, whereas </span>GroupNormalizer <span class="p">does the same but with each group separately (a group is defined by </span>group_ids<span class="p">). </span>EncoderNormalizer <span class="p">does the scaling at runtime by normalizing using the values in each window.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">categorical_encoders <span class="p">– This parameter takes in a dictionary of scikit-learn transformers as a category encoder. By default, the category encoding is similar to </span>LabelEncoder<span class="p">, which replaces each unique categorical value with a number, adding an additional category for unknown and </span>NaN <span class="p">values.</span></p></li></ul><p class="s27" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="https://pytorch-forecasting.readthedocs.io/en/stable/data.html#time-series-data-set" class="s140" target="_blank">For the full documentation, please refer to </a><a href="https://pytorch-forecasting.readthedocs.io/en/stable/data.html#time-series-data-set" class="a" target="_blank">https://pytorch-forecasting.readthedocs. </a>io/en/stable/data.html#time-series-data-set<span class="p">.</span></p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Initializing TimeSeriesDataset</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now that we know the major parameters, let’s initialize a time series dataset using our data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">training = TimeSeriesDataSet( train_df, time_idx=&quot;time_idx&quot;, target=feat_config.target,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">group_ids=feat_config.group_ids, max_encoder_length=max_encoder_length, max_prediction_length=max_prediction_length, time_varying_unknown_reals=[</p><p class="s28" style="padding-left: 57pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&quot;energy_consumption&quot;,</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">],</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: -24pt;line-height: 131%;text-align: left;">target_normalizer=GroupNormalizer( groups=feat_config.group_ids, transformation=None</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Note that we have used <span class="s20">GroupNormalizer </span>so that each household is scaled separately using its own mean and standard deviation using the following well-known formula:</p><p class="s80" style="padding-top: 6pt;padding-bottom: 2pt;padding-left: 172pt;text-indent: 0pt;text-align: center;">𝑥𝑥  − 𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚</p><p style="padding-left: 175pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="134" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_788.png"/></span></p><p class="s80" style="padding-left: 172pt;text-indent: 0pt;text-align: center;">standard deviation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark586">TimeSeriesDataset </a><span class="p">also makes it easy to declare validation and test datasets as well using a factory method, </span>from_dataset<span class="p">. It takes in another time series dataset as an argument and uses the same parameters, scalers, and so on, and creates new datasets:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Defining the validation dataset with the same parameters as training</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">validation = TimeSeriesDataSet.from_dataset(training, pd.concat([val_history,val_df]).reset_index(drop=True), stop_ randomization=True)</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Defining the test dataset with the same parameters as training</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">test = TimeSeriesDataSet.from_dataset(training, pd.concat([hist_df, test_df]).reset_index(drop=True), stop_ randomization=True)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Notice that we concatenate the history to both <span class="s20">val_df </span>and <span class="s20">test_df </span>to make sure we can predict on the entire validation and test period.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Creating the dataloader</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">All that is left to do is to create the dataloader from <span class="s20">TimeSeriesDataset</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">train_dataloader = training.to_dataloader(train=True, batch_ size=batch_size, num_workers=0)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">val_dataloader = validation.to_dataloader(train=False, batch_ size=batch_size, num_workers=0)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Before we proceed, let’s solidify our understanding of the dataloader with the help of an example. The train dataloader we just created has split the dataframe into a dictionary of PyTorch tensors. We have chosen <span class="s20">512 </span>as a batch size and can inspect the dataloader using the following code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Testing the dataloader</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">x, y = next(iter(train_dataloader)) print(&quot;\nsizes of x =&quot;)</p><p class="s28" style="padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">for key, value in x.items(): print(f&quot;\t{key} = {value.size()}&quot;)</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">print(&quot;\nsize of y =&quot;) print(f&quot;\ty = {y[0].size()}&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark587">We will get an output as follows:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 73pt;text-indent: 0pt;text-align: left;"><span><img width="408" height="306" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_789.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.2 – Shapes of tensors in a batch of a train dataloader</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can see that the dataloader and <span class="s20">TimeSeriesDataset </span>have split the dataframe into PyTorch tensors and packed them into a dictionary with the encoder and decoder sequences separate. We can also see that the categorical and continuous features are also separated. The main <i>keys </i>we will be using from this dictionary are <span class="s20">encoder_cat</span>, <span class="s20">encoder_cont</span>, <span class="s20">decoder_cat</span>, and <span class="s20">decoder_cont</span>. The <span class="s20">encoder_cat </span>and <span class="s20">decoder_cat </span>keys have zero dimensions because we haven’t declared any categorical features.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Visualizing how the dataloader works</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Let’s try to unpeel what happened here one level deeper and understand what <span class="s20">TimeSeriesDataset</span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">has done visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a name="bookmark588"><span><img width="526" height="454" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_790.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.3 – TimeSeriesDataset – an illustration of how it works</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 77%;text-align: justify;">Let’s assume we have a time series, <span class="s41">𝑥𝑥</span><span class="s88">1</span><span class="s65"> </span>to <span class="s41">𝑥𝑥</span><span class="s42">6</span><span class="s43"> </span>(this would be the target as well as <span class="s20">time_varying_ unknown </span>in the <span class="s20">TimeSeriesDataset </span>terminology). We have a time-varying real, <span class="s86">𝑓𝑓</span><span class="s241">1</span><span class="s43"> </span>to <span class="s271">𝑓𝑓</span><span class="s158">6</span>, and a time-varying categorical, <span class="s248">𝑐𝑐</span><span class="s82">1</span><span class="s50"> </span>to <span class="s80">𝑐𝑐</span><span class="s82">2</span>. In addition to that, we also have a static real, <span class="s137">𝑟𝑟</span>, and a static</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">categorical, . If we chose the encoder and decoder length as <span class="s20">3</span>, we will have the tensors constructed</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">as shown in <i>Figure 15.3</i>. Notice how the static categorical and real are repeated for all timesteps. These different tensors are constructed so that the model encoder can be trained using the encoder tensors and the decoder tensors are used in the decoding process.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s proceed with building our first global model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark589">Building the first global deep learning forecasting model</a><a name="bookmark568">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="s20">PyTorch Forecasting </span>uses PyTorch and PyTorch Lightning in the backend to define and train deep learning models. The models that can be used seamlessly with <span class="s20">PyTorch Forecasting </span>are essentially PyTorch Lightning models. But the recommended approach is to inherit <span class="s20">BaseModel </span>from <span class="s20">PyTorch Forecasting</span>. The developer of <span class="s20">PyTorch Forecasting </span>has excellent documentation and tutorials to help new users use it the way they want. One tutorial worth mentioning here is titled <i>How to use custom data and implement custom models and metrics </i>(link in the <i>Further reading </i>section).</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">I have slightly modified the basic model from the tutorial to make it more flexible. The implementation can be found in <span class="s20">src/dl/ptf_models.py </span>under the name <span class="s20">SingleStepRNNModel</span>. The class takes in two parameters:</p><ul id="l141"><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">network_callable <span class="p">– This is a callable that, when initialized, becomes a PyTorch model (inheriting </span>nn.Module<span class="p">).</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">model_params <span class="p">– This is a dictionary containing all the parameters necessary to initialize </span>network_callable<span class="p">.</span></p><p style="padding-top: 8pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">The structure is pretty simple. The<u> </u><span class="s20">init </span>function initializes <span class="s20">network_callable </span>into a PyTorch model under the <span class="s20">network </span>attribute. And the <span class="s20">forward </span>function sends the input to the network, formats the returned output the way <span class="s20">PyTorch Forecasting </span>wants, and returns it. It is a very short model because the bulk of the heavy lifting is done by <span class="s20">BaseModel</span>, which handles the loss calculation, logging, gradient descent, and so on. The benefit we get by defining a model this way is that we can now define standard PyTorch models and pass it to this model to make it work well with <span class="s20">PyTorch Forecasting</span>.</p><p style="padding-top: 6pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">In addition to this, we also define an abstract class called <span class="s20">SingleStepRNN</span>, which takes in a set of parameters and initializes the corresponding network that is specified by the parameters. If the parameter specifies an LSTM, with two layers, then it will be initialized and saved under the <span class="s20">rnn </span>attribute. It also defines a fully connected layer under the <span class="s20">fc </span>attribute, which turns the output of the RNN into the prediction. The <span class="s20">forward </span>method is an abstract method that needs to be overwritten in any class subclassing this class.</p><p class="s24" style="padding-top: 10pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">Defining our first RNN model</p><p style="padding-top: 8pt;padding-left: 27pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Now that we have the necessary setup, let’s define our first model inheriting the <span class="s20">SingleStepRNN</span></p><p style="padding-left: 27pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">class we defined:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 32pt;text-indent: -24pt;line-height: 131%;text-align: left;">class SimpleRNNModel(SingleStepRNN): def<u> </u>init<u> </u>(</p><p class="s28" style="padding-left: 56pt;text-indent: 0pt;line-height: 131%;text-align: left;">self, rnn_type: str,</p><p class="s28" style="padding-left: 56pt;text-indent: 0pt;line-height: 11pt;text-align: left;">input_size: int,</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 57pt;text-indent: 0pt;line-height: 131%;text-align: left;">hidden_size: int, num_layers: int, bidirectional: bool,</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">):</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 48pt;line-height: 106%;text-align: left;">super().<u> </u>init<u> </u>(rnn_type, input_size, hidden_size, num_layers, bidirectional)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;text-align: left;">def forward(self, x: Dict):</p><p class="s38" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;"># Using the encoder continuous which has the history</p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">window</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 48pt;line-height: 106%;text-align: left;">x = x[&quot;encoder_cont&quot;] <span class="s38"># x --&gt; (batch_size, seq_len, input_size)</span></p><p class="s38" style="padding-top: 2pt;padding-left: 57pt;text-indent: 0pt;text-align: left;"># Processing through the RNN</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">x, _ = self.rnn(x) <span class="s38"># --&gt; (batch_size, seq_len, hidden_</span></p><p class="s38" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">size)</p><p class="s38" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;"># Using a FC layer on last hidden state</p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">x = self.fc(x[:,-1,:]) <span class="s38"># --&gt; (batch_size, seq_len, 1)</span></p><p class="s28" style="padding-top: 3pt;padding-left: 57pt;text-indent: 0pt;text-align: left;">return x</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark590"/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 91%;text-align: justify;">This is the most straightforward implementation. We take <span class="s20">encoder_cont </span>from the dictionary and pass it through the RNN, and then use a fully connected layer on the last hidden state from the RNN to generate the prediction. If we take the example in <i>Figure 15.3</i>, we used <span class="s157">𝑥𝑥1 </span>to <span class="s41">𝑥𝑥3 </span>as the history and trained the model to predict <span class="s41">𝑥𝑥4 </span>(because we are using <span class="s20">min_decoder_length=1</span>, there will be just one timestep in the decoder and target).</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Initializing the RNN model</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now let’s initialize the model using some parameters. I have defined two dictionaries for parameters:</p><ul id="l142"><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">model_params <span class="p">– This has all the parameters necessary for the </span>SingleStepRNN <span class="p">model to be initialized.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">other_params <span class="p">– These are all the parameters such as </span>learning_rate<span class="p">, </span>loss<span class="p">, and so on, which we pass on to </span>SingleStepRNNModel<span class="p">.</span></p></li></ul></li></ul><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now we can initialize the <span class="s20">PyTorch Forecasting </span>model using a factory method it supports – <span class="s20">from_dataset</span>. This factory method lets us pass a dataset and infer some parameters from the dataset instead of us filling everything in all the time:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: -24pt;line-height: 131%;text-align: left;">model = SingleStepRNNModel.from_dataset( training, network_callable=SimpleRNNModel,</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">model_params=model_params,</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">**other_params</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark591"/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Training the RNN model</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Training the model is just like we have been doing in previous chapters because this is a PyTorch Lightning model. We can follow these steps:</p><ol id="l143"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Initialize the trainer with early stopping and model checkpoints:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 37pt;text-indent: -24pt;line-height: 131%;text-align: left;">trainer = pl.Trainer( auto_select_gpus=True, gpus=-1,</p><p class="s28" style="padding-left: 37pt;text-indent: 0pt;line-height: 131%;text-align: left;">min_epochs=1, max_epochs=20, callbacks=[</p><p class="s28" style="padding-left: 13pt;text-indent: 48pt;line-height: 106%;text-align: left;">pl.callbacks.EarlyStopping(monitor=&quot;val_loss&quot;, patience=4*3),</p><p class="s28" style="padding-top: 2pt;padding-left: 61pt;text-indent: 0pt;text-align: left;">pl.callbacks.ModelCheckpoint(</p><p class="s28" style="padding-top: 3pt;padding-left: 13pt;text-indent: 72pt;line-height: 106%;text-align: left;">monitor=&quot;val_loss&quot;, save_last=True, mode=&quot;min&quot;, auto_insert_metric_name=True</p><p class="s28" style="padding-top: 2pt;padding-left: 61pt;text-indent: 0pt;text-align: left;">),</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">],</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 131%;text-align: left;">val_check_interval=2000, log_every_n_steps=2000,</p><p class="s28" style="padding-left: 13pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Fit the model:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">trainer.fit(</p><p class="s28" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;line-height: 131%;text-align: left;">model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,</p><p class="s28" style="padding-left: 13pt;text-indent: 0pt;line-height: 11pt;text-align: left;">)</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Load the best model after training:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 106%;text-align: left;">best_model_path = trainer.checkpoint_callback.best_model_ path</p><p class="s28" style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 106%;text-align: left;">best_model = SingleStepRNNModel.load_from_ checkpoint(best_model_path)</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark592">The training can run for some time. To save you some time, I have included the trained weights for each of the models we are using, and if the </a><span class="s20">train_model </span>flag is <span class="s20">False</span>, it will skip training and load the saved weights.<a name="bookmark569">&zwnj;</a></p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Forecasting with the trained model</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, after training, we can predict on the test dataset as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">pred, index = best_model.predict(test, return_index=True, show_ progress_bar=True)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 36pt;text-indent: 0pt;text-align: justify;">We store the predictions in a dataframe and evaluate them using our standard metrics: <span class="s5">MAE</span>, <span class="s5">MSE</span>, <span class="s5">meanMASE</span>, and <span class="s5">Forecast Bias</span>. Let’s see the results:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 90pt;text-indent: 0pt;text-align: left;"><span><img width="384" height="45" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_791.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.4 – Aggregate results using the baseline global model</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 36pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140">This is not a very good model because we know from </a>Chapter 10<span class="p">, </span>Global Forecasting Models, <span class="p">that the baseline global model using LightGBM was as follows:</span></p><ul id="l144"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">MAE = 0.079581</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">MSE = 0.027326</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">meanMASE = 1.013393</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Forecast Bias = 28.718087</p><p style="padding-top: 8pt;padding-left: 36pt;text-indent: 0pt;text-align: justify;">Apart from Forecast Bias, our global model is nowhere close to the best. Let’s refer to the <span class="s5">global machine learning model </span>as <span class="s5">GFM(ML) </span>and the current model as <span class="s5">GFM(DL) </span>for the rest of our discussion. Now, let’s start looking at some strategies to make the global model better.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 36pt;text-indent: 0pt;text-align: justify;">Using time-varying information</p><p style="padding-top: 7pt;padding-left: 36pt;text-indent: 0pt;text-align: justify;">The GFM(ML) used all the available features. So obviously, that model had access to a lot more information than the GFM(DL) we have built till now. The GFM(DL) we just built only takes in the history and nothing else. Let’s change that by including time-varying information. We will just use time-varying real features this time because dealing with categorical features is a topic I want to leave for the next section.</p><p style="padding-top: 7pt;padding-left: 36pt;text-indent: 0pt;text-align: justify;">We initialize the training dataset the same way as before, but we add <span class="s20">time_varying_known_ reals=feat_config.time_varying_known_reals </span>to the initialization parameters. Now that we have all the datasets created, let’s move on to setting up the model.</p><p class="s37" style="padding-top: 4pt;padding-left: 306pt;text-indent: 0pt;text-align: left;">Using time-varying information 395</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_792.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">To set up the model, we need to understand one concept. We are now using the history of the target and time-varying known features. In <i>Figure 15.3</i>, we saw how <span class="s20">TimeSeriesDataset </span>arranges the different kinds of variables in PyTorch tensors. In the previous section, we used only <span class="s20">encoder_cont </span>because there were no other variables to worry about. But now, we have time-varying variables along with it, which brings an added complication. If we take a step back and think about it, in the single- step-ahead forecasting context, we can see that the time-varying variables and the history of the target cannot be of the same timestep. Let’s use a visual example to elucidate:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><span><img width="434" height="414" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_793.jpg"/></span></p><p class="s37" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.5 – Using time-varying variables for training</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 91%;text-align: justify;">Following the same spirit of the example from <i>Figure 15.3</i>, but reducing it to fit our context here, we have a time series, <span class="s41">𝑥𝑥1 </span>to <span class="s137">𝑥𝑥</span><span class="s42">4</span>, and time-varying real variable, <span class="s137">𝑓𝑓1 </span>to <span class="s137">𝑓𝑓4</span>. So, for <span class="s20">max_encoder_length=3 </span>and <span class="s20">min_decoder_length=1</span>, we would have <span class="s20">TimeSeriesDataset </span>make the tensors as shown in <i>Step 1 </i>in <i>Figure 15.5</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 90%;text-align: justify;">Now, for each timestep, we have the time-varying variable, <span class="s74">𝑓𝑓</span>, and the history, <span class="s137">𝑥𝑥</span>, in <span class="s20">encoder_cont</span>. The time-varying variable, <span class="s109">𝑓𝑓</span>, is a variable for which we know the future values as well and therefore, there is no causal constraint on that variable. That means that for predicting the timestep, <span class="s41">𝑡𝑡</span>, we can use <span class="s137">𝑓𝑓𝑡𝑡 </span>because it is known. But the history of the target variable is not. We do not know the future because it is the very quantity we are trying to forecast. That means that there is a causal constraint on <span class="s137">𝑥𝑥 </span>and, because of this, we cannot use <span class="s41">𝑥𝑥𝑡𝑡 </span>to predict timestep <span class="s157">𝑡𝑡</span>. But the way the tensors are formed right now, we have <span class="s74">𝑓𝑓 </span>and <span class="s137">𝑥𝑥 </span>aligned on timesteps and if we passed them through a model, we would be essentially cheating because we would be using <span class="s41">𝑥𝑥𝑡𝑡 </span>to predict timestep <span class="s80">𝑡𝑡</span>. Ideally, there should be an offset between the history, <span class="s137">𝑥𝑥</span>, and the time-varying feature, <span class="s74">𝑓𝑓</span>, such that at timestep <span class="s41">𝑡𝑡</span>, the model sees <span class="s41">𝑥𝑥𝑡𝑡−1 </span>and then sees <span class="s137">𝑓𝑓𝑡𝑡 </span>and then predicts <span class="s41">𝑥𝑥</span><span class="s42">𝑡𝑡</span>.</p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">To achieve that, we do the following:</p><ol id="l145"><li><p style="padding-top: 9pt;padding-left: 64pt;text-indent: -18pt;line-height: 87%;text-align: left;">Concatenate <span class="s20">encoder_cont </span>and <span class="s20">decoder_cont </span>because we need to use <span class="s137">𝑓𝑓</span><span class="s42">4</span><span class="s43"> </span>to predict timestep <span class="s137">𝑡𝑡 = 4 </span>(<i>Step 2 </i>in <i>Figure 15.5</i>).</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;line-height: 14pt;text-align: left;">Shift the target history, <span class="s137">𝑥𝑥</span>, forward by one timestep so that <span class="s137">𝑓𝑓</span><span class="s42">𝑡𝑡 </span>and <span class="s41">𝑥𝑥𝑡𝑡−1</span>are aligned (<i>Step 3 </i>in</p><p class="s4" style="padding-left: 64pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Figure 15.5<span class="p">).</span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;line-height: 13pt;text-align: left;">Drop the first timestep because we don’t have the history to go with the first timestep (<i>Step 4</i></p></li></ol><p style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: left;">in <i>Figure 15.5</i>).</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">This is exactly what we need to implement in our <span class="s20">forward </span>method in the new model we defined,</p><p class="s20" style="padding-left: 36pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">DynamicFeatureRNNModel<span class="p">, as well:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Def forward(self, x: Dict):</p><p class="s38" style="padding-top: 3pt;text-indent: 0pt;text-align: right;"># Step 2 in Figure 15.5</p><p class="s28" style="padding-top: 3pt;padding-left: 8pt;text-indent: 24pt;line-height: 106%;text-align: left;">x_cont = torch.cat([x[&quot;encoder_cont&quot;],x[&quot;decoder_cont&quot;]], dim=1)</p><p class="s38" style="padding-top: 2pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># Step 3 in Figure 15.5</p><p class="s28" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;line-height: 131%;text-align: left;">x_cont[:,:,-1] = torch.roll(x_cont[:,:,-1], 1, dims=1) x = x_cont</p><p class="s38" style="padding-left: 32pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># Step 4 in Figure 15.5</p><p class="s28" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;line-height: 131%;text-align: left;">x = x[:,1:,:] <span class="s38"># x -&gt; (batch_size, seq_len, input_size) # Processing through the RNN</span></p><p class="s28" style="padding-left: 8pt;text-indent: 24pt;line-height: 106%;text-align: left;">x, _ = self.rnn(x) <span class="s38"># --&gt; (batch_size, seq_len, hidden_ size)</span></p><p class="s38" style="padding-top: 2pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"># Using a FC layer on last hidden state</p><p class="s28" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;">x = self.fc(x[:,-1,:]) <span class="s38"># --&gt; (batch_size, seq_len, 1)</span></p><p class="s28" style="padding-top: 3pt;padding-left: 32pt;text-indent: 0pt;text-align: left;">return x</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 36pt;text-indent: 0pt;text-align: left;">Now, let’s train this new model and see how it performs. The exact code is in the notebook and is exactly the same as before:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;text-align: left;"><a name="bookmark593"><span><img width="377" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_794.jpg"/></span></a><a name="bookmark571">&zwnj;</a><a name="bookmark570">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.6 – Aggregate results using the time-varying features</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">It looks like having temperature as a feature did make the model slightly better, but there&#39;s still a long way to go. Not to worry, we have other features to use.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Using static/meta information</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140">There are some features such as the Acorn group, whether dynamic pricing is enabled, and so on, that are specific to a household, which will help the model learn patterns specific to these groups. Naturally, including that information makes intuitive sense. But as we discussed in </a><i>Chapter 10</i>, <i>Global Forecasting Models</i>, categorical features do not play well with machine learning models because they aren’t numerical. In that chapter, we discussed a few ways of encoding categorical features into numerical representations. We can use any of those in a deep learning model as well. But there is one way of handling categorical features that is unique to deep learning models – <span class="s5">embedding vectors</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">One-hot encoding and why it is not ideal</p><p class="s4" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140">One of the ways of converting categorical features to numerical representation is one-hot encoding. It encodes the categorical features in a higher dimension, placing the categorical values equally distant in that space. The size of the dimension it requires to encode the categorical values is equal to the cardinality of the categorical variable. For a more detailed discussion on one-hot encoding, refer to </a>Chapter 10<span class="p">, </span>Global Forecasting Models<span class="p">.</span></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The representation that we would get after one-hot encoding of a categorical feature is what we call a <span class="s5">sparse representation</span>. If the cardinality of the categorical feature (number of unique values) is <span class="s90">𝐶𝐶</span>, each row representing a value of the categorical feature would have <span class="s146">𝐶𝐶 − 1 </span>zeros. So, the representation is predominantly zeros and hence is called a sparse representation. This causes the overall dimension required to effectively encode a categorical feature to be equal to the cardinality of the vector. Therefore, one-hot encoding of a categorical feature with 5,000 unique values instantly adds 5,000 dimensions to the problem you are solving.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In addition to that, one-hot encoding is also completely uninformed. It places each categorical value equidistant from each other without any regard for the possible similarity between those values. For instance, if we are encoding the days in a week, one-hot encoding would place each day in a completely different dimension, making them equidistant from each other. But if we think about it, Saturday and Sunday should be closer together than the other weekdays on account of them being the weekend, right? This kind of information is not captured through one-hot encoding.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark594">Embedding vectors and dense representations</a><a name="bookmark573">&zwnj;</a><a name="bookmark572">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">An embedding vector is a similar representation, but instead of a sparse representation, it strives to give us a dense representation of a categorical feature. We can achieve this by using an embedding layer. The embedding layer can be thought of as a mapping between each categorical value and a numerical vector, and this vector can have a much lower dimension than the cardinality of the categorical feature. The only question that remains is “<i>How do we know what vector to choose for each categorical value?</i>”</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The good news is that we need not because the embedding layer is trained along with the rest of the network. So, while training a model for some task, the model itself figures out what the best vector representation is for each categorical value. This approach is really popular in natural language processing, where thousands of words are embedded into dimensions as small as 200 or 300. In PyTorch, we can accomplish this by using <span class="s20">nn.Embedding</span>, which is a module that is a simple lookup table that stores the embeddings of fixed discrete values and size. There are two mandatory parameters while initializing:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">num_embeddings <span class="p">– This is the size of the dictionary of embeddings. In other words, this is the cardinality of the categorical feature.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">embedding_dim <span class="p">– This is the size of each embedding vector.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s come back to global modeling. Let’s first introduce the static categorical features. Please note that we are also including the time-varying categorical because now we know how to deal with categorical features in a deep learning model. The code to initialize the dataset is the same, with the addition of the following two parameters to the initialization:</p></li><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">static_categoricals=feat_config.static_categoricals</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;line-height: 111%;text-align: left;">time_varying_known_categoricals=feat_config.time_varying_known_ categoricals</p></li></ul></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Defining a model with categorical features</p><p style="padding-top: 8pt;padding-left: 36pt;text-indent: 0pt;text-align: justify;">Now that we have the datasets, let’s look at how we can define the<u> </u><span class="s20">init </span>function in our new model, <span class="s20">StaticDynamicFeatureRNNModel</span>. In addition to invoking the parent model, which sets up the standard RNN and fully connected layer, we also set up the embedding layers using an input, <span class="s20">embedding_sizes</span>. <span class="s20">embedding_sizes </span>is a list of tuples (<i>cardinality and embedding size</i>) for each categorical feature:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 32pt;text-indent: -24pt;line-height: 131%;text-align: left;">def <u>  </u>init<u>  </u>( self, rnn_type: str,</p><p class="s28" style="padding-left: 32pt;text-indent: 0pt;line-height: 131%;text-align: left;">input_size: int, hidden_size: int,</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">num_layers: int, bidirectional: bool, embedding_sizes = []</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">):</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">super().<u> </u>init<u> </u>(rnn_type, input_size, hidden_size, num_ layers, bidirectional)</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">self.embeddings = torch.nn.ModuleList(</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 48pt;line-height: 106%;text-align: left;">[torch.nn.Embedding(card, size) for card, size in embedding_sizes]</p><p class="s28" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We used <span class="s20">nn.ModuleList </span>to store a list of <span class="s20">nn.Embedding </span>modules, one for each categorical feature. While initializing this model, we will need to give <span class="s20">embedding_sizes </span>as input. The embedding size required for each categorical feature is technically a hyperparameter that we can tune. But there are a few rules of thumb to get you started. The idea behind these thumb rules is that the bigger the cardinality of the categorical feature, the larger the embedding size required to encode the information in them. And also, the embedding size can be much smaller than the cardinality of the categorical feature. The rule of thumb that we have adopted is as follows:</p><p class="s41" style="padding-top: 7pt;padding-left: 173pt;text-indent: 0pt;line-height: 11pt;text-align: center;">𝑐𝑐  + 1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="35" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_795.png"/></span></p><p class="s41" style="padding-top: 2pt;padding-left: 259pt;text-indent: -95pt;line-height: 59%;text-align: left;">min (50, 𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟 ( )) 2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Therefore, we create the <span class="s20">embedding_sizes </span>list of tuples using the following code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Finding the cardinality using the categorical encoders in the dataset</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">cardinality = [len(training.categorical_encoders[c].classes_) for c in training.categoricals]</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># using the cardinality list to create embedding sizes</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">embedding_sizes = [</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">(x, min(50, (x + 1) // 2)) for x in cardinality</p><p class="s28" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">]</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And now, turning our attention toward the <span class="s20">forward </span>method, it is going to be similar to the previous model, but with an additional part to handle the categorical features. We essentially use the embedding layers to convert the categorical features into embeddings and concatenate them with the continuous features:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">def forward(self, x: Dict):</p><p class="s38" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"># Using the encoder and decoder sequence</p><p class="s28" style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">x_cont = torch.cat([x[&quot;encoder_cont&quot;],x[&quot;decoder_cont&quot;]],</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="528" height="435" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_796.png"/></span></p><p class="s28" style="padding-top: 5pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a name="bookmark595">dim=1)</a></p><p class="s38" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;"># Roll target by 1</p><p class="s28" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">x_cont[:,:,-1] = torch.roll(x_cont[:,:,-1], 1, dims=1)</p><p class="s38" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;"># Combine the encoder and decoder categoricals</p><p class="s28" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">cat = torch.cat([x[&quot;encoder_cat&quot;],x[&quot;decoder_cat&quot;]], dim=1)</p><p class="s38" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;"># if there are categorical features</p><p class="s28" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">if cat.size(-1)&gt;0:</p><p class="s38" style="padding-top: 3pt;padding-left: 94pt;text-indent: 0pt;text-align: left;"># concatenating all the embedding vectors</p><p class="s28" style="padding-top: 3pt;padding-left: 46pt;text-indent: 48pt;line-height: 106%;text-align: left;">x_cat = torch.cat([emb(cat[:,:,i]) for i, emb in enumerate(self.embeddings)], dim=-1)</p><p class="s38" style="padding-top: 2pt;padding-left: 94pt;text-indent: 0pt;text-align: left;"># concatenating continuous and categorical</p><p class="s28" style="padding-top: 3pt;padding-left: 70pt;text-indent: 24pt;line-height: 131%;text-align: left;">x = torch.cat([x_cont, x_cat], dim=-1) else:</p><p class="s28" style="padding-left: 94pt;text-indent: 0pt;line-height: 11pt;text-align: left;">x = x_cont</p><p class="s38" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;"># dropping first timestep</p><p class="s28" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;line-height: 131%;text-align: left;">x = x[:,1:,:] <span class="s38"># x --&gt; (batch_size, seq_len, input_size) # Processing through the RNN</span></p><p class="s28" style="padding-left: 46pt;text-indent: 24pt;line-height: 106%;text-align: left;">x, _ = self.rnn(x) <span class="s38"># --&gt; (batch_size, seq_len, hidden_ size)</span></p><p class="s38" style="padding-top: 2pt;padding-left: 70pt;text-indent: 0pt;text-align: left;"># Using a FC layer on last hidden state</p><p class="s28" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">x = self.fc(x[:,-1,:]) <span class="s38"># --&gt; (batch_size, seq_len, 1)</span></p><p class="s28" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">return x</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, let’s train this new model with static features and see how it performs:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 77pt;text-indent: 0pt;text-align: left;"><span><img width="414" height="103" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_797.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 98pt;text-indent: 0pt;text-align: left;">Figure 15.7 – Aggregate results using the static and time-varying features</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Adding the static variables also improved our model. Now let’s look at another strategy.</p><p class="s37" style="padding-top: 4pt;padding-left: 301pt;text-indent: 0pt;text-align: left;"><a name="bookmark596">Using the scale of the time series 401</a><a name="bookmark574">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_798.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Using the scale of the time series</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We used <span class="s20">GroupNormlizer </span>in <span class="s20">TimeSeriesDataset </span>to scale each household using its own mean and standard deviation. We did this because we wanted to make the target zero mean and unit variance so that the model does not waste effort trying to change its parameters to capture the scale of individual household consumption. Although this is a good strategy, we do have some information loss here. There may be patterns that are specific to households whose consumption is on the larger side and some other patterns that are specific to households that consume much less. But now, they are both lumped in together and the model tries to learn common patterns. In such a scenario, these unique patterns seem like noise to the model because there is no variable to explain those.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The bottom line is that there is information in the scale that we removed, and adding that information back would be beneficial. So, how do we add it back? Definitely not by including the unscaled targets, which brings back the disadvantage that we were trying to get away from in the first place. A way to do it is to add the scale information as static-real features to the model. We would have kept track of the mean and standard deviation of each household when we scaled them in the first place (because we need them to do the inverse transformation and get back the original targets). All we need to do is make sure we include them as a static real variable so that the model has access to the scale information while learning the patterns in the time series dataset.</p><p class="s20" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">PyTorch Forecasting <span class="p">makes this easier for us by having a handy parameter in </span>TimeSeriesDataset <span class="p">called </span>add_target_scales<span class="p">. If you make it </span>True<span class="p">, then </span>encoder_cont <span class="p">and </span>decoder_cont <span class="p">will also have the mean and standard deviation of individual time series as.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Nothing changes in our existing model; all we need to do is add this parameter to <span class="s20">TimeSeriesDataset</span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">while initializing it and train and predict using the model. Let’s see how that worked out for us:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 70pt;text-indent: 0pt;text-align: left;"><span><img width="419" height="122" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_799.jpg"/></span></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.8 – Aggregate results using the static, time-varying, and scale features</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The scale information has improved the model yet again. And with that, let’s look at one of the last strategies we will be covering in this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark597">Balancing the sampling procedure</a><a name="bookmark575">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We saw a few strategies for improving a global deep learning model by adding new types of features. Now, let’s look at a different aspect that is relevant in a global modeling context. In an earlier section, when we were talking about global deep learning models, we talked about how the process by which we sample a window of sequence to feed to our model can be thought of as a two-step process:</p><ol id="l146"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Sampling a time series out of a set of time series</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Sampling a window out of that time series</p></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Let’s use an analogy to make the concept clearer. Imagine we have a large bowl that we have filled with</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 87%;text-align: justify;"><span class="s137">𝑁𝑁 </span>balls. Each ball in the bowl represents a time series in the dataset (a household in our dataset). Now, each ball, <span class="s157">𝑖𝑖</span>, has <span class="s41">𝑀𝑀</span><span class="s42">𝑖𝑖 </span>chits of paper representing all the different windows of samples we can draw from it.</p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In the batch sampling we use by default, we open all the balls and dump all the chits into the bowl and discard the balls. Now, with our eyes closed, we pick <span class="s90">𝐵𝐵 </span>chits out of this bowl and set them aside. This is a batch that we sample from our dataset. We do not have any information that separates the chits from each other and so the probability of picking any chit is equal, which can be formulated as:</p><p style="text-indent: 0pt;text-align: left;"><span><img width="58" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_800.png"/></span></p><p class="s242" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: center;">1</p><p class="s242" style="text-indent: 0pt;line-height: 14pt;text-align: left;">∑</p><p style="text-indent: 0pt;text-align: left;"/><p class="s146" style="padding-top: 2pt;padding-left: 223pt;text-indent: 0pt;line-height: 10pt;text-align: left;">𝑁𝑁</p><p class="s146" style="padding-left: 223pt;text-indent: 0pt;line-height: 10pt;text-align: left;">𝑖𝑖=0</p><p class="s242" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">𝑀𝑀<span class="s247">𝑖𝑖</span></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s add something to our analogy to the data. We know that we have different kinds of time series – different lengths, different levels of consumption, and so on. Let’s pick one aspect, the length of the series, for our example (although it applies to other aspects as well). So, if we discretize the length of our time series, we end up with different bins; let’s assign a color for each bin. So, now we have <i>C </i>different colored balls in the bowl and the chits of paper also are colored accordingly.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In our current sampling strategy (where we dump all the chits of paper, now colored, and pick <i>B </i>chits at random), we would end up replicating the probability distribution of our bowl in a batch. It is not a stretch to understand that if the bowl has more of the longer time series than shorter ones, the chits we draw will also have that bias. And consequently, the batch will also be biased toward a long time series. What happens because of that?</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In mini-batch stochastic gradient descent (we saw this in <a href="#bookmark414" class="s21">Ancho</a><i>Chapter 11</i>, <i>Introduction to Deep Learning</i>), we do a gradient update every mini-batch, and we use this gradient update to move closer to our minima in the optimization process. Therefore, if a mini-batch is biased toward a particular type of case, then the gradient updates would be biased toward a solution that works better for them. There are good parallels to be drawn here to imbalanced learning. Longer time series and shorter time series may have different patterns, and having this sampling imbalance causes the model to learn patterns that work well for the longer time series and not so well for the shorter ones.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark598">Visualizing the data distribution</a><a name="bookmark576">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">We calculated the length of each household (<span class="s20">LCLid</span>) and binned them into <span class="s20">10 </span>bins – <span class="s20">bin_0 </span>for the shortest bin and <span class="s20">bin_9 </span>for the longest bin:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">n_bins= 10</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Calculating the length of each LCLid</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">counts = train_df.groupby(&quot;LCLid&quot;)[&#39;timestamp&#39;].count()</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Binning the counts and renaming</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">out, bins = pd.cut(counts, bins=n_bins, retbins=True) out = out.cat.rename_categories({</p><p class="s28" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: left;">c:f&quot;bin_{i}&quot; for i, c in enumerate(out.cat.categories)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">})</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s visualize the distribution of the bins in the original data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="516" height="252" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_801.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.9 – Distribution of length of time series</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: justify;">We can see that <span class="s20">bin_5 </span>and <span class="s20">bin_6 </span>are the most common lengths while <span class="s20">bin_0 </span>is the least common. Now, let’s get the first 50 batches from the dataloader and plot them as a stacked bar chart to check the distribution in each batch:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><a name="bookmark599"><span><img width="510" height="209" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_802.gif"/></span></a><a name="bookmark577">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.10 – Stacked bar chart of batch distribution</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see that the same distribution you saw in <i>Figure 15.9 </i>is replicated in the batch distributions as well with <span class="s20">bin_5 </span>and <span class="s20">bin_6 </span>leading the pack. <span class="s20">bin_0 </span>is barely making an appearance and LCLids that are in <span class="s20">bin_0 </span>would not have been learned that well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Tweaking the sampling procedure</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, what do we do? Let’s step into the analogy of bowls with chits inside for a bit. We were picking a ball at random, and we saw that the resulting distribution is identical to the original distribution of colors. Therefore, to get a more balanced distribution of colors in a batch, we need to sample different colored chits at different probabilities. In other words, we should be sampling more from colors that have low representation in the original distribution and less from colors that dominate the original representation.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s look at the process by which we are selecting the chits from the bowl from another perspective. We know that the probability of selecting each chit in the bowl is equal. So, another way to select chits from the bowl is by using a uniform random number generator. We pick a chit from the bowl, generate a random number between 0 and 1 (<span class="s137">𝑝𝑝</span>), and select the chit if the random number is less than 0.5 (<span class="s65">𝑝𝑝 &lt; 0.5</span>). So, it is equally likely that we select or reject the chit. We continue this until we get <span class="s90">𝐵𝐵 </span>samples. Although a bit more inefficient than the previous procedure, this sampling process approximates the original procedure closely. The advantage here is that we have a threshold now with which we can tweak our sampling to suit our needs. Having a lower threshold makes the chit harder to accept under this sampling procedure, and having a higher threshold makes it easier to accept.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that we have a threshold with which we can tweak the sampling procedure, all we need to do is find out the right thresholds for each of the chits so that the resulting batch has a uniform representation of all the colors. In other words, we need to find and assign the right weight to each LCLid such that the resulting batch will have an even distribution of all length bins.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_803.png"/></span></p><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝐶𝐶</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark600">How do we do that? There is a very simple strategy for that. We want the weights to be lower for length bins that have a lot of samples, and higher for length bins that have fewer samples. We can get this kind of weight by taking the inverse of the count of each bin. If there are </a><span class="s137">𝐶𝐶 </span>LCLids in a bin, the weight of the bin can be <span class="s74">1</span>. The <i>Further reading </i>section has a link where you can read more about weighted random sampling and the different algorithms used for the purpose.</p><p class="s20" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">TimeSeriesDataset <span class="p">has an internal index, which is a dataframe with all the samples it can draw from the dataset. We can use that to construct our array of weights:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># TimeSeriesDataset stores a df as the index over which it samples</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">df = training.index.copy()</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Adding a bin column to it to represent the bins we have created</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">df[&#39;bins&#39;] = [f&quot;bin_{i}&quot; for i in np.digitize(df[&quot;count&quot;]. values, bins)]</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Calculate Weights as inverse counts of the bins</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">weights = 1/df[&#39;bins&#39;].value_counts(normalize=True)</p><p class="s38" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># Assigning the weights back to the df so that we have an array of</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;"># weights in the same shape as the index over which we are going to sample</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">weights = weights.reset_index().rename(columns={&quot;index&quot;:&quot;bins&quot;, &quot;bins&quot;:&quot;weight&quot;})</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">df = df.merge(weights, on=&#39;bins&#39;, how=&#39;left&#39;) probabilities = df.weight.values</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This way ensures that the <span class="s20">probabilities </span>array has the same length as the internal index over which <span class="s20">TimeSeriesDataset </span>samples, and that is a mandatory requirement when using this technique – each possible window should have a corresponding weight attached to it.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Now that we have this weight, there is an easy way to put this into practice. We can use</p><p class="s20" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">WeightedRandomSampler <span class="p">from PyTorch, which has been created specifically for this purpose:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">from torch.utils.data import WeightedRandomSampler</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">sampler = WeightedRandomSampler(probabilities, len(probabilities))</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark601">Using and visualizing the dataloader with WeightedRandomSampler</a><a name="bookmark578">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, we can use this sampler in the dataloaders we create from <span class="s20">TimeSeriesDataset</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">train_dataloader = training.to_dataloader(train=True, batch_ size=batch_size, num_workers=0, sampler=sampler)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s visualize the first 50 batches like before and see the difference:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span><img width="520" height="213" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_804.gif"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 15.11 – Stacked bar chart of batch distribution with weighted random sampling</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Now we can see a more uniform distribution of bins in each batch. Let’s also see the results after training the model using this new data loader:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span><img width="483" height="174" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_805.jpg"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 118pt;text-indent: 0pt;line-height: 127%;text-align: center;">Figure 15.12 – Aggregate results using the static, time-varying, and scale features along with batch samplers</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark602">Summary 407</a><a name="bookmark580">&zwnj;</a><a name="bookmark579">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_806.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Looks like the sampler also made a good improvement in the model in all metrics, except <span class="s20">Forecast Bias</span>. Although we have not achieved better results than the GFM(ML) (which had an MAE of 0.079581), we are close enough. Maybe with some hyperparameter tuning, partitioning, or stronger models, we might reach closer to that number, or we may not. We used a custom sampling option to make the length of the time series balanced in a batch. We can use the same techniques to balance it on other aspects such as the level of consumption, region, or any other aspect that seems relevant. As always in machine learning, we will need to go with our experiments to say anything for sure, and all we need to do is form our hypothesis about the problem statement and construct experiments to validate those hypotheses.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And with that, we have come to the end of yet another practical-heavy (and compute-heavy) chapter. Congratulations on making it through the chapter; feel free to go back and refer to any points that didn’t quite land yet.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">After having built a strong foundation on deep learning models in the last few chapters, we started to look at a new paradigm of global models in the context of deep learning models. We learned how to use <span class="s20">PyTorch Forecasting</span>, an open source library for forecasting using deep learning, and used the feature-filled <span class="s20">TimeSeriesDataset </span>to start developing our own models.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We started off with a very simple LSTM in the global context and saw how we can add time-varying information, static information, and the scale of individual time series to the features to make models better. We closed by looking at an alternating sampling procedure for mini-batches that helps us present a more balanced view of the problem in each batch. This chapter is by no means an exhaustive list of all such techniques to make the forecasting models better. Instead, this chapter aims to build the right kind of thinking that is necessary to work on your own models and make them work better than before.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And now that we have a strong foundation in deep learning and global models, it is time to take a look at a few specialized deep learning architectures that have been proposed over the years for time series forecasting in the next chapter.</p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You can check out the following sources for further reading:</p><ul id="l147"><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">How to use custom data and implement custom models and metrics <a href="https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/building.html" class="s140" target="_blank">(PyTorch Forecasting): </a><a href="https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/building.html" class="a" target="_blank">https:// pytorch-forecasting.readthedocs.io/en/stable/tutorials/building. </a><a href="https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/building.html" target="_blank">html</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: justify;">Random Sampling from Databases <a href="https://dsf.berkeley.edu/papers/UCB-PhD-olken.pdf" class="s140" target="_blank">by Frank Olken, Page 22-23: </a><a href="https://dsf.berkeley.edu/papers/UCB-PhD-olken.pdf" class="a" target="_blank">https://dsf.berkeley. </a><a href="https://dsf.berkeley.edu/papers/UCB-PhD-olken.pdf" target="_blank">edu/papers/UCB-PhD-olken.pdf</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark603">16</a><a name="bookmark604">&zwnj;</a><a name="bookmark605">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 60pt;text-indent: 33pt;line-height: 114%;text-align: left;">Specialized Deep Learning Architectures for Forecasting</h4><p style="padding-top: 24pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Our journey through the world of <span class="s5">deep learning </span>(<span class="s5">DL</span>) is coming to an end. In the previous chapter, we were introduced to the global paradigm of forecasting and saw how we can make a simple model such as a <span class="s5">Recurrent Neural Network </span>(<span class="s5">RNN</span>) perform close to the high benchmark set by global machine learning models. In this chapter, we are going to review a few popular DL architectures that were designed specifically for time series forecasting. With these more sophisticated model architectures, we will be better equipped at handling problems in the wild that call for more powerful models than vanilla RNNs and LSTMs.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The need for specialized architectures</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with Exogenous Variables (N-BEATSx)</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Informer</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Autoformer</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Temporal Fusion Transformer (TFT)</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Interpretability</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Probabilistic forecasting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark636">Technical requirements</a><a name="bookmark607">&zwnj;</a><a name="bookmark606">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">You will need to set up an Anaconda environment by following the instructions in the <i>Preface </i>to get a working environment with all the packages and datasets required for the code in this book.</p><p class="s27" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter16" class="s140" target="_blank">The code associated with this chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter16" class="a" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/ </a>Chapter16<span class="p">.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">You will need to run the following notebooks for this chapter:</p><ul id="l148"><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">02-Preprocessing London Smart Meter Dataset.ipynb <span class="p">in </span>Chapter02</p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">01-Setting up Experiment Harness.ipynb <span class="p">in </span>Chapter04</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">01-Feature Engineering.ipynb <span class="p">in </span>Chapter06</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The need for specialized architectures</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Inductive bias, or learning bias, refers to a set of assumptions a learning algorithm makes to generalize the function it learns on training data to unseen data. Deep learning is thought to be a completely data- driven approach where the feature engineering and final task are learned end-to-end, thus avoiding the inductive bias that the modelers bake in while designing the features. But that view is not entirely correct. These inductive biases, which used to be put in through the features, now make their way through the design of architecture. Every DL architecture has its own inductive biases, which is why some types of models perform better on some types of data. For instance, a <span class="s5">Convolutional Neural Network </span>(<span class="s5">CNN</span>) works well on images, but not as much on sequences because the spatial inductive bias and translational equivariance that the CNN brings to the table are most effective on images.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In an ideal world, we would have an infinite supply of good, annotated data and we would be able to learn entirely data-driven networks with no strong inductive biases. But sadly, in the real world, we will never have enough data to learn such complex functions. This is where designing the right kind of inductive biases makes or breaks the DL system. We used to heavily rely on RNNs for sequences and they had a strong auto-regressive inductive bias baked into them. But later, Transformers, which have a much weaker inductive bias for sequences, came in and with large amounts of data, they were able to learn better functions for sequences. Therefore, this decision about how strong an inductive bias we bake into models is an important question in designing DL architectures.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Over the years, many DL architectures have been proposed specifically for time series forecasting and each of them has its own inductive biases attached to it. We’ll not be able to review every single one of those models, but we will cover the major ones that made a lasting impact on the field. We will also look at how we can use a few open source libraries to train those models on our data. We will exclusively focus on models that can handle the global modeling paradigm, directly or indirectly. This is because of the infeasibility of training separate models for each time series when we are forecasting at scale.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark637">We are going to look at a few popular architectures developed for time series forecasting. One of the major factors influencing the inclusion of a model is also the availability of stable open source frameworks that support these models. This is in no way a complete list because there are many architectures we are not covering here. I’ll try and share a few links in the </a><i>Further reading </i>section to get you started on your journey of exploration.<a name="bookmark609">&zwnj;</a><a name="bookmark608">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, without further ado, let’s get started on the first model on the list.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;line-height: 119%;text-align: left;">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)</p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The first model that used some components from DL (we can’t call it DL because it is essentially a mix of DL and classical statistics) and made a splash in the field was a model that won the M4 competition (univariate) in 2018. This was a model by <i>Slawek Smyl </i>from Uber (at the time) and was a Frankenstein-style mix of exponential smoothing and an RNN, dubbed <span class="s5">ES-RNN </span>(<i>Further reading </i>has links to a newer and faster implementation of the model that uses GPU acceleration). This led to Makridakis et al. putting forward an argument that “<i>hybrid approaches and combinations of methods are the way forward</i>.” The creators of the <span class="s5">N-BEATS </span>model aspired to challenge this conclusion by designing a pure DL architecture for time series forecasting. They succeeded in this when they created a model that beat all other methods in the M4 competition (although they didn’t publish it in time to participate in the competition). It is a very unique architecture, taking a lot of inspiration from signal processing. Let’s take a deeper look and understand the architecture.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="74" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_807.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">The research paper by Makridakis et al. and the blog post by Slawek Smyl are cited in the</p><p class="s4" style="padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: left;">References <span class="p">section as </span>1 <span class="p">and </span>2<span class="p">, respectively.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We need to establish a bit of context and terminology before moving ahead with the explanation. The core problem that they are solving is univariate forecasting, which means it is similar to classical methods such as exponential smoothing and ARIMA in the sense that it takes only the history of the time series to generate a forecast. There is no provision to include other covariates in the model. The model is shown a window from the history and is asked to predict the next few timesteps. The window of history is referred to as the <span class="s5">lookback period </span>and the future timesteps are the <span class="s5">forecast period</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The architecture of N-BEATS</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The N-BEATS architecture is different from the existing architectures (at the time) in a few aspects:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Instead of the common encoder-decoder (or sequence-to-sequence) formulation, N-BEATS formulates the problem as a multivariate regression problem.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l149"><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a name="bookmark638">Most of the other architectures at the time were relatively shallow (~5 LSTM layers). However, N-BEATS used the residual principle to stack many basic blocks (we will explain this shortly) and the paper has shown that we can stack up to 150 layers and still facilitate efficient learning.</a></p></li><li><p style="padding-top: 5pt;padding-bottom: 3pt;padding-left: 37pt;text-indent: 13pt;line-height: 162%;text-align: justify;">The model lets us extend it to human interpretable output, still in a principled way. Let’s look at the architecture and go deeper:</p></li></ul></li></ul><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span><img width="526" height="339" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_808.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 16.1 – N-BEATS architecture</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can see three columns of <i>rectangular blocks</i>, each one an exploded view of another. Let’s start at the left most (which is the most granular view) and then go up step by step, building up to the architecture. At the top, there is a representative time series, which has a lookback window and a forecast period.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Blocks</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;">The fundamental learning unit in N-BEATS is a <span class="s5">block</span>. Each block, <span class="s90">𝑙𝑙</span>, takes in an input, (<span class="s41">𝑥𝑥𝑙𝑙</span>), of the size of the lookback period and generates two outputs: a forecast, (<span class="s137">𝑦𝑦̂𝑙𝑙</span>), and a backcast, (<span class="s41">𝑥𝑥̂</span><span class="s42">𝑙</span><span class="s43">𝑙</span>). The backcast is the block’s own best prediction of the lookback period. It is synonymous with fitted values in the classical sense; they tell us how the stack would have predicted the lookback window using the function it has learned. The block input is first processed by a stack of four standard fully connected layers (complete with a bias term and non-linear activation), transforming the input into a hidden representation, <span class="s157">ℎ</span><span class="s88">𝑙𝑙</span>. Now, this hidden representation is transformed by two separate linear layers (no</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a name="bookmark639">bias or non-linear a</a><span class="s272">𝑓𝑓</span>ctivation) to something the paper calls expansion coefficients for the backcast and</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 8pt;text-align: left;">forecast, <span class="s157">𝜃𝜃𝑏𝑏 </span>and <span class="s259">𝜃𝜃 </span>, respectively. The last part of the block takes these expansion coefficients and</p><p class="s65" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑙𝑙</p><p class="s65" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑙𝑙</p><p style="padding-top: 5pt;padding-left: 70pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span class="s157">𝑔𝑔𝑏𝑏 </span>and <span class="s41">𝑔𝑔𝑓𝑓</span>). We will talk about the basis layers</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">maps them to the output using a set of basis layers (</p><p class="s65" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝑙𝑙</p><p class="s43" style="padding-top: 5pt;padding-left: 27pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑙𝑙</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 89%;text-align: justify;">in a bit more detail later, but for now, just understand that they take the expansion coefficients and transform them into the desired outputs (<span class="s90">𝑦𝑦̂𝑙𝑙 </span>and <span class="s41">𝑥𝑥̂𝑙𝑙</span>).</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Stacks</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 88%;text-align: justify;">Now, let’s move one layer up the abstraction to the middle column of <i>Figure 16.1</i>. It shows how different blocks are arranged in a <span class="s5">stack</span>, <span class="s41">𝑠𝑠</span>. All the blocks in a stack share the same kind of basis layers and therefore are grouped as a stack. As we saw earlier, each block has two outputs, <span class="s41">𝑦𝑦̂</span><span class="s42">𝑙𝑙</span><span class="s43"> </span>and <span class="s150">𝑥𝑥</span><span class="s41">̂𝑙𝑙</span>. The blocks are arranged in a residual manner, each block processing and cleaning the time series step by step. The input to a block, <span class="s248">𝑙𝑙</span>, is <span class="s137">𝑥𝑥𝑙𝑙  = 𝑥𝑥𝑙𝑙−1 − 𝑥𝑥̂𝑙𝑙−1</span>. At each step, the backcast generated by the block  is subtracted from the input to that block before it’s passed on to the next layer. And all the forecast outputs of all the blocks in a stack are added up to make the <i>stack forecast</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝑙𝑙</p><p style="text-indent: 0pt;text-align: left;"/><p class="s74" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑦𝑦<span class="s224">̂</span><span class="s273">𝑠𝑠</span><span class="s75">  </span>= ∑ 𝑦𝑦<span class="s224">̂</span><span class="s274">𝑠𝑠</span></p><p class="s75" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑙𝑙</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The residual backcast from the last block in a stack is the <i>stack residual </i>(<span class="s41">𝑥𝑥𝑠𝑠</span>).</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The overall architecture</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 87%;text-align: justify;">With that, we can move to the rightmost column of <i>Figure 16.1</i>, which shows the top-level view of the architecture. We saw that each stack has two outputs – a stack forecast (<span class="s252">𝑦</span><span class="s137">𝑦</span><span class="s258">𝑠𝑠</span>) and stack residual (<span class="s41">𝑥𝑥𝑠𝑠</span>). There can be <i>N </i>stacks that make up the N-BEATS model. Each stack is chained together so that for any stack (<i>s</i>), the stack residual out of the previous stack (<span class="s41">𝑥𝑥𝑠𝑠−1</span>) is the input and the stack generates two outputs: the stack forecast (<span class="s252">𝑦</span><span class="s137">𝑦</span><span class="s275">𝑠</span><span class="s43">𝑠</span>) and the stack residual (<span class="s41">𝑥𝑥𝑠𝑠</span>). Finally, the N-BEATS forecast, <span class="s93">𝑦𝑦</span><span class="s146">̂</span>, is the additive sum of all the stack forecasts:</p><p class="s146" style="padding-top: 3pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑁𝑁</p><p class="s242" style="padding-top: 2pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑦𝑦̂ = ∑ 𝑦𝑦<span class="s276">̂</span><span class="s244">𝑠𝑠</span></p><p class="s146" style="padding-top: 5pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑠𝑠=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="123" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_809.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Disclaimer</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The explanation here is to mostly aid intuition, so we might be hand-waving over a few mathematical concepts. For a more rigorous treatment of the subject, you should refer to mathematical books/articles that cover the topic. For example, <i>Functions as Vector Spaces </i>from the <i>Further reading </i>section and <i>Function Spaces </i><a href="https://cns.gatech.edu/%7Epredrag/courses/PHYS-6124-12/StGoChap2.pdf" class="s140" target="_blank">(</a><a href="https://cns.gatech.edu/%7Epredrag/courses/PHYS-6124-12/StGoChap2.pdf" class="a" target="_blank">https://cns.gatech.edu/~predrag/ </a><span class="s27">courses/PHYS-6124-12/StGoChap2.pdf</span>).</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Now that we have understood what the model is doing, we need to come back to one point that we left for later – <span class="s5">basis functions</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark640">Basis functions and interpretability</a></p><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: left;">5</p><p style="text-indent: 0pt;text-align: left;"/><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><a href="#bookmark414" class="s140">To understand what basis functions are, we need to understand a concept from linear algebra. We talked about vector spaces in </a><a href="#bookmark414" class="s21">Chapter </a><i>11</i>, <i>Introduction to Deep Learning</i>, and gave you a geometric interpretation of vectors and vector spaces. We talked about how a vector is a point in the <i>n</i>-dimensional vector space. We had that discussion regarding regular Euclidean space (<span class="s137">𝑅𝑅𝑛𝑛</span>), which is intended to represent physical space. Euclidean spaces are defined with an origin and an orthonormal basis. An orthonormal basis is a unit vector (magnitude=1) and they are orthogonal (in simple intuition, at 90 degrees) to each other. Therefore, a vector, <span class="s277">𝐴𝐴</span><span class="s75"> = [   ]</span>, can be written as <span class="s43">5𝑖𝑖̂ + 2𝑗𝑗̂</span>, where <span class="s150">𝑖</span><span class="s41">𝑖̂ </span>and  <span class="s157">𝑗𝑗̂ </span>are the orthonormal basis. You may remember this from high school.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, there is a branch of mathematics that views a function as a point in a vector space (at which point we call it a functional space). This comes from the fact that all the mathematical conditions that need to be satisfied for a vector space (things such as additivity, associativity, and so on) are valid if we consider functions instead of points. To better drive that intuition, let’s consider a function,</p><p class="s50" style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><span class="s278">𝑓</span>𝑓<span class="s279">(</span><span class="s278">𝑥</span>𝑥<span class="s279">)</span> <span class="s278">=</span> 2𝑥𝑥 + 4𝑥𝑥<span class="s280">2</span><span class="s54"> </span><span class="p">. We can consider this function as a vector in the function space with basis </span><span class="s90">𝑥𝑥 </span><span class="p">and</span></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><span class="s41">𝑥𝑥2</span>. Now, the coefficients, 2 and 4, can be changed to give us different functions; this can be any real number from - to + . This space of all functions that can have a basis of <span class="s137">𝑥𝑥 </span>and <span class="s137">𝑥𝑥2 </span>is the functional space, and every function in the function space can be defined as a linear combination of the basis functions. We can have the basis of any arbitrary function, which gives us a lot of flexibility. From a machine learning perspective, searching for the best function in this functional space automatically means that we are restricting the function search so that we have some properties defined by the basis functions.</p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Coming back to N-BEATS, we talked about the expansion coefficients, <span class="s157">𝜃𝜃𝑏𝑏 </span>and <span class="s41">𝜃𝜃𝑓𝑓</span>, which are mapped</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 8pt;text-align: justify;">to the output using a set of basis layers (<span class="s157">𝑔𝑔𝑏𝑏 </span>and <span class="s137">𝑔𝑔</span><span class="s258">𝑓</span><span class="s43">𝑓</span>). A basis layer can also be thought of as a basis</p><p class="s65" style="padding-left: 166pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝑙𝑙                        <span class="s43">𝑙𝑙</span></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">function because we know that a layer is nothing but a function that maps its inputs to its outputs. Therefore, by learning the expansion coefficients, we are essentially searching for the best function that can represent the output but is constrained by the basis functions we choose.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are two modes in which N-BEATS operates: <i>generic </i>and <i>interpretable</i>. The N-BEATS paper shows that under both modes, N-BEATS managed to beat the best in the M4 competition. Generic mode is where we do not have any basis function constraining the function search. We can also think of this as setting the basis function to be the identity function. So, in this mode, we are leaving the function completely learned by the model through a linear projection of the basis coefficients. This mode lacks human interpretability because we don’t have any idea how the different functions are learned and what each stack signifies.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">But if we have fixed basis functions that constrain the function space, we can bring in more interpretability. For instance, if we have a basis function that constrains the output to represent the trends for all the blocks in a stack, we can say that the forecast output of that stack represents the trend component. Similarly, if we have another basis function that constrains the output to represent the seasonality for all the blocks in a stack, we can say that the forecast output of the stack represents seasonality.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark641"><span class="p">This is exactly what the paper has proposed as well. They have defined specific basis functions that capture trend and seasonality, and including such blocks makes the final forecast more interpretable by giving us a decomposition. The trend basis function is a polynomial of a small degree, </span></a>p<span class="p">. So, as long as </span>p <a href="#bookmark211" class="s140">is low, such as 1, 2, or 3, it forces the forecast output to mimic the trend component. And for the seasonality basis function, the authors chose a Fourier basis (similar to the one we saw in </a>Chapter 6<span class="p">, </span>Feature Engineering for Time Series Forecasting<span class="p">). This forces the forecast output to be functions of these sinusoidal basis functions that mimic seasonality. In other words, the model learns to combine these sinusoidal waves with different coefficients to reconstruct the seasonality pattern as best as possible.</span><a name="bookmark610">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">For a deeper understanding of these basis functions and how they are structured, I have linked to a <i>Kaggle notebook </i>in the <i>Further reading </i>section that provides a clear explanation of the trend and seasonality basis functions. The associated notebook also has an additional section that visualizes the first few basis functions of seasonality. Along with the original paper, these additional readings will help you solidify your understanding.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_810.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Boris Oreshkin et al (N-BEATS) is cited in the <i>References </i>section as <i>3</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">N-BEATS wasn’t designed to be a global model, but it does well in the global setting. The M4 competition was a collection of unrelated time series and the N-BEATS model was trained so that the model was exposed to all those series and learned a common function to forecast each time series in the dataset. This, along with ensembling multiple N-BEATS models with different lookback windows, was the success formula for the M4 competition.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Forecasting with N-BEATS</p><ol id="l150"><ol id="l151"><li><p class="s4" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark561" class="s140">EATS is implemented in PyTorch Forecasting. We can use the same framework we worked with in </a><a href="#bookmark561" class="s21">Chapter </a>15<span class="p">, </span>Strategies for Global Deep Learning Forecasting Models<span class="p">, and extend it to train N-BEATS on our data. First, let’s look at the initialization parameters of the implementation.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The <span class="s20">Nbeats </span>class in PyTorch Forecasting has the following parameters:</p><ul id="l152"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s20">stack_types</span>: This defines the number of stacks that we need to have in the N-BEATS model. This should be a list of strings (<i>generic</i>, <i>trend</i>, or <i>seasonality</i>) denoting the number and type of stacks. Examples include <span class="s20">[&quot;trend&quot;, &quot;seasonality&quot;]</span>, <span class="s20">[&quot;trend&quot;, &quot;seasonality&quot;, &quot;generic&quot;]</span>, <span class="s20">[&quot;generic&quot;, &quot;generic&quot;, &quot;generic&quot;]</span>, and so on. However, if the entire network is generic, we can just have a single generic stack with more blocks as well.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">num_blocks<span class="p">: This is a list of integers signifying the number of blocks in each stack that we have defined. If we had defined </span>stack_types <span class="p">as </span>[&quot;trend&quot;, &quot;seasonality&quot;]<span class="p">, and we want three blocks each, we can set </span>num_blocks <span class="p">to </span>[3,3]<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l153"><li><p class="s20" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a name="bookmark642">num_block_layers</a><span class="p">: This is a list of integers signifying the number of FC layers with ReLU activation in each block. The recommended value is </span>4 <span class="p">and the length of the list should be equal to the number of stacks we have defined.</span><a name="bookmark611">&zwnj;</a></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">width<span class="p">: This sets the width or the number of units in the FC layers in each block. This is also a list of integers with lengths equal to the number of stacks defined.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">sharing<span class="p">: This is a list of Booleans signifying whether the weights generating the expansion coefficients are shared with other blocks in a stack. It is recommended to share the weights in the interpretable stacks and not share them in the generic stacks.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 94%;text-align: justify;"><span class="s20">expansion_coefficient_length</span>: This represents the size of the expansion coefficients (<span class="s281">𝜃</span><span class="s80">𝜃</span>). Depending on the kind of block, the intuitive meaning of this parameter changes. For the trend block, this means the number of polynomials we are using in our basis functions. And for the seasonality, this lets us control how quickly the underlying Fourier basis functions vary. The Fourier basis functions are sinusoidal basis functions with different frequencies; if they have a large <span class="s20">expansion_coefficient_length</span>, this means that subsequent basis functions will have a larger frequency than if you had a smaller <span class="s20">expansion_coefficient_length</span>. This is a parameter that we can tune as a hyperparameter. A typical range can be between 2 and 10.</p></li></ul></li></ul></li></ol></ol><p class="s27" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nbeats.NBeats.html" class="s140" target="_blank">There are a few other parameters, but these are not as important. A full list of parameters and their descriptions can be found at </a><a href="https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nbeats.NBeats.html" class="a" target="_blank">https://pytorch-forecasting.readthedocs.io/en/ </a>stable/api/pytorch_forecasting.models.nbeats.NBeats.html<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="138" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_811.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The complete code for training N-BEATS can be found in the <span class="s20">01-N-BEATS.ipynb </span>notebook in the <span class="s20">Chapter16 </span>folder. There are two variables in the notebook that act as switches: <span class="s20">TRAIN_SUBSAMPLE = True </span>makes the notebook run for a subset of 10 households, while <span class="s20">train_model = True </span>makes the notebook train different models (warning: training the model on full data takes hours). <span class="s20">train_model = False </span>loads the trained model weights (not included in the repository but saved every time you run training) and predicts on them.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Since the strength of the model is in forecasting slightly longer durations, we move from one-step ahead to one-day ahead (48 steps) forecasting. The only change we have to implement is changing the <span class="s20">max_prediction_length </span>parameter to <span class="s20">48 </span>instead of <span class="s20">1 </span>while initializing <span class="s20">TimeSeriesDataset</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Interpreting N-BEATS forecasting</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">N-BEATS, if we are running it in the interpretable model, also gives us more interpretability by separating the forecast into trend and seasonality. To get the interpretable output, we only need to make a small change in the <span class="s20">predict </span>function – we must change <span class="s20">mode=&quot;prediction&quot; </span>to <span class="s20">mode=&quot;raw&quot; </span>in the parameters:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">best_model.predict(val_dataloader, mode=&quot;raw&quot;)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p class="s37" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark643">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with Exogenous Variables (N-BEATSx) 417</a><a name="bookmark612">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_812.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">This will return us a <span class="s20">namedtuple </span>from which trend can be accessed using the <i>trend </i>key, seasonality from the <i>seasonality </i>key, and total predictions from the <i>prediction </i>key. Let’s see how one of the household predictions decomposed:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 61pt;text-indent: 0pt;text-align: left;"><span><img width="446" height="296" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_813.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 16.2 – Decomposed predictions from N-BEATS (interpretable)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">With all its success, N-BEATS was still a univariate model. It was not able to take in any external information, apart from its history. This was fine for the M4 competition, where all the time series in question were also univariate. But many real-world time series problems come with additional explanatory variables (or exogenous variables). Let’s look at a slight modification that was made to N-BEATS that enabled exogenous variables.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;line-height: 119%;text-align: left;">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with Exogenous Variables (N-BEATSx)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_814.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Olivares et al. (N-BEATSx) is cited in the <i>References </i>section as <i>4</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Olivares et al. proposed an extension of the N-BEATS model by making it compatible with exogenous variables. The overall structure is the same (with blocks, stacks, and residual connections) as N-BEATS (<i>Figure 16.1</i>), so we will only be focusing on the key differences and additions that the <span class="s5">N-BEATSx </span>model puts forward.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark644">Handling exogenous variables</a><a name="bookmark614">&zwnj;</a><a name="bookmark613">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><span class="s138">In N-BEATS, the input to a block was the lookback window, </span><span class="s41">𝑦𝑦𝑏𝑏</span><span class="s138">. But here, the input to a block is both </span>the lookback window, <span class="s137">𝑦𝑦𝑏𝑏</span>, and the array of exogenous variables, <span class="s137">𝒳𝒳</span>. These exogenous variables can be of two types: time-varying and static. The static variables are encoded using a static feature encoder. This is nothing but a single-layer FC that encodes the static information into a dimension specified by the user. Now, the encoded static information, the time-varying exogenous variables, and the lookback window are concatenated to form the input for a block so that the hidden state representation, <span class="s90">ℎ𝑙𝑙</span>, of block <span class="s137">𝑙𝑙 </span>is not</p><p class="s109" style="padding-left: 36pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">𝐹<span class="s146">𝐹𝐹𝐹</span>(𝑦<span class="s146">𝑦𝑏𝑏</span>)<span class="s146"> </span><span class="p">like in N-BEATS, but </span>𝐹𝐹𝐹<span class="s146">𝐹</span>(<span class="s146">[</span>𝑦<span class="s146">𝑦𝑏𝑏; 𝒳𝒳</span>]<span class="s146">)</span><span class="p">, where </span><span class="s144">[;</span><span class="s90"> </span><span class="s144">]</span><span class="s90"> </span><span class="p">represents concatenation. This way, the exogenous</span></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">information is part of the input to every block as it is concatenated with the residual at each step.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Exogenous blocks</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In addition to this, the paper also proposes a new kind of block – an <i>exogenous block</i>. The exogenous block takes in the concatenated lookback window and exogenous variables (just like any other block) as input and produces a backcast and forecast:</p><p class="s75" style="padding-top: 5pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑁𝑁<span class="s282">𝑥𝑥</span></p><p class="s78" style="padding-left: 41pt;text-indent: 0pt;line-height: 10pt;text-align: center;">𝑦𝑦<span class="s250">̂</span><span class="s75">𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒     </span>= <span class="s74">∑ 𝒳𝒳</span><span class="s283">𝒾𝒾</span>𝜃𝜃<span class="s75">𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒</span></p><p class="s75" style="padding-left: 173pt;text-indent: 0pt;line-height: 8pt;text-align: center;">𝑙𝑙                                        <span class="s277">ℓ    </span>𝑙𝑙</p><p class="s75" style="padding-top: 1pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑖𝑖=0</p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here, <span class="s90">𝑁𝑁𝑥𝑥 </span>is the number of exogenous features.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, we can see that the exogenous forecast is the linear combination of the exogenous variables and that the weights for this linear combination are learned by the expansion coefficients, <span class="s74">𝜃𝜃𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒</span>. The paper refers to this configuration as the interpretable exogenous block because by using the expansion weights, we can define the importance of each exogenous variable and even figure out the exact part of the forecast, which is because of a particular exogenous variable.</p><ol id="l154"><ol id="l155"><li><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">EATSx also has a generic version (which is not interpretable) of the exogenous block. In this block, the exogenous variables are passed through an encoder that learns a context vector, <span class="s146">𝒞𝒞𝑙𝑙</span>, and the forecast is generated using the following formula:</p><p class="s65" style="padding-top: 5pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑁𝑁<span class="s79">𝑥𝑥</span></p><p class="s284" style="padding-top: 4pt;padding-left: 41pt;text-indent: 0pt;line-height: 26%;text-align: center;">𝑦𝑦<span class="s271">̂</span><span class="s65">𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒  </span>=<span class="s157"> ∑ 𝒞𝒞 </span><span class="s285">𝒾</span><span class="s65">𝒾</span>𝜃<span class="s157">𝜃</span><span class="s65">𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒</span></p><p class="s65" style="text-indent: 0pt;text-align: right;">𝑙𝑙</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s65" style="text-indent: 0pt;text-align: right;">𝑖𝑖=0</p><p class="s214" style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">ℓ <span class="s65">𝑙𝑙</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">They proposed two encoders: a <span class="s5">Temporal Convolutional Network </span>(<span class="s5">TCN</span>) and <span class="s5">WaveNet </span>(a network similar to the TCN, but with dilation to expand the receptive field). The <i>Further reading </i>section contains resources if you wish to learn more about WaveNet, an architecture that originated in the sound domain.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="154" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_815.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional information</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">N-BEATSx is not implemented in PyTorch Forecasting, but we can find it in another library for forecasting using DL – <span class="s20">neuralforecast </span>by Nixtla. One feature that <span class="s20">neuralforecast</span><span class="s27"> </span><a href="#bookmark367" class="s140">lacks (which is kind of a deal breaker to me) is that it doesn’t support categorical features. So, we will have to encode the categorical features into numerical representations (like we did in </a><a href="#bookmark367" class="s21">Chapter </a><i>10, Global Forecasting Models</i>) before using <span class="s20">neuralforecast</span>. Also, the documentation of the library isn’t great, which means we need to dive into the code base and hack it to make it work.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark645">The research paper also showed that N-BEATSx outperformed N-BEATS, ES-RNN, and other benchmarks on electricity price forecasting considerably.</a><a name="bookmark616">&zwnj;</a><a name="bookmark615">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Continuing with the legacy of N-BEATS, we will now talk about another modification to the architecture that makes it suitable for long-term forecasting.</p><p class="s3" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;line-height: 119%;text-align: left;">Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)</p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Although there has been a good amount of work from DL to tackle time series forecasting, very little focus has been on long-horizon forecasting. Despite recent progress, long-horizon forecasting remains a challenge because of two reasons:</p><ul id="l156"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The expressiveness required to truly capture the variation</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The computational complexity</p></li></ul></li></ol></ol><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Attention-based methods (Transformers) and N-BEATS-like methods scale quadratically in memory and the computational cost concerning the forecasting horizon.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_816.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Challu et al. on N-HiTS is cited in the <i>References </i>section as <i>5</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The authors claim that N-HiTS drastically cuts long-forecasting compute costs while simultaneously showing 25% accuracy improvements compared to existing Transformer-based architectures across a large array of multi-variate forecasting datasets.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The Architecture of N-HiTS</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">N-HiTS can be considered as an alteration to N-BEATS because the two share a large part of their architectures. <i>Figure 16.1</i>, which shows the N-BEATS architecture, is still valid for N-HiTS. N-HiTS also has stacks of blocks arranged in a residual manner; it differs only in the kind of blocks it uses. For instance, there is no provision for interpretable blocks. All the blocks in N-HiTS are generic. While</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l157"><ol id="l158"><li><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark646">EATS tries to decompose the signal into different patterns (trend, seasonality, and so on), N-HiTS tries to decompose the signal into multiple frequencies and forecast them separately. To enable this, a few key improvements have been proposed:</a></p><ul id="l159"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Multi-rate data sampling</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Hierarchical interpolation</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Synchronizing the rate of input sampling with a scale of output interpolation across the blocks</p></li></ul></li></ol></ol><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Multi-rate data sampling</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">N-HiTS incorporates sub-sampling layers before the fully connected blocks so that the resolution of the input to each block is different. This is similar to smoothing the signal with different resolutions so that each block is looking at a pattern that occurs at different resolutions – for instance, if one block looks at the input every day, another block looks at the output every week, and so on. This way, when arranged with different blocks looking at different resolutions, the model will be able to predict patterns that occur in those resolutions. This significantly reduces the memory footprint and the computation required as well, because instead of looking at all <i>H </i>steps of the lookback window, we are looking at smaller series (such as H/2, H/4, and so on).</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">N-HiTS accomplishes this using a Max Pooling or Average Pooling layer of kernel size <span class="s157">𝑘𝑘𝑙𝑙 </span><a href="#bookmark454" class="s140">on the lookback window. A pooling operation is similar to a convolution operation, but the function that is used is non-learnable. In </a><a href="#bookmark454" class="s21">Chapter </a><i>12</i>, <i>Building Blocks of Deep Learning for Time Series</i>, we learned about convolutions, kernels, stride, and so on. While a convolution uses weights that are learned from data while training, a pooling operation uses a non-learnable and fixed function to aggregate the data in the receptive field of a kernel. Common examples of these functions are the maximum, average, sum, and so on. N-HiTS uses <span class="s20">MaxPool1d </span>or <span class="s20">AvgPool1d </span>(in <span class="s20">PyTorch </span>terminology) with different kernel sizes for different blocks. Each pooling operation also has a stride equal to the kernel, resulting in non-overlapping windows over which we do the aggregation operation. To refresh our memory, let’s see what max pooling with <span class="s20">kernel=2 </span>and <span class="s20">stride=2 </span>looks like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span><img width="414" height="103" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_817.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 16.3 – Max pooling on one dimension – kernel=2, stride=2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark647">Therefore, a larger kernel size will tend to cut more high-frequency (or small-timescale) components from the input. This way, the block is forced to focus on larger-scale patterns. The paper calls this </a><span class="s5">multi-rate signal sampling</span>.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Hierarchical interpolation</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In a standard multi-step forecasting setting, the model must forecast <i>H </i>timesteps. And as <i>H </i>becomes larger, the compute requirements increase and lead to an explosion of expressive power the model needs to have. Training a model with such a large expressive power, without overfitting, is a challenge in itself. To combat these issues, N-HiTS proposes a technique called <span class="s5">temporal interpolation</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The pooled input (which we saw in the previous section) goes into the block along with the usual mechanism to generate expansion coefficients and finally gets converted into forecast output. But here, instead of setting the dimension of the expansion coefficients as <i>H</i>, N-HiTS sets them as <span class="s146">𝑟𝑟𝑙𝑙 × 𝐻𝐻</span>, where</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><span class="s252">𝑟𝑟</span><span class="s145">𝑙𝑙 </span>is the <span class="s5">expressiveness ratio</span>. This parameter essentially reduces the forecast output dimension and</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">thus controls the issues we discussed in the previous paragraph. To recover the original sampling rate and predict all the <i>H </i>points in the forecast horizon, we can use an interpolation function. There are many options for the interpolation functions – linear, nearest neighbor, cubic, and so on. All these options can easily be implemented in <span class="s20">PyTorch </span>using the <span class="s20">interpolate </span>function.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Synchronizing the input sampling and output interpolation</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">In addition to proposing the input sampling through pooling and output interpolation, N-HiTS also proposes to arrange them in different blocks in a particular way. The authors argue that hierarchical interpolation can only happen the right way if the expressiveness ratios are distributed across blocks in a manner that is synchronized with the multi-rate sampling. Blocks closer to the input should have a smaller expressiveness ratio, <span class="s137">𝑟𝑟𝑙𝑙</span>, and larger kernel sizes, <span class="s90">𝑘𝑘𝑙𝑙</span>. This means that the blocks closer to the input will generate larger resolution patterns (because of aggressive interpolation) while being forced to look at aggressively subsampled input signals. The paper proposes exponentially increasing expressiveness ratios as we move from the initial block to the last block to handle a wide range of frequency bands. The official N-HiTS implementation uses the following formula to set the expressiveness ratios and pooling kernels:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">pooling_sizes = np.exp2(</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">np.round(np.linspace(0.49, np.log2(prediction_length / 2), n_stacks))</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">)</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 131%;text-align: left;">pooling_sizes = [int(x) for x in pooling_sizes[::-1]] downsample_frequencies = [</p><p class="s28" style="padding-left: 9pt;text-indent: 24pt;line-height: 106%;text-align: left;">min(prediction_length, int(np.power(x, 1.5))) for x in pooling_sizes</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">]</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark648">We can also provide explicit </a><span class="s20">pooling_sizes </span>and <span class="s20">downsampling_fequencies </span>to reflect known cycles of the time series (weekly seasonality, monthly seasonality, and so on). The core principle of N-BEATS (one block removing the effect it captures from the signal and passing it on to the next block) is used here as well so that, at each level, the patterns or frequencies that a block captures are removed from the input signal before being passed on to the next block. In the end, the final forecast is the sum of all such individual block forecasts.<a name="bookmark617">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Forecasting with N-HiTS</p><ol id="l160"><ol id="l161"><li><p class="s4" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark561" class="s140">iTS is implemented in PyTorch Forecasting. We can use the same framework we were working with in </a><a href="#bookmark561" class="s21">Chapter </a>15<span class="p">, </span>Strategies for Global Deep Learning Forecasting Models<span class="p">, and extend it to train </span>N-HiTS <span class="p">on our data. What’s even better is that the implementation supports exogenous variables, the same way N-BEATSx handles exogenous variables (although without the exogenous block). First, let’s look at the initialization parameters of the implementation.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The <span class="s20">NHiTS </span>class in PyTorch Forecasting has the following parameters:</p><ul id="l162"><li><p class="s20" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">n_blocks<span class="p">: This is a list of integers signifying the number of blocks to be used in each stack. For instance, </span>[1,1,1] <span class="p">means there will be three stacks with one block each.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">n_layers<span class="p">: This is either a list of integers or a single integer signifying the number of FC layers with a ReLU activation in each block. The recommended value is </span>2<span class="p">.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">hidden_size<span class="p">: This sets the width or the number of units in the FC layers in each block.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s20">static_hidden_size</span>: The static features are encoded using an FC encoder into a dimension that is set by this parameter. We covered this in detail in the <i>Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with Exogenous Variables (N-BEATSx) </i>section.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">shared_weights<span class="p">: This signifies whether the weights generating the expansion coefficients are shared with other blocks in a stack. It is recommended to share the weights in the interpretable stacks and not share them in the generic stacks.</span></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 90%;text-align: justify;"><span class="s20">pooling_sizes</span>: This is a list of integers that defines the pooling size (<span class="s157">𝑘𝑘</span><span class="s88">𝑙</span><span class="s65">𝑙</span>) for each stack. This is an optional parameter, and if provided, we can have more control over how the pooling happens in the different stacks. Using an ordering of higher to lower improves results.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">pooling_mode<span class="p">: This defines the kind of pooling to be used. It should be either </span>&#39;max&#39;</p><p style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: left;">or <span class="s20">&#39;average&#39;</span>.</p></li><li><p style="padding-top: 6pt;padding-left: 64pt;text-indent: -13pt;line-height: 90%;text-align: justify;"><span class="s20">downsample_frequencies</span>: This is a list of integers that defines the expressiveness ratios (<span class="s157">𝑟𝑟𝑙𝑙</span>) for each stack. This is an optional parameter, and if provided, we can have more control over how the interpolation happens in the different stacks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="138" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_818.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The complete code for training N-HiTS can be found in the <span class="s20">02-N-HiTS.ipynb </span>notebook in the <span class="s20">Chapter16 </span>folder. There are two variables in the notebook that act as switches – <span class="s20">TRAIN_SUBSAMPLE = True </span>makes the notebook run for a subset of 10 households, while <span class="s20">train_model = True </span>makes the notebook train different models (warning: training the model on full data takes hours). <span class="s20">train_model = False </span>loads the trained model weights (not included in the repository but saved every time you run training) and predicts on them.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark649">Now, let’s shift our focus and look at a few modifications of the Transformer model to make it better for time series forecasting.</a><a name="bookmark618">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Informer</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Recently, Transformer models have shown superior performance in capturing long-term patterns than standard RNNs. One of the major factors of that is the fact that self-attention, which powers Transformers, can reduce the length that the relevant sequence information has to be held on to before it can be used for prediction. In other words, in an RNN, if the timestep 12 steps before holds important information, that information has to be stored in the RNN through 12 updates before it can be used for prediction. But with self-attention in Transformers, the model is free to create a shortcut between lag 12 and the current step directly because of the lack of recurrence in the structure.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">But the same self-attention is also the reason why we can’t scale vanilla Transformers to long sequences. In the previous section, we discussed how long-term forecasting is a challenge because of two reasons: the expressiveness required to truly capture the variation and computational complexity. Self-attention, with its quadratic computational complexity, contributes to the second reason. Scaling Transformers on very long sequences will require us to pour computation into the model using multi-GPU setups, which makes real-world deployment a challenge, especially when good alternative models such as ARIMA, LightGBM, and N-BEATS exist.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_819.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Zhou et al. on the Informer model is cited in the <i>References </i>section as <i>8</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The research community has recognized this challenge and has put a lot of effort into devising efficient transformers through many techniques such as downsampling, low-rank approximations, sparse attention, and so on. For a detailed account of such techniques, refer to the link for <i>Efficient Transformers: A Survey </i>in the <i>Further reading </i>section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark650">The architecture of the Informer model</a><a name="bookmark619">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The Informer model is a modification of Transformers. The following are its major contributions:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Uniform Input Representation<span class="p">: A methodical way to include the history of the series along with other information, which will help in capturing long-term signals such as the week, month, holidays, and so on</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">ProbSparse<span class="p">: An efficient attention mechanism based on information theory</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Attention distillation<span class="p">: A mechanism to provide dominating attention scores while stacking multiple layers and also reduce computational complexity</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Generative-style decoder<span class="p">: Used to generate the long-term horizon in a single forward pass instead of via dynamic recurrence</span></p></li></ul></li></ol></ol><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s take a look at the overall architecture (<i>Figure 16.4</i>) to see how they fit together, and then look at them in more detail:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: left;"><span><img width="475" height="306" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_820.gif"/></span></p><p class="s37" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 16.4 – Informer model architecture</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The overall architecture is akin to a standard encoder-decoder transformer model. The encoder takes in the inputs and uses multi-headed attention to generate features that are passed to the decoder, which, in turn, uses these features to generate the forecast. Special modifications are made to the architecture in each of these steps. Let’s review them in detail.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark651">Uniform Input Representation</a></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">RNNs capture time series patterns with their recurrent structure, so they only need the sequence; they don’t need information about the timestamp to extract the patterns. However, the self-attention in Transformers is done via point-wise operations that are performed in sets (the order doesn’t matter in a set). Typically, we include positional encodings to capture the order of the sequence. Instead of using positional encodings, we can use richer information, such as hierarchical timestamp information (such as weeks, months, years, and so on). This is what the authors proposed through <span class="s5">Uniform Input Representation</span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Uniform Input Representation uses three types of embeddings to capture the history of the time series, the sequence of values in the time series, and the global timestamp information. The sequence of values in the time series is captured by the standard positional embedding of the <span class="s5">d_model </span>dimension.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Uniform Input Representation uses a one-dimensional convolutional layer with <span class="s20">kernel=3 </span>and <span class="s20">stride=1 </span>to project the history (which is scalar or one-dimensional) into an embedding of <span class="s20">d_model </span>dimensions. This is referred to as <span class="s5">value embedding</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The global timestamp information is embedded by a learnable embedding of <span class="s20">d_model </span><a href="#bookmark561" class="s140">dimensions with limited vocabulary in a mechanism that is identical to embedding categorical variables into fixed-size vectors (</a><i>Chapter 15</i>, <i>Strategies for Global Deep Learning Forecasting Models</i>). This is referred to as <span class="s5">temporal embedding</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now that we have three embeddings of the same dimension, <span class="s20">d_model</span>, all we need to do is add them together to get the Uniform Input Representation.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">ProbSparse attention</p><p class="s4" style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark522" class="s140">In </a>Chapter 14<span class="p">, </span>Attention and Transformers for Time Series<span class="p">, we defined a generalized attention model as follows:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s248" style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝒜𝒜(𝑞𝑞, 𝐾𝐾, 𝑉𝑉) = ∑ 𝑝𝑝(𝑎𝑎(𝑘𝑘<span class="s82">𝑖𝑖</span><span class="s50"> </span>, 𝑞𝑞)) × 𝑣𝑣<span class="s82">𝑖𝑖</span></p><p class="s50" style="padding-top: 3pt;padding-left: 159pt;text-indent: 0pt;text-align: center;">𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;">Here, is an <span class="s5">alignment function </span>that calculates a similarity or a notion of similarity between the queries and keys, <span class="s137">𝑝𝑝 </span>is a <span class="s5">distribution function </span>that converts this score into attention weights that sum up to 1, and <i>q</i>, <i>k</i>, and <i>v </i>are the query, key, and values of the attention mechanism, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="534" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_821.png"/></span></p><p class="s29" style="padding-top: 4pt;padding-left: 50pt;text-indent: 0pt;text-align: left;">Additional information</p><p style="padding-top: 2pt;padding-left: 50pt;text-indent: 0pt;line-height: 89%;text-align: left;">The original Transformer uses scaled dot product attention, along with different projection matrices for the query, key, and values. The formula can be written like so:</p><p class="s43" style="padding-top: 5pt;padding-left: 103pt;text-indent: 0pt;line-height: 7pt;text-align: center;">𝑞𝑞𝑊𝑊<span class="s286">𝑞</span><span class="s64">𝑞</span>(𝑘𝑘𝑊𝑊<span class="s286">𝑘𝑘</span><span class="s64"> </span>)𝑇𝑇</p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: right;">𝒜𝒜(𝑞𝑞, 𝐾𝐾, 𝑉𝑉) = 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 (           <span class="s39">    </span></p><p class="s43" style="text-indent: 0pt;line-height: 9pt;text-align: right;">√𝑑𝑑𝑘𝑘</p><p class="s43" style="padding-left: 11pt;text-indent: 0pt;line-height: 10pt;text-align: left;">) 𝑣𝑣W<span class="s286">v</span></p><p style="padding-top: 4pt;padding-left: 50pt;text-indent: 0pt;line-height: 81%;text-align: left;">Here, <span class="s146">𝑊𝑊𝑞𝑞</span>, <span class="s146">𝑊𝑊</span><span class="s79">𝑘</span><span class="s75">𝑘</span>, and <span class="s50">𝑊𝑊𝑣𝑣 </span>are learnable projection matrices for the query, key, and values, respectively, and <span class="s94">𝑑</span><span class="s74">𝑑𝑘𝑘 </span>is the attention dimension. We know that <span class="s109">𝑠𝑠𝑠𝑠𝑠</span><span class="s146">𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠</span><span class="s109">(𝑧</span><span class="s146">𝑧</span><span class="s283">𝑖</span><span class="s75">𝑖</span><span class="s109">)</span><span class="s146"> </span>is defined as follows:</p><p class="s207" style="padding-left: 173pt;text-indent: 0pt;line-height: 40%;text-align: center;">𝑒𝑒<span class="s43">𝑧𝑧</span><span class="s240">𝑖𝑖</span></p><p class="s41" style="text-indent: 0pt;line-height: 11pt;text-align: left;">∑</p><p style="text-indent: 0pt;text-align: left;"/><p class="s43" style="padding-top: 6pt;padding-left: 223pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝐾𝐾</p><p class="s43" style="padding-left: 223pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝑗𝑗=1</p><p class="s207" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">𝑒𝑒<span class="s43">𝑧𝑧</span><span class="s240">𝑗𝑗</span></p><p style="padding-top: 1pt;padding-left: 50pt;text-indent: 0pt;line-height: 89%;text-align: left;">Let’s denote <span class="s260">𝑞𝑞𝑊𝑊</span><span class="s43">𝑞𝑞 </span>as <span class="s157">𝑞𝑞∗ </span>and <span class="s150">𝑘𝑘𝑊𝑊</span><span class="s41">𝑘𝑘 </span>as <span class="s157">𝑘𝑘∗</span>. So, using the <i>softmax </i>expansion, we can write the previous formula like so:</p><p class="s75" style="padding-top: 2pt;padding-left: 108pt;text-indent: 0pt;line-height: 6pt;text-align: center;">⟨𝑞𝑞<span class="s287">∗</span>, 𝑘𝑘<span class="s287">∗ </span>⟩/𝑑𝑑<span class="s282">𝑘𝑘</span></p><p class="s74" style="padding-top: 6pt;text-indent: 0pt;line-height: 7pt;text-align: right;">𝒜𝒜(𝑞𝑞<span class="s79">𝑖𝑖 </span>, 𝐾𝐾, 𝑉𝑉) = ∑</p><p class="s74" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">𝑒𝑒</p><p class="s220" style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">𝑖𝑖 𝑗𝑗</p><p class="s288" style="padding-top: 5pt;padding-left: 23pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑣<span class="s74">𝑣</span><span class="s75">∗</span></p><p class="s75" style="text-indent: 0pt;line-height: 3pt;text-align: right;">⟨𝑞𝑞<span class="s287">∗</span>, 𝑘𝑘<span class="s287">∗</span>⟩/𝑑𝑑<span class="s282">𝑘𝑘</span></p><p class="s75" style="padding-left: 8pt;text-indent: 0pt;line-height: 3pt;text-align: left;">𝑗𝑗</p><p class="s75" style="padding-top: 5pt;text-indent: 0pt;text-align: right;">𝑗𝑗</p><p class="s74" style="padding-left: 4pt;text-indent: 0pt;line-height: 12pt;text-align: left;">∑<span class="s79">𝑙𝑙</span><span class="s75"> </span>𝑒𝑒</p><p class="s220" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">𝑖𝑖 𝑙𝑙</p><p style="padding-top: 6pt;padding-left: 50pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Here, <span class="s109">⟨. ⟩ </span>denotes the inner product. In 2019, Tsai et al. proposed an alternate view of the attention mechanism using kernels. The math and history behind kernels are quite extensive and are outside the scope of this book. Just know that a kernel is a special kind of function, similar to</p><p class="s289" style="padding-left: 50pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="p">a similarity function. In this case, if we define   </span>⟨<span class="s64">𝑞𝑞</span><span class="s290">∗</span>,<span class="s64"> 𝑘𝑘</span><span class="s290">∗</span><span class="s231"> </span>⟩<span class="s64">/𝑑𝑑</span><span class="s231">𝑘𝑘  </span><span class="p">as </span><span class="s50">𝑘𝑘(𝑞𝑞∗, 𝑘𝑘∗) </span><span class="p">(which is an asymmetric</span></p><p class="s65" style="text-indent: 0pt;line-height: 5pt;text-align: right;">𝑒𝑒</p><p class="s231" style="text-indent: 0pt;line-height: 5pt;text-align: right;">𝑖𝑖</p><p class="s231" style="text-indent: 0pt;line-height: 5pt;text-align: right;">𝑗𝑗</p><p class="s54" style="text-indent: 0pt;line-height: 5pt;text-align: right;">𝑖𝑖</p><p class="s54" style="padding-left: 7pt;text-indent: 0pt;line-height: 5pt;text-align: left;">𝑗𝑗</p><p style="padding-left: 50pt;text-indent: 0pt;line-height: 13pt;text-align: left;">exponential kernel), the attention equation becomes like this:</p><p class="s146" style="padding-top: 4pt;padding-left: 98pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝑘𝑘(𝑞𝑞∗, 𝑘𝑘∗)</p><p class="s146" style="padding-top: 1pt;text-indent: 0pt;line-height: 6pt;text-align: right;">𝒜𝒜(𝑞𝑞 , 𝐾𝐾, 𝑉𝑉) = ∑</p><p class="s75" style="padding-left: 21pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑖𝑖 𝑗𝑗</p><p class="s247" style="padding-left: 14pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑣𝑣<span class="s75">∗</span></p><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑖𝑖</p><p class="s146" style="padding-left: 50pt;text-indent: 0pt;line-height: 7pt;text-align: left;">∑<span class="s79">𝑙𝑙</span><span class="s75"> </span>𝑘𝑘(𝑞𝑞∗, 𝑘𝑘∗)</p><p class="s75" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑗𝑗</p><p class="s75" style="padding-top: 2pt;text-indent: 0pt;text-align: right;">𝑗𝑗</p><p class="s75" style="padding-left: 30pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑖𝑖 𝑙𝑙</p><p style="padding-top: 8pt;padding-left: 50pt;text-indent: 0pt;text-align: left;">This interpretation leads to a probabilistic view of attention where the first term is:</p><p class="s248" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;line-height: 8pt;text-align: center;">𝑘𝑘(𝑞𝑞∗, 𝑘𝑘∗)</p><p class="s291" style="padding-top: 1pt;padding-left: 41pt;text-indent: 0pt;line-height: 39%;text-align: center;">∑     <span class="s248"> </span><span class="s50">𝑖𝑖        𝑗𝑗        </span>)</p><p class="s248" style="padding-left: 173pt;text-indent: 0pt;line-height: 9pt;text-align: center;">∑<span class="s82">𝑙𝑙</span><span class="s50"> </span>𝑘𝑘(𝑞𝑞∗, 𝑘𝑘∗)</p><p class="s50" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">𝑗𝑗</p><p class="s50" style="padding-left: 41pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑖𝑖 𝑙𝑙</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 50pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Which can be interpreted as the probability of <span class="s90">𝑘𝑘∗</span>, given <span class="s50">𝑞𝑞</span><span class="s292">∗</span>, <span class="s249">𝑃</span><span class="s50">𝑃(𝑘𝑘</span><span class="s292">∗</span><span class="s249">|</span><span class="s50">𝑞𝑞</span><span class="s292">∗</span><span class="s249">)</span>.</p><p class="s54" style="padding-left: 301pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑗𝑗 𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 36pt;text-indent: 0pt;text-align: left;">The attention equation can also be written as follows:</p><p class="s242" style="padding-top: 2pt;padding-left: 12pt;text-indent: 0pt;line-height: 15pt;text-align: center;">𝒜𝒜<span class="s293">(</span>𝑞𝑞𝑖𝑖, 𝐾𝐾, 𝑉𝑉<span class="s293">)</span> = 𝔼𝔼<span class="s294">P</span><span class="s146">(</span>𝑘𝑘<span class="s146">∗</span><span class="s294">|</span>𝑞𝑞<span class="s146">∗</span><span class="s294">)</span>𝑣𝑣𝑗𝑗</p><p class="s146" style="padding-left: 129pt;text-indent: 0pt;line-height: 10pt;text-align: center;">𝑗𝑗       𝑖𝑖</p><p style="padding-top: 6pt;padding-left: 36pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Here, <span class="s43">𝐸𝐸 </span><span class="s220">∗ ∗ </span>is the expectation of the probability of <span class="s157">𝑘𝑘∗</span>, given <span class="s259">𝑞𝑞</span><span class="s157">∗</span>. The quadratic computational</p><p class="s220" style="padding-left: 66pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑃𝑃(<span class="s43">𝑘𝑘</span><span class="s295">𝑗𝑗</span> |<span class="s43">𝑞𝑞</span><span class="s295">𝑖𝑖</span> )</p><p style="padding-left: 36pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">complexity stems from the calculation of this expectation. We will have to use all the elements in the</p><p style="padding-left: 36pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">query and the key to calculate a matrix of probabilities.</p><p style="padding-top: 6pt;padding-left: 36pt;text-indent: 0pt;text-align: justify;">Previous studies had revealed that this distribution of self-attention probability has potential sparsity. The authors of the paper also reaffirmed this through their experiments. The essence of this sparsity is that there are only a few query-key pairs that absorb the majority of the probability mass. In other</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark652">words, there will be a few query-key pairs that will have a high probability; the others will be closer to zero. But the key question is to identify which query-key pairs contribute to major attention without doing the actual calculation.</a></p><p class="s64" style="text-indent: 0pt;line-height: 6pt;text-align: left;">𝑗𝑗</p><p style="text-indent: 0pt;text-align: left;"/><p class="s64" style="text-indent: 0pt;line-height: 6pt;text-align: left;">𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"/><p class="s64" style="text-indent: 0pt;line-height: 6pt;text-align: left;">𝑗𝑗</p><p style="text-indent: 0pt;text-align: left;"/><p class="s64" style="text-indent: 0pt;line-height: 6pt;text-align: left;">𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">From the re-written attention equation, the <span class="s137">𝑖𝑖</span>-th query’s attention on all the keys is defined as a probability <span class="s65">𝑃𝑃(𝑘𝑘∗|𝑞𝑞∗) </span>and the output is its composition with values, <span class="s90">𝑣𝑣</span>. If <span class="s65">𝑃𝑃(𝑘𝑘∗|𝑞𝑞∗) </span>is close to a uniform</p><p style="padding-bottom: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 7pt;text-align: left;">distribution, <span class="s65">𝑃𝑃(𝑘𝑘∗|𝑞𝑞∗) </span>will be <span class="s219">1</span><span class="s220"> </span>. This means that self-attention becomes a simple sum of all values. The</p><p style="padding-left: 140pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="7" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_822.png"/></span></p><p class="s64" style="text-indent: 0pt;line-height: 3pt;text-align: right;">𝑗𝑗</p><p class="s64" style="text-indent: 0pt;line-height: 3pt;text-align: right;">𝑖𝑖</p><p class="s220" style="padding-left: 32pt;text-indent: 0pt;line-height: 3pt;text-align: left;">𝐿𝐿<span class="s296">𝑞𝑞</span></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">dominant dot-product query-key pairs encourage the corresponding query’s probability distribution to deviate away from the uniform distribution. So, we can measure how <i>different </i>the attention distribution</p><p class="s64" style="text-indent: 0pt;line-height: 6pt;text-align: left;">𝑗𝑗</p><p style="text-indent: 0pt;text-align: left;"/><p class="s64" style="text-indent: 0pt;line-height: 6pt;text-align: left;">𝑖𝑖</p><p style="text-indent: 0pt;text-align: left;"/><p class="s65" style="padding-left: 27pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝑃𝑃(𝑘𝑘∗|𝑞𝑞∗) <span class="p">is from the uniform distribution to measure how dominant a query-key pair is. We can use</span></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: justify;"><span class="s5">Kullback-Liebler </span>(<span class="s5">KL</span>) <span class="s5">Divergence </span>to measure this <i>difference</i>. KL Divergence is based on information theory and is defined as the information loss that happens when one distribution is approximated using the other. Therefore, the more different the two distributions are, the larger the loss, and thereby KL Divergence. In this manner, it measures how much one distribution diverges from another.</p><p style="padding-top: 6pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">The formula for calculating KL Divergence with the uniform distribution works out to be as follows:</p><p class="s50" style="padding-top: 6pt;text-indent: 0pt;line-height: 11pt;text-align: right;">𝐿𝐿<span class="s79">𝐾𝐾</span></p><p class="s50" style="padding-top: 10pt;padding-left: 10pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑞𝑞<span class="s79">𝑖</span><span class="s75">𝑖</span>𝑘𝑘<span class="s274">⊤</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 10pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="27" height="3" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_823.png"/></span></p><p class="s248" style="padding-top: 13pt;padding-left: 16pt;text-indent: 0pt;line-height: 4pt;text-align: left;">1 𝑞𝑞 𝑘𝑘⊤</p><p class="s75" style="text-indent: 0pt;line-height: 2pt;text-align: right;">𝑙𝑙</p><p class="s248" style="text-indent: 0pt;line-height: 11pt;text-align: right;">𝑙𝑙𝑙𝑙 ∑ 𝑒𝑒 <span class="s279">√𝑑𝑑</span></p><p class="s248" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"><span><img width="55" height="6" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_824.png"/></span></p><p class="s248" style="text-indent: 0pt;line-height: 1pt;text-align: right;">𝐿𝐿</p><p class="s50" style="padding-left: 13pt;text-indent: 0pt;line-height: 28%;text-align: left;">𝑖𝑖 𝑗𝑗</p><p class="s248" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">− 𝑙𝑙𝑙𝑙𝐿𝐿<span class="s82">𝑘𝑘</span></p><p class="s50" style="padding-top: 5pt;text-indent: 0pt;text-align: right;">𝑖𝑖=1</p><p class="s50" style="text-indent: 0pt;text-align: right;">𝑘𝑘</p><p class="s248" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">√𝑑𝑑</p><p style="padding-top: 7pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">The first term here is the <span class="s5">Log-Sum-Exp </span>(<span class="s5">LSE</span>) of <span class="s20">q_i</span>, on all the keys. LSE is known to have numerical stability issues, so the authors proposed an empirical approximation. The complete proof is in the paper for those who are interested. So, after the approximation, the measure of divergence, <span class="s146">𝑀𝑀(𝑞𝑞𝑘𝑘, 𝐾𝐾)</span>, becomes as follows:</p><p class="s297" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝑙 𝑙  </p><p style="text-indent: 0pt;text-align: left;"/><p class="s74" style="padding-top: 5pt;text-indent: 0pt;line-height: 10pt;text-align: right;">𝑞𝑞<u> </u><span class="s297">𝑖𝑖 </span>𝑘𝑘⊤</p><p class="s127" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝑗𝑗</p><p style="text-indent: 0pt;text-align: left;"/><p class="s74" style="padding-bottom: 3pt;padding-left: 150pt;text-indent: 0pt;line-height: 3pt;text-align: left;">𝑀𝑀(𝑞𝑞 , 𝐾𝐾) = 𝑚𝑚𝑚𝑚𝑥𝑥 [ ] −</p><p style="padding-left: 242pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="8" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_825.png"/></span></p><p class="s74" style="padding-top: 5pt;padding-bottom: 2pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">1 𝑞𝑞𝑖𝑖𝑘𝑘⊤</p><p style="text-indent: 0pt;line-height: 3pt;text-align: left;"><span><img width="44" height="5" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_826.png"/></span></p><p class="s127" style="text-indent: 0pt;line-height: 3pt;text-align: right;">𝑘𝑘</p><p class="s127" style="text-indent: 0pt;line-height: 3pt;text-align: right;">𝑗𝑗</p><p class="s74" style="padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: left;">√𝑑𝑑</p><p class="s74" style="padding-left: 17pt;text-indent: 0pt;line-height: 10pt;text-align: left;">𝐿𝐿<span class="s128">𝑘𝑘</span></p><p class="s74" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">√𝑑𝑑</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 27pt;text-indent: 0pt;line-height: 93%;text-align: justify;">This still doesn’t absolve us of the quadratic calculation of the dot product of all query-key pairs. But the authors further prove that to approximate this measure of divergence, we only need to randomly sample <span class="s279">𝑈𝑈 </span><span class="s50">= 𝐿𝐿</span><span class="s54">𝐾𝐾 </span><span class="s279">𝑙𝑙𝑙𝑙𝐿𝐿</span><span class="s54">𝑄𝑄 </span>query-key pairs, where <span class="s150">𝐿𝐿</span><span class="s285">𝑄𝑄 </span>is the length of the query and <span class="s150">𝐿𝐿</span><span class="s253">𝐾𝐾 </span>is the length of the keys. We only calculate the dot product on these sampled pairs an<u>d </u>fill zero for t<u>h</u>e rest of it. Furthermore, we select a sparse <i>Top-u </i>from the calculated probabilities as <span class="s74">𝑄𝑄</span>. It is on this <span class="s146">𝑄𝑄</span>, for which we already have the dot products, we calculate the attention distribution. This considerably reduces the computational load on the self-attention calculation.</p><p class="s24" style="padding-top: 9pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">Attention distillation</p><p style="padding-top: 9pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;">One of the consequences of using ProbSparse attention is that we end up with redundant combinations of values. This is mainly because we might keep sampling the same dominant query-key pairs. The authors propose using a distilling operation to privilege the superior ones with dominating features and make the self-attention feature maps more focused layer after layer. They do this by using a mechanism similar</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark653">to dilated convolutions. The attention output from each layer is passed through a </a><span class="s20">Conv1d </span>filter with <span class="s20">kernel = 3 </span>on the time dimension, an activation (the paper suggests <span class="s20">ELU</span>), and a <span class="s20">MaxPool1d </span>with <span class="s20">kernel = 3 </span>and <span class="s20">stride = 2</span>. More formally, the output of a layer j+1 is as follows:<a name="bookmark620">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s137" style="padding-top: 5pt;padding-left: 131pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑋𝑋𝑡𝑡     = 𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀 (𝐸𝐸𝐸𝐸𝐸𝐸 (𝐶𝐶𝑀𝑀𝐶𝐶𝐶𝐶1𝑑𝑑 ([𝑋𝑋𝑡𝑡]</p><p class="s137" style="padding-top: 5pt;padding-left: 9pt;text-indent: 0pt;line-height: 7pt;text-align: left;">)))</p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: right;">𝑗𝑗+1</p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: right;">𝑗𝑗</p><p class="s43" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">𝐴𝐴𝐴𝐴</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <span class="s143">[. ]</span><span class="s127">𝐴𝐴𝐴𝐴 </span>represents the attention block.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Generative-style decoder</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The standard way of inferencing a Transformer model is by decoding one token at a time. This autoregressive process is time-consuming and repeats a lot of calculations for each step. To alleviate this problem, the Informer model adopts a more generative fashion where the entire forecasting horizon is generated in a single forward pass.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In NLP, it is a popular technique to use a special token (START) to start the dynamic decoding process. Instead of choosing a special token for this purpose, the Informer model chooses a sample from the input sequence, such as an earlier slice before the output window. For instance, if we say the input window is</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 89%;text-align: justify;"><span class="s80">𝑡𝑡</span><span class="s82">1</span><span class="s50"> </span>to <span class="s41">𝑡𝑡𝑊𝑊</span>, we will sample a sequence of length <span class="s41">𝐶𝐶 </span>from the input, <span class="s41">𝑡𝑡𝑊𝑊−𝐶𝐶 </span>to <span class="s41">𝑡𝑡</span><span class="s88">𝑊</span><span class="s65">𝑊</span>, and include this sequence as the starting sequence of the decoder. And to make the model predict the entire horizon in a single forward pass, we can extend the decoder input tensor so that its length is <span class="s41">𝐶𝐶 + 𝐻𝐻</span>, where <span class="s41">𝐻𝐻 </span>is the length of the prediction horizon. The initial C tokens are filled with the sample sequence from the input, and the</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 6pt;text-align: justify;">rest is filled as zeros – that is, <span class="s146">𝑋𝑋𝑡𝑡    = 𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶(𝑋𝑋𝑡𝑡        , 𝑋𝑋𝑡𝑡)</span>. This is just the target. Although <span class="s41">𝑋𝑋𝑡𝑡 </span>has zeros</p><p class="s75" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝑑𝑑𝑑𝑑</p><p class="s75" style="padding-left: 51pt;text-indent: 0pt;line-height: 6pt;text-align: left;">𝑡𝑡𝑡𝑡𝑡𝑡𝑑𝑑𝑡𝑡 0 <span class="s43">0</span></p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: left;">0</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">filled in for the prediction horizon, this is just for the target. The other information, such as the global timestamps, is included in <span class="s137">𝑋𝑋𝑡𝑡</span>. Sufficient masking of the attention matrix is also employed so that each position does not attend to future positions, thus maintaining the autoregressive nature of the prediction.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Forecasting with the Informer model</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Unfortunately, the <span class="s5">Informer </span>model has not been implemented in PyTorch Forecasting. However, we have adapted the original implementation by the authors of the paper so that it can work with PyTorch Forecasting; it can be found in <span class="s20">src/dl/ptf_models.py </span>in a class named <span class="s20">InformerModel</span><a href="#bookmark561" class="s140">. We can use the same framework we worked with in </a><i>Chapter 15</i>, <i>Strategies for Global Deep Learning Forecasting Models</i>, with this implementation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="106" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_827.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">We have to keep in mind that the Informer model does not support exogenous variables. The only additional information it officially supports is global timestamp information such as the week, month, and so on, along with holiday information. We can technically extend this to use any categorical feature (static or dynamic), but no real-valued information is currently supported.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Let’s look at the initialization parameters of the implementation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark654">The </a><span class="s20">InformerModel </span>class has the following major parameters:<a name="bookmark621">&zwnj;</a></p><ul id="l163"><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">label_len<span class="p">: This is an integer representing the number of timesteps from the input sequence to sample as a START token while decoding.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">distil<span class="p">: This is a Boolean flag for turning the attention distillation off and on.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">e_layers<span class="p">: This is an integer representing the number of encoder layers.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">d_layers<span class="p">: This is an integer representing the number of decoder layers.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">n_heads<span class="p">: This is an integer representing the number of attention heads.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">d_ff<span class="p">: This is an integer representing the number of kernels in the one-dimensional convolutional layers used in the encoder and decoder layers.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">activation<span class="p">: This is a string that takes in one of two values – </span>relu <span class="p">or </span>gelu<span class="p">. This is the activation to be used in the encoder and decoder layers.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">factor<span class="p">: This is a float value that controls the sparsity of the attention calculation. For a value less than 1, it reduces the number of query-value pairs to calculate the divergence measure and reduces the number of Top-u samples taken than the standard formula for these quantities.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">dropout<span class="p">: This is a float between 0 and 1, which determines the strength of the dropout in the network.</span></p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="154" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_828.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The complete code for training the Informer model can be found in the <span class="s20">03-Informer. ipynb </span>notebook in the <span class="s20">Chapter16 </span>folder. There are two variables in the notebook that act as switches – <span class="s20">TRAIN_SUBSAMPLE = True </span>makes the notebook run for a subset of 10 households, while <span class="s20">train_model = True </span>makes the notebook train different models (warning: training the model on the full data takes hours). <span class="s20">train_model = False </span>loads the trained model weights (not included in the repository but saved every time you run training) and predicts on them.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at another modification of the Transformer architecture that uses autocorrelation and time series decomposition more effectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Autoformer</p><p class="s5" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Autoformer <span class="p">is another model that is designed for long-term forecasting. While the Informer model focuses on making the attention computation more efficient, Autoformer invents a new kind of attention and couples it with aspects from time series decomposition.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark655">The architecture of the Autoformer model</a><a name="bookmark622">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Autoformer has a lot of similarities with the Informer model, so much so that it can be thought of as an extension of the Informer model. Uniform Input Representation and the generative-style decoder have been reused in Autoformer. But instead of ProbSparse attention, Autoformer has an AutoCorrelation mechanism. And instead of attention distillation, Autoformer has a time series decomposition-inspired encoder-decoder setup.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_829.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Wu et al. on Autoformer is cited in the <i>References </i>section as <i>9</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s look at the time series decomposition architecture first.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Decomposition architecture</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We saw this idea of decomposition back in <i>Chapter 3, Analyzing and Visualizing Time Series Data</i>, and even in this chapter (N-BEATS). Autoformer successfully renovated the Transformer architecture into a deep-decomposition architecture:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="528" height="205" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_830.jpg"/></span></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 16.5 – Autoformer architecture</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">It is easier to understand the overall architecture first and then dive deeper into the details. In <i>Figure 16.5</i>, there are boxes labeled <span class="s5">Auto-Correlation </span>and <span class="s5">Series Decomp</span>. For now, just know that auto-correlation is a type of attention and that series decomposition is a particular block that decomposes the signal into trend-cyclical and seasonal components.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s233" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark656">Encoder</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">With the level of abstraction discussed in the preceding section, let’s understand what is happening in the encoder:</p><p style="padding-top: 9pt;padding-left: 55pt;text-indent: -18pt;line-height: 84%;text-align: left;">1. The uniform representation of the time series, <span class="s41">𝒳𝒳</span><span class="s42">𝑒𝑒𝑒𝑒</span>, is the input to the encoder. The input is passed through an <span class="s5">Auto-Correlation </span>block (for self-attention) whose output is <span class="s41">𝒳𝒳</span><span class="s42">𝑎𝑎𝑎𝑎</span>.</p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">2. The uniform representation, <span class="s146">𝒳𝒳</span><span class="s79">𝑒𝑒𝑒𝑒</span>, is added back to <span class="s50">𝒳𝒳𝑎𝑎𝑎𝑎 </span>as a residual connection, <span class="s43">𝒳𝒳𝑎𝑎𝑎𝑎 = 𝒳𝒳𝑎𝑎𝑎𝑎 + 𝒳𝒳𝑒𝑒𝑒𝑒</span>..</p><ol id="l164"><li><p style="padding-top: 2pt;padding-left: 55pt;text-indent: -18pt;line-height: 94%;text-align: left;">Now, <span class="s41">𝒳𝒳𝑎𝑎𝑎𝑎 </span>is passed through a <span class="s5">Series Decomp </span>block, which decomposes the signal into a trend-cyclical component (<span class="s137">𝒳𝒳</span><span class="s42">𝑇</span><span class="s43">𝑇</span>) and a seasonal component, <span class="s41">𝒳𝒳𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠</span>.</p></li><li><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 119%;text-align: left;">We discard <span class="s41">𝒳𝒳𝑇𝑇 </span>and pass <span class="s41">𝒳𝒳𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 </span>to a <span class="s5">Feed Forward </span>network, which gives <span class="s252">𝒳𝒳</span><span class="s253">𝐹𝐹𝐹𝐹 </span>as an output. 5. <span class="s41">𝒳𝒳𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 </span>is again added to <span class="s137">𝒳𝒳𝐹𝐹𝐹𝐹 </span>as a residual connection, <span class="s137">𝒳𝒳𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 = 𝒳𝒳𝐹𝐹𝐹𝐹 + 𝒳𝒳𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠</span>.</p></li></ol><ol id="l165"><li><p style="padding-top: 1pt;padding-left: 55pt;text-indent: -18pt;line-height: 89%;text-align: left;">Finally, this <span class="s41">𝒳𝒳𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 </span>is passed through another <span class="s5">Series Decomp </span>layer, which again decomposes the signal into the trend, <span class="s252">𝒳𝒳</span><span class="s253">𝑇𝑇</span><span class="s43">̃ </span>, and a seasonal component, <span class="s41">𝒳𝒳𝑠𝑠̃𝑠𝑠𝑠𝑠𝑠𝑠</span>.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">We discard <span class="s41">𝒳𝒳</span><span class="s42">𝑇𝑇</span><span class="s253">̃</span>, and pass on <span class="s41">𝒳𝒳𝑠𝑠̃𝑠𝑠𝑠𝑠𝑠𝑠 </span>as the final output from one block of the encoder.</p></li><li><p style="padding-top: 2pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">There may be <i>N </i>blocks of encoders stacked together, one taking in the output of the previous encoder as input.</p></li></ol><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s shift our attention to the decoder block.</p><p class="s233" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Decoder</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Like the Informer model, the Autoformer model uses a START token-like mechanism by including a sampled window from the input sequence. But instead of just taking the sequence, Autoformer does a bit of special processing on it. Autoformer uses the bulk of its learning power to learn seasonality. The output of the transformer is also just the seasonality. Therefore, instead of including the complete window from the input sequence, Autoformer decomposes the signal and only includes the seasonal component in the START token. Let’s look at this process step by step:</p><ol id="l166"><li><p style="padding-top: 7pt;padding-left: 55pt;text-indent: -18pt;line-height: 13pt;text-align: left;">If the input (the context window) is <span class="s137">𝒳𝒳</span>, we decompose it with the <span class="s5">Series Decomp </span>block into</p><p class="s43" style="padding-left: 55pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="s299">𝒳𝒳</span>𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖<span class="s300">and </span><span class="s41">𝒳𝒳</span>𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖 <span class="s300">.</span></p><p class="s43" style="padding-left: 64pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝑇𝑇 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠</p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 1pt;padding-left: 55pt;text-indent: -18pt;line-height: 14pt;text-align: left;">Now, we sample <span class="s41">𝐶𝐶 </span>timesteps from the end of <span class="s41">𝒳𝒳𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖 </span>and append <span class="s41">𝐻𝐻 </span>zeros, where <span class="s41">𝐻𝐻 </span>is the</p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 13pt;text-align: left;">forecast horizon, and construct <span class="s41">𝒳𝒳𝑑𝑑𝑑𝑑</span>.</p></li><li><p style="padding-top: 2pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">This <span class="s41">𝒳𝒳𝑑𝑑𝑑𝑑 </span>is then used to create a uniform representation, <span class="s41">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑</span>.</p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑇𝑇</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;line-height: 92%;text-align: left;">Meanwhile, we sample <span class="s157">𝐶𝐶 </span>timesteps from the end of <span class="s252">𝒳</span><span class="s137">𝒳𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖 </span>and append <span class="s41">𝐻𝐻 </span>timesteps with the series mean (<span class="s146">𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚</span><span class="s109">(</span><span class="s146">𝒳𝒳</span><span class="s109">)</span>), where <span class="s41">𝐻𝐻 </span>is the forecast horizon, and construct <span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑</span>.</p></li></ol><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">This <span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑 </span>is then used as the input for the decoder. This is what happens in the decoder:</p><ol id="l167"><li><p style="padding-top: 9pt;padding-left: 55pt;text-indent: -18pt;line-height: 89%;text-align: left;">The input, <span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑 </span>, is first passed through an <span class="s5">Auto-Correlation </span>(for self-attention) block whose output is <span class="s41">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s138" style="padding-top: 4pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a name="bookmark657">2. The uniform representation, </a><span class="s65">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑 </span>, is added back to <span class="s278">𝒳𝒳</span><span class="s54">𝑑𝑑𝑑𝑑𝑑𝑑 </span>as a residual connection, <span class="s277">𝒳𝒳</span><span class="s231">𝑑𝑑𝑑𝑑𝑑𝑑 </span><span class="s277">= </span><span class="s75">𝒳𝒳</span><span class="s231">𝑑𝑑𝑑𝑑𝑑𝑑 </span><span class="s277">+ </span><span class="s75">𝒳𝒳</span><span class="s231">𝑑𝑑𝑑𝑑</span>.<span class="s231">𝑑𝑑</span>.</p><ol id="l168"><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;line-height: 93%;text-align: justify;">Now, <span class="s41">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑 </span>is passed through a <span class="s5">Series Decomp </span>block that decomposes the signal into a trend- cyclical component (<span class="s41">𝒳𝒳</span><span class="s42">𝑑𝑑𝑑</span><span class="s43">𝑑1</span>) and a seasonal component, <span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑</span>.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;line-height: 93%;text-align: justify;">In the decoder, we do not discard the trend component; instead, we save it. This is because we will be adding all the trend components with the trend in it (<span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑</span>) to come up with the overall trend part (<span class="s90">𝒯𝒯</span>).</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -18pt;line-height: 87%;text-align: justify;">The seasonal output from the <span class="s5">Series Decomp </span>block (<span class="s137">𝒳𝒳</span><span class="s42">𝑑</span><span class="s43">𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑</span><span class="s137">)</span>, along with the output from the <span class="s138">e</span>ncoder (<span class="s41">𝒳𝒳</span><span class="s42">𝑠𝑠̃𝑠</span><span class="s43">𝑠𝑠𝑠𝑠𝑠</span><span class="s138">),</span> is then passed into another <span class="s5">Auto-Correlation </span>block where cross-attention between the decoder sequence and encoder sequence is calculated. Let the output of this block <span class="s301">b</span>e <span class="s41">𝒳𝒳</span><span class="s43">𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐</span><span class="s301">.</span></p></li></ol></li></ol><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;">6. Now, <span class="s74">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑 </span>is added back to <span class="s41">𝒳𝒳𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 </span>as a residual connection, <span class="s146">𝒳𝒳</span><span class="s79">𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 </span><span class="s146">= 𝒳𝒳</span><span class="s79">𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 </span><span class="s146">+ 𝒳𝒳</span><span class="s79">𝑑𝑑𝑐𝑐𝑑𝑑𝑑𝑑𝑐𝑐</span>.</p><p style="padding-top: 2pt;padding-left: 64pt;text-indent: -18pt;line-height: 86%;text-align: left;">7.    <span class="s41">𝒳𝒳𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐  </span>is again passed through a <span class="s5">Series Decomp </span>block, which splits <span class="s157">𝒳𝒳</span><span class="s88">𝑐𝑐𝑐</span><span class="s65">𝑐𝑐𝑐𝑐𝑐𝑐𝑐  </span>into two components – <span class="s86">𝒳</span><span class="s137">𝒳</span><span class="s238">𝑑𝑑𝑑</span><span class="s43">𝑑2 </span>and <span class="s86">𝒳</span><span class="s137">𝒳</span><span class="s241">𝑑</span><span class="s43">𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑2</span>.</p><p style="padding-top: 2pt;padding-left: 64pt;text-indent: -18pt;line-height: 87%;text-align: left;">8.     <span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑 </span>is then transformed using a <span class="s5">Feed Forward </span>network into <span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑 </span>and <span class="s137">𝒳𝒳</span><span class="s42">𝑑</span><span class="s43">𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑 </span>is added to it in a residual connection, <span class="s44">𝒳</span><span class="s43">𝒳</span><span class="s64">𝑑𝑑𝑑𝑑𝑑𝑑   </span><span class="s44">=</span><span class="s43">  𝒳𝒳</span><span class="s64">𝑑𝑑𝑑𝑑𝑑𝑑   </span><span class="s44">+</span><span class="s43">  𝒳𝒳</span><span class="s64">𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑</span>.</p><ol id="l169"><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;line-height: 89%;text-align: justify;">Finally, <span class="s150">𝒳</span><span class="s41">𝒳𝑑𝑑𝑑𝑑𝑑𝑑 </span>is passed through yet another <span class="s5">Series Decomp </span>block, which decomposes it into two components – <span class="s137">𝒳𝒳</span><span class="s42">𝑑𝑑𝑑</span><span class="s43">𝑑3  </span>and <span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑3</span>. <span class="s137">𝒳𝒳</span><span class="s42">𝑑</span><span class="s43">𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑3  </span>is the final output of the decoder, which captures seasonality.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;line-height: 91%;text-align: justify;"><span class="s138">Another</span> output is the residual trend, <span class="s137">𝑋𝑋</span><span class="s253">𝑡𝑡𝑟𝑟</span><span class="s43">̃</span><span class="s253">𝑟𝑟𝑟𝑟𝑟𝑟</span><span class="s138">,</span> which is a projection of the summation of all the trend components extracted in the decoder’s <span class="s5">Series Decomp </span>blocks. The projection layer is a <span class="s5">Conv1d </span>layer, which projects the extracted trend to the desired output dimension:</p><p class="s43" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;"><span class="s41">𝑋𝑋</span><span class="s241">𝑡</span>𝑡𝑡𝑡<span class="s253">̃</span><span class="s241">𝑡</span>𝑡𝑡𝑡𝑑𝑑  <span class="s41">= 𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶1𝑑𝑑(𝒳𝒳</span><span class="s42">𝑑𝑑</span>𝑑𝑑1  <span class="s41">+ 𝒳𝒳</span><span class="s42">𝑑𝑑</span>𝑑𝑑2  <span class="s41">+ 𝒳𝒳</span><span class="s42">𝑑𝑑</span>𝑑𝑑3<span class="s41">)</span></p></li><li><p class="s41" style="padding-top: 12pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">𝑀𝑀 <span class="p">such decoder layers are stacked on top of each other, each one feeding its output as the input to the next one.</span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;line-height: 91%;text-align: left;">The residual trend, <span class="s252">𝒳</span><span class="s137">𝒳</span><span class="s253">𝑡𝑡</span><span class="s43">𝑡𝑡̃</span><span class="s253">𝑡</span><span class="s43">𝑡𝑡𝑡𝑡𝑡</span>, of each decoder layer gets added to the trend init, <span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑</span>, to model the overall trend component (<span class="s74">𝒯𝒯</span>).</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;line-height: 92%;text-align: left;">The <span class="s137">𝒳𝒳𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑3 </span>of the final decoder layer is considered to be the overall seasonality component and is projected to the desired output dimension (<span class="s137">𝒮𝒮</span>) using a linear layer.</p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;text-align: left;">Finally, the prediction or the forecast <span class="s137">𝑋𝑋𝑜𝑜𝑜𝑜𝑜𝑜 = 𝒯𝒯 + 𝒮𝒮</span>.</p></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The whole architecture is cleverly designed so that the relatively stable and easy-to-predict part of the time series (the trend-cyclical) is removed and the difficult-to-capture seasonality can be modeled well.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, how does the <span class="s5">Series Decomp </span>block decompose the series? The mechanism may be familiar to you already: <span class="s20">AvgPool1d </span>with some padding so that it maintains the same size as the input. This acts like a moving average over the specified kernel width.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have been talking about the <span class="s5">Auto-Correlation </span>block throughout this explanation. Now, let’s understand the ingenuity of the <span class="s5">Auto-Correlation </span>block.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark658">Auto-correlation mechanism</a></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Autoformer uses an auto-correlation mechanism in place of standard scaled dot product attention. This discovers sub-series similarity based on periodicity and uses this similarity to aggregate similar sub-series. This clever mechanism breaks the information bottleneck by expanding the point-wise operation of the scaled dot product attention to a sub-series level operation. The initial part of the overall mechanism is similar to the standard attention procedure, where we project the query, key, and values into the same dimension using weight matrices. The key difference is the attention weight calculation and how they are used to calculate the values. This mechanism achieves this by using two salient sub-mechanisms: discovering period-based dependencies and time delay aggregation.</p><p class="s233" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Period-based dependencies</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Autoformer uses autocorrelation as the key measure of similarity. Auto-correlation, as we know, represents the similarity between a given time series, <span class="s255">𝑋</span><span class="s137">𝑋</span><span class="s43">𝑡𝑡</span>, and its lagged series. For instance, <span class="s137">ℛ𝑥𝑥𝑥𝑥</span><span class="s252">(</span><span class="s137">𝜏𝜏</span><span class="s252">) </span>is the autocorrelation between the time series <span class="s252">𝑋</span><span class="s137">𝑋</span><span class="s253">𝑡𝑡</span><span class="s43"> </span>and <span class="s252">𝑋</span><span class="s137">𝑋</span><span class="s253">𝑡</span><span class="s43">𝑡−𝜏𝜏</span>. Autoformer considers this autocorrelation as the unnormalized confidence of the particular lag. Therefore, from the list of all <span class="s137">𝜏𝜏</span>, we choose <span class="s137">𝑘𝑘 </span>most possible lags and use <i>softmax </i>to convert these unnormalized confidences into probabilities. We use these probabilities as weights to aggregate relevant sub-series (we will talk about this in the next section).</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The autocorrelation calculation is not the most efficient operation and Autoformer suggests an alternative to make the calculation faster. Based on the <span class="s5">Wiener–Khinchin theorem </span>in <span class="s5">Stochastic Processes </span>(this is outside the scope of the book, but for those who are interested, I have included a link in the <i>Further reading </i>section), autocorrelation can also be calculated using <span class="s5">Fast Fourier Transforms </span>(<span class="s5">FFTs</span>). The process can be seen as follows:</p><p class="s248" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝒮𝒮<span class="s246">𝑥𝑥𝑥𝑥</span>(𝜏𝜏) = ℱ(𝑋𝑋<span class="s246">𝑡𝑡</span>)ℱ∗(𝑋𝑋<span class="s246">𝑡𝑡</span>)</p><p style="padding-top: 13pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, <span class="s137">ℱ </span>denotes the FFT and <span class="s41">ℱ∗ </span>denotes the conjugate operation (the conjugate of a complex number is the number with the same real part and an imaginary part, which is equal in magnitude but with the sign reversed. The mathematics around this is outside the scope of this book). This can easily be written in PyTorch as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s38" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># calculating the FFT of Query and Key</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)</p><p class="s38" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;"># Multiplying the FFT of Query with Conjugate FFT of Key</p><p class="s28" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">res = q_fft * torch.conj(k_fft)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark659">Now, </a><span class="s137">𝒮𝒮𝑥𝑥𝑥𝑥(𝜏𝜏)</span>is in the spectral domain. To bring it back to the real domain, we need to do an inverse FFT:<a name="bookmark623">&zwnj;</a></p><p class="s239" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">ℛ<span class="s302">𝓍</span><span class="s74">𝓍𝓍𝓍</span>(𝜏𝜏) = ℱ−1(𝒮𝒮<span class="s302">𝓍</span><span class="s74">𝓍𝓍𝓍</span>(𝜏𝜏))</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Here, <span class="s41">ℱ−1 </span>denotes the inverse FFT. In PyTorch, we can do this easily:</p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;text-align: center;"><span class="s49" style=" background-color: #F3F2F1;">  corr = torch.fft.irfft(res, dim=-1)                             </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">When the query and key are the same, this calculates self-attention; when they are different, they calculate cross-attention.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, all we need to do is take the top-k values from <span class="s20">corr </span>and use them to aggregate the sub-series.</p><p class="s233" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Time delay aggregation</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have identified the major lags that are auto-correlated using the FFT and inverse-FFT. For a more concrete example, the dataset we have been working on (<i>London Smart Meter Dataset</i>) has a half-hourly frequency and has strong daily and weekly seasonality. Therefore, the auto-correlation identification may have picked out 48 and 48*7 as the two most important lags. In the standard attention mechanism, we use the calculated probability as weights to aggregate the value. Autoformer also does something similar, but instead of applying the weights to points, it applies them to sub-series.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Autoformer does this by shifting the time series by the lag, <span class="s137">𝜏𝜏</span>, and then using the lag’s weight to aggregate them:</p><p class="s127" style="padding-top: 2pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑘𝑘</p><p class="s90" style="padding-top: 1pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴 − 𝐶𝐶𝐴𝐴𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐴𝐴𝐶𝐶𝐴𝐴𝐶𝐶(𝑄𝑄, 𝐾𝐾, 𝑉𝑉) = ∑ 𝑅𝑅𝐴𝐴𝐶𝐶𝐶𝐶(𝑉𝑉, 𝜏𝜏<span class="s128">𝑖𝑖</span><span class="s127"> </span>)𝑅𝑅<span class="s143">̂</span><span class="s128">𝑄</span><span class="s127">𝑄,𝐾𝐾</span>(𝜏𝜏<span class="s128">𝑖𝑖</span><span class="s127"> </span>)</p><p class="s127" style="padding-top: 2pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑖𝑖=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Here, <span class="s137">𝑅𝑅</span><span class="s255">̂</span><span class="s42">𝑄</span><span class="s43">𝑄,𝐾𝐾</span><span class="s137">(𝜏𝜏</span><span class="s42">𝑖𝑖</span><span class="s43"> </span><span class="s137">) </span>is the <i>softmax</i>-ed probabilities on the <i>top-k </i>autocorrelations.</p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In our example, we can think of this as shifting the series by 48 timesteps so that the previous day’s timesteps are aligned with the current day and then using the weight of the 48 lag to scale it. Then, we can move on to the 48*7 lag and align the previous week’s timesteps with the current week, and then use the weight of the 48*7 lag to scale it. So, in the end, we will get a weighted mixture of the seasonality patterns that we can observe daily and weekly. Since these weights are learned by the model, we can hypothesize that different blocks learn to focus on different seasonalities, and thus as a whole, the blocks learn the overall pattern in the time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Forecasting with Autoformer</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Similar to the Informer model, the Autoformer model has also not been implemented in PyTorch Forecasting. However, we have adapted the original implementation by the authors of the paper so that it works with PyTorch Forecasting; this can be found in <span class="s20">src/dl/ptf_models.py </span>in a class named <span class="s20">AutoformerModel</span><a href="#bookmark561" class="s140">. We can use the same framework we worked with in </a><i>Chapter 15</i>, <i>Strategies for Global Deep Learning Forecasting Models</i>, with this implementation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="122" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_831.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">We have to keep in mind that the Autoformer model does not support exogenous variables. The only additional information it officially supports is global timestamp information such as the week, month, and so on, along with holiday information. We can technically extend this to any categorical feature (static or dynamic), but no real-valued information is currently supported. The Autoformer model is also more memory hungry, probably because of its sub-series aggregation.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark660">Let’s look at the initialization parameters of the implementation.</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The <span class="s20">AutoformerModel </span>class has the following major parameters:</p><ul id="l170"><li><p class="s20" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">label_len<span class="p">: This is an integer representing the number of timesteps from the input sequence to sample as a START token while decoding.</span></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><span class="s20">moving_avg</span>: This is an <i>odd </i>integer that determines the kernel size to be used in the <span class="s5">Series Decomp </span>block.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">e_layers<span class="p">: This is an integer representing the number of encoder layers.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">d_layers<span class="p">: This is an integer representing the number of decoder layers.</span></p></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">n_heads<span class="p">: This is an integer representing the number of attention heads.</span></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><span class="s20">d_ff</span>: This is an integer representing the number of kernels in the <span class="s5">Conv1d </span>layers used in the encoder and decoder layers.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">activation<span class="p">: This is a string that takes in one of two values – </span>relu <span class="p">or </span>gelu<span class="p">. This is the activation to be used in the encoder and decoder layers.</span></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;"><span class="s20">factor</span>: This is a float value that controls the top-k selection of autocorrelations. For a factor of 1, the top-k is calculated as <i>log(length of window)</i>. For a value less than 1, it selects a smaller top-k.</p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">dropout<span class="p">: This is a float between 0 and 1 that determines the strength of the dropout in the network.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="154" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_832.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The complete code for training the Autoformer model can be found in the <span class="s20">04-Autoformer. ipynb </span>notebook in the <span class="s20">Chapter16 </span>folder. There are two variables in the notebook that act as switches – <span class="s20">TRAIN_SUBSAMPLE = True </span>makes the notebook run for a subset of 10 households, while <span class="s20">train_model = True </span>makes the notebook train different models (warning: training the model on full data takes hours). <span class="s20">train_model = False </span>loads the trained model weights (not included in the repository but saved every time you run training) and predicts on them.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now, let’s look at one more, very successful, architecture that is well-designed to utilize all kinds of information in a global context.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark661">Temporal Fusion Transformer (TFT)</a><a name="bookmark625">&zwnj;</a><a name="bookmark624">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">TFT is a model that is thoughtfully designed from the ground up to make the most efficient use of all the different kinds of information in a global modeling context – static and dynamic variables. TFT also has interpretability at the heart of all design decisions. The result is a high-performing, interpretable, and global DL model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_833.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Lim et al. on TFT is cited in the <i>References </i>section as <i>10</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">At first glance, the model architecture looks complicated and daunting. But once you peel the onion, it is quite simple and ingenious. We will take this one level of abstraction at a time to ease you into the full model. Along the way, there will be many black boxes I’m going to ask you to take for granted, but don’t worry – we will open every one of them as we dive deeper.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The Architecture of TFT</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Let’s establish some notations and a setting before we start. We have a dataset with <span class="s137">𝐼𝐼 </span>unique time series and each entity, <span class="s137">𝑖𝑖</span>, has some static variables (<span class="s259">𝑠</span><span class="s157">𝑠</span><span class="s285">𝑖</span><span class="s65">𝑖</span>). The collection of all static variables of all entities can be represented by <span class="s41">𝒮𝒮</span>. We also have the context window of length <i>k</i>. Along with this, we have the time- varying variables, which have one distinction – for some variables, we do not have the future data (unknown), and for other variables, we know the future (known). Let’s denote all the time-varying information (the context window, known, and unknown time-varying variables) from the context window’s input, <span class="s252">𝒳</span><span class="s137">𝒳𝑡𝑡−𝑘𝑘 … 𝒳𝒳𝑡𝑡 </span>. The known time-varying variables for the future are denoted using</p><p class="s137" style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><span class="s303">𝑥</span>𝑥<span class="s43">𝑡𝑡+1 </span><span class="s303">…</span> 𝑥𝑥<span class="s43">𝑡𝑡+𝜏𝜏</span><span class="p">, where </span>𝜏𝜏 <span class="p">is the forecast horizon. With these notations, we are ready to look at the first</span></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">level of abstraction:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a name="bookmark662"><span><img width="513" height="450" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_834.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 16.6 – TFT – a high-level overview</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">There is a lot to unpack here. Let’s start with the static variables, <span class="s41">𝒮𝒮</span>. First, the static variables are passed through a <span class="s5">Variable Selection Network </span>(<span class="s5">VSN</span>). The VSN does instance-wise feature selection and performs some non-linear processing on the inputs. This processed input is fed into a bunch of <span class="s5">Static Covariate Encoders </span>(<span class="s5">SEs</span>). The SE block is designed to integrate the static metadata in a principled way.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">If you follow the arrows from the SE block in <i>Figure 16.6</i>, you will see that the static covariates are used in three (four distinct outputs) different places in the architecture. We will see how these are used in each of these places when we talk about them. But all these different places may be looking at different aspects of the static information. To allow the model this flexibility, the processed and variable- selected output is fed into four different <span class="s5">Gated Residual Networks </span>(<span class="s5">GRNs</span>), which, in turn, generate</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">four outputs – <span class="s304">𝑐</span><span class="s157">𝑐</span><span class="s65">𝑠𝑠</span>, <span class="s83">𝑐</span><span class="s80">𝑐</span><span class="s50">𝑒𝑒</span>, <span class="s83">𝑐</span><span class="s80">𝑐</span><span class="s50">𝑐𝑐</span>, and <span class="s304">𝑐</span><span class="s157">𝑐</span><span class="s65">ℎ</span>. We will explain what a GRN is later, but for now, just understand</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">that it is a block capable of non-linear processing, along with a residual connection, which enables it to bypass the non-linear processing if needed.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a name="bookmark663">The past inputs, </a><span class="s74">𝒳𝒳𝑡𝑡−𝑘𝑘 … 𝒳𝒳𝑡𝑡</span>, and the future known inputs, <span class="s255">𝑥</span><span class="s137">𝑥</span><span class="s43">𝑡𝑡+1 </span><span class="s255">…</span><span class="s137"> 𝑥𝑥</span><span class="s43">𝑡𝑡+𝜏𝜏</span>, are also passed through separate VSNs and these processed outputs are fed into a <span class="s5">Locality Enhancement </span>(<span class="s5">LE</span>) Seq2Seq layer. We can think of LE as a way to encode the local context and temporal ordering into the embeddings of each timestep. This is similar to the positional embeddings in vanilla Transformers. We can also see similar attempts in the <span class="s5">Conv1d </span>layers that were used to encode the history in the uniform representation in the Informer and Autoformer models. We will see what is happening inside the LE later, but for now, just understand it captures the local context conditioned on other observed variables and static information. Let’s call the output of the block <span class="s5">Locality Encoded Context Vectors </span>(<span class="s137">𝜙𝜙𝑡𝑡−𝑘𝑘 … 𝜙𝜙𝜙𝜙</span>, and <span class="s252">𝜙</span><span class="s137">𝜙𝑡𝑡+1 … 𝜙𝜙𝑡𝑡+𝜏𝜏</span>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="74" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_835.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The terminology, notation, and grouping of major blocks are not the same as in the original paper. I have changed these to make them more accessible and understandable.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 36pt;text-indent: 0pt;line-height: 93%;text-align: justify;">Now, these LE context vectors are fed into a <span class="s5">Temporal Fusion Decoder </span>(<span class="s5">TD</span>). The TD applies a slight variation of multi-headed self-attention in a Transfomer model-like manner and produces the <span class="s5">Decoded Representation </span>(<span class="s252">ψ</span><span class="s137">𝑡𝑡−𝑘𝑘 … ψ𝑡𝑡 + τ </span>). Finally, this Decoded Representation is passed through a <span class="s5">Gated Linear Unit </span>(<span class="s5">GLU</span>) and an <span class="s5">Add and Norm </span>block that adds the LE context vectors as a residual connection.</p><p style="padding-top: 7pt;padding-left: 36pt;text-indent: 0pt;text-align: justify;">A GLU is a unit that helps the model decide how much information it needs to allow to flow through. We can think of it as a learned information throttle that is widely used in <span class="s5">Natural Language Processing </span>(<span class="s5">NLP</span>) architectures. The formula is really simple:</p><p class="s157" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝐺𝐺𝐺𝐺𝐺𝐺(𝑋𝑋) = (𝑋𝑋 ∗ 𝑊𝑊 + 𝑏𝑏) ⊗ σ(𝑋𝑋 ∗ 𝑉𝑉 + 𝑐𝑐)</p><p style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Here, <span class="s157">𝑊𝑊 </span>and <span class="s157">𝑉𝑉 </span>are learnable weight matrices, <span class="s41">𝑏𝑏 </span>and are learnable biases, is an activation function, and <span class="s137">⊗ </span>is the Hadamard product operator (element-wise multiplication).</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The <span class="s5">Add &amp; Norm </span><a href="#bookmark522" class="s140">block is the same as in the vanilla Transformer; we discussed this back in </a><i>Chapter 14</i>, <i>Attention and Transformers for Time Series</i>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, to top it all off, we have a <span class="s5">Dense </span>layer (linear layer with bias) that projects the output of the <span class="s5">Add &amp; Norm </span>block to the desired output dimensions.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">And with that, it is time for us to step one level down in our abstraction.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Locality Enhancement Seq2Seq layer</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s peel back the onion and see what’s happening inside the LE Seq2Seq layer. Let’s start with a figure:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><a name="bookmark664"><span><img width="522" height="324" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_836.jpg"/></span></a></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 16.7 – TFT – LE Seq2Seq layer</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 72%;text-align: justify;">The LE uses a Seq2Seq architecture to capture the local context. The process starts with the processed past inputs. The LSTM encoder takes in these past inputs, <span class="s137">𝒳𝒳𝑡𝑡−𝑘𝑘 … 𝒳𝒳𝑡𝑡</span>. <span class="s304">𝑐</span><span class="s157">𝑐</span><span class="s65">ℎ</span>; <span class="s83">𝑐</span><span class="s80">𝑐</span><span class="s50">𝑐𝑐 </span>from the static covariate</p><p class="s303" style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 78%;text-align: justify;"><span class="p">encoder acts as the initial hidden states of the LSTM. The encoder processes each timestep at a time, producing hidden states at each time step, </span>ℋ<span class="s43">𝑡𝑡−𝑘𝑘 </span>…<span class="s137"> ℋ</span><span class="s43">𝑡𝑡 </span><span class="p">. The last hidden states (context vector) are now passed on to the LSTM decoder, which processes the known future inputs, </span>𝑥<span class="s137">𝑥</span><span class="s43">𝑡𝑡+1 </span>…<span class="s137"> 𝑥𝑥</span><span class="s43">𝑡𝑡+𝜏𝜏 </span><span class="p">, and produces the hidden states at each of the future timesteps, </span>ℋ<span class="s43">𝑡𝑡+1 </span>…<span class="s137"> ℋ</span><span class="s43">𝑡𝑡+τ</span><span class="p">. Finally, all these hidden</span></p><p class="s303" style="padding-left: 28pt;text-indent: 0pt;line-height: 78%;text-align: justify;"><span class="p">states are passed through a </span><span class="s5">GLU + AddNorm </span><span class="p">block with the residual connection from before the LSTM processing. The outputs are the LE context vectors (</span>𝜙<span class="s137">𝜙</span><span class="s43">𝑡𝑡−𝑘𝑘 </span>…<span class="s137"> 𝜙𝜙𝜙𝜙</span><span class="p">, and </span>𝜙<span class="s137">𝜙</span><span class="s43">𝑡𝑡+1 </span>…<span class="s137"> 𝜙𝜙</span><span class="s43">𝑡𝑡+𝜏𝜏</span>)<span class="p">.</span></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at the next block: the TFD.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark665">Temporal fusion decoder</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s start this discussion with another figure:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: left;"><span><img width="469" height="362" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_837.jpg"/></span></p><p class="s37" style="padding-top: 11pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 16.8 – Temporal Fusion Transformer – Temporal Fusion Decoder</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The LE context vectors from both the past input and known future input are concatenated into a single LE context vector. Now, this can be thought of as the position-encoded tokens in the Transformer</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">paradigm. The first thing the TFD does is enrich these encodings with static information, <span class="s304">𝑐𝑐</span><span class="s65">𝑒𝑒</span>, that was</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">created from the static covariate encoder. This was concatenated with the embeddings. A position- wise GRN is used to enrich the embeddings. These enriched embeddings are now used as the query, key, and values for the <span class="s5">Masked Interpretable Multi-Head Attention </span>block.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The paper posits that the <span class="s5">Masked Interpretable Multi-Head Attention </span>block learns long-term dependencies across time steps. The local dependencies are already captured by the LE Seq2Seq layer in the embeddings, but the point-wise long-term dependencies are captured by <span class="s5">Masked Interpretable Multi-Head Attention</span>. This block also enhances the interpretability of the architecture. The attention weights that are generated in the process give us some indication of the major timesteps involved in the process. But the multi-head attention has one drawback from the interpretability perspective. In vanilla multi-head attention, we use separate projection weights for the values, which means that the values for each head are different and hence the attention weights are not straightforward to interpret.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark666">TFT gets over this limitation by employing a </a><i>single shared weight matrix </i><a href="#bookmark522" class="s140">for projecting the values into the attention dimension. Even with the shared value projection weights, because of the individual query and key projection weights, each head can learn different temporal patterns. In addition to this, TFT also employs masking to make sure information from the future is not used in operations. We discussed this type of causal masking in </a><i>Chapter 14</i>, <i>Attention and Transformers for Time Series</i>. With these two modifications, TFT names this layer <span class="s5">Masked Interpretable Multi-Head Attention</span>.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">And with that, it’s time to open the last and most granular level of abstraction we have been using.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Gated Residual Network</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We have been talking about GRNs for some time now; so far, we have just taken them at face value. Let’s understand what is happening inside a GRN – one of the most basic building blocks of a TFT.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s look at a schematic diagram of a GRN to understand it better:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><img width="528" height="239" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_838.png"/></span></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 16.9 – TFT – GRN (left) and VSN (right)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The GRN takes in two inputs: the primary input, , and the external context, . The context, , is an optional input and is treated as zero if it’s not present. First, both the inputs, and , are transformed by separate dense layers and a subsequent activation function – the <span class="s5">Exponential Linear Unit </span>(<span class="s5">ELU</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.ELU.html" class="s140" target="_blank">) (</a><span class="s27">https://pytorch.org/docs/stable/generated/torch.nn.ELU.html</span>).</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;">Now, the transformed and inputs are added together and then transformed again using another <span class="s5">Dense </span>layer. Finally, this is passed through a <span class="s5">GLU+Add &amp; Norm </span>layer with residual connections from the original <span class="s157">𝑎𝑎</span>. This structure bakes in enough non-linearity to learn complex interactions between the inputs, but at the same time lets the model ignore those non-linearities through a residual connection. Therefore, such a block allows the model to scale the computation required up or down based on the data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark667">Variable Selection Network</a><a name="bookmark626">&zwnj;</a></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The last building block of the TFT is the VSN. VSNs enable TFT to do instance-wise variable selection. Most real-world time series datasets have many variables that do not have a lot of predictive power, so being able to select the ones that do have predictive power automatically will help the model pick out relevant patterns. <i>Figure 16.9 </i>(right) shows this VSN.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 90%;text-align: justify;">These additional variables can be categorical or continuous. TFT uses Entity Embeddings to convert the categorical features into numerical vectors of the dimension that we desire (<span class="s41">𝑑𝑑</span><span class="s42">𝑚𝑚𝑚</span><span class="s43">𝑚𝑚𝑚𝑚𝑚𝑚𝑚</span><a href="#bookmark561" class="s140">). We talked about this in </a><a href="#bookmark561" class="s21">Chapter </a><i>15</i>, <i>Strategies for Global Deep Learning Forecasting Models</i>. The continuous features are linearly transformed (independently) into the same dimension, <span class="s41">𝑑𝑑</span><span class="s42">𝑚𝑚𝑚</span><span class="s43">𝑚𝑚𝑚𝑚𝑚𝑚𝑚</span>. This gives us the Transformed</p><p class="s138" style="padding-left: 37pt;text-indent: 0pt;line-height: 6pt;text-align: justify;">inputs, <span class="s41">ξ(1) … ξ(𝑚𝑚)</span>, where <span class="s137">𝑚𝑚 </span><span class="p">is the number of features and </span><span class="s259">𝑡𝑡 </span>is the timestep. We can concatenate all</p><p class="s43" style="padding-left: 70pt;text-indent: 0pt;line-height: 5pt;text-align: left;">𝑡𝑡 𝑡𝑡</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">these embeddings (flatten them) and that flattened representation can be represented as <span class="s157">Ξ𝑡𝑡</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Now, there are two parallel streams in which these embeddings are processed – one for non-linear processing of the embeddings and another to do feature selection. Each of these embeddings is processed by separate</p><p class="s65" style="text-indent: 0pt;line-height: 83%;text-align: left;">ξ<span class="s305">𝑡𝑡</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s65" style="text-indent: 0pt;line-height: 83%;text-align: left;">… ξ<span class="s305">𝑡𝑡</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s161" style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="p">GRNs (but shared for all timesteps) to give us the non-linearly processed ones, </span>̃<span class="s306">(1) </span>̃<span class="s306">(𝑚𝑚)</span><span class="p">. In another</span></p><p class="s65" style="text-indent: 0pt;line-height: 81%;text-align: left;">… ξ<span class="s305">𝑡𝑡</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;line-height: 84%;text-align: justify;">stream, the VSN processes the flattened representation, <span class="s279">Ξ</span><span class="s54">𝑡𝑡</span>, along with optional context information, , and processes it through a GRN with a softmax activation. This gives us a weight, <span class="s41">𝑣𝑣𝑡𝑡</span>, which is a vector of length <span class="s137">𝑚𝑚</span>. This <span class="s255">𝑣𝑣</span><span class="s43">𝑡𝑡 </span>is now used in a weighted sum of all the non-linearly processed feature embeddings,</p><p class="s65" style="padding-left: 39pt;text-indent: 0pt;line-height: 6pt;text-align: left;">̃<span class="s64">(1)</span></p><p class="s65" style="padding-left: 36pt;text-indent: 0pt;line-height: 67%;text-align: left;">ξ<span class="s305">𝑡𝑡</span></p><p class="s307" style="padding-left: 10pt;text-indent: 0pt;line-height: 13pt;text-align: left;">̃<span class="s308">(𝑚𝑚)</span><span class="p">, which is calculated as follows:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s75" style="padding-top: 5pt;padding-left: 35pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑚𝑚</p><p class="s74" style="padding-left: 12pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><span class="s78">ξ</span><span class="s302">̃</span>  <span class="s78">=</span> ∑ 𝑣𝑣<span class="s75">(𝑗𝑗)  </span>̃<span class="s75">(𝑗𝑗)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Forecasting with TFT</p><p class="s75" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s75" style="padding-top: 5pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">𝑗𝑗=1</p><p class="s75" style="padding-top: 3pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">𝑡𝑡</p><p class="s74" style="padding-left: 5pt;text-indent: 0pt;line-height: 81%;text-align: left;">ξ<span class="s251">𝑡𝑡</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">TFT is implemented in PyTorch Forecasting. We can use the same framework we worked with in</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 147%;text-align: justify;"><a href="#bookmark561" class="s21">Chapter </a><i>15</i>, <i>Strategies for Global Deep Learning Forecasting Models</i>, and extend it to train TFT on our data. The <span class="s20">TemporalFusionTransformer </span>class in PyTorch Forecasting has the following major parameters:</p><ul id="l171"><li><p class="s20" style="padding-top: 2pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">hidden_size<span class="p">: This is an integer representing the hidden dimension across the model. This is the dimension in which all the GRNs work, the VSN, the LSTM hidden sizes, the self-attention hidden sizes, and so on. Arguably, this is the most important hyperparameter in the model.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">lstm_layers<span class="p">: This is an integer that determines the number of layers in the LSTMs we use in the LE Seq2Seq block.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">attention_head_size<span class="p">: This is an integer representing the number of attention heads.</span></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">embedding_sizes<span class="p">: This is a dictionary of categorical feature names to a tuple of (</span>cardinality<span class="p">, </span>embedding size<span class="p">). Although the original paper suggests projecting all categorical and continuous variables to a single dimension, the PyTorch Forecasting implementation allows the flexibility to have separate dimensions for each variable.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p class="s20" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a name="bookmark668">hidden_continuous_size</a><span class="p">: This is an integer that is the default embedding size for continuous features.</span><a name="bookmark627">&zwnj;</a></p></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">hidden_continuous_sizes<span class="p">: This is a dictionary of continuous feature names to a hidden size for variable selection. This lets us override </span>hidden_continuous_size <span class="p">for specific features.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="138" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_839.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The complete code for training TFT can be found in the <span class="s20">05-TFT.ipynb </span>notebook in the <span class="s20">Chapter16 </span>folder. There are two variables in the notebook that act as switches – <span class="s20">TRAIN_ SUBSAMPLE = True </span>makes the notebook run for a subset of 10 households, while <span class="s20">train_ model = True </span>makes the notebook train different models (warning: training the model on the full data takes hours). <span class="s20">train_model = False </span>loads the trained model weights (not included in the repository but saved every time you run training) and predicts on them.</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p class="s20" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">dropout<span class="p">: This is a float between 0 and 1, which determines the strength of the dropout in the network.</span></p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Interpreting TFT</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">TFT approaches interpretability from a slightly different perspective than N-BEATS. While N-BEATS gives us a decomposed output for interpretability, TFT gives us visibility into how the model has interpreted the variables it has used. On account of the VSNs, we have ready access to feature weights. Like the feature importance we get from tree-based models, TFT gives us access to similar scores. And because of the self-attention layer, the attention weights can also be interpreted to help us understand which time steps hold a large enough weightage in the attention mechanism.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">PyTorch Forecasting makes this possible by performing a few steps. First, we get the <span class="s5">raw predictions </span>using <span class="s20">mode=&quot;raw&quot; </span>in the <span class="s20">predict </span>function. Then, we use those raw predictions in the <span class="s20">interpret_ output </span>function. There is a parameter called <span class="s20">reduction </span>in the <span class="s20">interpret_output </span>function that decides how to aggregate the weights across different instances. We know that TFT does instance- wise feature selection in VSNs and attention is also done instance-wise. <span class="s20">&#39;mean&#39; </span>is a good option for looking at the global interpretability:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">raw_predictions, x = best_model.predict(val_dataloader, mode=&quot;raw&quot;, return_x=True)</p><p class="s28" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 106%;text-align: left;">interpretation = best_model.interpret_output(raw_predictions, reduction=&quot;sum&quot;)</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark669">This </a><span class="s20">interpretation </span>variable is a dictionary with weights for different aspects of the model, such as <span class="s20">attention</span>, <span class="s20">static_variables</span>, <span class="s20">encoder_variables</span>, and <span class="s20">decoder_variables</span>. PyTorch Forecasting also provides us with an easy way to visualize this importance:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="s49" style=" background-color: #F3F2F1;">  best_model.plot_interpretation(interpretation)                  </span></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This generates four plots:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;"><span><img width="514" height="338" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_840.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 40pt;text-indent: 0pt;text-align: center;">Figure 16.10 – Interpreting TFT</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can also look at each instance and plot similar visualizations for each prediction we make. All we need to do is use <span class="s20">reduction=&quot;none&quot; </span>and then plot it ourselves. The accompanying notebook explores how to do that and more.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have covered a few popular specialized architectures for time series forecasting, but this is in no way a complete list. There are so many model architectures and techniques out there. I have included a few in the <i>Further reading </i>section to get you started.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark670">Interpretability 445</a><a name="bookmark629">&zwnj;</a><a name="bookmark628">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_841.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Interpretability</p><p class="s4" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark367" class="s140">I directed you toward a few interpretability techniques for machine learning models back in </a>Chapter 10<span class="p">, </span>Global Forecasting Models<span class="p">. While some of those, such as SHAP and LIME, can still be applied to deep learning models, none of them considers the temporal aspect by design. This is because all those techniques were developed for more general purposes, such as classification and regression. That being said, there has been some work in interpretability for DL models and time series models. Here, I’ll list a few promising papers that tackle the temporal aspect head-on:</span></p><ul id="l172"><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">TimeSHAP<span class="p">: This is a model-agnostic recurrent explainer that builds upon </span>KernelSHAP <span class="p">and extends it to the time series domain. </span>Research paper<a href="https://dl.acm.org/doi/10.1145/3447548.3467166" class="s140" target="_blank">: </a><a href="https://dl.acm.org/doi/10.1145/3447548.3467166" class="a" target="_blank">https://dl.acm.org/ </a><span class="s27">doi/10.1145/3447548.3467166</span><span class="p">. </span>GitHub<a href="https://github.com/feedzai/timeshap" class="s140" target="_blank">: </a><a href="https://github.com/feedzai/timeshap" class="a" target="_blank">https://github.com/feedzai/ </a><span class="s27">timeshap</span><span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">Instance-wise Feature Importance in Time (FIT)<span class="p">: This is an interpretability technique that relies on the distribution shift between the predictive distribution and a counterfactual where all but the feature under inspection are unobserved. </span>Research paper<a href="https://proceedings.neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper.pdf" class="s140" target="_blank">: </a><a href="https://proceedings.neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper.pdf" class="a" target="_blank">https://proceedings. neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper. </a><span class="s27">pdf</span><span class="p">. </span>GitHub<a href="https://github.com/sanatonek/time_series_explainability" class="s140" target="_blank">: </a><span class="s27">https://github.com/sanatonek/time_series_explainability</span><span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Dynamask - Explaining Time Series Predictions with Dynamic Masks<span class="p">: A technique that produces instance-wise importance scores for each feature at each time step by fitting a perturbation mask to the input sequence. </span>Research paper<a href="http://proceedings.mlr.press/v139/crabbe21a.html" class="s140" target="_blank">: </a><a href="http://proceedings.mlr.press/v139/crabbe21a.html" class="a" target="_blank">http://proceedings.mlr.press/v139/ </a><span class="s27">crabbe21a.html</span><span class="p">. </span>GitHub<a href="https://github.com/JonathanCrabbe/Dynamask" class="s140" target="_blank">: </a><span class="s27">https://github.com/JonathanCrabbe/Dynamask</span><span class="p">.</span></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">While this is not an exhaustive list, these are a few works that I feel are important and promising. This is an area of active research and new techniques will come up as time goes on.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s take a look at another aspect, although not a specialized architecture, but a key component that can elevate the forecasts you generate to another dimension.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Probabilistic forecasting</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">So far, we have been talking about the forecast as a single number. We have been projecting our DL models to a single dimension and training the model using a loss such as mean squared loss. This paradigm is what we call a <span class="s5">point forecast</span>. A probabilistic forecast is when the forecast, instead of having a single-point prediction, captures the uncertainty of that forecast as well. This means that the model doesn’t output a single number, but an output that reflects the probabilities associated with all possible future outcomes.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the econometrics and classical time series world, the prediction intervals were already baked into the formulation. The statistical grounding of those methods made sure that the output of those models was readily interpreted in a probabilistic way as well (so long as you could satisfy the assumptions that were stipulated by those models). But in the modern machine/DL world, probabilistic forecasting is not an afterthought. A combination of factors such as less rigid assumptions and the way we train the models leads to this predicament.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 88%;text-align: justify;"><a name="bookmark671">From a probabilistic point of view, a forecast at position </a><span class="s157">𝑡𝑡</span>, <span class="s137">𝑦𝑦̂𝑡𝑡</span>, can be seen as the realization of a probability distribution, <span class="s137">𝑝𝑝(𝑦𝑦𝑡𝑡)</span>. And instead of estimating <span class="s41">𝑦𝑦̂</span><span class="s42">𝑡</span><span class="s43">𝑡</span>, the model estimates <span class="s137">𝑝𝑝(𝑦𝑦𝑡𝑡)</span>. There are   a few popular ways in DL to estimate <span class="s137">𝑝𝑝(𝑦𝑦</span><span class="s42">𝑡</span><span class="s43">𝑡</span><span class="s137">)</span>. Let’s look at them one by one.<a name="bookmark630">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Probability Density Function (PDF)</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is the most common way of representing probability distribution in forecasting. Standard parametric probability distributions have a few parameters that define the full distribution. For instance, the Gaussian distribution can be fully parameterized by the mean and the standard deviation. So, if we assume that the forecast is from one of these parametric distributions, we can tweak the model so that it outputs the parameters of the distribution, rather than a single-point estimate. For instance, the final projection to the output space of a DL model can be tweaked to output two parameters – the</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">mean, <span class="s293">μ</span><span class="s246">𝑡𝑡</span>, and the standard deviation, <span class="s157">σ𝑡𝑡</span>, at time <span class="s137">𝑡𝑡</span>. There are specific constraints to these outputs that</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 90%;text-align: justify;">are typically handled by the right activation function. For instance, <span class="s157">𝜎𝜎</span><span class="s88">𝑡𝑡 </span>should be a positive number because a negative standard deviation doesn’t make sense. So, we can apply an activation function, such as a <span class="s5">ReLU </span>or a <span class="s5">SoftPlus </span>function, to make sure those considerations are met.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now that the model has two outputs, the mean and standard deviation, we also need to change the loss function. It is not as if the target that we are training the model with is in PDF form. In other words, the targets are still real numbers and not means and standard deviations. So, instead of a loss function such as mean squared error, we need to use something such as a <span class="s5">negative log-likelihood </span>(<span class="s5">NLL</span>). Let’s understand this a bit more.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 91%;text-align: justify;">For instance, let’s say you have a set of iid observations (in our case, the target), <span class="s137">𝑦𝑦1, … , 𝑦𝑦𝑛𝑛 </span>. With PDF, we will be able to tell the probability of each of those observations, <span class="s41">𝑝𝑝(𝑦𝑦</span><span class="s42">𝑥𝑥</span><span class="s41">)</span>. From high school probability, we know that when two independent events occur, to get the joint probability, we can just multiply them together. Using the same logic, to calculate the joint probability of all <i>n </i>iid observations (the probability that all of them occur), we can just multiply all of them together:</p><p class="s65" style="padding-top: 1pt;padding-left: 162pt;text-indent: 0pt;text-align: center;">𝑛𝑛</p><p class="s157" style="padding-top: 4pt;padding-left: 211pt;text-indent: 0pt;text-align: left;">∏ 𝑝𝑝(𝑦𝑦<span class="s88">𝑖𝑖</span><span class="s65"> </span>)</p><p class="s65" style="padding-top: 3pt;padding-left: 213pt;text-indent: 0pt;text-align: left;">𝑖𝑖=1</p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">However, this operation is not numerically stable – if you start multiplying numbers less than zero, it gets smaller and smaller and eventually leads to numerical underflow. Therefore, a more stable version of the same likelihood is used – log-likelihood. Optimizing a function and a log of the same function is synonymous. By using the log, the series of multiplications becomes an addition and instantly more stable:</p><p class="s50" style="padding-top: 4pt;padding-left: 139pt;text-indent: 0pt;text-align: center;">𝑛𝑛</p><p class="s80" style="padding-top: 4pt;padding-left: 200pt;text-indent: 0pt;text-align: left;">∑ 𝑙𝑙𝑙𝑙(𝑝𝑝(𝑦𝑦<span class="s82">𝑖𝑖</span><span class="s50"> </span>))</p><p class="s50" style="padding-top: 3pt;padding-left: 201pt;text-indent: 0pt;text-align: left;">𝑖𝑖=1</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark672">Probabilistic forecasting 447</a><a name="bookmark631">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_842.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s137" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="p">So, all we need is </span>𝑝𝑝<span class="s252">(</span>𝑦𝑦𝑖𝑖<span class="s252">)</span> <span class="p">to plug into the NLL loss. This comes from our assumption regarding the distribution the output will be in and the estimated parameters of the model. This allows the model to compute the likelihood that a particular point (the target) is under the predicted distribution and then minimize the NLL (or maximize the likelihood) by changing the model parameters.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_843.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by Salinas et al. on DeepAR is cited in the <i>References </i>section as <i>11</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Apart from the Gaussian assumption, several differentiable parametric distributions have been used by different researchers for forecasting problems – student-t distribution, Tweedie distribution, negative binomial distribution, and so on. <span class="s5">DeepAR </span>by Salinas et al. is a prominent example where this approach has been used to great success.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Quantile functions</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 91%;text-align: justify;">Another way <span class="s137">𝑝𝑝(𝑦𝑦</span><span class="s42">𝑡𝑡</span><span class="s137">) </span>can be represented is by using quantile functions. Before talking about the quantile function, let’s spend a minute on the <span class="s5">Cumulative Distribution Function </span>(<span class="s5">CDF</span>). This, again, is high school probability. In simple words, a CDF returns the probability of some random variable, <span class="s90">𝑋𝑋</span>, being less than or equal to some value, <span class="s90">𝑥𝑥</span>:</p><p class="s90" style="padding-top: 5pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝐹𝐹(𝑥𝑥) = 𝑃𝑃(𝑋𝑋 ≤  𝑥𝑥)</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 83%;text-align: left;">Here, <span class="s80">𝐹𝐹 </span>is the CDF. This function takes in an input, <span class="s137">𝑥𝑥</span>, and returns a value between 0 and 1. Let’s call this value <span class="s143">𝑝𝑝</span>.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 80%;text-align: left;">A quantile function is an inverse of the CDF. This function tells you the value of x, which would make F(x) return a particular value, <span class="s255">𝑝𝑝</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s157" style="padding-top: 4pt;padding-left: 172pt;text-indent: 0pt;text-align: center;">𝐹𝐹−1(𝑝𝑝)  = 𝑥𝑥</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">This function, <span class="s41">𝐹𝐹−1</span>, is the quantile function. Similar to the PDF, the quantile function also provides a complete description of the distribution. Therefore, repurposing the model to learn the quantile function for specific quantiles that are of interest to us also gives us probabilistic forecasts.</p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;line-height: 94%;text-align: justify;">From an implementation perspective, we would have to choose <span class="s157">𝑞𝑞 </span>quantiles we want to estimate (0.1, 0.5, and 0.9 are popular choices) and tweak the last layer that projects to the output space so that there are <span class="s137">𝑞𝑞 </span>outputs. In this case, the most common choice for a loss function is the Quantile Loss or the Pinball Loss.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The Quantile Loss can be defined as follows:</p><p class="s65" style="padding-top: 5pt;padding-left: 75pt;text-indent: 0pt;line-height: 5pt;text-align: center;">(𝑦𝑦<span class="s305">𝑡𝑡</span><span class="s64">  </span>− 𝑦𝑦𝑞𝑞)𝑞𝑞, 𝑖𝑖𝑖𝑖 𝑦𝑦<span class="s305">𝑡𝑡</span><span class="s64">  </span>≥ 𝑦𝑦𝑞𝑞</p><p class="s65" style="padding-top: 1pt;text-indent: 0pt;line-height: 5pt;text-align: right;">𝐿𝐿<span class="s305">𝑞𝑞</span><span class="s64"> </span>(𝑦𝑦<span class="s305">𝑡</span><span class="s64">𝑡</span>, 𝑦𝑦𝑞𝑞) = {</p><p class="s64" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑡𝑡</p><p class="s64" style="padding-left: 46pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑡𝑡</p><p class="s64" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑡𝑡</p><p class="s65" style="padding-left: 20pt;text-indent: 0pt;line-height: 6pt;text-align: left;">(𝑦𝑦𝑞𝑞 − 𝑦𝑦<span class="s305">𝑡𝑡</span><span class="s64"> </span>)(1 − 𝑞𝑞), 𝑖𝑖𝑖𝑖 𝑦𝑦<span class="s305">𝑡𝑡</span><span class="s64">  </span>&lt; 𝑦𝑦𝑞𝑞</p><p class="s64" style="padding-left: 81pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝑡𝑡                                                                                               𝑡𝑡</p><p style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a name="bookmark673">Here, </a><span class="s41">𝑦𝑦𝑡𝑡</span><a name="bookmark632">&zwnj;</a></p><p style="padding-top: 8pt;text-indent: 0pt;line-height: 17pt;text-align: left;">is the target value at time t, <span class="s41">𝑦𝑦</span><span class="s258">𝑞𝑞 </span>is the quantile forecast, and <span class="s259">𝑞𝑞 </span>is the quantile we are forecasting.</p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;">While implementing this, we can easily replace the branched equation with a maximum operation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">def quantile_loss(q, y, y_hat_q):</p><p class="s38" style="padding-top: 3pt;padding-left: 21pt;text-indent: 0pt;line-height: 131%;text-align: left;"># q: Quantile to be evaluated, e.g., 0.5 for median. # y: Target Value</p><p class="s38" style="padding-left: 21pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># y_hat_q: Quantile Forecast.</p><p class="s28" style="padding-top: 3pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">e = y - f</p><p class="s28" style="padding-top: 3pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">return np.maximum(q * e, (q - 1) * e)</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Typically, we train with multiple quantiles. In this case, the loss the model optimizes for will be the sum of all the quantile loss for each of the quantiles we are training for.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">TFTs are a great example where uncertainty quantification is done using a quantile function. The paper originally proposed TFT as a probabilistic forecasting option. For a more detailed explanation of quantile loss, head over to the <i>Further reading </i>section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Other approaches</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Apart from PDF and quantile functions, there are a lot of other approaches to probabilistic forecasting. We will not be able to cover all of them here, but I will still try to mention the major ones and give you references so that you can read more about the technique:</p><ul id="l173"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s5">Monte Carlo dropout</span>: This trick only works for networks with dropout. Dropouts are generally turned off while inferencing. However, in Monte Carlo dropout, we keep the dropout on and predict multiple times. This approximates Bayesian computation and estimates model uncertainty. A popular research paper implementing this idea for time series is <i>Deep and Confident Prediction for Time Series at Uber </i><a href="https://doi.org/10.1109/ICDMW.2017.19" class="s140" target="_blank">by Zhu et al. at </a><span class="s27">https://doi.org/10.1109/ICDMW.2017.19</span>.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s5">Normalizing flows</span>: Normalizing flows are a method for learning complex data distributions by transforming a simple distribution into a complex distribution. They do this by learning a series of invertible and differentiable functions, such that we have a one-to-one correspondence between points in the simple distribution to points in the complex distribution. Therefore, we can have a bijective function (a function mapping between domain A to domain B such that for every element, <i>b</i>, in B, there is exactly one element, <i>a</i>, in A) that maps the data distribution (complex) to a simple distribution (such as a Gaussian distribution). A recent paper that applies normalizing flows to multivariate time series forecasting is <i>Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows </i><a href="https://openreview.net/forum?id=WiGQBFuVRv" class="s140" target="_blank">by Rasul et al. This was presented in ICLR 2021 and can be accessed at </a><span class="s27">https://openreview.net/forum?id=WiGQBFuVRv</span>.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark674">Summary 449</a><a name="bookmark634">&zwnj;</a><a name="bookmark633">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_844.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p style="padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: justify;"><span class="s5">Conformal prediction</span>: Conformal prediction is an increasingly popular way to create distribution- free prediction intervals. It uses a trained model’s experience in past data (calibration data) to recalibrate the confidence in new predictions. There is a large body of work and it is a very rapidly advancing field. I’m citing an example where the authors apply conformal predictions to a time series setting. For more information, you can refer to <i>Conformal Time-series Forecasting </i><a href="https://proceedings.neurips.cc/paper/2021/hash/312f1ba2a72318edaaa995a67835fad5-Abstract.html" class="s140" target="_blank">by Stankeviciute et al., which was presented at NeurIPS 2021 and is accessible at </a><a href="https://proceedings.neurips.cc/paper/2021/hash/312f1ba2a72318edaaa995a67835fad5-Abstract.html" class="a" target="_blank">https:// proceedings.neurips.cc/paper/2021/hash/312f1ba2a72318edaaa995</a><span class="s27"> a67835fad5-Abstract.html</span>.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Congratulations on making it through probably, one of the toughest and densest chapters in this book. Give yourself a pat on the back, sit back, and relax.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Our journey with deep learning for time series has finally reached a conclusion with us reviewing a few specialized architectures for time series forecasting. We now understand how different models such as N-BEATS, N-BEATSx, N-HiTS, Informer, Autoformer, and TFT work.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We also looked at how we can apply those models using PyTorch Forecasting. For the models such as <i>Informer </i>and <i>Autoformer </i>that were not implemented in PyTorch Forecasting, we saw how we can port normal <span class="s20">PyTorch </span>models into a form that can be used with PyTorch Forecasting. Models such as N-BEATS and TFT also offer interpretability and we explored those use cases as well.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">To top this off, we covered probabilistic forecasting at a high level and provided references so that you can start your journey of looking at them. This brings this part of this book to a close. At this point, you should be much more comfortable with using DL for time series forecasting problems.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the next part of this book, we will look at a few mechanics of forecasting, such as multi-step forecasting, cross-validation, and evaluation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The following is a list of the references that we used throughout this chapter:</p><ol id="l174"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. (2020). <i>The M4 Competition: 100,000 time series and 61 forecasting methods</i><a href="https://doi.org/10.1016/j.ijforecast.2019.04.014" class="s140" target="_blank">. International Journal of Forecasting, Volume 36, Issue 1. Pages 54-74. </a><span class="s27">https://doi.org/10.1016/j.ijforecast.2019.04.014</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Slawek Smyl. (2018). <i>M4 Forecasting Competition: Introducing a New Hybrid ES-RNN Model</i><a href="https://www.uber.com/blog/m4-forecasting-competition/" class="s140" target="_blank">. </a><span class="s27">https://www.uber.com/blog/m4-forecasting-competition/</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. (2020). <i>N-BEATS: Neural basis expansion analysis for interpretable time series forecasting</i><a href="https://openreview.net/forum?id=r1ecqn4YwB" class="s140" target="_blank">. 8th International Conference on Learning Representations, (ICLR). </a><span class="s27">https://openreview.net/forum?id=r1ecqn4YwB</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Kin G. Olivares and Cristian Challu and Grzegorz Marcjasz and R. Weron and A. Dubrawski. (2022). <i>Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx</i><a href="https://www.sciencedirect.com/science/article/pii/S0169207022000413" class="s140" target="_blank">. International Journal of Forecasting, 2022. </a><a href="https://www.sciencedirect.com/science/article/pii/S0169207022000413" class="a" target="_blank">https://www.sciencedirect. </a><span class="s27">com/science/article/pii/S0169207022000413</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Cristian Challu and Kin G. Olivares and Boris N. Oreshkin and Federico Garza and Max Mergenthaler-Canseco and Artur Dubrawski. (2022). <i>N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting</i><a href="https://arxiv.org/abs/2201.12886" class="s140" target="_blank">. arXiv preprint arXiv: Arxiv-2201.12886. </a><a href="https://arxiv.org/abs/2201.12886" class="a" target="_blank">https://arxiv.org/ </a><span class="s27">abs/2201.12886</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Lukasz, and Polosukhin, Illia. (2017). <i>Attention is All you Need. </i><a href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" class="s140" target="_blank">Advances in Neural Information Processing Systems. </a><a href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" class="a" target="_blank">https://papers.nips.cc/paper/2017/hash/</a><span class="s27"> 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. (2019). <i>Transformer Dissection: An Unified Understanding for Transformer’s Attention via the Lens of Kernel. </i><a href="https://aclanthology.org/D19-1443/" class="s140" target="_blank">N Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4,344–4,353. </a><a href="https://aclanthology.org/D19-1443/" class="a" target="_blank">https://aclanthology.</a><span class="s27"> org/D19-1443/</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. (2021). <i>Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</i><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17325" class="s140" target="_blank">. Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021. </a><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17325" class="a" target="_blank">https://ojs. </a><span class="s27">aaai.org/index.php/AAAI/article/view/17325</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. (2021). <i>Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</i><a href="https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html" class="s140" target="_blank">. Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021. </a><a href="https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html" class="a" target="_blank">https://proceedings.neurips. </a><span class="s27">cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Bryan Lim, Sercan Ö. Arik, Nicolas Loeff, and Tomas Pfister. (2019). <i>Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting</i><a href="https://www.sciencedirect.com/science/article/pii/S0169207021000637" class="s140" target="_blank">. International Journal of Forecasting, Volume 37, Issue 4, 2021, Pages 1,748-1,764. </a><a href="https://www.sciencedirect.com/science/article/pii/S0169207021000637" class="a" target="_blank">https://www.sciencedirect.com/ </a><span class="s27">science/article/pii/S0169207021000637</span>.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">David Salinas, Valentin Flunkert, and Jan Gasthaus. (2017). <i>DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks</i><a href="https://www.sciencedirect.com/science/article/pii/S0169207019301888" class="s140" target="_blank">. International Journal of Forecasting, 2017. </a><a href="https://www.sciencedirect.com/science/article/pii/S0169207019301888" class="a" target="_blank">https:// </a><span class="s27">www.sciencedirect.com/science/article/pii/S0169207019301888</span>.</p></li></ol><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark675">Further reading 451</a><a name="bookmark635">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_845.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">You can check out the following resources for further reading:</p></li><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Fast ES-RNN: A GPU Implementation of the ES-RNN Algorithm<a href="https://arxiv.org/abs/1907.03329" class="s140" target="_blank">: </a><a href="https://arxiv.org/abs/1907.03329" class="a" target="_blank">https://arxiv.org/ abs/1907.03329</a><span class="s27"> </span><a href="https://github.com/damitkwr/ESRNN-GPU" class="s140" target="_blank">and </a><a href="https://github.com/damitkwr/ESRNN-GPU" target="_blank">https://github.com/damitkwr/ESRNN-GPU</a></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Functions as Vector Spaces<a href="https://www.youtube.com/watch?v=NvEZol2Q8rs" class="s140" target="_blank">: </a><a href="https://www.youtube.com/watch?v=NvEZol2Q8rs" target="_blank">https://www.youtube.com/watch?v=NvEZol2Q8rs</a></p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: left;">Forecast with N-BEATS<a href="https://www.kaggle.com/code/gatandubuc/forecast-with-n-beats-interpretable-model/notebook" class="s140" target="_blank">, by Gaetan Dubuc: </a><a href="https://www.kaggle.com/code/gatandubuc/forecast-with-n-beats-interpretable-model/notebook" class="a" target="_blank">https://www.kaggle.com/code/ </a><a href="https://www.kaggle.com/code/gatandubuc/forecast-with-n-beats-interpretable-model/notebook" target="_blank">gatandubuc/forecast-with-n-beats-interpretable-model/notebook</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: left;">WaveNet: A Generative Model for Audio<a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio" class="s140" target="_blank">, by DeepMind: </a><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio" class="a" target="_blank">https://www.deepmind.com/ </a><a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio" target="_blank">blog/wavenet-a-generative-model-for-raw-audio</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;line-height: 111%;text-align: left;">What is Residual Connection?<a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55" class="s140" target="_blank">, by Wanshun Wong: </a><a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55" class="a" target="_blank">https://towardsdatascience. </a><a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55" target="_blank">com/what-is-residual-connection-efb07cab0d55</a></p></li><li><p class="s4" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Efficient Transformers: A Survey<a href="https://arxiv.org/abs/2009.06732" class="s140" target="_blank">, by Tay et al.: </a><a href="https://arxiv.org/abs/2009.06732" target="_blank">https://arxiv.org/abs/2009.06732</a></p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">Autocorrelation and the Wiener-Khinchin theorem<a href="https://www.itp.tu-berlin.de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_part1.pdf" class="s140" target="_blank">: </a><a href="https://www.itp.tu-berlin.de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_part1.pdf" class="a" target="_blank">https://www.itp.tu-berlin. de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_ </a><a href="https://www.itp.tu-berlin.de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_part1.pdf" target="_blank">part1.pdf</a></p></li><li><p class="s27" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s4">Modelling Long- and Short-Term Temporal Patterns with Deep Neural Networks</span><a href="https://dl.acm.org/doi/10.1145/3209978.3210006" class="s140" target="_blank">, by Lai et al.: </a><a href="https://dl.acm.org/doi/10.1145/3209978.3210006" class="a" target="_blank">https://dl.acm.org/doi/10.1145/3209978.3210006</a> <a href="https://github.com/cure-lab/SCINet" class="s140" target="_blank">and </a><a href="https://github.com/cure-lab/SCINet" class="a" target="_blank">https://github. </a>com/cure-lab/SCINet<span class="p">.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: justify;">Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting<a href="https://proceedings.neurips.cc/paper/2019/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html" class="s140" target="_blank">, by Sen et al.: </a><a href="https://proceedings.neurips.cc/paper/2019/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html" class="a" target="_blank">https://proceedings.neurips.cc/paper/2019/ hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html</a><span class="s27"> </span><a href="https://github.com/rajatsen91/deepglo" class="s140" target="_blank">and </a><a href="https://github.com/rajatsen91/deepglo" class="a" target="_blank">https:// </a><a href="https://github.com/rajatsen91/deepglo" target="_blank">github.com/rajatsen91/deepglo</a></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting<a href="https://proceedings.neurips.cc/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html" class="s140" target="_blank">, by Li et al.: </a><a href="https://proceedings.neurips.cc/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html" target="_blank">https://proceedings.neurips.cc/paper/2019/hash</a></p><p style="padding-top: 1pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><a href="https://proceedings.neurips.cc/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html">/6775a0635c302542da2c32aa19d86be0-Abstract.html</a></p></li><li><p class="s4" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;line-height: 112%;text-align: justify;">Quantile loss function for machine learning<a href="https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/" class="s140" target="_blank">, by Evergreen Innovations: </a><a href="https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/" class="a" target="_blank">https://www. evergreeninnovations.co/blog-quantile-loss-function-for-machine- </a><a href="https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/" target="_blank">learning/</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s31" style="padding-top: 5pt;padding-left: 109pt;text-indent: 221pt;line-height: 114%;text-align: left;"><a name="bookmark676">Part 4 – Mechanics of Forecasting</a><a name="bookmark677">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s32" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this last part, we cover a few concepts that are essential for creating an industry-ready forecasting system. We discuss rarely talked about concepts such as multi-step forecasting and delve into the details of evaluating a forecast.</p><p class="s32" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This part comprises the following chapters:</p></li><li><p class="s34" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 17<span class="s32">, </span>Multi-Step Forecasting</p></li><li><p class="s34" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 18<span class="s32">, </span>Evaluating Forecasts – Forecast Metrics</p></li><li><p class="s34" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Chapter 19<span class="s32">, </span>Evaluating Forecasts – Validation Strategies</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark678">17</a><a name="bookmark679">&zwnj;</a><a name="bookmark680">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">Multi-Step Forecasting</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the previous parts, we covered some basics of forecasting and different types of modeling techniques for time series forecasting. But a complete forecasting system is not just the model. There are a few mechanics of time series forecasting that make a lot of difference. These topics cannot be called <i>basics </i>because they require a nuanced understanding of the forecasting paradigm, and that is why we didn’t cover these upfront.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now that you have worked on some forecasting models and are familiar with time series, it’s time to get more nuanced in our approach. Most of the forecasting exercises we have done throughout the book focus on forecasting the next timestep.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 92%;text-align: justify;">In this chapter, we will look at strategies to generate multi-step forecasting. In other words, how to forecast the next <span class="s41">𝐻𝐻 </span>timesteps.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Why multi-step forecasting?</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Recursive strategy</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Direct strategy</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Joint strategy</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Hybrid strategies</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">How to choose a multi-step forecasting strategy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark699">Why multi-step forecasting?</a><a name="bookmark681">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 90%;text-align: justify;">A multi-step forecasting task consists of forecasting the next <span class="s41">𝐻𝐻 </span>timesteps, <span class="s137">𝑦𝑦</span><span class="s42">𝑡</span><span class="s43">𝑡+1</span><span class="s137">, … , 𝑦𝑦</span><span class="s42">𝑡</span><span class="s43">𝑡+𝐻𝐻</span>, of a time series, <span class="s137">𝑦𝑦1, … 𝑦𝑦𝑡𝑡</span>, where <span class="s74">𝐻𝐻 &gt; 1</span>. Most real-world applications of time series forecasting demand multi- step forecasting, whether it is the energy consumption of a household or the sales of a product. This is because forecasts are never created to know what will happen in the future, but to take some action</p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">using the visibility we get. To effectively take any action, we would want to know the forecast a little ahead of time. For instance, the dataset we have been using throughout the book is about the energy consumption of households, logged every half an hour. If the energy provider wants to plan its energy production to meet customer demand, the next half an hour doesn’t help at all. Similarly, if we look at the retail scenario, where we want to forecast the sales of a product, we will want to forecast a few days ahead so that we can purchase necessary goods, ship them to the store, and so on, in time for the demand.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Despite a more prevalent use case, multi-step forecasting has not received the attention it deserves. One of the reasons for that is the existence of classical statistical models or econometrics models such as the <i>ARIMA </i>and <i>exponential smoothing </i>methods, which include the multi-step strategy bundled within what we call a model; because of that, these models can generate multiple timesteps without breaking a sweat (although, as we will see in the chapter, they rely on one specific multi-step strategy to generate their forecast). Because these models were the most popular models used, practitioners need not have worried about multi-step forecasting strategies. But the advent of <span class="s5">machine learning </span>(<span class="s5">ML</span>) and <span class="s5">deep learning </span>(<span class="s5">DL</span>) methods for time series forecasting has opened up the need for a more focused study of multi-step forecasting strategies once again.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Another reason for the lower popularity of multi-step forecasting is that it is simply harder than single- step forecasting. This is because the more steps we extrapolate into the future, the more uncertainty in the predictions due to complex interactions between the different steps ahead.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are many strategies that can be used to generate multi-step forecasting, and the following figure summarizes them neatly:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 152pt;text-indent: 0pt;text-align: left;"><span><img width="223" height="122" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_846.jpg"/></span></p><p class="s37" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 17.1 – Multi-step forecasting strategies</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Each node of the graph in <i>Figure 17.1 </i>is a strategy, and different strategies that have common elements have been linked together with edges in the graph. In the rest of the chapter, we will be covering each of these nodes (strategies) and explaining them in detail.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark700">Recursive strategy 457</a><a name="bookmark682">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_847.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Let’s establish a few basic notations to help us understand these strategies. We have a time series, <span class="s41">𝕐𝕐𝑇𝑇</span>, of <span class="s41">𝑇𝑇</span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 15pt;text-align: justify;">timesteps, <span class="s137">𝑦𝑦1, … , 𝑦𝑦𝑇𝑇</span>. <span class="s86">𝕐𝕐</span><span class="s238">𝑡𝑡</span><span class="s43"> </span>denotes the same series, but ending at timestep <span class="s80">𝑡𝑡</span>. We also consider a function,</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><span class="s137">𝒲𝒲</span>, which generates a window of size <span class="s90">𝑘𝑘 &gt; 0 </span>from a time series. This function is a proxy for how we</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 92%;text-align: justify;">prepare the inputs for the different models we have seen throughout the book. So, if we see <span class="s74">𝒲𝒲</span><span class="s94">(</span><span class="s74">𝕐𝕐𝑡𝑡</span><span class="s94">)</span>, it means the function would draw a window from <span class="s41">𝕐𝕐𝑡𝑡 </span>that ends at timestep <span class="s41">𝑡𝑡</span>. We will also consider <span class="s41">𝐻𝐻 </span>to be the forecast horizon, where <span class="s50">𝐻𝐻 &gt; 1</span>. We will also be using <span class="s74">; </span>as an operator, which denotes concatenation.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at the different strategies (#1 in <i>References </i>is a good survey paper for different strategies). The discussion about merits and where we can use each of them is bundled in another upcoming section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Recursive strategy</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The recursive strategy is the oldest, most intuitive, and most popular technique to generate multi-step forecasts. To understand a strategy, there are two major regimes we have to understand:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">How is the training of the models done?</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">How are the trained models used to generate forecasts?</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s take the help of a diagram to understand the recursive strategy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 61pt;text-indent: 0pt;text-align: left;"><span><img width="443" height="281" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_848.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 17.2 – Recursive strategy for multi-step forecasting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s discuss these regimes in detail.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark701">Training regime</a><a name="bookmark685">&zwnj;</a><a name="bookmark684">&zwnj;</a><a name="bookmark683">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;">The recursive strategy involves training a single model to perform a <i>one-step-ahead </i>forecast. We can see in <i>Figure 17.1 </i>that we use the window function, <span class="s65">𝒲𝒲(𝕐𝕐</span><span class="s305">𝑡</span><span class="s64">𝑡</span><span class="s65">)</span>, to draw a window from <span class="s90">𝕐𝕐𝑡𝑡 </span>and train the model to predict <span class="s137">𝑦𝑦𝑡𝑡+1</span>. And during training, a loss function (which measures the divergence between the output of the model, <span class="s90">𝑦𝑦̂𝑡𝑡+1</span>, and the actual value, <span class="s137">𝑦𝑦</span><span class="s42">𝑡</span><span class="s43">𝑡+1</span>) is used to optimize the parameters of the model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Forecasting regime</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have trained a model to do <i>one-step-ahead </i>predictions. Now, we use this model in a recursive fashion to generate forecasts <span class="s146">𝐻𝐻 </span>timesteps ahead. For the first step, we use <span class="s65">𝒲𝒲(𝕐𝕐</span><span class="s286">𝑇</span><span class="s64">𝑇</span><span class="s65">)</span>, the window using the latest timestamp in training data, and generate the forecast one step ahead, <span class="s109">𝑦𝑦</span><span class="s146">̂𝑇𝑇+1</span>. Now, this generated forecast is added to the history and a new window is drawn from this history, <span class="s65">𝒲𝒲(𝕐𝕐</span><span class="s286">𝑇</span><span class="s64">𝑇</span><span class="s65">; 𝑦𝑦̂</span><span class="s286">𝑇</span><span class="s64">𝑇+1</span><span class="s65">)</span>. This window is given as the input to the same <i>one-step-ahead </i>model, and the forecast for the next timestep, <span class="s146">𝑦𝑦̂𝑇𝑇+2</span>,  is generated. This process is repeated until we get forecasts for all <span class="s146">𝐻𝐻 </span>timesteps.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This is the strategy that classical models that have stood the test of time (such as <i>ARIMA </i>and <i>Exponential Smoothing</i>) use internally when they generate multi-step forecasts. In the ML context, this means that we will train a model to predict one step ahead (as we have done all through this book), and then do a recursive operation where we forecast one step ahead, use the new forecast to recalculate all the features such as lags, rolling windows, and so on, and forecast the next step. In the context of the DL models, we can think of this as adding the forecast into the context window and using the trained model to generate the next step.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Direct strategy</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The direct strategy, also called the independent strategy, is a popular strategy in forecasting using ML. This involves forecasting each horizon independently of each other. Let’s look at the diagram first:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 138pt;text-indent: 0pt;text-align: left;"><span><img width="254" height="247" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_849.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 17.3 – Direct strategy for multi-step forecasting</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark702">Joint strategy 459</a><a name="bookmark688">&zwnj;</a><a name="bookmark687">&zwnj;</a><a name="bookmark686">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_850.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Next, let’s discuss the regimes in detail.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Training regime</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Under the direct strategy, we train <span class="s74">𝐻𝐻 </span>different models, which take in the same window function but are trained to predict different timesteps in the forecast horizon. Therefore, we are learning a separate set of parameters, one for each timestep in the horizon, such that all the models combined learn a direct and independent mapping from the window, <span class="s109">𝒲</span><span class="s146">𝒲(𝕐𝕐𝑡𝑡)</span>, to the forecast horizon, <span class="s137">𝐻𝐻</span>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">This strategy has gained ground along with the popularity of ML-based time series forecasting. From the ML context, we can practically implement it in two ways:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Shifting targets <span class="p">– Each model in the horizon is trained by shifting the target by that many steps as the horizon we are training the model to forecast.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="218" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_851.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;"><a href="#bookmark211" class="s140">The two ways mentioned in the preceding list work nicely if we only have lags as features. For instance, for eliminating features, we can just drop the offending lags and train the model. But in cases where we are using rolling features and other more sophisticated features, simple dropping doesn’t work because lag 1 is already used in calculating the rolling features. This leads to data leakage. In such scenarios, we can make a dynamic function that calculates these features taking in a parameter to specify the horizon we are creating these features for. All the helper methods we used in </a><a href="#bookmark211" class="s21">Chapter </a><i>6</i>, <i>Feature Engineering for Time Series Forecasting</i>, (<span class="s20">add_ rolling_features</span>, <span class="s20">add_seasonal_rolling_features</span>, and <span class="s20">add_ewma</span>) have a parameter called <span class="s20">n_shift</span>, which handles this condition. If we are training a model for <span class="s110">𝐻𝐻 = 2</span>, we need to pass <span class="s20">n_shift=2 </span>and the method will take care of the rest. Now, while training the models, we use this dynamic method to recalculate these features for each horizon separately.</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="s5">Eliminating features </span>– Each model in the horizon is trained by using only the features that are allowable to use according to the rules. For instance, when predicting <span class="s168">𝐻𝐻 = 2</span>, we can’t use lag 1 (because to predict <span class="s309">𝐻𝐻 = 2</span>, we will not have actuals for <span class="s146">𝐻𝐻 = 1</span>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Forecasting regime</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The forecasting regime is also fairly straightforward. We have the <i>H </i>trained models, one for each timestep in the horizon, and we use <span class="s146">𝒲𝒲(𝕐𝕐</span><span class="s126">𝑇𝑇</span><span class="s146">) </span>to forecast each of them independently.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Joint strategy</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The previous two strategies consider the model to have a single output. This is the case with most ML models; we formulate the model to predict a single scalar value as the prediction after taking in an array of inputs: <span class="s5">multiple input, single output </span>(<span class="s5">MISO</span>). But there are some models, such as the DL</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark703">models, which can be configured to give us multiple outputs. Therefore, the joint strategy, also called </a><span class="s5">multiple input, multiple output </span>(<span class="s5">MIMO</span>), aims to learn a single model that produces the entire forecasting horizon as an output:<a name="bookmark690">&zwnj;</a><a name="bookmark689">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><span><img width="467" height="301" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_852.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 17.4 – Joint strategy for multi-step forecasting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s see how these regimes work.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Training regime</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;">The joint strategy involves training a single multi-output model to forecast all the timesteps in the horizon at once. We can see in <i>Figure 17.1 </i>that we use the window function, <span class="s74">𝒲𝒲(𝕐𝕐𝑡𝑡)</span>, to draw a window from <span class="s74">𝕐𝕐𝑡𝑡 </span>and train the model to predict <span class="s74">𝑦𝑦</span><span class="s128">𝑡</span><span class="s127">𝑡+1</span><span class="s74">, … , 𝑦𝑦</span><span class="s128">𝑡</span><span class="s127">𝑡+𝐻𝐻 </span>. And during training, a loss function that measures the divergence between all the outputs of the model, <span class="s146">𝑦𝑦̂𝑡𝑡+1, … , 𝑦𝑦̂𝑡𝑡+𝐻𝐻</span>, and the actual values,</p><p class="s90" style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">𝑦𝑦𝑡𝑡+1, … , 𝑦𝑦𝑡𝑡+𝐻𝐻<span class="p">, is used to optimize the parameters of the model.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Forecasting regime</p><p class="s43" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><span class="p">The forecasting regime is also very simple. We have a trained model that is able to forecast all the timesteps in the horizon and we use </span>𝒲𝒲(𝕐𝕐<span class="s286">𝑇</span><span class="s64">𝑇</span>) <span class="p">to forecast them at once.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">This strategy is typically used in DL models where we configure the last layer to output <span class="s146">𝐻𝐻 </span>scalars instead of 1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark704">We have already seen this strategy in action at multiple places in the book:</a><a name="bookmark692">&zwnj;</a><a name="bookmark691">&zwnj;</a></p></li><li><p class="s4" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a href="#bookmark495" class="s140">The Tabular Regression (</a><a href="#bookmark495" class="s21">Chapter </a>13<span class="p">, </span>Common Modeling Patterns for Time Series<span class="p">) paradigm can easily be extended to output the whole horizon.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><span class="p">We have seen </span>Sequence-to-Sequence <span class="p">models with a </span>fully connected <a href="#bookmark495" class="s140">decoder (</a><a href="#bookmark495" class="s21">Chapter </a>13<span class="p">, </span>Common Modeling Patterns for Time Series<span class="p">) using this strategy for multi-step forecasting.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a href="#bookmark522" class="s140">In </a>Chapter 14<span class="p">, </span>Attention and Transformers for Time Series<span class="p">, we used this strategy to forecast using transformers.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a href="#bookmark603" class="s140">In </a><a href="#bookmark603" class="s21">Chapter </a>16<span class="p">, </span>Specialized Deep Learning Architectures for Forecasting<span class="p">, we saw models such as </span>N-BEATS<span class="p">, </span>N-HiTS<span class="p">, and </span>Temporal Fusion Transformer<span class="p">, which used this strategy to generate multi-step forecasts.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Hybrid strategies</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The three strategies we have already covered are the three basic strategies for multi-step forecasting, each with its own merits and demerits. Over the years, researchers have tried to combine these into hybrid strategies that try to capture the good parts of all these strategies. Let’s go through a few of them here. This will not be a comprehensive list because there is none. Anyone with enough creativity can come up with alternate strategies, but we will just cover a few that have received some attention and deep study from the forecasting community.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">DirRec Strategy</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">As the name suggests, the <span class="s5">DirRec </span>strategy is the combination of <i>direct </i>and <i>recursive </i>strategies for multi-step forecasting. First, let’s look at the following diagram:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 116pt;text-indent: 0pt;text-align: left;"><span><img width="293" height="260" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_853.jpg"/></span></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 17.5 – DirRec strategy for multi-step forecasting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark705">Now, let’s see how these regimes work for the DirRec strategy.</a><a name="bookmark693">&zwnj;</a></p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Training regime</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">Similar to the direct strategy, the DirRec strategy also has <span class="s41">𝐻𝐻 </span>models for a forecasting horizon of <span class="s41">𝐻𝐻</span>, but with a twist. We start the process by using <span class="s50">𝒲𝒲(𝕐𝕐𝑡𝑡) </span>and train a model to predict one step ahead. In the recursive strategy, we used this forecasted timestep in the same model to predict the next timestep. But in DirRec, we train a separate model for <span class="s310">𝐻𝐻 = 2</span>, but using the forecast we generated in <span class="s50">𝐻𝐻 = 1</span>. To generalize at timestep <span class="s74">ℎ &lt; 𝐻𝐻</span>, in addition to <span class="s50">𝒲𝒲(𝕐𝕐</span><span class="s126">𝑡</span><span class="s54">𝑡</span><span class="s50">)</span>, we include all the forecasts generated by different models at timesteps <span class="s90">1 </span>to <span class="s137">ℎ</span>.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Forecasting regime</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">The forecasting regime is just like the training regime, but instead of training the models, we use the</p><p class="s137" style="padding-left: 36pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">𝐻𝐻 <span class="p">trained models to generate the forecasts in a recursive manner.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Iterative block-wise direct strategy</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The <span class="s5">iterative block-wise direct </span>(<span class="s5">IBD</span>) strategy is also called the <span class="s5">iterative multi-SVR strategy</span>, paying homage to the research paper that suggested this (<i>#2 </i>in <i>References</i>). The direct strategy requires <span class="s41">𝐻𝐻 </span>different models to train, and that makes it difficult to scale for long-horizon forecasting. The IBD strategy tries to tackle that shortcoming by using a block-wise iterative style of forecasting:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 62pt;text-indent: 0pt;text-align: left;"><span><img width="457" height="275" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_854.jpg"/></span></p><p class="s37" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 17.6 – IBD strategy for multi-step forecasting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Let’s understand the training and forecasting regimes for this strategy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark706">Training regime</a><a name="bookmark694">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">In the IBD strategy, we split the forecast horizon, <span class="s137">𝐻𝐻</span>, into <span class="s157">𝑅𝑅 </span>blocks of length <span class="s41">𝐿𝐿</span>, such that <span class="s74">𝐻𝐻 = 𝐿𝐿 × 𝑅𝑅</span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">. And instead of training <span class="s137">𝐻𝐻 </span>direct models, we train <span class="s41">𝐿𝐿 </span>direct models.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Forecasting regime</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 92%;text-align: justify;">While forecasting, we use the <span class="s137">𝐿𝐿 </span>trained models to generate the forecast for the first <span class="s137">𝐿𝐿 </span>timesteps (<span class="s50">𝑇𝑇 + 1 </span>to <span class="s109">𝑇𝑇</span><span class="s146"> + 𝐿𝐿</span>) in <span class="s41">𝐻𝐻 </span>using the window <span class="s65">𝒲𝒲(𝕐𝕐</span><span class="s286">𝑇</span><span class="s64">𝑇</span><span class="s65">)</span>. Let’s denote this <i>L </i>forecast as <span class="s137">𝕐𝕐𝑇𝑇+𝐿𝐿</span>. Now, we will use <span class="s146">𝕐𝕐𝑇𝑇+𝐿𝐿</span>, along with <span class="s90">𝕐𝕐𝑇𝑇</span>, in the window function to draw a new window, <span class="s65">𝒲𝒲(𝕐𝕐</span><span class="s305">𝑇</span><span class="s64">𝑇</span><span class="s65">; 𝕐𝕐</span><span class="s305">𝑇</span><span class="s64">𝑇+𝐿𝐿</span><span class="s65">)</span>. This new window is used to generate the forecast for the next <span class="s146">𝐿𝐿 </span>timesteps (<span class="s74">𝑇𝑇 + 𝐿𝐿 </span>to <span class="s50">𝑇𝑇 + 2𝐿𝐿</span>). This process is repeated many times to complete the full horizon forecast.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Rectify strategy</p><p class="s4" style="padding-top: 8pt;padding-left: 27pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark337" class="s140">The rectify strategy is another way we can combine direct and recursive strategies. It strikes a middle ground between the two by forming a two-stage training and inferencing methodology. We can see this as a model stacking approach (</a>Chapter 9<span class="p">, </span>Ensembling and Stacking<span class="p">) but between different multi- step forecasting strategies:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;"><span><img width="445" height="319" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_855.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 17.7 – Rectify strategy for multi-step forecasting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: justify;">Let’s understand how this strategy works.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark707">Training regime</a><a name="bookmark695">&zwnj;</a></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The training happens in two steps. The recursive strategy is applied to the horizon and the forecast for all <i>H </i>timesteps is generated. Let’s call this <span class="s50">𝕐𝕐</span><span class="s279">̃</span><span class="s126">𝑡</span><span class="s54">𝑡+𝐻𝐻</span>. Now, we train direct models for each horizon using the original history, <span class="s50">𝕐𝕐𝑡𝑡</span>, and the recursive forecasts, <span class="s50">𝕐𝕐</span><span class="s279">̃</span><span class="s126">𝑡</span><span class="s54">𝑡+𝐻𝐻</span>, as inputs.</p><p class="s24" style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Forecasting regime</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The forecasting regime is similar to the training, where the recursive forecasts are generated first and they, along with the original history, are used to generate the final forecasts.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">RecJoint</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">True to its name, RecJoint is a mashup between the recursive and joint strategies, but applicable for single-output models:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 115pt;text-indent: 0pt;text-align: left;"><span><img width="318" height="247" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_856.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 17.8 – RecJoint strategy for multi-step forecasting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The following sections detail the working of this strategy.</p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Training regime</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;line-height: 94%;text-align: justify;">The training in the RecJoint strategy is very similar to the recursive strategy in the way it trains a single model and recursively uses prediction at <span class="s41">𝑡𝑡 + 1 </span>as an input to train <span class="s41">𝑡𝑡 + 2</span>, and so on. But the recursive strategy trains the model on just the next timestep. RecJoint generates the predictions for the entire horizon, and jointly optimizes the entire horizon forecasts while training. This forces the model to look at the next <i>H </i><a href="#bookmark495" class="s140">timesteps and jointly optimize the entire horizon instead of the myopic one-step-ahead objective. We saw this strategy at play when we trained Seq2Seq models using an RNN encoder and decoder (</a><i>Chapter 13</i>, <i>Common Modeling Patterns for Time Series</i>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark708">Forecasting regime</a><a name="bookmark696">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The forecasting regime for RecJoint is exactly the same as for the recursive strategy.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Now that we have understood a few strategies, let’s also discuss the merits and demerits of these strategies.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">How to choose a multi-step forecasting strategy?</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Let’s summarize all the different strategies that we learned now in a table:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span><img width="531" height="196" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_857.jpg"/></span></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 17.9 – Multi-step forecasting strategies – a summary</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Here, the following applies:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">S.O<span class="p">: Single output</span></p></li><li><p class="s5" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">M.O<span class="p">: Multi-output</span></p></li><li><p style="padding-top: 3pt;padding-left: 54pt;text-indent: -13pt;text-align: left;"><span class="s150">𝑻𝑻</span><span class="s253">𝒔𝒔𝒔𝒔 </span>and <span class="s157">𝑰𝑰𝒔𝒔𝒔𝒔</span>: Training and inferencing time of a single output model</p></li><li><p class="s41" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;line-height: 87%;text-align: justify;"><span class="s150">𝑻</span>𝑻<span class="s253">𝒎𝒎𝒎</span><span class="s43">𝒎 </span><span class="p">and </span><span class="s150">𝑰</span>𝑰<span class="s253">𝒎𝒎𝒎</span><span class="s43">𝒎</span><span class="p">: Training and inferencing time of a multi-output model (practically, </span>𝑻𝑻<span class="s42">𝒎𝒎𝒎</span><span class="s43">𝒎 </span><span class="p">is larger than </span>𝑻𝑻𝒔𝒔𝒔𝒔 <span class="p">mostly because multi-output models are typically DL models and their training time is higher than standard ML models)</span></p></li><li><p class="s80" style="padding-top: 3pt;padding-left: 54pt;text-indent: -13pt;line-height: 14pt;text-align: justify;">𝐻𝐻<span class="p">: The horizon</span></p><p class="s74" style="padding-left: 73pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝑯𝑯</p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_858.png"/></span></p></li><li><p style="padding-left: 54pt;text-indent: -13pt;line-height: 78%;text-align: justify;"><span class="s74">𝑳𝑳 = </span><span class="s77">𝑹𝑹 </span>, where <span class="s137">𝑹𝑹 </span>is the number of blocks in the IBD strategy</p></li><li><p class="s248" style="padding-left: 54pt;text-indent: -13pt;line-height: 15pt;text-align: justify;">𝛅𝛅 <span class="p">is some positive real number</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The table will help us understand and decide which strategy is better from multiple perspectives:</p><ul id="l175"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;"><span class="s5">Engineering complexity</span>: <i>Recursive</i>, <i>Joint</i>, <i>RecJoint </i>&lt;&lt; <i>IBD </i>&lt;&lt; <i>Direct</i>, <i>DirRec </i>&lt;&lt; <i>Rectify</i></p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;line-height: 14pt;text-align: left;"><span class="s5">Training time</span>: <i>Recursive </i>&lt;&lt; <i>Joint </i>(typically <span class="s41">𝑇𝑇𝑚𝑚𝑚𝑚 &gt; 𝑇𝑇𝑠𝑠𝑚𝑚</span>) &lt;&lt; <i>RecJoint </i>&lt;&lt; <i>IBD </i>&lt;&lt; <i>Direct</i>, <i>DirRec</i></p><p style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: left;">&lt;&lt; <i>Rectify</i></p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;"><span class="s5">Inference time</span>: <i>Joint </i>&lt;&lt; <i>Direct</i>, <i>Recursive</i>, <i>DirRec</i>, <i>IBD</i>, <i>RecJoint </i>&lt;&lt; <i>Rectify</i></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">It also helps us to decide the kind of model we can use for each strategy. For instance, a joint strategy can only be implemented with a model that supports multi-output, such as a DL model. But we are yet to discuss how these strategies affect accuracies.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Although in ML, the final word goes to empirical evidence, there are ways we can analyze the different methods to provide us with some guidelines. <i>Taieb et al. </i>analyzed the bias and variance of these multi- step forecasting strategies, both theoretically and using simulated data. With this analysis, along with other empirical findings over the years, we do have an understanding of the strengths and weaknesses of these strategies, and some guidelines from these findings.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="58" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_859.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Reference check</p><p style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">The research paper by <i>Taieb et al</i>. is cited in <i>References </i>under <i>3</i>.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s4" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Taieb et al. <span class="p">point out several disadvantages of the recursive strategy, contrasting with the direct strategy, based on the bias and variance components of error analysis. They further corroborated these observations through an empirical study as well.</span></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The key points that elucidate the difference in performance are as follows:</p></li><li><p style="padding-top: 8pt;padding-left: 63pt;text-indent: -13pt;line-height: 94%;text-align: justify;">For the recursive strategy, the bias and variance components of error in step <span class="s41">ℎ = 1 </span>affect step <span class="s311">ℎ = 2</span>. Because of this phenomenon, the errors that a recursive model makes tend to accumulate as we move further in the forecast horizon. But for the direct strategy, this dependence is not explicit and therefore doesn’t suffer the same deterioration that we see in the recursive strategy. This was also seen in the empirical study where the recursive strategy was very erratic and had the highest variance, which increased significantly as we moved further in the horizon.</p></li><li><p style="padding-top: 4pt;padding-left: 63pt;text-indent: -13pt;line-height: 94%;text-align: justify;">For the direct strategy, the bias and variance components of error in step <span class="s41">ℎ = 1 </span>do not affect <span class="s311">ℎ = 2</span>. This is because each horizon, <span class="s41">ℎ</span>, is forecasted in isolation. Intuitively, a downside of this approach is the fact that this strategy can produce completely unrelated forecasts across the horizon leading to unrealistic forecasts. The complex dependencies that may exist between the forecast in the horizon are not captured in the direct strategy. For instance, a direct strategy on a time series with a non-linear trend may result in a broken curve because of the independence of each timestep in the horizon.</p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Practically, in most cases, a direct strategy produces coherent forecasts.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a name="bookmark709">The bias for the recursive strategy was also amplified when the forecasting model produces forecasts that have large variations. Highly complex models are known to have low bias but a high amount of variations, and these high variations seem to amplify the bias for recursive strategy models.</a></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">When we have very large datasets, the bias term of the direct strategy becomes zero, but the recursive strategy bias was still non-zero. This was further demonstrated in experiments – for long time series, the direct strategy almost always outperformed the recursive strategy. From the learning theory perspective, we are learning <span class="s50">𝐻𝐻 </span>functions using the data for the direct strategy, whereas for recursive, we are just learning <span class="s137">1</span>. So, with the same amount of data, it is harder to learn <span class="s146">𝐻𝐻 </span>true functions than <span class="s137">1</span>. This is amplified in low-data situations.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Although the recursive strategy seems inferior to the direct strategy theoretically and empirically, it is not without some advantages:</p><ul id="l176"><li><p style="padding-top: 9pt;padding-left: 65pt;text-indent: -11pt;text-align: left;">For highly non-linear and noisy time series, learning direct functions for all the horizons can be hard. And in such situations, recursive can work better.</p></li><li><p style="padding-top: 5pt;padding-left: 65pt;text-indent: -11pt;text-align: left;">If the underlying <span class="s5">data-generating process </span>(<span class="s5">DGP</span>) is very smooth and can be easily approximated, the recursive strategy can work better.</p></li><li><p style="padding-top: 5pt;padding-left: 65pt;text-indent: -11pt;text-align: left;">When the time series is shorter, the recursive strategy can work better.</p></li></ul></li><li><p style="padding-top: 9pt;padding-left: 55pt;text-indent: -13pt;line-height: 92%;text-align: justify;">We talked about the direct strategy generating possible unrelated forecasts for the horizon, but this is exactly the part that the joint strategy takes care of. The joint strategy can be thought of as an extension of the direct strategy, but instead of having <span class="s41">𝐻𝐻 </span>different models, we have a single model produce <span class="s41">𝐻𝐻 </span>outputs. We are learning a single function instead of <i>H </i>functions from the given data. Therefore, the joint strategy doesn’t have the same weakness as the direct strategy in short time series.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;line-height: 94%;text-align: justify;">One of the weaknesses of the joint strategy (and RecJoint) is the high bias on very short horizons (such as <span class="s312">𝐻𝐻 = 2</span>, <span class="s252">𝐻𝐻</span><span class="s137"> = 3, </span>and so on). We are learning a model that optimizes across all the <span class="s41">𝐻𝐻</span><span class="s107"> </span>timesteps in the horizon using a standard loss function such as the mean squared error. But these errors are at different scales. The errors that can occur further down the horizon are larger than the ones close by, and this implicitly puts more weight on the longer horizons, and the model learns a function that is skewed toward getting the longer horizons right.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">The joint and RecJoint strategies are comparable from the variance perspective. But the joint strategy can give us a lower bias because the RecJoint strategy learns a recursive function and it may not be flexible enough to capture the pattern. But the joint strategy uses the full power of the forecasting model to directly forecast the horizon.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Hybrid strategies, such as DirRec, IBD, and so on, try to balance the merits and demerits of fundamental strategies such as direct, recursive, and joint. With these merits and demerits, we can make an informed experimentation framework to come up with the best strategy for the problem at hand.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark710">Summary</a><a name="bookmark698">&zwnj;</a><a name="bookmark697">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We touched upon a particular aspect of forecasting that is highly relevant for real-world use cases, but rarely talked about and studied. We saw why we needed multi-step forecasting and then went on to review a few popular strategies we can use. We understood the popular and fundamental strategies such as direct, recursive, and joint, and then went on to look at a few hybrid strategies such as DirRec, rectify, and so on. To top it off, we looked at the merits and demerits of these strategies and discussed a few guidelines for selecting the right strategy for your problem.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In the next chapter, we will be looking at another important aspect of forecasting – evaluation.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The following is the list of the references that we used throughout the chapter:</p><ol id="l177"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Taieb, S.B., Bontempi, G., Atiya, A.F., and Sorjamaa, A. (2012). <i>A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition</i><a href="https://arxiv.org/pdf/1108.3259.pdf" class="s140" target="_blank">. Expert Syst. Appl., 39, 7067-7083: </a><a href="https://arxiv.org/pdf/1108.3259.pdf" target="_blank">https://arxiv.org/pdf/1108.3259.pdf</a></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Li Zhang, Wei-Da Zhou, Pei-Chann Chang, Ji-Wen Yang, Fan-Zhang Li. (2013). <i>Iterated time series prediction with multiple support vector regression models. </i><a href="https://www.sciencedirect.com/science/article/pii/S0925231212005863" class="s140" target="_blank">Neurocomputing, Volume 99, 2013: </a><a href="https://www.sciencedirect.com/science/article/pii/S0925231212005863" class="a" target="_blank">https://www.sciencedirect.com/science/article/pii/ </a><a href="https://www.sciencedirect.com/science/article/pii/S0925231212005863" target="_blank">S0925231212005863</a></p></li><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -18pt;text-align: justify;">Taieb, S.B. and Atiya, A.F. (2016). <i>A Bias and Variance Analysis for Multistep-Ahead Time Series Forecasting. </i><a href="https://ieeexplore.ieee.org/document/7064712" class="s140" target="_blank">in IEEE Transactions on Neural Networks and Learning Systems, vol. 27, no. 1, pp. 62-76, Jan. 2016: </a><a href="https://ieeexplore.ieee.org/document/7064712" target="_blank">https://ieeexplore.ieee.org/document/7064712</a></p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark711">18</a><a name="bookmark712">&zwnj;</a><a name="bookmark714">&zwnj;</a><a name="bookmark713">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 220pt;text-indent: -74pt;line-height: 114%;text-align: left;">Evaluating Forecasts – Forecast Metrics</h4><p style="padding-top: 24pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We started getting into the nuances of forecasting in the previous chapter where we saw how to generate multi-step forecasts. While that covers one of the aspects, there is another aspect of forecasting that is as important as it is confusing – <i>how to evaluate forecasts</i>.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the real world, we generate forecasts to enable some downstream processes to plan better and take relevant actions. For instance, the operations manager at a bike rental company should decide how many bikes he should make available at the metro station the next day at 4 p.m. However, instead of using the forecasts blindly, he may want to know which forecasts he should trust and which ones he shouldn’t. This can only be done by measuring how good a forecast is.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We have been using a few metrics throughout the book and it is now time to get down into the details to understand those metrics, when to use them, and when to not use some metrics. We will also elucidate a few aspects of these metrics experimentally.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Taxonomy of forecast error measures</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Investigating the error measures</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Experimental study of the error measures</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Guidelines for choosing a metric</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all packages and datasets required for the code in this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s27" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter18" class="s140" target="_blank" name="bookmark727">The associated code for the chapter can be found here: </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter18" class="a" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/ </a>Chapter18.<a name="bookmark715">&zwnj;</a></p><p style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">For this chapter, you need to run the notebooks in the <span class="s20">Chapters02 </span>and <span class="s20">Chapter04 </span>folders from the book’s GitHub repository.</p><p class="s3" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Taxonomy of forecast error measures</p><p class="s4" style="padding-top: 6pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Measurement is the first step that leads to control and eventually improvement.</p><p class="s4" style="padding-top: 7pt;padding-left: 191pt;text-indent: 0pt;text-align: justify;">–H. James Harrington</p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Traditionally, in regression problems, we have very few, general loss functions such as the mean squared error or the mean absolute error, but when you step into the world of time series forecasting, you will be hit with a myriad of different metrics.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="74" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_860.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">Since the focus of the book is on point predictions (and not probabilistic predictions), we will stick to reviewing point forecast metrics.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are a few key factors that distinguish the metrics in time series forecasting:</p><ul id="l178"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Temporal relevance<span class="p">: The temporal aspect of the prediction we make is an essential aspect of a forecasting paradigm. Metrics such as forecast bias and the tracking signal take this aspect into account.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Aggregate metrics<span class="p">: In most business use cases, we would not be forecasting a single time series, but rather a set of time series, related or unrelated. In these situations, looking at the metrics of individual time series becomes infeasible. Therefore, there should be metrics that capture the idiosyncrasies of this mix of time series.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Over- or under-forecasting<span class="p">: Another key concept in time series forecasting is over- and under- forecasting. In a traditional regression problem, we do not really worry whether the predictions are more than or less than expected, but in the forecasting paradigm, we must be careful about structural biases that always over- or under-forecast. This, when combined with the temporal aspect of time series, accumulates errors and leads to problems in downstream planning.</span></p></li></ul></li></ul><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">These aforementioned factors, along with a few others, have led to an explosion in the number of forecast metrics. In a recent survey paper by <i>Hewamalage et al. </i>(#1 in <i>References</i>), the number of metrics that was covered stands at <i>38</i>. Let’s try and unify these metrics under some structure. <i>Figure 18.1 </i>depicts a taxonomy of forecast error measures:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><a name="bookmark728"><span><img width="527" height="282" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_861.gif"/></span></a><a name="bookmark716">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 18.1 – Taxonomy of forecast error measures</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">We can semantically separate the different forecast metrics into two buckets – <span class="s5">intrinsic </span>and <span class="s5">extrinsic</span>. <i>Intrinsic </i>metrics measure the generated forecast using nothing but the generated forecast and the corresponding actuals. As the name suggests, it is a very inward-looking metric. <i>Extrinsic </i>metrics, on the other hand, use an external reference or benchmark in addition to the generated forecast and ground truth to measure the quality of the forecast.</p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 86%;text-align: justify;">Before we start with the metrics, let’s establish a notation to help us understand. <span class="s313">𝑦</span><span class="s41">𝑦</span><span class="s43">𝑡𝑡 </span>and <span class="s255">𝑦𝑦</span><span class="s137">̂</span><span class="s127">𝑡𝑡 </span>are the actual observation and the forecast at time <span class="s157">𝑡𝑡</span>. The forecast horizon is denoted by <span class="s41">𝐻𝐻</span>. In cases where we have a dataset of time series, we assume there are <span class="s41">𝑀𝑀 </span>time series, indexed by <span class="s41">𝑚𝑚</span>, and finally, <span class="s74">𝑒𝑒</span><span class="s79">𝑡𝑡</span><span class="s75">  </span><span class="s74">= 𝑦𝑦</span><span class="s79">𝑡𝑡</span><span class="s75">  </span><span class="s74">− 𝑦𝑦̂</span><span class="s79">𝑡𝑡 </span>denotes the error at timestep <span class="s157">𝑡𝑡</span>. Now, let’s start with the intrinsic metrics.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Intrinsic metrics</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">There are four major base errors – absolute error, squared error, percent error, and symmetric error</p></li></ul></li><li><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">that are aggregated or summarized in different ways in a variety of metrics. Therefore, any property of these base errors also applies to the aggregate ones, so let’s look at these base errors first.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Absolute error</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 90%;text-align: justify;">The error, <span class="s90">𝑒𝑒</span><span class="s128">𝑡𝑡</span>, can be positive or negative, depending on whether <span class="s146">𝑦𝑦𝑡𝑡 &lt; 𝑦𝑦̂𝑡𝑡 </span>or not, but then when we are calculating and adding this error over the horizon, the positive and negative errors may cancel each other out and that paints a rosier picture. Therefore, we include a function on top of <span class="s74">𝑒𝑒</span><span class="s79">𝑡𝑡 </span>to ensure that the errors do not cancel each other out.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s146" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark729"><span class="p">The absolute function is one of these functions: </span></a>𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴 𝐸𝐸𝐸𝐸𝐸𝐸𝐴𝐴𝐸𝐸<span class="s109">(</span>𝐴𝐴𝐸𝐸<span class="s109">)</span> = <span class="s109">|</span>𝐴𝐴𝑡𝑡<span class="s109">|</span><span class="p">. The absolute error is a scale-dependent error. This means that the magnitude of the error depends on the scale of the time series. For instance, if you have an </span><span class="s4">AE </span><span class="p">of 10, it doesn’t mean anything until you put it in context. For a time series with values of around 500 to 1,000, an </span><span class="s4">AE </span><span class="p">of 10 may be a very good number, but if the time series has values around 50 to 70, then it is bad.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="272" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_862.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Scale dependence is not a deal breaker when we are looking at individual time series, but when we are aggregating or comparing across multiple time series, scale-dependent errors skew the metric in favor of the large-scale time series. The interesting thing to note here is that this is not necessarily bad. Sometimes, the scale in the time series is meaningful and it makes sense from the business perspective to focus more on the large-scale time series than the smaller ones. For instance, in a retail scenario, one would be more interested in getting the high-selling product forecast right than those of the low-selling ones. In these cases, using a scale-dependent error automatically favors the high-selling products.</p><p style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">You can see this by carrying out an experiment on your own. Generate a random time series,</p><p style="padding-left: 13pt;text-indent: 0pt;line-height: 83%;text-align: justify;"><span class="s252">𝐴𝐴</span>. Now, similarly, generate a random forecast for the time series, <span class="s41">𝐹𝐹</span>. Now, we multiply the forecast, <span class="s80">𝐹𝐹</span>, and time series, <span class="s90">𝐴𝐴</span>, by 100 to get two new time series and their forecasts, <span class="s74">𝐴𝐴</span><span class="s79">𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 </span>and <span class="s90">𝐹𝐹</span><span class="s128">𝑠</span><span class="s127">𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠</span>. If we calculate the forecast metric for both these sets of time series and forecasts, the scaled-dependent metrics will give very different values, whereas the scale-independent ones will give the same values.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Many metrics are based on this error:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s54" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;line-height: 6pt;text-align: left;">𝐻𝐻</p><p class="s146" style="padding-left: 31pt;text-indent: 0pt;line-height: 6pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="10" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_863.png"/></span></p><ul id="l179"><li><p class="s146" style="padding-left: 64pt;text-indent: -13pt;line-height: 81%;text-align: left;"><span class="s5">Mean Absolute Error </span><span class="p">(</span><span class="s5">MAE</span><span class="p">): </span><span class="s93">MAE</span> = <span class="s314">𝐻𝐻</span> <span class="s93">∑|𝑒</span>𝑒<span class="s54">𝑡𝑡</span><span class="s93">|</span></p><p class="s54" style="padding-left: 173pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝑡𝑡=1</p></li><li><p class="s5" style="padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: left;">Median Absolute Error<span class="p">: </span><span class="s50">𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀 = 𝑚𝑚𝑚𝑚𝑀𝑀𝑚𝑚𝑚𝑚𝑚𝑚(|𝑚𝑚𝑡𝑡|)</span></p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;line-height: 6pt;text-align: left;">Geometric Mean Absolute Error<span class="p">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s211" style="text-indent: 0pt;text-align: right;">𝐻𝐻</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="24" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_864.png"/></span></p><p class="s234" style="padding-top: 2pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">𝐻𝐻</p><p class="s64" style="padding-left: 173pt;text-indent: 0pt;line-height: 7pt;text-align: center;">𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺  =   √∏|𝑒𝑒<span class="s296">𝑡</span><span class="s234">𝑡</span>|</p><p class="s234" style="padding-top: 2pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑡𝑡=1</p></li><li><p class="s5" style="padding-top: 2pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Weighted Mean Absolute Error<span class="p">: This is a more esoteric method that lets you put more weight on a particular timestep in the horizon:</span></p><p class="s315" style="padding-top: 2pt;text-indent: 0pt;line-height: 8pt;text-align: right;">∑<span class="s65">𝐻𝐻</span></p><p class="s157" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑤𝑤<span class="s88">𝑡</span><span class="s65">𝑡</span>|𝑒𝑒<span class="s88">𝑡𝑡</span><span class="s65"> </span>|</p><p class="s157" style="padding-top: 3pt;padding-left: 40pt;text-indent: 0pt;line-height: 8pt;text-align: center;">𝑊𝑊𝑊𝑊𝑊𝑊𝑊𝑊  = <u>    𝑡𝑡=1            </u></p><p class="s157" style="text-indent: 0pt;line-height: 12pt;text-align: left;">∑</p><p style="text-indent: 0pt;text-align: left;"/><p class="s65" style="padding-left: 250pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝐻𝐻</p><p class="s65" style="padding-left: 250pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑡𝑡=1</p><p class="s157" style="text-indent: 0pt;text-align: left;">𝑤𝑤<span class="s88">𝑡𝑡</span></p><p style="padding-top: 8pt;padding-left: 64pt;text-indent: 0pt;line-height: 85%;text-align: justify;">Here, <span class="s157">𝑤𝑤</span><span class="s88">𝑡𝑡 </span>is the weight of a particular timestep. This can be used to assign more weight to special days (such as weekends or promotion days).</p></li><li><p class="s5" style="padding-top: 6pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Normalized Deviation <span class="p">(</span>ND<span class="p">): This is a metric that is strictly used to calculate aggregate measures across a dataset of time series. This is also one of the popular metrics used in the industry to measure aggregate performance across different time series. This is not scale-free and will be skewed toward large-scale time series. This metric has strong connections with another metric called the </span>Weighted Average Percent Error <span class="p">(</span>WAPE<span class="p">). We will discuss these connections when we talk about the WAPE in the following sections.</span></p></li></ul></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><a name="bookmark730">To calculate ND, we just sum all the absolute errors across the horizons and time series and scale it by the actual observations across the horizons and time series:</a></p><p class="s317" style="padding-top: 4pt;text-indent: 0pt;line-height: 9pt;text-align: right;">∑<span class="s50">M </span>∑<span class="s50">H</span></p><p class="s85" style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">|e<span class="s50">(t,m)</span>|</p><p class="s80" style="padding-top: 2pt;text-indent: 0pt;text-align: right;">ND =</p><p class="s50" style="padding-left: 10pt;text-indent: 0pt;line-height: 9pt;text-align: left;">m=1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="121" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_865.png"/></span></p><p class="s317" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;line-height: 9pt;text-align: left;">∑<span class="s50">M</span></p><p class="s50" style="padding-left: 8pt;text-indent: 0pt;line-height: 9pt;text-align: left;">t=1</p><p class="s317" style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;line-height: 9pt;text-align: left;">∑<span class="s50">H</span></p><p class="s80" style="padding-top: 11pt;padding-left: 1pt;text-indent: 0pt;line-height: 8pt;text-align: left;">|y<span class="s82">t,m</span>|</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Squared error</p><p class="s50" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;">m=1</p><p class="s50" style="padding-left: 8pt;text-indent: 0pt;line-height: 9pt;text-align: left;">t=1</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Squaring is another function that makes the error positive and thereby prevents the errors from canceling each other out:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s65" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"/><p class="s41" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆 𝐸𝐸𝑆𝑆𝑆𝑆𝐸𝐸𝑆𝑆(𝑆𝑆𝐸𝐸) = 𝑆𝑆2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">There are many metrics that are based on this error:</p><p class="s75" style="padding-left: 102pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝐻𝐻</p><ul id="l180"><li><p class="s146" style="padding-bottom: 1pt;padding-left: 55pt;text-indent: -13pt;line-height: 11pt;text-align: left;"><span class="s5">Mean Squared Error</span><span class="p">: </span>MSE =  <span class="s195">1</span> ∑(𝑒𝑒2)</p><p style="padding-left: 175pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="10" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_866.png"/></span></p><p class="s146" style="text-indent: 0pt;line-height: 11pt;text-align: right;">𝐻𝐻</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝑡𝑡=1</p><p class="s75" style="padding-left: 6pt;text-indent: 0pt;line-height: 6pt;text-align: left;">𝑡𝑡                  <span class="s318">                 </span></p><p class="s231" style="padding-top: 2pt;padding-left: 47pt;text-indent: 0pt;line-height: 5pt;text-align: left;">𝐻𝐻</p><p class="s75" style="padding-left: 38pt;text-indent: 0pt;line-height: 2pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_867.png"/></span></p></li><li><p class="s5" style="padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: left;">Root Mean Squared Error <span class="p">(</span>RMSE<span class="p">): </span><span class="s277">𝑅𝑅</span><span class="s75">MSE = √</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s75" style="text-indent: 0pt;text-align: left;">𝐻𝐻</p><p class="s75" style="padding-top: 1pt;text-indent: 0pt;text-align: left;">∑(𝑒𝑒2)</p><p class="s231" style="text-indent: 0pt;line-height: 5pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"/><p class="s231" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">𝑡𝑡=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 241pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="9" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_868.png"/></span></p></li><li><p class="s5" style="padding-left: 55pt;text-indent: -13pt;text-align: left;">Root Median Squared Error<span class="p">: </span><span class="s75">𝑅𝑅MdSE = 𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚 (√𝑚𝑚2)          </span><span class="s318">                </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s231" style="text-indent: 0pt;line-height: 5pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"/><p class="s213" style="text-indent: 0pt;line-height: 3pt;text-align: right;">2𝑛𝑛</p><p class="s64" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">𝐻𝐻</p><p class="s64" style="text-indent: 0pt;line-height: 6pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p class="s65" style="padding-left: 55pt;text-indent: -13pt;line-height: 13pt;text-align: left;"><span class="s5">Geometric Root Mean Squared Error</span><span class="p">: </span>𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺  =    <span class="s214">√</span>∏<span class="s214">(</span>𝑒𝑒2<span class="s214">)</span></p><p class="s64" style="padding-top: 1pt;padding-left: 117pt;text-indent: 0pt;text-align: center;">𝑡𝑡=1</p></li><li><p class="s5" style="padding-top: 2pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Normalized Root Mean Squared Error <span class="p">(</span>NRMSE<span class="p">): This is a metric that is very similar to ND in spirit. The only difference is that we take the square root of the squared errors in the numerator rather than the absolute error:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 206pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="118" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_869.png"/></span></p><p class="s109" style="text-indent: 0pt;line-height: 11pt;text-align: right;">√ <span class="s319">1 </span><span class="s146">∑</span><span class="s273">M</span></p><p style="padding-left: 206pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="19" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_870.png"/></span></p><p class="s146" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 7pt;text-align: left;">∑H (e2 )</p><p style="text-indent: 0pt;text-align: left;"><span><img width="129" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_871.png"/></span></p><p class="s146" style="padding-top: 3pt;text-indent: 0pt;line-height: 6pt;text-align: right;">NRMSE =</p><p class="s246" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 49%;text-align: left;">MH<span class="s146"> </span><span class="s75">m=1</span></p><p class="s75" style="padding-left: 6pt;text-indent: 0pt;line-height: 6pt;text-align: left;">t=1 (t,m)</p><p class="s320" style="padding-top: 2pt;text-indent: 0pt;line-height: 30%;text-align: right;">  <span class="s321">1</span><span class="s146"> </span><span class="s294">∑</span><span class="s79">M</span></p><p class="s146" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 6pt;text-align: left;">∑H |y |</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Percent error</p><p class="s246" style="padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: left;">MH <span class="s75">m=1</span></p><p class="s75" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">t=1 t,m</p><p style="text-indent: 0pt;text-align: left;"><span><img width="20" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_872.png"/></span></p><p class="s64" style="text-indent: 0pt;line-height: 7pt;text-align: left;">𝑦𝑦<span class="s296">𝑡𝑡</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s64" style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 90%;text-align: left;"><span class="p">While absolute error and squared error are scale-dependent, percent error is a scale-free error measure. In percent error, we scale the error using the actual time series observations: </span><span class="s289">𝑃</span>𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃 𝐸𝐸𝑃𝑃𝑃𝑃𝐸𝐸𝑃𝑃(𝑃𝑃𝐸𝐸) = <span class="s322">100</span>𝑃𝑃<span class="s226">𝑡</span><span class="s234">𝑡</span><span class="p">. Some</span></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">of the metrics that use percent error are as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s54" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;line-height: 5pt;text-align: left;">𝐻𝐻</p></li><li><p class="s5" style="padding-left: 55pt;text-indent: -13pt;line-height: 14pt;text-align: left;">Mean Absolute Percent Error <span class="p">(</span>MAPE<span class="p">) – </span><span class="s50">MAPE = ∑</span><span class="s84"> </span><span class="s52">100|𝑒𝑒</span><span class="s323">𝑡𝑡</span><span class="s324">|</span></p><p class="s50" style="padding-left: 139pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝑦𝑦<span class="s126">𝑡𝑡</span></p><p class="s54" style="padding-left: 173pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝑡𝑡=1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="43" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_873.png"/></span></p></li><li><p class="s74" style="padding-left: 55pt;text-indent: -13pt;line-height: 17pt;text-align: left;"><span class="s5">Median Absolute Percent Error </span><span class="p">– </span><span class="s250">MdA</span>PE = median (<span class="s325">1</span>00<span class="s222">|</span><span class="s325">𝑒</span>𝑒<span class="s274">𝑡</span><span class="s75">𝑡</span><span class="s222">|</span><span class="s250">)</span></p><p class="s74" style="padding-left: 161pt;text-indent: 0pt;line-height: 11pt;text-align: center;">𝑦𝑦<span class="s79">𝑡𝑡</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l181"><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><a name="bookmark731">WAPE </a><span class="p">– WAPE is a metric that embraces scale dependency and explicitly weights the errors with the scale of the timestep. If we want to give more focus to high values in the horizon, we can weigh those timesteps more than the others. Instead of taking a simple mean, we use a weighted mean on the absolute percent error. We can choose the weight to be anything but more often than not, it is chosen as the quantity of the observation itself. And in that special case, the math (with some assumptions) works out to be a simple formula which reminds us of ND. The difference is that ND is a metric which aggregates across multiple time series, and WAPE is a metric which weights across timestep:</span></p><p class="s157" style="padding-top: 5pt;padding-left: 88pt;text-indent: 0pt;line-height: 7pt;text-align: center;">∑H   |𝑒𝑒<span class="s88">𝑡𝑡 </span>|</p><p class="s157" style="padding-left: 173pt;text-indent: 0pt;line-height: 12pt;text-align: center;">WAPE =<u>    </u><span class="s160">t=1          </span></p><p class="s157" style="padding-left: 87pt;text-indent: 0pt;line-height: 8pt;text-align: center;">∑H   |𝑦𝑦<span class="s88">𝑡𝑡 </span>|</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Symmetric error</p><p class="s65" style="padding-left: 37pt;text-indent: 0pt;line-height: 9pt;text-align: left;">t=1</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Percent error has a few problems – it is asymmetrical (we will see this in detail later in the chapter), and it breaks down when the actual observation is zero (due to division by zero). Symmetric error was proposed as an alternative to avoid this asymmetry, but as it turns out, symmetric error is itself asymmetric – more on that later, but for now, let’s see what symmetric error is:</p><p class="s52" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;line-height: 16pt;text-align: center;"><span class="s50">𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆 𝐸𝐸𝑆𝑆𝑆𝑆𝐸𝐸𝑆𝑆(𝑆𝑆𝐸𝐸) = </span><span class="s327"> </span><span class="s51"> </span>200<span class="s324">|𝑆</span>𝑆<span class="s323">𝑡𝑡</span><span class="s324">|</span>   </p><p class="s50" style="padding-left: 141pt;text-indent: 0pt;line-height: 10pt;text-align: center;">|𝑆𝑆<span class="s126">𝑡𝑡</span><span class="s54"> </span>| + |𝑆𝑆̂<span class="s126">𝑡</span><span class="s54">𝑡</span>|</p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are only two metrics that are popularly used under this base error:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Symmetric Mean Absolute Percent Error <span class="p">(</span>sMAPE<span class="p">):</span></p><p class="s43" style="padding-left: 173pt;text-indent: 0pt;line-height: 6pt;text-align: center;">𝐻𝐻</p><p class="s137" style="padding-left: 85pt;text-indent: 0pt;line-height: 9pt;text-align: center;">1          200|𝑒𝑒<span class="s42">𝑡𝑡</span>|</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_874.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="63" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_875.png"/></span></p><p class="s328" style="padding-left: 40pt;text-indent: 0pt;line-height: 12pt;text-align: center;">𝑠𝑠𝑠𝑠𝑠𝑠<span class="s137">𝑠𝑠𝑠𝑠 = 𝐻𝐻 </span>∑<span class="s137"> |𝑦𝑦  | + |𝑦𝑦̂ |</span></p><p class="s43" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">𝑡𝑡=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s5" style="padding-left: 64pt;text-indent: -13pt;text-align: left;">Symmetric Median Absolute Percent Error<span class="p">:</span></p><p class="s43" style="text-indent: 0pt;line-height: 8pt;text-align: right;">𝑡𝑡</p><p class="s43" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="57" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_876.png"/></span></p><p class="s74" style="text-indent: 0pt;line-height: 10pt;text-align: left;">200|𝑚𝑚<span class="s79">𝑡</span><span class="s75">𝑡</span>|</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 10pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Other intrinsic metrics</p><p class="s74" style="padding-left: 12pt;text-indent: 0pt;line-height: 83%;text-align: left;">𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 = 𝑚𝑚𝑚𝑚𝑠𝑠𝑚𝑚𝑚𝑚𝑚𝑚 (<span class="s77">|</span></p><p class="s74" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">𝑦𝑦<span class="s79">𝑡</span><span class="s75">𝑡</span>|</p><p class="s74" style="padding-left: 25pt;text-indent: 0pt;line-height: 7pt;text-align: left;">)</p><p class="s74" style="text-indent: 0pt;line-height: 11pt;text-align: left;">+ |𝑦𝑦̂<span class="s79">𝑡𝑡</span><span class="s75"> </span>|</p><p style="padding-top: 9pt;padding-left: 36pt;text-indent: 0pt;text-align: center;">There are a few other metrics that are intrinsic in nature but don’t conform to the other metrics. Notable among those are three metrics that measure the over- or under-forecasting aspect of forecasts:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Cumulative Forecast Error <span class="p">(</span>CFE<span class="p">) – CFE is simply the sum of all the errors, including the sign of the error. Here, we want the positives and negatives to cancel each other out so that we understand whether a forecast is consistently over- or under-forecasting in a given horizon. A CFE close to zero means the forecasting model is neither over- nor under-forecasting:</span></p><p class="s65" style="padding-top: 6pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝐻𝐻</p><p class="s157" style="padding-top: 4pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝐶𝐶𝐹𝐹𝐹𝐹  = ∑ 𝑒𝑒<span class="s88">𝑡𝑡</span></p><p class="s65" style="padding-top: 2pt;padding-left: 173pt;text-indent: 0pt;text-align: center;">𝑡𝑡=1</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a name="bookmark732">Forecast Bias </a><span class="p">– While CFE measures the degree of over- and under-forecasting, it is still scale- dependent. When we want to compare across time series or have an intuitive understanding of the degree of over- or under-forecasting, we can scale CFE by the actual observations. This is called Forecast Bias:</span><a name="bookmark717">&zwnj;</a></p><p class="s329" style="padding-top: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">∑<span class="s127">𝐻𝐻</span></p><p class="s90" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 5pt;text-align: left;">𝐹𝐹<span class="s128">𝑡𝑡</span></p><p class="s90" style="padding-top: 2pt;padding-left: 12pt;text-indent: 0pt;line-height: 7pt;text-align: center;">𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹 𝐵𝐵𝐵𝐵𝐹𝐹𝐹𝐹  = <u>    𝑡𝑡=1     </u></p><p class="s90" style="text-indent: 0pt;line-height: 11pt;text-align: left;">∑</p><p style="text-indent: 0pt;text-align: left;"/><p class="s127" style="padding-left: 256pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝐻𝐻</p><p class="s127" style="padding-left: 256pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝑡𝑡=1</p><p class="s90" style="text-indent: 0pt;text-align: left;">𝑦𝑦<span class="s128">𝑡𝑡</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="8" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_877.gif"/></span></p></li><li><p class="s5" style="padding-top: 3pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Tracking Signal <span class="p">– The Tracking Signal is another metric that is used to measure the same over- and under-forecasting in forecasts. While CFE and Forecast Bias are used more offline, the Tracking Signal finds its place in an online setting where we are tracking over- and under- forecasting over periodic time intervals, such as every hour or every week. It helps us detect structural biases in the forecasting model. Typically, the Tracking Signal is used along with a threshold value so that going above or below it throws a warning. Although a thumb rule is to use 3.75, it is totally up to you to decide the right threshold for your problem:</span></p><p class="s315" style="padding-top: 2pt;text-indent: 0pt;line-height: 8pt;text-align: right;">∑<span class="s65">𝑤𝑤</span></p><p class="s157" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 6pt;text-align: left;">𝑒𝑒<span class="s88">𝑡𝑡</span></p><p class="s157" style="padding-top: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">𝑇𝑇𝑆𝑆</p><p class="s284" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 24%;text-align: left;">=<u> </u><span class="s160">𝑡𝑡=0           </span></p><p class="s65" style="text-indent: 0pt;line-height: 9pt;text-align: right;">𝑤𝑤</p><p class="s330" style="padding-top: 2pt;padding-left: 14pt;text-indent: 0pt;line-height: 32%;text-align: left;"> <span class="s316">1</span><span class="s157"> </span><span class="s284">∑</span><span class="s88">𝑤𝑤</span></p><p class="s157" style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">|𝑒𝑒 |</p><p class="s157" style="text-indent: 0pt;line-height: 13pt;text-align: right;">𝑤𝑤</p><p class="s65" style="padding-left: 9pt;text-indent: 0pt;line-height: 10pt;text-align: left;">𝑡𝑡=0</p><p class="s65" style="padding-left: 7pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 55pt;text-indent: 0pt;text-align: left;">Here, <span class="s137">𝑤𝑤 </span>is the past window over which <i>TS </i>is calculated.</p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s turn our attention to a few extrinsic metrics.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Extrinsic metrics</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are two major buckets of metrics under the extrinsic umbrella – relative error and scaled error.</p><p class="s24" style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Relative error</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">One of the problems of intrinsic metrics is that they don’t mean a lot unless a benchmark score exists. For instance, if we hear that the MAPE is 5%, it doesn’t mean a lot because we don’t know how forecastable that time series is. Maybe 5% is a bad error rate. Relative error solves this by including a benchmark forecast in the calculation so that the errors of the forecast we are measuring are measured against the benchmark and thus show the relative gains of the forecast. Therefore, in addition to the notation that we have established, we need to add a few more.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 7pt;text-align: justify;">Let <span class="s41">𝑦𝑦∗ </span>be the forecast from the benchmark and <span class="s137">𝑒𝑒∗ = 𝑦𝑦</span><span class="s42">𝑡𝑡 </span><span class="s137">− 𝑦𝑦∗ </span>be the benchmark error. There are two</p><p class="s43" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑡𝑡</p><p class="s43" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑡𝑡</p><p class="s43" style="padding-left: 43pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑡𝑡</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;">ways we can include the benchmark in the metric:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Using errors from the benchmark forecast to scale the error of the forecast</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Using forecast measures from the benchmark forecast to scale the forecast measure of the forecast we are measuring</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a name="bookmark733">Let’s look at a few metrics which follow these:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s231" style="text-indent: 0pt;line-height: 4pt;text-align: right;">𝐻𝐻</p><p class="s127" style="text-indent: 0pt;line-height: 1pt;text-align: right;">1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s127" style="padding-left: 2pt;text-indent: 0pt;line-height: 3pt;text-align: left;">|𝑒𝑒<span class="s331">𝑡𝑡 </span>|</p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_878.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_879.png"/></span></p><ul id="l182"><li><p class="s127" style="padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: left;"><span class="s5">Mean Relative Absolute Error </span><span class="p">(</span><span class="s5">MRAE</span><span class="p">): </span>𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀 = <span class="s332">𝐻𝐻</span> ∑<span class="s87">|</span><span class="s332">𝑒</span>𝑒<span class="s333">∗</span><span class="s87">|</span></p><p class="s231" style="padding-top: 2pt;text-indent: 0pt;line-height: 4pt;text-align: right;">𝑡𝑡=1</p><p class="s231" style="padding-left: 5pt;text-indent: 0pt;line-height: 5pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"><span><img width="23" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_880.png"/></span></p></li><li><p class="s90" style="padding-left: 64pt;text-indent: -13pt;line-height: 17pt;text-align: left;"><span class="s5">Median Relative Absolute Error</span><span class="p">: </span>𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀 = 𝑚𝑚𝑚𝑚𝑀𝑀𝑚𝑚𝑚𝑚𝑚𝑚 (<span class="s334">|</span><span class="s335">𝑚</span>𝑚<span class="s152">𝑡𝑡</span><span class="s127"> </span><span class="s334">|</span>)</p><p class="s127" style="text-indent: 0pt;line-height: 8pt;text-align: left;">𝑡𝑡</p><p style="text-indent: 0pt;text-align: left;"/><p class="s336" style="padding-left: 272pt;text-indent: 0pt;line-height: 10pt;text-align: left;"> <span class="s337">        </span><span class="s338">   </span><span class="s90">|𝑚𝑚∗|</span></p><p class="s234" style="text-indent: 0pt;line-height: 4pt;text-align: left;">𝐻𝐻</p><p style="text-indent: 0pt;text-align: left;"/></li><li><p class="s220" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;line-height: 1pt;text-align: left;"><span class="s5">Geometric Mean Relative Absolute Error</span><span class="p">: </span>𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺 = <span class="s235">√</span>∏<span class="s339">|</span> <span class="s296">∗</span><span class="s339">|</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 281pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="11" height="0" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_881.png"/></span></p><p class="s228" style="text-indent: 0pt;line-height: 4pt;text-align: right;">𝐻𝐻</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s234" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">𝑡𝑡=1</p><p class="s220" style="text-indent: 0pt;text-align: left;">|𝑒𝑒<span class="s296">𝑡𝑡</span><span class="s234"> </span>|</p><p class="s220" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">𝑒𝑒<span class="s296">𝑡𝑡</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="31" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_882.png"/></span></p></li><li><p style="padding-top: 3pt;padding-left: 64pt;text-indent: -13pt;line-height: 14pt;text-align: left;"><span class="s5">Relative Mean Absolute Error </span>(<span class="s5">RelMAE</span>): <span class="s50">𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅 =  </span><span class="s84">𝑅𝑅𝑅𝑅𝑅</span><span class="s50">𝑅 </span>, where <span class="s146">𝑀𝑀𝑀𝑀𝐸𝐸∗ </span>is the MAE of the</p><p style="padding-top: 3pt;padding-left: 64pt;text-indent: 0pt;text-align: left;">benchmark forecast</p><p class="s50" style="text-indent: 0pt;line-height: 10pt;text-align: right;">𝑅𝑅𝑅𝑅𝑅𝑅<span class="s292">∗</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅</p><p style="text-indent: 0pt;text-align: left;"><span><img width="41" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_883.png"/></span></p></li><li><p class="s5" style="padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: left;">Relative Root Mean Squared Error <span class="p">(</span>RelRMSE<span class="p">): </span><span class="s74">RelRMSE =</span></p><p style="padding-left: 64pt;text-indent: 0pt;line-height: 13pt;text-align: left;">RMSE of the benchmark forecast</p><p class="s74" style="padding-top: 7pt;text-indent: 0pt;text-align: left;">𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅</p><p style="text-indent: 0pt;line-height: 15pt;text-align: left;"><span class="s251">∗ </span>, where <span class="s65">𝑅𝑅𝑅𝑅𝑅𝑅𝐸𝐸∗ </span>is the</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;"><span class="s5">Average Relative Root Mean Squared Error</span>: Davydenko and Fildes (#2 in <i>References</i>) proposed another metric that is strictly for calculating aggregate scores across time series. They argued that using a geometric mean over the RelMAEs of individual time series is better than an arithmetic mean, so they defined the Average Relative Root Mean Squared Error as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 259pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="113" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_884.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: left;">∑</p><p style="text-indent: 0pt;text-align: left;"/><p class="s75" style="padding-top: 3pt;padding-left: 222pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑀𝑀</p><p class="s75" style="padding-left: 222pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑚𝑚=1</p><p class="s75" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">ℎ<span class="s283">𝑚𝑚</span></p><p class="s50" style="padding-top: 1pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">𝑀𝑀</p><p class="s80" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;line-height: 11pt;text-align: left;">𝐴𝐴𝐴𝐴𝐴𝐴<span class="s82">𝑚𝑚</span></p><p class="s50" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">ℎ<span class="s79">𝑚𝑚</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="47" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_885.png"/></span></p><p class="s80" style="padding-left: 12pt;text-indent: 0pt;line-height: 35%;text-align: center;">𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴  =              √∏ (<span class="s340">𝐴𝐴𝐴</span>𝐴𝐴𝐴<span class="s341">∗</span><span class="s50">  </span>)</p><p class="s50" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">𝑚𝑚=1</p><p class="s50" style="padding-left: 32pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑚𝑚</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Scaled error</p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;">Hyndman and Koehler introduced the idea of scaled error in 2006. This was an alternative to relative error and measures and tries to get over some of the drawbacks and subjectivity of choosing the benchmark forecast. Scaled error scales the forecast error using an in-sample MAE of a benchmark method such as naïve forecasting. Let the entire training history be of <span class="s137">𝑇𝑇 </span>timesteps, indexed by <span class="s41">𝑖𝑖</span>.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">So, the scaled error is defined as follows:</p><p class="s74" style="padding-top: 12pt;text-indent: 0pt;line-height: 38%;text-align: right;">𝑆𝑆𝑆𝑆 =<u> 1</u><u>    </u></p><p class="s74" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">|𝑒𝑒<span class="s79">𝑡𝑡 </span>|</p><p style="text-indent: 0pt;text-align: left;"><span><img width="113" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_886.png"/></span></p><p class="s75" style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;line-height: 2pt;text-align: left;">𝑇𝑇</p><p class="s74" style="padding-left: 64pt;text-indent: 0pt;text-align: center;"><span class="s343">𝑇𝑇</span> − 1 <span class="s344">∑</span><span class="s75">𝑖𝑖=2</span><span class="s344">|</span><span class="s224">𝑦</span>𝑦<span class="s75">𝑡𝑡</span><span class="s224">–</span> 𝑦𝑦<span class="s75">𝑡𝑡−1</span><span class="s344">|</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">There are a couple of metrics that adopt this principle:</p><p class="s64" style="padding-left: 78pt;text-indent: 0pt;line-height: 5pt;text-align: center;">H</p><p class="s43" style="padding-left: 58pt;text-indent: 0pt;line-height: 3pt;text-align: center;">1</p></li><li><p class="s5" style="padding-left: 64pt;text-indent: -13pt;line-height: 13pt;text-align: left;">Mean Absolute Scaled Error <span class="p">(</span>MASE<span class="p">): </span><span class="s44">MASE</span><span class="s43"> =</span></p><p class="s43" style="padding-top: 1pt;padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;">∑|𝑆𝑆𝑆𝑆|</p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_887.png"/></span></p><p class="s43" style="padding-left: 3pt;text-indent: 0pt;line-height: 6pt;text-align: left;">H</p><p class="s64" style="padding-left: 10pt;text-indent: 0pt;line-height: 3pt;text-align: left;">t=1</p></li><li><p class="s5" style="padding-left: 64pt;text-indent: -13pt;text-align: left;">Root Mean Squared Scaled Error <span class="p">(</span>RMSSE<span class="p">): A similar scaled error was developed for the squared error and was used in the M5 Forecasting Competition in 2020:</span></p><p class="s75" style="text-indent: 0pt;line-height: 6pt;text-align: right;">𝐻𝐻</p><p class="s146" style="text-indent: 0pt;line-height: 3pt;text-align: right;">1</p><p class="s247" style="padding-top: 3pt;padding-left: 45pt;text-indent: 0pt;line-height: 6pt;text-align: left;">𝑒<span class="s146">𝑒</span><span class="s75">2</span></p><p class="s146" style="padding-top: 2pt;text-indent: 0pt;line-height: 6pt;text-align: right;">𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅 =</p><p class="s294" style="padding-top: 2pt;padding-left: 9pt;text-indent: 0pt;line-height: 24%;text-align: left;">∑<u> </u><span class="s345">𝑡 𝑡                             </span></p><p style="text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="10" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_888.png"/></span></p><p class="s245" style="text-indent: 0pt;line-height: 10pt;text-align: right;">𝐻𝐻 <span class="s75">𝑡𝑡=1</span><span class="s346"> </span><u>1</u><span class="s146"> </span><span class="s93">∑</span><span class="s221">𝑇𝑇</span></p><p class="s146" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">(𝑦𝑦</p><p class="s146" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 6pt;text-align: left;">− 𝑦𝑦 )<span class="s273">2</span></p><p class="s146" style="text-indent: 0pt;line-height: 10pt;text-align: right;">𝑇𝑇 − 1</p><p class="s75" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">𝑖𝑖=2</p><p class="s75" style="text-indent: 0pt;line-height: 7pt;text-align: right;">𝑡𝑡</p><p class="s75" style="padding-left: 15pt;text-indent: 0pt;line-height: 7pt;text-align: left;">𝑡𝑡−1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark734">Other extrinsic metrics</a><a name="bookmark719">&zwnj;</a><a name="bookmark718">&zwnj;</a></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are other extrinsic metrics that don’t fall into the categorization of errors we have made. One such error measure is the following:</p><p class="s5" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Percent Better <span class="p">(</span>PB<span class="p">) is a method that is based on counts and can be applied to individual time series as well as a dataset of time series. The idea here is to use a benchmark method and count how many times a given method is better than the benchmark and report it as a percentage. Formally, we can define it using MAE as the reference error as follows:</span></p><p class="s41" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">𝑃𝑃𝐵𝐵<span class="s88">𝑀</span><span class="s65">𝑀𝑀𝑀𝑀𝑀  </span>= 100 𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚(𝕀𝕀{𝑀𝑀𝑀𝑀𝑀𝑀 &lt; 𝑀𝑀𝑀𝑀𝑀𝑀∗})</p><p style="padding-top: 10pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Here, <span class="s80">𝕀𝕀 </span>is an indicator function that returns 1 if the condition is true and 0 otherwise.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We have seen a lot of metrics in the previous sections, but now it’s time to understand a bit more about the way they work and what they are suited for.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Investigating the error measures</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">It’s not enough to know the different metrics since we also need to understand how these work, what are they good for, and what are they not good for. We can start with the basic errors and work our way up because understanding the properties of basic errors such as <i>absolute error</i>, <i>squared error</i>, <i>percent error</i>, and <i>symmetric error </i>will help us understand the others as well because most of the other metrics are derivatives of these primary errors; either aggregating them or using relative benchmarks.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_889.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The notebook for running these experiments on your own is <span class="s20">01-Loss Curves and Symmetry.ipynb </span>in the <span class="s20">Chapter18 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s do this investigation using a few experiments and understand them through the results.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Loss curves and complementarity</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">All these base errors depend on two factors – forecasts and actual observations. We can examine the behavior of these several metrics if we fix one and alter the other in a symmetric range of potential errors. The expectation is that the metric will behave the same way on both sides because deviation from the actual observation on either side should be equally penalized in an unbiased metric. We can also swap the forecasts and actual observations and that also should not affect the metric.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the notebook, we did exactly these experiments – loss curves and complementary pairs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark735">Absolute error</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">When we plot these for absolute error, we get <i>Figure 18.2</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 42pt;text-indent: 0pt;text-align: left;"><span><img width="511" height="502" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_890.jpg"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 18.2 – The loss curves and complementary pairs for absolute error</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The first chart plots the signed error against the absolute error and the second one plots the absolute error with all the combinations of actuals and forecast, which add up to 10. The two charts are obviously symmetrical, which means that an equal deviation from the actual observed on either side is penalized equally, and if we swap the actual observation and the forecast, the metric remains unchanged.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark736">Squared error</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at squared error:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="521" height="512" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_891.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 18.3 – The loss curves and complementary pairs for squared error</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">These charts also look symmetrical, so the squared error also doesn’t have an issue with asymmetric error distribution – but we can notice one thing here. The squared error increases exponentially as the error increases. This points to a property of the squared error – it gives undue weightage to outliers. If there are a few timesteps for which the forecast is really bad and excellent at all other points, the squared error inflates the impact of those outlying errors.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark737">Percent error</a></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now, let’s look at percent error:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 54pt;text-indent: 0pt;text-align: left;"><span><img width="478" height="474" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_892.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 18.4 – The loss curves and complementary pairs for percent error</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">There goes our symmetry. The percent error is symmetrical when you move away from the actuals on both sides (mostly because we are keeping the actuals constant), but the complementary pairs tell us a whole different story. When the actual is 1 and the forecast is 9, the percent error is 8, but when we swap them, the percent error drops to 1. This kind of asymmetry can cause the metric to favor under-forecasting. The right half of the second chart in <i>Figure 18.4 </i>are all cases where we are under- forecasting and we can see that the error is very low there when compared to the left half.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We will look at under- and over-forecasting in detail in another experiment.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark738">Symmetric error</a></p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">For now, let’s move on and look at the last error we had – symmetric error:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><span><img width="502" height="494" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_893.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 18.5 – The loss curves and complementary pairs for symmetric error</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Symmetric error was proposed mainly because of the asymmetry we saw in the percent error. MAPE, which uses percent error, is one of the most popular metrics used and sMAPE was proposed to directly challenge and replace MAPE – true to its claim, it did resolve the asymmetry that was present in percent error. However, it introduced its own asymmetry. In the first chart, we can see that for a particular actual value, if the forecast moves on either side, it is penalized differently, so in effect, this metric favors over-forecasting (which is in direct contrast to percent error, which favors under-forecasting).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark739">Extrinsic errors</a><a name="bookmark720">&zwnj;</a></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">With all the intrinsic measures done, we can also take a look at the extrinsic ones. With extrinsic measures, plotting the loss curves and checking symmetry is not as easy. Instead of two variables, we now have three – the actual observation, the forecast, and the reference forecast; the value of the measure can vary with any of these. We can use a contour plot for this as shown in <i>Figure 18.6</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span><img width="519" height="229" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_894.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 18.6 – Contour plot of the loss surface – relative absolute error and absolute scaled error</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The contour plot enables us to plot three dimensions in a 2D plot. The two dimensions (error and reference forecast) are on the <i>x</i>- and <i>y</i>-axes. The third dimension (the relative absolute error and absolute scaled error values) is represented as color, with contour lines bordering same-colored areas. The errors are symmetric around the error (horizontal) axis. This means that if we keep the reference forecast constant and vary the error, both measures vary equally on both sides of the errors. This is not surprising since both these errors have their base in absolute error, which we know was symmetric.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">The interesting observation is the dependency on the reference forecast. We can see that for the same error, <i>relative absolute error </i>has different values for different reference forecasts, but <i>scaled error </i>doesn’t have this problem. This is because it is not directly dependent on the reference forecast and rather uses the MAE of a naïve forecast. This value is fixed for a time series and eliminates the task of choosing a reference forecast. Therefore, scaled error has good symmetry for absolute error and very little or fixed dependency on the reference forecast.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Bias towards over- or under-forecasting</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have seen indications of bias toward over- or under-forecasting in a few metrics that we saw. In fact, it looked like the popular metric, MAPE, favors under-forecasting. To finally put that to test, we can perform another experiment with synthetically generated time series and we included a lot more metrics in this experiment so that we know which are safe to use and which need to be looked at carefully.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_895.png"/></span></p><p class="s29" style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The notebook to run these experiments on your own is <span class="s20">02-Over and Under Forecasting. ipynb </span>in the <span class="s20">Chapter18 </span>folder.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The experiment is simple and detailed as follows:</p><ol id="l183"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">We randomly sample a count time series of integers with a length of 100 from a uniform distribution between <span class="s20">2 </span>and <span class="s20">5</span>:</p><p style="padding-top: 9pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><span class="s49" style=" background-color: #F3F2F1;">  np.random.randint(2,5,n)                                   </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 55pt;text-indent: -18pt;text-align: left;">We use the same process to generate a forecast, which is also drawn from a uniform distribution between <span class="s20">2 </span>and <span class="s20">5</span>:</p><p style="padding-top: 9pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><span class="s49" style=" background-color: #F3F2F1;">  np.random.randint(2,5,n)                                   </span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 55pt;text-indent: -18pt;text-align: left;">Now, we generate two additional forecasts, one from the uniform distribution between <span class="s20">0 </span>to <span class="s20">4 </span>and another between <span class="s20">3 </span>and <span class="s20">7</span>. The former predominantly under-forecasts and the latter over-forecasts:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">np.random.randint(0,4,n)<span class="s38"># Underforecast</span></p><p class="s28" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">np.random.randint(3,7,n) <span class="s38"># Overforecast</span></p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">We calculate all the measures we want to investigate using all three forecasts.</p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">We repeat it 10,000 times to average out the effect of random draws.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">After the experiment is done, we can plot a box plot of different metrics so that it shows the distribution of each metric for each of those three forecasts over these 10,000 runs of the experiment. Let’s see the box plot in <i>Figure 18.7</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: left;"><span><img width="441" height="234" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_896.jpg"/></span></p><p class="s37" style="padding-top: 9pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 18.7 – Over- and under-forecasting experiment</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark740">Let’s go over what we would expect from this experiment first. The over- (green) and under- (red) forecasted forecasts would have a higher error than the baseline (blue). The over- and under-forecasted errors would be similar.</a><a name="bookmark721">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">With that, let’s summarize our major findings:</p><ul id="l184"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">MAPE clearly favors the under-forecasted with a lower MAPE than the over-forecasted.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">WAPE, although based on percent error, managed to get over the problem by having explicit weighting. This may be counteracting the bias that percent error has.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">sMAPE, in its attempt to fix MAPE, does a worse job in the opposite direction. sMAPE highly favors over-forecasting.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Metrics such as MAE and RMSE, which are based on absolute error and squared error respectively, don’t show any preference for either over- or under-forecasting.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">MASE and RMSSE (both using versions of scaled error) are also fine.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">MRAE, in spite of some asymmetry regarding the reference forecast, turns out to be unbiased from the over- and under-forecasting perspective.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">The relative measures with absolute and squared error bases (RelMAE and RelRMSE) also do not have any bias toward over- or under-forecasting.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">The relative measure of mean absolute percentage error, RelMAPE, carries MAPE’s bias toward under-forecasting.</p></li></ul></li></ol><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We have investigated a few properties of different error measures and understood the basic properties of some of them. To further that understanding and move closer to helping us select the right measure for our problem, let’s do one more experiment using the London Smart Meters dataset we have been using through this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Experimental study of the error measures</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">As we discussed earlier, there are a lot of metrics for forecasting that people have come up with over the years. Although there are many different formulations of these metrics, there can be similarities in what they are measuring. Therefore, if we are going to choose a primary and secondary metric while modeling, we should pick some metrics that are diverse and measure different aspects of the forecast.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Through this experiment, we are going to try and figure out which of these metrics are similar to each other. We are going to use the subset of the <i>London Smart Meters </i>dataset we have been using all through the book and generate some forecasts for each household. I have chosen to do this exercise with the <span class="s20">darts </span>library because I wanted multi-step forecasting. I’ve used five different forecasting methods – <i>seasonal naïve</i>, <i>exponential smoothing</i>, <i>Theta</i>, <i>FFT</i>, and <i>LightGBM (local) </i>– and generated forecasts. On top of that, I have also calculated the following metrics on all of these forecasts – <i>MAPE</i>,</p><p class="s37" style="padding-top: 4pt;padding-left: 269pt;text-indent: 0pt;text-align: left;"><a name="bookmark741">Experimental study of the error measures 485</a><a name="bookmark722">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_897.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">WAPE<span class="p">, </span>sMAPE<span class="p">, </span>MAE<span class="p">, </span>MdAE<span class="p">, </span>MSE<span class="p">, </span>RMSE<span class="p">, </span>MRAE<span class="p">, </span>MASE<span class="p">, </span>RMSSE<span class="p">, </span>RelMAE<span class="p">, </span>RelRMSE<span class="p">, </span>RelMAPE<span class="p">,</span></p><p class="s4" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">CFE<span class="p">, </span>Forecast Bias<span class="p">, and </span>PB(MAE)<span class="p">. In addition to this, we also calculated a few aggregate metrics –</span></p><p class="s4" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">meanMASE<span class="p">, </span>meanRMSSE<span class="p">, </span>meanWAPE<span class="p">, </span>meanMRAE<span class="p">, </span>AvgRelRMSE<span class="p">, </span>ND<span class="p">, and </span>NRMSE<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Using Spearman’s rank correlation</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The basis of the experiment is that if different metrics measure the same underlying factor, then they will also rank forecasts on different households similarly. For instance, if we say that MAE and MASE are measuring one latent property of the forecast, then those two metrics would give similar rankings to different households. At the aggregated level, there are five different models and aggregate metrics that measure the same underlying latent factor and should also rank them in similar ways.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s look at the aggregate metrics first. We ranked the different forecast methods at the aggregate level using each of the metrics and then we calculated the Pearson correlation of the ranks. This gives us Spearman’s rank correlation between the forecasting methods and metrics. The heatmap of the correlation matrix is in <i>Figure 18.8</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 107pt;text-indent: 0pt;text-align: left;"><span><img width="318" height="273" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_898.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 47pt;text-indent: 0pt;text-align: left;">Figure 18.8 – Spearman’s rank correlation between the forecast methods and aggregate metrics</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">These are the major observations:</p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">We can see that <i>meanMASE</i>, <i>meanWAPE, </i>and <i>ND </i>(all based on absolute error) are highly correlated, indicating that they might be measuring similar latent factors of the forecast.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">The other pair that is highly correlated is <i>meanRMSSE </i>and <i>NRMSE</i>, which are both based on squared error.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l185"><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">There is a weak correlation between <i>meanMASE </i>and <i>meanRMSSE</i>, maybe because they are both using scaled error.</p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">meanMRAE <span class="p">and </span>Forecast Bias <span class="p">seem to be highly correlated, although there is no strong basis for that shared behavior. Some correlations can be because of chance and this needs to be validated further on more datasets.</span></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">meanMRAE <span class="p">and </span>AvgRelRMSE <span class="p">seem to be measuring very different latent factors from the rest of the metrics and each other.</span></p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Similarly, we calculated Spearman’s rank correlation between the forecast methods and metrics across all the households (<i>Figure 18.9</i>). This enables us to have the same kind of comparison as before at the item level:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 120pt;text-indent: 0pt;text-align: left;"><span><img width="306" height="279" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_899.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 18.9 – Spearman’s rank correlation between the forecast methods and item-level metrics</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">The major observations are as follows:</p></li><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">We can see there are five clusters of highly correlated metrics (the five green boxes).</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">The first group is <i>MASE </i>and <i>RMSSE</i>, which are highly correlated. This can be because of the scaled error formulation of both metrics.</p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">WAPE<span class="p">, </span>MAPE<span class="p">, and </span>sMAPE <span class="p">are the second group. Frankly, this is a bit confusing because I would have expected </span>MAPE <span class="p">and </span>sMAPE <span class="p">to have less correlated results. They do behave in the opposite way from an over- and under-forecasting perspective. Maybe all the forecasts we have used to check this correlation don’t over- or under-forecast and therefore the similarity came out through the shared percent error base. This needs to be investigated further.</span></p><p class="s37" style="padding-top: 4pt;padding-left: 303pt;text-indent: 0pt;text-align: left;"><a name="bookmark742">Guidelines for choosing a metric 487</a><a name="bookmark723">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_900.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p class="s4" style="padding-left: 55pt;text-indent: -13pt;text-align: justify;">MAE<span class="p">, </span>MdAE<span class="p">, </span>MSE<span class="p">, and </span>RMSE <span class="p">form the third group of highly similar metrics. </span>MAE <span class="p">and </span>MdAE <span class="p">are both absolute error metrics and </span>MSE <span class="p">and </span>RMSE <span class="p">are both squared error metrics. The similarity between these two can be because of the lack of outlying errors in the forecasts. The only difference between these two base errors is that squared error puts a much greater weight on outlying errors.</span></p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">The next group of similar metrics is the motley crew of relative measures – <i>MRAE, RelMAE</i>, <i>RelRMSE</i>, <i>RelMAPE</i>, and <i>PB(MAE)</i>, but the intercorrelation among this group is not as strong as the other groups. The pairs of metrics that stand out in terms of having low inter-correlations are <i>MRAE </i>and <i>RelRMSE </i>and <i>RelMAPE </i>and <i>RelRMSE</i>.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">The last group that stands totally apart with much less correlation with any other metric but a higher correlation with each other is <i>Forecast Bias </i>and <i>CFE</i>. Both are calculated on unsigned errors and measure the amount of over- or under-forecasting.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">If we look at intergroup similarities, the only thing that stands out is the similarity between the scaled error group and absolute error and squared error group.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="122" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_901.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Spearman’s rank correlation on aggregate metrics is done using a single dataset and has to be taken with a grain of salt. The item-level correlation has a bit more significance because it is made across many households, but there are still a few things in there that warrant further investigation. I urge you to repeat this experiment on some other datasets and check whether we see the same patterns repeated before adopting them as rules.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now that we have explored the different metrics, it is time to summarize and probably leave you with a few guidelines for choosing a metric.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Guidelines for choosing a metric</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Throughout this chapter, we have come to understand that it is difficult to choose one forecast metric and apply it universally. There are advantages and disadvantages for each metric and being cognizant of these while selecting a metric is the only rational way to go about it.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s summarize and note a few points we have seen through different experiments in the chapter:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Absolute error and squared error are both symmetric losses and are unbiased from the under- or over-forecasting perspective.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Squared error does have a tendency to magnify the outlying error because of the square term in it. Therefore, if we use a squared-error-based metric, we will be penalizing high errors much more than small errors.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">RMSE is generally preferred over MSE because RMSE is on the same scale as the original input and therefore is a bit more interpretable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l186"><li><p style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Percent error and symmetric error are not symmetric in the complete sense and favor under- forecasting and over-forecasting, respectively. MAPE, which is a very popular metric, is plagued by this shortcoming. For instance, if we are forecasting demand, optimizing for MAPE will lead you to select a forecast that is conservative and therefore under-forecast. This will lead to an inventory shortage and out-of-stock situations. sMAPE, with all its shortcomings, has fallen out of favor with practitioners.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Relative measures are a good alternative to percent-error-based metrics because they are also inherently interpretable, but relative measures depend on the quality of the benchmark method. If the benchmark method performs poorly, the relative measures will tend to dampen the impact of errors from the model under evaluation. On the other hand, if the benchmark forecast is close to an oracle forecast with close to zero errors, the relative measure will exaggerate the errors of the model. Therefore, you have to be careful when choosing the benchmark forecast, which is an additional thing to worry about.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Although a geometric mean offers a few advantages over an arithmetic mean (such as resistance to outliers and better approximation when there is high variation in data), it is not without its own problems. Geometric mean-based measures mean that even if a single series (when aggregating across time series) or a single timestep (when aggregating across timesteps) performs really well, it will make the overall error come down drastically due to the multiplication.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">PB, although an intuitive metric, has one disadvantage. We are simply counting the instances in which we perform better. However, it doesn’t assess how well or poorly we are doing. The effect on the PB score is the same whether our error is 50% less than the reference error or 1% less.</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Hewamalage et al. (#1 in <i>References</i>) have proposed a very detailed flowchart to aid in decision-making, but that is also more of a guideline as to what not to use. The selection of a single metric is a very debatable task. There are a lot of conflicting opinions out there and I’m just adding another to that noise. Here are a few guidelines I propose to help you pick a forecasting metric:</p></li><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Avoid <i>MAPE</i>. In any situation, there is always a better metric to measure what you want. At the very least, stick to <i>WAPE </i>for single time series datasets.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">For a single time series dataset, the best metrics to choose are <i>MAE </i>or <i>RMSE </i>(depending on whether you want to penalize large errors more or not).</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">For multiple time series datasets, use <i>ND </i>or <i>NRMSSE </i>(depending on whether you want to penalize large errors more or not). As a second choice, <i>meanMASE </i>or <i>meanRMSSE </i>can also be used.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">If there are large changes in the time series (in the horizon we are measuring, there is a huge shift in time series levels), then something such as <i>PB </i>or <i>MRAE </i>can be used.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Whichever metric you choose, always make sure to use <i>Forecast Bias</i>, <i>CFE</i>, or Tracking Signal to keep an eye on structural over- or under-forecasting problems.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark743">Summary 489</a><a name="bookmark725">&zwnj;</a><a name="bookmark724">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_902.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p style="padding-left: 55pt;text-indent: -13pt;text-align: justify;">If the time series you are forecasting is intermittent (as in, has a lot of time steps with zero values), use <i>RMSE </i>and avoid <i>MAE</i>. <i>MAE </i>favors forecasts that generate all zeros. Avoid all percent-error- based metrics because intermittency brings to light another one of their shortcomings – it is undefined when actual observations are zero (<i>Further reading </i>has a link to a blog that explores other metrics for intermittent series).</p></li></ul><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Congratulations on finishing a chapter full of new terms and metrics and I hope you have gained the necessary intuition to intelligently select the metric to focus on for your next forecasting assignment!</p><p class="s3" style="padding-top: 11pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we looked at the thickly populated and highly controversial area of forecast metrics. We started with a basic taxonomy of forecast measures to help you categorize and organize all the metrics in the field.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Then, we launched a few experiments through which we learned about the different properties of these metrics, slowly approaching a better understanding of what these metrics are measuring, but looking at synthetic time series experiments, we learned how <i>MAPE </i>and <i>sMAPE </i>favor under- and over-forecasting, respectively.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">We also analyzed the rank correlations between these metrics on real data to see how similar the different metrics are and finally, rounded off by laying out a few guidelines that can help you pick a forecasting metric for your problem.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the next chapter, we will look at cross-validation strategies for time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The following are the references that we used throughout the chapter:</p><ol id="l187"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Hewamalage, Hansika; Ackermann, Klaus; and Bergmeir, Christoph. (2022). <i>Forecast Evaluation for Data Scientists: Common Pitfalls and Best Practices</i><a href="https://arxiv.org/abs/2203.10716v2" class="s140" target="_blank">. arXiv preprint arXiv: Arxiv-2203.10716: </a><a href="https://arxiv.org/abs/2203.10716v2" target="_blank">https://arxiv.org/abs/2203.10716v2</a></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Davydenko, Andrey and Fildes, Robert. (2013). <i>Measuring forecasting accuracy: the case of judgmental adjustments to SKU-level demand forecasts</i>. In <i>International Journal of Forecasting</i><a href="https://doi.org/10.1016/j.ijforecast.2012.09.002" class="s140" target="_blank">. Vol. 29, No. 3., 2013, pp. 510-522: </a><a href="https://doi.org/10.1016/j.ijforecast.2012.09.002" target="_blank">https://doi.org/10.1016/j.ijforecast.2012.09.002</a></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Hyndman, Rob J. and Koehler, Anne B.. (2006). <i>Another look at measures of forecast accuracy</i>. In <i>International Journal of Forecasting</i><a href="https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/" class="s140" target="_blank">, Vol. 22, Issue 4, 2006, pp. 679-688: </a><a href="https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/" class="a" target="_blank">https:// robjhyndman.com/publications/another-look-at-measures-of- </a><a href="https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/" target="_blank">forecast-accuracy/</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark744">Further reading</a><a name="bookmark726">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">If you wish to read further about forecast metrics, you can check out the blog post <i>Forecast Error Measures: Intermittent Demand </i><a href="https://deep-and-shallow.com/2020/10/07/forecast-error-measures-intermittent-demand/" class="s140" target="_blank">by Manu Joseph – </a><a href="https://deep-and-shallow.com/2020/10/07/forecast-error-measures-intermittent-demand/" class="a" target="_blank">https://deep-and-shallow.com/2020/10/07/ </a><span class="s27">forecast-error-measures-intermittent-demand/</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 6pt;text-indent: 0pt;text-align: right;"><a name="bookmark745">19</a><a name="bookmark746">&zwnj;</a><a name="bookmark748">&zwnj;</a><a name="bookmark747">&zwnj;</a></h2><h4 style="padding-top: 2pt;padding-left: 161pt;text-indent: -15pt;line-height: 114%;text-align: left;">Evaluating Forecasts – Validation Strategies</h4><p style="padding-top: 24pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Throughout the last few chapters, we have been looking at a few relevant, but seldom discussed, aspects of time series forecasting. While we learned about different forecasting metrics in the previous chapter, we now move on to the final piece of the puzzle – validation strategies. This is another integral part of evaluating forecasts.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we try to answer the question <i>How do we choose the validation strategy to evaluate models from a time series forecasting perspective? </i>We will look at different strategies and their merits and demerits so that by the end of the chapter, you can make an informed decision to set up the validation strategy for your time series problem.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In this chapter, we will be covering these main topics:</p><ul id="l188"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Model validation</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Holdout strategies</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Cross-validation strategies</p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Choosing a validation strategy</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Validation strategies for datasets with multiple time series</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Technical requirements</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">You will need to set up the Anaconda environment following the instructions in the <i>Preface </i>of the book to get a working environment with all the packages and datasets required for the code in this book.</p><p class="s27" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 112%;text-align: justify;"><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter19" class="s140" target="_blank">The associated code for the chapter can be found at </a><a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter19" class="a" target="_blank">https://github.com/PacktPublishing/ Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/ </a>Chapter19.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark760">Model validation</a><a name="bookmark749">&zwnj;</a></p><p class="s4" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark711" class="s140">In </a>Chapter 18<span class="p">, </span>Evaluating Forecasts – Forecast Metrics<span class="p">, we learned about different forecast metrics that can be used to measure the quality of a forecast. One of the main uses for this is to measure how well our forecast is doing on test data (new and unseen data), but this comes after we train a model, tweak it, and tinker with it until we are happy with it. How do we know whether a model we are training or tweaking is good enough?</span></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark711" class="s140">Model validation is the process of evaluating a trained model using data to assess how good the model is. We use the metrics we learned about in </a><i>Chapter 18</i>, <i>Evaluating Forecasts – Forecast Metrics</i>, to calculate the goodness of the forecast. But, there is one question we haven’t answered. Which part of the data do we use to evaluate? In a standard machine learning setup (classification or regression), we randomly sample a portion of the training data and call it validation data, and it is based on this data that all the modeling decisions are taken. The best practice in the field is to use cross-validation. <span class="s5">Cross-validation </span>is a resampling procedure where we sample different portions of the training dataset to train and test in multiple iterations. In addition to repeated evaluation, cross-validation also makes the most efficient use of the data.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">But, in the field of time series forecasting, such a consensus on best practice does not exist. This is mainly because of the temporal nature and the sheer variety of ways we can go about it. Different time series might have different lengths of history and we may choose different ways to model it, or there might be different horizons to forecast for, and so on. Because of the temporal dependence in the data, standard assumptions of i.i.d. don’t hold true, therefore techniques such as cross-validation have their own complications. When randomly chosen, the validation and training datasets may not be independent, which will lead to an optimistic and misleading estimate of error.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">There are two main paradigms of validation:</p><ul id="l189"><li><p class="s5" style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">In-sample validation <span class="p">– As the name suggests, the model is evaluated on the same or a subset of the same data that was used to train it.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Out-of-sample validation <span class="p">– Under this paradigm, the data we use to evaluate the model has no intersection with the data used to train the model.</span></p><p style="padding-top: 9pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In-sample validation helps you understand how well your model has fit the data you have. This was very popular in the era of statistics, where the models were meticulously designed and primarily used for inferencing and not predicting. In such cases, the in-sample error shows how well the specified model fits the data and how valid the inferences we make from that model are. But in a predictive paradigm, like most of machine learning, the in-sample error is not the right measure of the <i>goodness </i>of a model. Complex models can easily fit the training data, memorize it, and not work well on new and unseen data. Therefore, out-of-sample validations are almost exclusively used in today’s predictive model evaluations. Since this book is solely concerned with forecasting, which is a predictive task, we will be sticking to out-of-sample evaluations only.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark761">As discussed earlier, deciding on a validation strategy for forecasting problems is not as trivial as standard machine learning. There are two major schools of thought here:</a><a name="bookmark751">&zwnj;</a><a name="bookmark750">&zwnj;</a></p></li></ul></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Holdout-based strategies, which respect the temporal integrity of the problem</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Cross-validation-based strategies, which sample validation splits with a very loose or no sense of temporal ordering</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;line-height: 93%;text-align: justify;">Let’s discuss the major ones in each category. What we have to keep in mind is that all the validation strategies that we discuss in the book are not exhaustive. They are merely a few popular ones. In the explanations that follow, the length of the validation period is <span class="s90">𝐿𝐿</span><span class="s128">𝑣𝑣 </span>and the length of the training period is <span class="s74">𝐿𝐿</span><span class="s79">𝑡𝑡</span>.</p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now, let’s look at the first school of thought.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Holdout strategies</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">There are three aspects of a holdout strategy, and they can be mixed and matched to create many variations of the strategy. The three aspects are as follows:</p></li><li><p class="s5" style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Sampling strategy <span class="p">– A sampling strategy is how we sample the validation split(s) from training data.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Window strategy <span class="p">– A window strategy decides how we sample the window of training split(s) from training data.</span></p></li><li><p class="s5" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Calibration strategy <span class="p">– A calibration strategy decides whether a model should be recalibrated or not.</span></p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">That said, designing a holdout validation strategy for a time series problem includes making decisions on these three aspects.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Sampling strategies are ways to pick one or more origins in the training data. These <span class="s5">origins </span>are point(s) in time that determine the starting point of the validation split and the ending point of the training split. The exact length of the validation split is governed by a parameter <span class="s90">𝐿𝐿𝑣𝑣</span>, which is the horizon chosen for validation. The length of the training split depends on the window strategy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Window strategy</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">There are two ways we can draw the windows of training split – expanding window and rolling window.</p><p class="s4" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Figure 19.1 <span class="p">shows the difference between the two setups:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><a name="bookmark762"><span><img width="507" height="226" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_903.gif"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 19.1 – Expanding (left) versus rolling (right) strategy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Under the expanding window strategy, the training split expands as the origin moves forward in time. In other words, under the expanding window strategy, we choose all the data that is available before the origin as the training split. This effectively increases the training length every time the origin moves forward in time.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 91%;text-align: justify;">In the rolling window strategy, we keep the length of the training split constant (<span class="s90">𝐿𝐿</span><span class="s128">𝑡</span><span class="s127">𝑡</span>). Therefore, when we move the origin forward by three timesteps, the training split drops three timesteps from the start of the time series.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="122" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_904.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Important note</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Although the expanding and rolling window concept may remind you of the window we use for feature engineering or use as the context in deep learning models, this window is not the same. The window we talk about in this chapter is the window of training data that we chose to train our model. For instance, the features of a machine learning model may only extend to the 5 days before, and we can have the training split use the last 5 years of data.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">There are merits and demerits to both of these window strategies. Let’s summarize them in a few key points:</p><ul id="l190"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Expanding window is a good setup for a short time series, where the expanding window leads to more data being available for the models.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Rolling window removes the oldest data from training. If the time series is non-stationary and the behavior is bound to change as time passes, having a rolling window will be beneficial to keep the model up to date.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;"><a name="bookmark763">When we use the expanding window strategy for repeated evaluation, such as in cross-validation, the increase in time series length used for training can introduce some bias toward windows with a longer history. The rolling window strategy takes care of that bias by maintaining the same length of the series.</a><a name="bookmark753">&zwnj;</a><a name="bookmark752">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Calibration strategy</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The calibration strategy is only valid in cases where we do multiple evaluations with different origins. There are two ways we can do evaluations with different origins – recalibrate every origin, or update every origin (terminology from Tashman, #1 in <i>References</i>).</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Under the <i>recalibrate </i>strategy, the model is retrained with the new training split for every origin. This retrained model is used to evaluate the validation split. But, for the <i>update </i>strategy, we do not retrain the model, but use the trained model to evaluate the new validation split.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Let’s summarize a few key points to be considered for choosing a strategy here:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">The golden standard is to recalibrate every new origin, but many times this may not be feasible. In the econometrics/classical statistical models, the norm was to recalibrate every origin. That was feasible because those models are relatively less compute-intensive and the datasets at the time were also small. So, one could refit a model in a very short time. Nowadays, the datasets have grown in size, and so have the models. Retraining a deep learning model every time we move the origin may not be as easy.</p><p style="padding-top: 5pt;padding-left: 55pt;text-indent: 0pt;text-align: justify;">Therefore, if you are using modern, complex models with long training times, an update strategy might be better.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">For classical models that run fast, we can explore the recalibration strategy.</p><p style="padding-top: 5pt;padding-left: 55pt;text-indent: 0pt;text-align: justify;">But, if the time series you are forecasting is so dynamic that the behavior changes very frequently, then the recalibration strategy might be the way to go.</p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Now let’s get on to the third part of the validation strategy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Sampling strategy</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">In the holdout strategy, we sample a point (<i>origin</i>) on the time series, preferably toward the end, such that the portion of the time series after the origin is shorter than the portion of the time series before. From this origin, we can use either the <i>expanding </i>or <i>rolling window </i>strategy to generate training and validation splits. The model is trained on the training split and tested on the held-out validation split. This strategy is simply called the <span class="s5">holdout </span>strategy. The calibration strategy is fixed at <i>recalibrate </i>because we are only testing and evaluating the model once.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark764">The simple holdout strategy has one disadvantage – the forecast measure we have calculated on the held-out data may not be robust enough because of the single evaluation paradigm. We are relying on a single split of data to calculate the predictive performance of the model. For non-stationary series, this can be a problem because we might be selecting a model that captures the idiosyncrasies of the split that we have chosen.</a></p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 93%;text-align: justify;">We can get over this problem by repeating the holdout evaluation multiple times. We can either hand- tailor the different origins using business domain knowledge, such as taking into account seasonality, or some other factor. Or we could sample the origin points randomly. If we repeat this <span class="s41">𝑛𝑛 </span>times, there will be <span class="s41">𝑛𝑛 </span>validation splits, and they may or may not overlap with each other. The performance metric from these repeated trials can be aggregated using a function such as the mean, maximum, and minimum. This is called the <span class="s5">repeated holdout </span>(<span class="s5">Rep-Holdout</span>) strategy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="122" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_905.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Note on implementation</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">The simple holdout strategy is very simple to implement because we decide the size of the validation split, and keep that much from the end of the time series aside as the validation. The <i>Rep-Holdout </i>strategy involves sampling multiple windows at random or using predefined windows as validation splits. We can make use of the <span class="s20">PredefinedSplit </span>class from scikit- learn to this effect.</p><p style="text-indent: 0pt;text-align: left;"/><p class="s4" style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Figure 19.2 <span class="p">shows the two holdout strategies using an expanding window approach:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 112pt;text-indent: 0pt;text-align: left;"><span><img width="328" height="300" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_906.gif"/></span></p><p class="s37" style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 19.2 – Holdout strategy (a) and Rep-Holdout strategy (b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a name="bookmark765">The Rep-Holdout strategy has a few more variants. The vanilla </a><i>Rep-Holdout </i>strategy evaluates multiple validation datasets, is mostly hand-crafted, and can have overlapping validation datasets. A variation of the Rep-Holdout strategy that insists that multiple validation splits should have no overlap is a more popular option. We call this <span class="s5">Repeated Holdout (No Overlap) </span>(<span class="s5">Rep-Holdout-O</span>). This has some properties from the cross-validation family and tries to use more data systematically. <i>Figure 19.3(a) </i>shows this strategy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 76pt;text-indent: 0pt;text-align: left;"><span><img width="404" height="426" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_907.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">Figure 19.3 – Variations of Rep-Holdout strategy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="75" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_908.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Notebook alert</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: left;">The associated notebook that shows how to implement different validation strategies can be found in the <span class="s20">Chapter19 </span>folder under the name <span class="s20">01-Validation Strategies.ipynb</span>.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">The <i>Rep-Holdout-O </i>strategy is easy to implement in scikit-learn using the <span class="s20">TimeSeriesSplit </span>class for single time series datasets.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a name="bookmark766">The </a><span class="s20">TimeSeriesSplit </span>class from <span class="s20">sklearn.model_selection </span>implements the Rep-Holdout validation strategy and even supports expanding or rolling window variants. The main parameter is <span class="s20">n_splits</span>. This determines how many splits you want from the data, and the validation split size is decided automatically, according to this formula:<a name="bookmark754">&zwnj;</a></p><p class="s90" style="padding-top: 8pt;padding-left: 173pt;text-indent: 0pt;line-height: 10pt;text-align: center;">𝑟𝑟_𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠</p><p class="s90" style="text-indent: 0pt;line-height: 11pt;text-align: right;">𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟 (</p><p class="s90" style="padding-left: 53pt;text-indent: 0pt;line-height: 8pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="74" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_909.png"/></span></p><p class="s90" style="text-indent: 0pt;line-height: 10pt;text-align: left;">𝑟𝑟_𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 + 1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In the default configuration, this implements an expanding window Rep-Holdout-O strategy. But, there is a parameter, <span class="s20">max_train_size</span>. If we set this parameter, then it will use a window of <span class="s20">max_train_size </span>in a rolling window manner.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;line-height: 92%;text-align: justify;">Yet another variant of the Rep-Holdout strategy introduces a gap of length <span class="s74">𝐿𝐿</span><span class="s79">𝑔𝑔 </span>between the train and validation splits. This is to increase the independence between the train and validation splits, hence getting a better error estimate through the procedure. We call this strategy <span class="s5">Repeated Holdout (No Overlap) with Gaps </span>(<span class="s5">Rep-Holdout-O(G)</span>). This strategy is depicted in <i>Figure 19.3(b)</i>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We can implement this using the <span class="s20">TimeSeriesSplit </span>class as well. All we need to do is use a parameter called <span class="s20">gap</span>. By default, the gap is set to 0. But if we change to a non-zero number, it inserts that much timestep gap between the end of the training and the beginning of validation.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Before we move on to the next set of strategies, let’s summarize and discuss some key points about the holdout strategies:</p><ul id="l191"><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">Holdout strategies respect the temporal integrity of the problem and have been the preferred way of evaluating forecasting models for a long time.</p><p style="padding-top: 5pt;padding-left: 64pt;text-indent: 0pt;text-align: justify;">But, it does have a weakness in the inefficient use of available data. For short time series, holdout or Rep-Holdout may not have enough training data to train a model.</p></li><li><p style="padding-top: 5pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">A simple holdout depends on a single evaluation, and the error estimate is not robust. Even in a stationary series, this procedure does not guarantee a good estimate of the error. In non-stationary time series, such as a seasonal time series, this problem exacerbates. But the Rep-Holdout and its variants take care of that issue.</p><p style="padding-top: 8pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now let’s look at the other major school of thought.</p><p class="s3" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Cross-validation strategies</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Cross-validation is one of the most important tools when evaluating standard regression and classification methods. This is because of two reasons:</p></li><li><p style="padding-top: 8pt;padding-left: 64pt;text-indent: -13pt;text-align: justify;">A simple holdout approach doesn’t use all the data available and in cases where data is scarce, cross-validation makes the best use of the available data.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark767">Cross-validation strategies 499</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_910.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ul></li><li><p style="padding-left: 55pt;text-indent: -13pt;text-align: justify;">Theoretically, the time series we have observed is one realization of a stochastic process, and so the acquired error measure of the data is also a stochastic variable. Therefore, it is essential to sample multiple error estimates to get an idea about the distribution of the stochastic variable. Intuitively, we can think of this as a “lack of reliability” on the error measure derived from a single slice of data.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The most common strategy that is used in standard machine learning is called <span class="s5">k-fold cross-validation</span>. Under this strategy, we randomly shuffle and partition the training data into <i>k </i>equal parts. Now, the whole training of the model and calculating the error is repeated <i>k </i>times, such that every <i>k </i>subset we have kept aside is used as a test set once, and only once. When we use a particular subset as testing data, we use all the other subsets as the training data. After we acquire <i>k </i>different estimates of the error measure, we aggregate it using a function such as an average. This mean will typically be more robust than a single error measure.</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">But, there is one assumption that is central to the validity procedure: <i>i.i.d samples</i>. This is one assumption that is invalid in time series problems because, by definition, the different samples in time series are dependent on each other through autocorrelation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="138" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_911.png"/></span></p><p class="s29" style="padding-top: 6pt;padding-left: 13pt;text-indent: 0pt;text-align: justify;">Additional information</p><p style="padding-top: 2pt;padding-left: 13pt;text-indent: 0pt;line-height: 89%;text-align: justify;">Some argue that when we use time delay embedding to convert time series to a regression problem, we can start to use k-fold cross-validation on time series problems. While there are obvious theoretical problems, <i>Bergmeir et al</i>. (#2 in <i>References</i>) showed that empirically, the k-fold cross-validation is not a bad option. But, the caveat is that the time series needs to be stationary. We will talk more about this in the next section, where we will discuss the merits and demerits of these strategies.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">But there have been modifications to the k-fold strategy, specifically aimed at sequential data.</p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><i>Snijders et al. </i>(#4 in <i>References</i>) proposed a modification we call the <span class="s5">Blocked Cross-Validation </span>(<span class="s5">Bl-CV</span>) strategy. It is similar to the standard <i>k-fold </i>strategy, but we do not randomly shuffle the dataset before partitioning into <i>k </i>subsets of length <span class="s65">𝐿𝐿𝑣𝑣</span>. So, this partitioning strategy results in <i>k </i>contiguous blocks of observations. Then, like a standard k-fold strategy, we train and test each of these <i>k </i>blocks and aggregate the error measure over these multiple evaluations. So, the temporal integrity of the problem is satisfied partially. In other words, temporal integrity is maintained within each of the blocks, but not between the blocks. <i>Figure 19.4(a) </i>shows this strategy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 115pt;text-indent: 0pt;text-align: left;"><a name="bookmark768"><span><img width="318" height="375" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_912.jpg"/></span></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="padding-top: 5pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">Figure 19.4 – Bl-CV strategies</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">To implement the <span class="s5">Bl-CV strategy</span>, we can use the same <span class="s20">Kfold </span>class from scikit-learn. As we saw earlier, the main parameter the cross-validation classes in scikit-learn takes in is <span class="s20">n_splits</span>. Here, <span class="s20">n_splits </span>also defines the number of equally sized folds it selects. There is another parameter, <span class="s20">shuffle</span>, which is set to <span class="s20">True </span>by default. If we make sure our data is sorted according to time, then use the <span class="s20">Kfold </span>class with <span class="s20">shuffle=False</span>, it will imitate the <i>Bl-CV </i>strategy. The associated notebook shows this usage. I urge you to check the notebook for a better understanding of how this is implemented.</p><p style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">In the previous section, we talked about introducing gaps between train and validation splits, to increase independence between them. Another variant of the Bl-CV is a version that uses these gaps. We call it <span class="s5">Blocked Cross-Validation with Gaps </span>(<span class="s5">Bl-CV(G)</span>). We can see this in action in <i>Figure 19.4(b)</i>.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Unfortunately, the <span class="s20">Kfold </span>implementation in scikit-learn does not support this variant. But, it’s simple to extend the <span class="s20">Kfold </span>implementation to include gaps as well. The associated notebook has an implementation of this. It has an additional parameter, <span class="s20">gap</span>, that lets us set the gap between the train and validation splits.</p><p style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">We saw many different strategies for validation; now let’s try and lay down a few points that will help you in deciding the right strategy for your problem.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark769">Choosing a validation strategy 501</a><a name="bookmark755">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_913.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Choosing a validation strategy</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Choosing the right validation strategy is one of the most important, but overlooked tasks in the machine learning workflow. A good validation setup will go a long way in all the different steps in the modeling process, such as feature engineering, feature selection, model selection, and hyperparameter tuning. Although there are no hard and fast rules in setting up a validation strategy, there are a few guidelines we can follow. Some of them are from experience (both mine and others) and some of them are from empirical and theoretical studies that have been published as research papers:</p></li><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">One guiding principle in the design is that we try to make the validation strategy replicate the real use of the model as much as possible. For instance, if the model is going to be used to predict the next 24 timesteps, we make the length of the validation split 24 timesteps. Of course, it’s not as simple as that, because other practical constraints such as the availability of enough data, time, and computers have to be kept in mind while designing a validation strategy.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">Rep-Holdout strategies that respect the temporal order of the time series problem are the preferred option, especially in cases where there is sufficient data available.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">For purely autoregressive formulations of stationary time series, regular Kfold can also be used and <i>Bergmeir et al</i>. (#2 in <i>References</i>) empirically show that they perform better than holdout strategies. But, Blocked Cross Validation is a better alternative among cross-validated strategies. <i>Cerqueira et al. </i>(#3 in <i>References</i>) corroborated the findings in their empirical study for stationary time series.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">If the time series is non-stationary, then <i>Cerqueira et al. </i>showed empirically that the holdout strategies (especially Rep-Holdout strategies) are the best ones to choose.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">If the time series is short, using Bl-CV after making the time series stationary is a good strategy for autoregressive formulations, such as time delay embedding. But, for models that use some kind of memory of the history to forecast, such as exponential smoothing or deep learning models such as RNN, cross-validation strategies may not be safe to use.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">If we have exogenous variables, in addition to the autoregressive part, it may not be safe to use cross-validation strategies. It is best to stick to holdout-based strategies.</p></li><li><p style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: justify;">For a strongly seasonal time series, it is beneficial to use validation periods that mimic the forecast horizon. For instance, if we are forecasting for October, November, and December, it is beneficial to check the performance of the model in October, November, and December of last year.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Up until now, we were talking about validation strategies for a single time series. But in the context of global models, we are at a point where we need to think about validation strategies for such cases as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a name="bookmark770">Validation strategies for datasets with multiple time series</a><a name="bookmark757">&zwnj;</a><a name="bookmark756">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 38pt;text-indent: 0pt;text-align: justify;">All the strategies we have seen till now are perfectly valid for datasets with multiple time series, such as the London Smart Meters dataset we have been working with in this book. The insights we discussed in the last section are also valid. The implementation of such strategies can be slightly tricky because the scikit learn classes we discussed work for single time series. Those implementations assume that we have a single time series, sorted according to the temporal order. If there are multiple time series, the splits will be haphazard and messy.</p><p style="padding-top: 6pt;padding-left: 38pt;text-indent: 0pt;text-align: justify;">There are a couple of options we can adopt for datasets with multiple time series:</p><ul id="l192"><li><p style="padding-top: 8pt;padding-left: 65pt;text-indent: -13pt;text-align: justify;">We can loop over the different time series and use the methods we discussed to do the train- validation split, and then concatenate the resulting sets across all the time series. But, that is not going to be so efficient.</p></li><li><p style="padding-top: 5pt;padding-left: 65pt;text-indent: -13pt;text-align: justify;"><a href="#bookmark561" class="s140">We can write some code and design the validation strategies to use datetime or a time index (such as the one we saw in PyTorch forecasting in </a><a href="#bookmark561" class="s21">Chapter </a><i>15</i>, <i>Strategies for Global Deep Learning Forecasting Models</i>). I have linked to a brilliant notebook from <i>Konrad Banachewicz </i>in the <i>Further reading </i>section of this chapter, where he uses a custom <span class="s20">GroupSplit </span>class that uses the time index as the group. This is one way to use Rep-Holdout strategies on a dataset with multiple time series.</p><p style="padding-top: 8pt;padding-left: 38pt;text-indent: 0pt;text-align: justify;">There are a few points that we need to keep in mind for datasets with multiple time series:</p></li><li><p style="padding-top: 8pt;padding-left: 65pt;text-indent: -13pt;text-align: justify;">Do not use different time windows for different time series. This is because different windows in time would have different errors, and that would skew the aggregate error metric we are tracking.</p></li><li><p style="padding-top: 5pt;padding-left: 65pt;text-indent: -13pt;text-align: justify;">If different time series have different lengths, align the length of the validation period across all the series. Training length can be different, but validation windows should be the same so that every time series equally contributes to the aggregate error metric.</p></li><li><p style="padding-top: 5pt;padding-left: 65pt;text-indent: -13pt;text-align: justify;">It is easy to get carried away by complicated validation schemes, but always keep the technical debt you incur by choosing a specific technique in mind.</p></li></ul></li></ul></li></ol><p style="padding-top: 8pt;padding-left: 38pt;text-indent: 0pt;text-align: justify;">With that, we have come to the end of a short but important chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 38pt;text-indent: 0pt;text-align: left;">Summary</p><p style="padding-top: 7pt;padding-left: 38pt;text-indent: 0pt;text-align: justify;">We have come to the end of our journey through the world of time series forecasting. In the last couple of chapters, we addressed a few mechanics of forecasting, such as how to do multi-step forecasting, and how to evaluate forecasts. Different validation strategies for evaluating forecasts and forecasting models were the topics of the current chapter. We started by enlightening you as to why model validation is an important task. Then, we looked at a few different validation strategies, such as the holdout strategies, and navigated the controversial use of cross-validation for time series. We spent some time summarizing and laying down a few guidelines to be used to select a validation strategy. To top it all off, we looked at how these validation strategies are applicable to datasets with multiple time series and talked about how to adapt them to such scenarios.</p><p class="s37" style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a name="bookmark771">References 503</a><a name="bookmark759">&zwnj;</a><a name="bookmark758">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_914.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">With that, we have come to the end of the book. Congratulations on making it all the way through, and I hope you have gained enough skills from the book to tackle the next time series problem that comes your way. I strongly urge you to start putting into practice the skills that you have gained from the book because, as Richard Feynman rightly put it, <i>&quot;You do not know anything until you have practiced.&quot;</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">References</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">The following are the references used in this chapter:</p><ol id="l193"><li><p style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Tashman, Len. (2000). <i>Out-of-sample tests of forecasting accuracy: An analysis and review</i><a href="https://www.researchgate.net/publication/223319987_Out-of-sample_tests_of_forecasting_accuracy_An_analysis_and_review" class="s140" target="_blank">. International Journal of Forecasting. 16. 437-450. 10.1016/S0169-2070(00)00065-0: </a><a href="https://www.researchgate.net/publication/223319987_Out-of-sample_tests_of_forecasting_accuracy_An_analysis_and_review" class="a" target="_blank">https:// www.researchgate.net/publication/223319987_Out-of-sample_tests_ </a><a href="https://www.researchgate.net/publication/223319987_Out-of-sample_tests_of_forecasting_accuracy_An_analysis_and_review" target="_blank">of_forecasting_accuracy_An_analysis_and_review</a></p></li><li><p style="padding-top: 4pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Bergmeir, Christoph and Benítez, José M. (2012). <i>On the use of cross-validation for time series predictor evaluation</i><a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025511006773" class="s140" target="_blank">. In Information Sciences, Volume 191, 2012, Pages 192-213: </a><a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025511006773" class="a" target="_blank">https://</a><a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025511006773" target="_blank"> www.sciencedirect.com/science/article/abs/pii/S0020025511006773</a></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: justify;">Cerqueira, V., Torgo, L., and Mozetič, I. (2020). <i>Evaluating time series forecasting models: an empirical study on performance estimation methods. </i><a href="https://doi.org/10.1007/s10994-020-05910-7" class="s140" target="_blank">Mach Learn 109, 1997–2028 (2020): </a><a href="https://doi.org/10.1007/s10994-020-05910-7" class="a" target="_blank">https:// </a><a href="https://doi.org/10.1007/s10994-020-05910-7" target="_blank">doi.org/10.1007/s10994-020-05910-7</a></p></li><li><p style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;line-height: 13pt;text-align: justify;">Snijders, T.A.B. (1988). <i>On Cross-Validation for Predictor Evaluation in Time Series. </i>In: <i>Dijkstra, T.K. </i>(eds) <i>On Model Uncertainty and its Statistical Implications</i><a href="https://doi.org/10.1007/978-3-642-61564-1_4" class="s140" target="_blank">. Lecture Notes in Economics and Mathematical Systems, vol 307. Springer, Berlin, Heidelberg. </a><a href="https://doi.org/10.1007/978-3-642-61564-1_4" class="a" target="_blank">https://doi. </a><a href="https://doi.org/10.1007/978-3-642-61564-1_4" target="_blank">org/10.1007/978-3-642-61564-1_4</a></p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Further reading</p><p style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;line-height: 111%;text-align: justify;">TS-10: <i>Validation methods for time series </i>by <i>Konrad Banachewicz </i><a href="https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series" class="s140" target="_blank">– </a><a href="https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series" class="a" target="_blank">https://www.kaggle.com/ </a><a href="https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series" target="_blank">code/konradb/ts-10-validation-methods-for-time-series</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 14pt;text-indent: 0pt;text-align: right;"><a name="bookmark772">Index</a><a name="bookmark773">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Symbols</p><p class="s36" style="padding-top: 6pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">.dt accessor</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark67" class="s140">using </a><a href="#bookmark68" class="s140">25, 26</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">A</p><p style="padding-top: 6pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark728" class="s348">absolute error (AE) </a><a href="#bookmark729" class="s348">471, 472</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark735" class="s140">loss curves and complementary pairs for 478</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark63" class="s348">Acorn classes 21</a></p><p class="s36" style="padding-top: 2pt;padding-left: 37pt;text-indent: -7pt;line-height: 108%;text-align: left;"><a href="#bookmark441" class="s348">activation functions </a>273 <a href="#bookmark443" class="s140">Hyperbolic tangent (tanh) </a><a href="#bookmark442" class="s140">275 sigmoid 274</a></p><p class="s36" style="padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark663" class="s348">Add and Norm block </a><a href="#bookmark542" class="s348">438 additive attention </a><a href="#bookmark543" class="s348">354, </a>355 Air Quality Monitoring Data</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark71" class="s140">reference link 29</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark407" class="s348">algorithmic partitioning </a><a href="#bookmark409" class="s348">251-255</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark539" class="s348">alignment function </a><a href="#bookmark540" class="s348">350, </a><a href="#bookmark651" class="s348">352, 425</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark542" class="s140">additive/concat attention </a><a href="#bookmark543" class="s140">354, 355</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark540" class="s140">dot product 352</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark541" class="s140">general attention </a><a href="#bookmark542" class="s140">353, </a><a href="#bookmark541" class="s140">354 scaled dot product attention 353</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark538" class="s348">attention </a><a href="#bookmark539" class="s348">348-350</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark544" class="s140">forecasting with </a><a href="#bookmark546" class="s140">356-360</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark652" class="s348">attention distillation 427</a></p><p style="padding-top: 6pt;padding-left: 38pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark269" class="s348">Augmented Dickey-Fuller (ADF) test </a><a href="#bookmark270" class="s348">143, 144</a></p><p style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark170" class="s348">Auto ARIMA 88</a></p><p style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark169" class="s348">autocorrelation 87</a></p><p class="s36" style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark278" class="s348">autocorrelation function (ACF) </a>152 auto-correlation mechanism,</p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark658" class="s348">Autoformer model 433</a></p><p style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark658" class="s140">period-based dependencies </a><a href="#bookmark659" class="s140">433, 434 time delay aggregation 434</a></p><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark121" class="s348">autocorrelation plot </a><a href="#bookmark123" class="s348">58-60</a></p><p style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark654" class="s348">Autoformer model 429</a></p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark659" class="s140">forecasting with </a><a href="#bookmark660" class="s140">434, 435</a></p><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark655" class="s348">Autoformer model, architecture 430</a></p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark658" class="s140">auto-correlation mechanism 433</a></p><p style="padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark655" class="s140">decomposition architecture 430</a></p><p class="s36" style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">AutoML approach</p><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 7pt;line-height: 117%;text-align: left;"><a href="#bookmark283" class="s140">to target transformation </a><a href="#bookmark285" class="s140">159-</a>162 <a href="#bookmark392" class="s348">Autoregressive (AR) </a><span class="s36">233 Autoregressive Integrated Moving</span></p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark169" class="s348">Average (ARIMA) </a><a href="#bookmark171" class="s348">87-89</a></p><p class="s36" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">AutoStationaryTransformer</p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark334" class="s140">using </a><a href="#bookmark335" class="s140">199, 200</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark733" class="s348">Average Relative Root Mean Squared Error 476</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-top: 5pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">B</p><p style="padding-top: 6pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark480" class="s348">Back Propagation Through Time (BPTT) 298</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark448" class="s348">Backward Propagation 280</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark325" class="s348">bagging 188</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark325" class="s348">base learners 188</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark138" class="s348">baseline forecasts 77</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark175" class="s140">evaluating </a><a href="#bookmark176" class="s140">94-96</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark163" class="s140">generating </a><a href="#bookmark164" class="s140">80-82</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark639" class="s348">basis functions 413</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark515" class="s348">batch 330</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark451" class="s348">batch gradient descent 285</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark451" class="s140">cons 285</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark451" class="s140">pros 285</a></p><p class="s36" style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark402" class="s348">Bayesian optimization </a><a href="#bookmark405" class="s348">246-</a>249 bias</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark739" class="s140">toward over- or under-forecasting </a><a href="#bookmark740" class="s140">482-484</a></p><p style="padding-top: 1pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark546" class="s348">Bidirectional Encoder Representations from Transformers (BERT) 360</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark480" class="s348">bi-directional RNNs 298</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark362" class="s348">blending </a><a href="#bookmark364" class="s348">219-</a>221 blending models</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark365" class="s140">aggregate metrics 222</a></p><p style="padding-top: 1pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark767" class="s348">Blocked Cross-Validation (Bl-CV) strategy </a><a href="#bookmark768" class="s348">499, 500</a></p><p style="padding-top: 2pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark768" class="s348">Blocked Cross-Validation with Gaps (Bl-CV(G)) strategy 500</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark638" class="s348">block, N-BEATS </a><a href="#bookmark639" class="s348">412, 413</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark180" class="s348">block shuffling 101</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark284" class="s348">boosting 161</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark282" class="s348">Box-Cox transform </a><a href="#bookmark283" class="s348">157-159</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark322" class="s348">branch 184</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">C</p><p style="padding-top: 6pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark237" class="s348">calendar features 134</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark121" class="s348">calendar heatmaps 58</a></p><p style="padding-top: 6pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark761" class="s348">calibration strategy </a><a href="#bookmark763" class="s348">493, 495</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark763" class="s140">key points 495</a></p><p class="s36" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark394" class="s348">cardinality </a>235 categorical features</p><p style="padding-left: 32pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark594" class="s140">used, for defining model </a><a href="#bookmark595" class="s140">398-400</a></p><p class="s36" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark163" class="s348">classical statistical models </a><a href="#bookmark202" class="s348">80 classification </a>110 Classification and Regression</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark323" class="s348">Trees (CART) 185</a></p><p style="padding-top: 3pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark176" class="s348">Coefficient of Variation (CoV) 96</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark177" class="s140">issues 97</a></p><p style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark75" class="s348">compact form data </a><a href="#bookmark546" class="s348">33 computer vision (CV) </a><a href="#bookmark542" class="s348">360 concat attention </a><a href="#bookmark543" class="s348">354, 355</a></p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark266" class="s348">concept drift 140</a></p><p class="s36" style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark674" class="s348">conformal prediction </a>449 Conformal Time-series Forecasting</p><p style="padding-left: 32pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark674" class="s140">reference link 449</a></p><p style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark398" class="s348">constraints 242</a></p><p style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark538" class="s348">context 348</a></p><p class="s36" style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark663" class="s348">Conv1d layers </a>438 convolutional neural networks</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark489" class="s348">(CNNs) </a><a href="#bookmark553" class="s348">307, </a><a href="#bookmark636" class="s348">369, 410</a></p><p style="padding-top: 2pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark491" class="s140">causal convolutions 311</a></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark489" class="s140">convolution operation </a><a href="#bookmark490" class="s140">307, 308</a></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark490" class="s140">dilations </a><a href="#bookmark491" class="s140">308-311</a></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark490" class="s140">padding </a><a href="#bookmark491" class="s140">308-311</a></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark491" class="s140">PyTorch implementation </a><a href="#bookmark492" class="s140">311, 312</a></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark490" class="s140">stride </a><a href="#bookmark491" class="s140">308-311</a></p><p style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark447" class="s348">cross-entropy loss 279</a></p><p style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark208" class="s348">cross-learning </a><a href="#bookmark387" class="s348">116, 227</a></p><p style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark205" class="s348">cross-validation </a><a href="#bookmark760" class="s348">113, 492</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark766" class="s140">strategy </a><a href="#bookmark767" class="s140">498, 499</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark672" class="s348">Cumulative Distribution Function (CDF) </a><a href="#bookmark673" class="s348">447, 448</a></p><p style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark731" class="s348">Cumulative Forecast Error (CFE) </a><a href="#bookmark395" class="s348">474 curse of dimensionality 236</a></p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark116" class="s348">cyclical component 51</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">D</p><p class="s36" style="padding-top: 6pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">data distribution</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark598" class="s140">visualizing </a><a href="#bookmark599" class="s140">403, 404</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark65" class="s348">DataFrame 23</a></p><p class="s36" style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark25" class="s348">data generating process (DGP) </a><a href="#bookmark26" class="s348">5, </a>6,</p><p style="padding-top: 1pt;padding-left: 50pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s348">28, </a><a href="#bookmark200" class="s348">72, </a><a href="#bookmark208" class="s348">108, </a><a href="#bookmark389" class="s348">116, </a><a href="#bookmark709" class="s348">229, 467</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark31" class="s140">components, combining </a><a href="#bookmark32" class="s140">12, 13</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s140">detecting 72</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark32" class="s140">non-stationary time series 13 stationarity time series </a><a href="#bookmark27" class="s140">13 synthetic time series, generating </a><a href="#bookmark133" class="s140">7 treating 72</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark229" class="s348">data leakage 123</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 7pt;line-height: 112%;text-align: right;"><a href="#bookmark229" class="s140">avoiding </a><a href="#bookmark230" class="s140">123, </a>124 <span class="s36">data leakage, types </span><a href="#bookmark229" class="s140">target leakage 123</a></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark229" class="s140">train-test contamination 123</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark512" class="s348">dataloader </a>325 data model</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark64" class="s140">preparing </a><a href="#bookmark65" class="s140">22, 23</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark512" class="s348">datamodule </a>325 date columns</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark66" class="s140">converting into DatetimeIndex </a><a href="#bookmark67" class="s140">24, 25</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark66" class="s140">converting into pd.Timestamp </a><a href="#bookmark67" class="s140">24, 25</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">date offsets</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark69" class="s140">managing </a><a href="#bookmark70" class="s140">27, 28</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">date sequences</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark69" class="s140">creating </a><a href="#bookmark70" class="s140">27, 28</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">DatetimeIndex</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark66" class="s140">date columns, converting, into </a><a href="#bookmark67" class="s140">24, 25</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">datetime properties</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark67" class="s140">using </a><a href="#bookmark68" class="s140">25, 26</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark444" class="s348">dead ReLUs 276</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark322" class="s348">decision node 184</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark321" class="s348">decision trees </a><a href="#bookmark325" class="s348">183-188</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark663" class="s348">Decoded Representation 438</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark474" class="s348">decoder 291</a></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark557" class="s348">decoder-only Transformer 373</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark555" class="s348">decoder, vanilla Transformer model 371</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark555" class="s140">masked self-attention </a><a href="#bookmark556" class="s140">371, 372</a></p><p class="s36" style="padding-top: 1pt;padding-left: 49pt;text-indent: -21pt;text-align: left;">decomposition architecture,</p><p style="padding-top: 1pt;padding-left: 49pt;text-indent: 0pt;text-align: left;"><a href="#bookmark655" class="s348">Autoformer model 430</a></p><p style="padding-top: 2pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark656" class="s140">decoder </a><a href="#bookmark657" class="s140">431, 432</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark656" class="s140">encoder 431</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark672" class="s348">DeepAR 447</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark415" class="s348">deep learning (DL) </a><a href="#bookmark432" class="s348">261-</a><a href="#bookmark435" class="s348">262, </a><a href="#bookmark436" class="s348">265-</a><a href="#bookmark699" class="s348">266, 456</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark441" class="s140">activation functions 273</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark448" class="s140">backward propagation 280</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark439" class="s140">components 271</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark432" class="s140">compute availability, increasing </a><a href="#bookmark433" class="s140">262-263 data availability, increasing </a><a href="#bookmark435" class="s140">263-</a><a href="#bookmark448" class="s140">265 forward propagation 280</a></p><p style="padding-left: 35pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark448" class="s140">gradient descent </a><a href="#bookmark451" class="s140">280-285</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark441" class="s140">linear transformation 273</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark447" class="s140">loss function 279</a></p><p style="padding-left: 28pt;text-indent: 7pt;line-height: 117%;text-align: left;"><a href="#bookmark440" class="s140">representation learning </a>272 <a href="#bookmark554" class="s348">deep NNs (DNNs) </a><a href="#bookmark663" class="s348">370 Dense layer </a><a href="#bookmark666" class="s348">438, 441</a></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark594" class="s348">dense representations 398</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark123" class="s348">deseasonalizing </a><a href="#bookmark124" class="s348">60, 61</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark124" class="s140">Fourier series </a><a href="#bookmark125" class="s140">61-63</a></p><p style="padding-left: 28pt;text-indent: 7pt;line-height: 119%;text-align: left;"><a href="#bookmark124" class="s140">period adjusted averages </a>61 <a href="#bookmark272" class="s348">deterministic trend </a><a href="#bookmark273" class="s348">146, </a><a href="#bookmark123" class="s348">147 detrended time series 60 detrending 60</a></p><p style="padding-left: 35pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark123" class="s140">LOESS algorithm 60</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark123" class="s140">moving average 60</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark170" class="s348">differencing 88</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark490" class="s348">dilation </a><a href="#bookmark491" class="s348">308-311</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark701" class="s348">direct strategy 458</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark702" class="s140">features, eliminating 459</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark702" class="s140">forecasting regime 459</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark702" class="s140">targets, shifting 459</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark702" class="s140">training regime 459</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark704" class="s348">DirRec strategy 461</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark705" class="s140">forecasting regime 462</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark705" class="s140">training regime 462</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark174" class="s348">Discrete Fourier Transform </a><a href="#bookmark132" class="s348">92 discrete step 71</a></p><p class="s36" style="padding-left: 38pt;text-indent: 0pt;line-height: 11pt;text-align: left;">disk</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark80" class="s140">files, loading 38</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark80" class="s140">files, saving 38</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark539" class="s348">distribution function </a><a href="#bookmark543" class="s348">350, </a><a href="#bookmark651" class="s348">355, 425</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark651" class="s348">d_model dimension </a><a href="#bookmark540" class="s348">425 dot product attention 352</a></p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark166" class="s348">double exponential smoothing (DES) </a><a href="#bookmark167" class="s348">84, </a><a href="#bookmark395" class="s348">85 dummy variable encoding 236</a></p><p class="s36" style="padding-left: 38pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Dynamask</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark670" class="s140">reference link 445</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark62" class="s348">dynamic time-of-use (dToU) </a><a href="#bookmark407" class="s348">20 Dynamic Time Warping (DTW) 251</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">E</p><p style="padding-top: 6pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark509" class="s348">early stopping </a><a href="#bookmark583" class="s348">320, 385</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark515" class="s140">reference link 330</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark163" class="s348">econometrics models 80</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark25" class="s348">electrocardiogram (EKG) 5</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark25" class="s348">electroencephalogram (EEG) 5</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark547" class="s348">embedding 361</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark593" class="s348">embedding vector </a><a href="#bookmark594" class="s348">397, 398</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark474" class="s348">encoder 291</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark473" class="s348">encoder-decoder paradigm </a><a href="#bookmark474" class="s348">290, </a>291 encoder, vanilla Transformer</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark552" class="s348">model </a><a href="#bookmark553" class="s348">368, 369</a></p><p style="padding-top: 2pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark554" class="s140">layer normalization 370</a></p><p style="padding-left: 38pt;text-indent: 7pt;line-height: 117%;text-align: left;"><a href="#bookmark553" class="s140">residual connections </a><a href="#bookmark554" class="s140">369, </a>370 <a href="#bookmark510" class="s348">end-to-end (E2E) model </a><a href="#bookmark325" class="s348">322 ensemble learning 188</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark178" class="s348">entropy 98</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark178" class="s348">entropy-based measures </a><a href="#bookmark179" class="s348">98-100</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark515" class="s348">epoch 330</a></p><p style="padding-top: 5pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a href="#bookmark116" class="s348">error term 51</a></p><p style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a href="#bookmark637" class="s348">ES-RNN 411</a></p><p class="s36" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark124" class="s348">Euler’s identity </a>61 evaluation metric</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark162" class="s140">selecting 79</a></p><p style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark76" class="s348">expanded form data </a><a href="#bookmark483" class="s348">34 exploding gradients 301</a></p><p style="padding-left: 20pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark401" class="s348">exploitation 245</a></p><p style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a href="#bookmark401" class="s348">exploration 245</a></p><p style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark62" class="s348">Exploratory Data Analysis (EDA) </a><a href="#bookmark666" class="s348">20 Exponential Linear Unit (ELU) 441</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark666" class="s140">reference link 441</a></p><p style="padding-top: 1pt;padding-left: 41pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark235" class="s348">exponentially weighted moving averages (EWMA) </a><a href="#bookmark237" class="s348">131-</a><a href="#bookmark393" class="s348">134, 234</a></p><p style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark166" class="s348">exponential smoothing (ETS) </a><a href="#bookmark647" class="s348">84 expressiveness ratio 421</a></p><p class="s36" style="padding-left: 20pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark134" class="s348">extreme studentized deviate (ESD) </a><a href="#bookmark135" class="s348">73, </a>74 extrinsic errors</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark739" class="s140">loss curves and complementary pairs 482</a></p><p class="s36" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">extrinsic metrics, forecast error measures</p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark734" class="s140">Percent Better (PB) </a><a href="#bookmark732" class="s140">477 relative error </a><a href="#bookmark733" class="s140">475, 476</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark733" class="s140">scaled error 476</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">F</p><p class="s36" style="padding-top: 6pt;padding-left: 20pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark476" class="s348">Facebook AI Research Lab (FAIR) </a>293 Fast Fourier Transform (FFT)</p><p style="padding-left: 41pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark173" class="s348">forecast </a><a href="#bookmark175" class="s348">91-</a><a href="#bookmark658" class="s348">94, 433</a></p><p style="padding-top: 3pt;padding-left: 41pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark365" class="s348">Feature-Based Forecast Model Averaging (FFORMA) 222</a></p><p class="s36" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">feature-based time-series analysis</p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark240" class="s140">reference link 138</a></p><p style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a href="#bookmark310" class="s348">FeatureConfig 168</a></p><p style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a href="#bookmark228" class="s348">feature engineering </a><a href="#bookmark229" class="s348">122, 123</a></p><p style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a href="#bookmark314" class="s348">feature_importance function 172</a></p><p style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a href="#bookmark473" class="s348">feature space 290</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark475" class="s348">feed-forward networks (FFNs) </a><a href="#bookmark476" class="s348">292, 293</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark477" class="s140">layers, defining </a><a href="#bookmark478" class="s140">294, 295</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark477" class="s140">tensor, using 294</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">files</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark80" class="s140">loading, to disk 38 saving, to disk 38</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark207" class="s348">finite memory models </a><a href="#bookmark313" class="s348">115 fit function </a>171 forecastability</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark176" class="s140">assessing, of time series 96</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark163" class="s348">Forecast Bias (FB) </a><a href="#bookmark592" class="s348">80, </a><a href="#bookmark732" class="s348">394, </a>475 forecast error measures</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark740" class="s140">experimental study 484</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark732" class="s140">extrinsic metrics 475</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark728" class="s140">intrinsic metrics 471</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark734" class="s140">investigating 477</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark741" class="s140">Spearman’s rank correlation, using </a><a href="#bookmark727" class="s140">485 taxonomy </a><a href="#bookmark728" class="s140">470, 471</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark230" class="s348">forecast horizons 124</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark230" class="s140">setting 124</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">forecast metric</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark742" class="s140">selecting, guidelines </a><a href="#bookmark743" class="s140">487-489</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark637" class="s348">forecast period </a>411 forecasts</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark352" class="s140">combining 204</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark484" class="s348">forget gate 302</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark448" class="s348">Forward Propagation 280</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark128" class="s348">Fourier decomposition </a><a href="#bookmark130" class="s348">67-69</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark129" class="s140">parameters 68</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark239" class="s348">Fourier terms </a><a href="#bookmark240" class="s348">136-138</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark395" class="s348">frequency encoding </a><a href="#bookmark396" class="s348">236, </a><a href="#bookmark475" class="s348">237 fully connected networks 292</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">G</p><p style="padding-top: 6pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark552" class="s348">Gated Linear Unit (GLU) </a><a href="#bookmark663" class="s348">368, 438</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark486" class="s348">gated recurrent unit (GRU) </a><a href="#bookmark487" class="s348">304, </a><a href="#bookmark510" class="s348">305, 322</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark488" class="s140">PyTorch implementation 306</a></p><p style="padding-top: 4pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark486" class="s140">reset gate 304</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark487" class="s140">update gate 305</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark662" class="s348">Gated Residual Networks (GRNs) </a><a href="#bookmark666" class="s348">437, 441</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark541" class="s348">general attention </a><a href="#bookmark542" class="s348">353, 354</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s348">generalization capability 110</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark391" class="s348">generalization error 232</a></p><p class="s36" style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark539" class="s348">generalized attention model </a><a href="#bookmark540" class="s348">350-</a><a href="#bookmark653" class="s348">352 generative-style decoder </a><a href="#bookmark729" class="s348">428 Geometric Mean Absolute Error </a>472 Geometric Mean Relative</p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark733" class="s348">Absolute Error 476</a></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark730" class="s348">Geometric Root Mean Squared Error </a><a href="#bookmark592" class="s348">473 GFM(DL) 394</a></p><p class="s36" style="padding-left: 35pt;text-indent: -7pt;line-height: 106%;text-align: left;">GFM memory, increasing ways <a href="#bookmark393" class="s140">EWMA features, adding </a><a href="#bookmark392" class="s140">234 lag features, adding </a><a href="#bookmark393" class="s140">233 rolling features, adding 234</a></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark592" class="s348">GFM(ML) 394</a></p><p class="s36" style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">global deep learning forecasting models</p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark589" class="s140">building 391</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark581" class="s140">creating 382</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark582" class="s140">data, preprocessing </a><a href="#bookmark584" class="s140">383-386</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark589" class="s140">RNN model, defining </a><a href="#bookmark590" class="s140">391, 392 RNN model, initializing </a><a href="#bookmark591" class="s140">392 RNN model, training </a><a href="#bookmark592" class="s140">393, 394</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark597" class="s140">sampling procedure </a><a href="#bookmark592" class="s140">402 trained model, forecasting 394</a></p><p style="padding-top: 1pt;padding-left: 49pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark208" class="s348">Global Forecasting Models (GFMs) </a><a href="#bookmark209" class="s348">116-</a><a href="#bookmark390" class="s348">118, </a><a href="#bookmark392" class="s348">231-233</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark389" class="s140">creating </a><a href="#bookmark390" class="s140">229-231</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark387" class="s140">cross-learning 227</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark388" class="s140">engineering complexity </a><a href="#bookmark389" class="s140">228, 229</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark388" class="s140">multi-task learning 228</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark386" class="s140">need for 226</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark387" class="s140">sample size 227</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark592" class="s348">global machine learning model </a><a href="#bookmark664" class="s348">394 GLU + AddNorm block </a><a href="#bookmark666" class="s348">439 GLU+Add &amp; Norm layer 441</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark328" class="s348">gradient boosted decision trees (GBDTs) </a><a href="#bookmark333" class="s348">192-198</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark331" class="s140">disadvantages 196</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark448" class="s348">gradient descent </a><a href="#bookmark451" class="s348">280-285</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark451" class="s140">batch gradient descent 285</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark452" class="s140">mini-batch gradient descent </a><a href="#bookmark451" class="s140">286 stochastic gradient descent (SGD) 285</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark484" class="s348">gradient highway </a><a href="#bookmark486" class="s348">302, 304</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark356" class="s348">greedy optimization 208</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark399" class="s348">grid search </a><a href="#bookmark400" class="s348">243, 244</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark36" class="s348">gross domestic product (GDP) </a><a href="#bookmark134" class="s348">17 Grubb’s Test 73</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">H</p><p class="s36" style="padding-top: 6pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">half-hourly block-level data (hhblock)</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" class="s140">converting, into time series data 33</a></p><p class="s36" style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">helper functions</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark314" class="s140">for evaluating models </a><a href="#bookmark315" class="s140">172, 173</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark34" class="s348">heteroscedasticity </a><a href="#bookmark280" class="s348">15, 155</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark282" class="s140">Box-Cox transform </a><a href="#bookmark283" class="s140">157-159</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark280" class="s140">correcting 155</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark280" class="s140">detecting </a><a href="#bookmark281" class="s140">155, 156</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark281" class="s140">log transform </a><a href="#bookmark282" class="s140">156, 157</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark475" class="s348">hidden layers 292</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark761" class="s348">holdout strategy </a><a href="#bookmark763" class="s348">493-</a>495 holdout (test) dataset</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark161" class="s140">creating </a><a href="#bookmark162" class="s140">78, 79</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark166" class="s348">Holt-Winters’ seasonal smoothing </a><a href="#bookmark704" class="s348">84 hybrid strategies 461</a></p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark704" class="s140">DirRec strategy 461</a></p><p style="padding-top: 1pt;padding-left: 59pt;text-indent: -13pt;text-align: left;"><a href="#bookmark705" class="s140">iterative block-wise direct (IBD) strategy 462</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark707" class="s140">RecJoint 464</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark706" class="s140">rectify strategy 463</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark205" class="s348">hyperparameters 113</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark398" class="s348">hyperparameter tuning </a><a href="#bookmark399" class="s348">242, 243</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark402" class="s140">Bayesian optimization </a><a href="#bookmark405" class="s140">246-249</a></p><p style="padding-top: 4pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark399" class="s140">grid search </a><a href="#bookmark400" class="s140">243, 244</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark400" class="s140">random search </a><a href="#bookmark401" class="s140">244, 245</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">I</p><p style="padding-top: 6pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark200" class="s348">ideal target function </a><a href="#bookmark312" class="s348">108 impute_missing_values 170</a></p><p class="s36" style="padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark493" class="s348">inbuilt padding </a>313 independent and identically</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark206" class="s348">distributed (iid) </a><a href="#bookmark363" class="s348">114, 220</a></p><p style="padding-top: 3pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark701" class="s348">independent strategy 458</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark636" class="s348">inductive bias </a>410 informations_households.csv file</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark78" class="s140">mapping 36</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark649" class="s348">Informer model 423</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark653" class="s140">forecasting with </a><a href="#bookmark654" class="s140">428, 429</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark650" class="s348">Informer model, architecture 424</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark652" class="s140">attention distillation 427</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark653" class="s140">generative-style decoder 428</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark651" class="s140">ProbSparse attention </a><a href="#bookmark652" class="s140">425-</a><a href="#bookmark651" class="s140">427 Uniform Input Representation 425</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark484" class="s348">input gate 302</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark473" class="s348">input space 290</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark760" class="s348">in-sample validation </a>492 Instance-wise Feature Importance</p><p class="s36" style="padding-left: 59pt;text-indent: 0pt;line-height: 10pt;text-align: left;">in Time (FIT)</p><p style="padding-top: 2pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark670" class="s140">reference link 445</a></p><p class="s36" style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">interpolation</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark72" class="s140">imputation techniques </a><a href="#bookmark73" class="s140">30, 31</a></p><p class="s36" style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">interpolation techniques</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark74" class="s140">reference link 32</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark410" class="s348">interpretability </a><a href="#bookmark670" class="s348">256, 445</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark120" class="s348">interquartile range (IQR) </a><a href="#bookmark134" class="s348">57, </a>73 intrinsic metrics, forecast</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark728" class="s348">error measures 471</a></p><p style="padding-top: 2pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark728" class="s140">absolute error </a><a href="#bookmark729" class="s140">471, 472</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark731" class="s140">cumulative Forecast Error (CFE) </a><a href="#bookmark732" class="s140">474 Forecast Bias 475</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark730" class="s140">squared error </a><a href="#bookmark731" class="s140">473, 474</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark731" class="s140">symmetric error 474</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark732" class="s140">Tracking Signal 475</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark174" class="s348">Inverse Fast Fourier Transform (IFFT) </a><a href="#bookmark116" class="s348">92 irregular component 51</a></p><p class="s36" style="padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark24" class="s348">irregular time series </a><a href="#bookmark134" class="s348">4 Isolation Forest </a>73 iterative block-wise direct</p><p style="padding-left: 50pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark705" class="s348">(IBD) strategy 462</a></p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark706" class="s140">forecasting regime 463</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark706" class="s140">training regime 463</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark705" class="s348">iterative multi-SVR strategy 462</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">J</p><p style="padding-top: 6pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark702" class="s348">joint strategy 459</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark703" class="s140">forecasting regime </a><a href="#bookmark704" class="s140">460, 461</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark703" class="s140">training regime 460</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark406" class="s348">judgmental partitioning </a><a href="#bookmark407" class="s348">250, 251</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">K</p><p style="padding-top: 6pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark180" class="s348">Kaboudan metric </a><a href="#bookmark181" class="s348">101, 102</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark63" class="s348">Kaggle dataset 21</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark273" class="s348">Kendall’s Tau </a><a href="#bookmark274" class="s348">147, 148</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark489" class="s348">kernel 307</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark205" class="s348">k-fold cross validation </a><a href="#bookmark767" class="s348">113, </a><a href="#bookmark652" class="s348">499 Kullback-Liebler (KL) Divergence 427</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">L</p><p style="padding-top: 6pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark317" class="s348">L1 norm </a><a href="#bookmark319" class="s348">176-179</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark317" class="s348">L2 norm </a><a href="#bookmark319" class="s348">176-179</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark201" class="s348">labels 109</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark281" class="s348">Lagrangian Multiplier (LM) statistic </a><a href="#bookmark231" class="s348">156 lags/backshift </a><a href="#bookmark232" class="s348">125, 126</a></p><p style="padding-left: 29pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark317" class="s348">lasso regression 176</a></p><p style="padding-top: 5pt;padding-left: 49pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark72" class="s348">Last Observation Carried Forward or Forward Fill 30</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark322" class="s348">leaf node 184</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark636" class="s348">learning bias 410</a></p><p class="s36" style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark330" class="s348">learning rate </a><a href="#bookmark450" class="s348">194, </a>282 LightGBM</p><p style="padding-left: 35pt;text-indent: 0pt;line-height: 12pt;text-align: left;">native handling of categorical</p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><a href="#bookmark397" class="s140">features </a><a href="#bookmark398" class="s140">240-242</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark72" class="s348">linear interpolation 30</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark169" class="s348">linear regression </a><a href="#bookmark315" class="s348">87, </a><a href="#bookmark316" class="s348">173-175</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark315" class="s140">assumptions 173</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark317" class="s140">feature importance 176</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark316" class="s140">forecast 175</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark117" class="s348">line charts </a><a href="#bookmark118" class="s348">52-55</a></p><p class="s36" style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 122%;text-align: justify;"><a href="#bookmark386" class="s348">Local Forecasting Models (LFMs) </a><a href="#bookmark663" class="s348">226 Locality Encoded Context Vectors </a>438 Locality Enhancement (LE)</p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 10pt;text-align: justify;"><a href="#bookmark663" class="s348">seq2seq layer 438</a></p><p style="padding-top: 3pt;padding-left: 49pt;text-indent: -21pt;line-height: 111%;text-align: justify;"><a href="#bookmark123" class="s348">locally estimated scatterplot smoothing (LOESS) regression 60</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 122%;text-align: justify;"><a href="#bookmark123" class="s348">locally weighted polynomial regression </a><a href="#bookmark208" class="s348">60 local models of forecasting 116</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a href="#bookmark442" class="s348">logistic function 274</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark652" class="s348">Log-Sum-Exp (LSE) 427</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark281" class="s348">log transform </a><a href="#bookmark282" class="s348">156, 157</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark281" class="s140">properties 156</a></p><p class="s36" style="padding-top: 1pt;padding-left: 35pt;text-indent: -7pt;line-height: 106%;text-align: left;">London Smart Meters dataset <a href="#bookmark78" class="s140">converting, into compact form </a><a href="#bookmark77" class="s140">36 converting, into expanded form 35 converting, into time series format 35</a></p><p style="padding-left: 49pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark484" class="s348">long short-term memory (LSTM) networks </a><a href="#bookmark485" class="s348">302, </a><a href="#bookmark510" class="s348">303, </a><a href="#bookmark553" class="s348">322, 369</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark484" class="s140">forget gate 302</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark484" class="s140">input gate 302</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark485" class="s140">output gate 303</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark485" class="s140">PyTorch implementation 303</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark637" class="s348">lookback period 411</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark449" class="s348">loss curve 281</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark734" class="s348">loss curves and complementary pairs 477</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark739" class="s140">extrinsic errors 482</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark735" class="s140">for absolute error (AE) </a><a href="#bookmark737" class="s140">478 for percent error (PE) </a><a href="#bookmark736" class="s140">480 for squared error (SE) </a><a href="#bookmark738" class="s140">479 for symmetric error (SE) 481</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">M</p><p style="padding-top: 6pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark507" class="s348">machine learning (ML) </a><a href="#bookmark545" class="s348">318, </a><a href="#bookmark699" class="s348">359, 456</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark187" class="s140">basics </a><a href="#bookmark202" class="s140">107-110</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark187" class="s140">defining 107</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark309" class="s140">evaluating, with standardized code </a><a href="#bookmark308" class="s140">167 predicting 166</a></p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark205" class="s140">reference link 113</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark308" class="s140">training 166</a></p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 108%;text-align: center;"><a href="#bookmark309" class="s140">training, with standardized code </a><a href="#bookmark187" class="s140">167 versus traditional programming </a>107 <a href="#bookmark275" class="s348">Mann-Kendall test (M-K test) </a><a href="#bookmark276" class="s348">149,  150</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark207" class="s348">Markov models 115</a></p><p style="padding-top: 2pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark665" class="s348">Masked Interpretable Multi-Head Attention </a><a href="#bookmark666" class="s348">440, 441</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark555" class="s348">masked self-attention </a><a href="#bookmark556" class="s348">371, 372</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark391" class="s348">Massive Open Online Course (MOOC) </a><a href="#bookmark438" class="s348">232 matrix 270</a></p><p class="s36" style="padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark446" class="s348">maximum likelihood estimation (MLE) </a><a href="#bookmark83" class="s348">278 Mean Absolute Error (MAE) </a>45,</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark398" class="s348">79, </a><a href="#bookmark447" class="s348">242, </a><a href="#bookmark592" class="s348">279, </a><a href="#bookmark729" class="s348">394, 472</a></p><p style="padding-top: 3pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark730" class="s348">Mean Absolute Percent Error (MAPE) </a><a href="#bookmark163" class="s348">473 Mean Absolute Scaled Error (MASE) </a><a href="#bookmark733" class="s348">80, </a><a href="#bookmark592" class="s348">476 meanMASE 394</a></p><p class="s36" style="padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark733" class="s348">Mean Relative Absolute Error (MRAE) </a><a href="#bookmark162" class="s348">476 Mean Squared Error (MSE) </a>79,</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark447" class="s348">173, </a><a href="#bookmark592" class="s348">279, </a><a href="#bookmark730" class="s348">394, 473</a></p><p style="padding-top: 3pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark72" class="s348">Mean Value Fill 30</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark729" class="s348">Median Absolute Error </a><a href="#bookmark730" class="s348">472 Median Absolute Percent Error 473</a></p><p style="padding-top: 5pt;padding-left: 18pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark733" class="s348">Median Relative Absolute Error </a><a href="#bookmark484" class="s348">476 memory cell 302</a></p><p style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark362" class="s348">meta model 219</a></p><p class="s36" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark452" class="s348">mini-batch gradient descent </a>286 missing data</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark70" class="s140">handling </a><a href="#bookmark74" class="s140">28-32</a></p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: -13pt;text-align: left;"><a href="#bookmark82" class="s140">imputing, with hourly average for each weekday </a><a href="#bookmark84" class="s140">43-46</a></p><p style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark81" class="s140">imputing, with previous day </a><a href="#bookmark82" class="s140">42, </a><a href="#bookmark84" class="s140">43 imputing, with seasonal interpolation </a><a href="#bookmark86" class="s140">46-</a><a href="#bookmark80" class="s140">48 longer periods, handling </a><a href="#bookmark81" class="s140">38-42</a></p><p style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><a href="#bookmark311" class="s348">MissingValueConfig </a><a href="#bookmark312" class="s348">169, 170</a></p><p style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><a href="#bookmark313" class="s348">MLForecast 171</a></p><p style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark314" class="s140">feature_importance function 172</a></p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark313" class="s140">fit function 171</a></p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark313" class="s140">predict function 171</a></p><p style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><a href="#bookmark25" class="s348">model </a><a href="#bookmark201" class="s348">5, 109</a></p><p style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark314" class="s140">evaluating, helper functions </a><a href="#bookmark315" class="s140">172, </a><a href="#bookmark594" class="s140">173 defining, with categorical features </a><a href="#bookmark595" class="s140">398-400</a></p><p style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><a href="#bookmark312" class="s348">ModelConfig 170</a></p><p style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><a href="#bookmark203" class="s348">model’s capacity 111</a></p><p style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><a href="#bookmark760" class="s348">model validation 492</a></p><p style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark180" class="s348">modified Kaboudan metric </a><a href="#bookmark673" class="s348">101 Monte Carlo dropout </a><a href="#bookmark123" class="s348">448 moving average </a><a href="#bookmark235" class="s348">60, </a><a href="#bookmark165" class="s348">131 moving average forecast 83</a></p><p style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark326" class="s348">M trees 189</a></p><p class="s36" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark549" class="s348">multi-headed attention (MHA) </a>363 multiple households</p><p style="padding-left: 25pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark333" class="s140">predicting 198</a></p><p style="padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark333" class="s140">training 198</a></p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark703" class="s348">multiple input, multiple output (MIMO) 460</a></p><p class="s36" style="padding-top: 2pt;padding-left: 18pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark702" class="s348">multiple input, single output (MISO) </a>459 multiple seasonality decomposition</p><p style="padding-left: 39pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark130" class="s348">using LOESS (MSTL) </a><a href="#bookmark132" class="s348">69-71</a></p><p style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="#bookmark130" class="s140">parameters </a><a href="#bookmark131" class="s140">69, 70</a></p><p style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><a href="#bookmark647" class="s348">multi-rate signal sampling 421</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s36" style="padding-top: 5pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">multi-step forecasting</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark699" class="s140">need for </a><a href="#bookmark700" class="s140">456, 457</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">multi-step forecasting strategy</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark708" class="s140">selecting </a><a href="#bookmark709" class="s140">465-467</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark388" class="s348">multi-task learning 228</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark388" class="s140">benefits 228</a></p><p class="s36" style="padding-top: 1pt;padding-left: 50pt;text-indent: -21pt;line-height: 111%;text-align: left;">Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows</p><p style="padding-top: 2pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark673" class="s140">reference link 448</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">N</p><p style="padding-top: 6pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark163" class="s348">naïve forecast </a><a href="#bookmark164" class="s348">80, 82</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark546" class="s348">natural language processing (NLP) </a><a href="#bookmark663" class="s348">360, 438</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark638" class="s348">N-BEATS, architecture 412</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark637" class="s140">aspects 411</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark640" class="s140">basis functions </a><a href="#bookmark641" class="s140">414, 415</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark638" class="s140">block </a><a href="#bookmark639" class="s140">412, 413</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark640" class="s140">interpretability </a><a href="#bookmark641" class="s140">414, 415</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark639" class="s140">overall architecture 413</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark639" class="s140">stacks 413</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">Nbeats class, in PyTorch Forecasting</p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 4pt;line-height: 112%;text-align: center;"><a href="#bookmark641" class="s140">parameters  </a><a href="#bookmark642" class="s140">415, </a>416 <span class="s36">N-BEATS, parameters </span><a href="#bookmark642" class="s140">reference link 416</a></p><p class="s36" style="padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark671" class="s348">negative log-likelihood (NLL) </a>446 Neural Basis Expansion Analysis</p><p class="s36" style="padding-left: 50pt;text-indent: 0pt;line-height: 10pt;text-align: left;">for Interpretable Timeseries</p><p style="padding-top: 1pt;padding-left: 50pt;text-indent: 0pt;text-align: left;"><a href="#bookmark637" class="s348">Forecasting (N-BEATS) 411</a></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark642" class="s140">forecasting, interpretability </a><a href="#bookmark643" class="s140">416, 417</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark641" class="s140">forecasting with </a><a href="#bookmark642" class="s140">415, 416</a></p><p style="padding-top: 1pt;padding-left: 50pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark643" class="s348">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with Exogenous Variables (N-BEATSx) 417</a></p><p style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark644" class="s140">exogenous blocks </a><a href="#bookmark645" class="s140">418, </a><a href="#bookmark644" class="s140">419 exogenous variables, handling 418</a></p><p style="padding-top: 5pt;padding-left: 42pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark645" class="s348">Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS) 419</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark648" class="s140">forecasting with </a><a href="#bookmark649" class="s140">422, 423</a></p><p class="s36" style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark508" class="s348">neural network (NN) </a><a href="#bookmark546" class="s348">319, </a><a href="#bookmark436" class="s348">360 neural network, Perceptron </a><a href="#bookmark437" class="s348">266-</a>269 Next observation Carried Backward</p><p style="text-indent: 0pt;line-height: 10pt;text-align: right;"><a href="#bookmark72" class="s348">of Backward Fill 30</a></p><p style="padding-top: 3pt;text-indent: 0pt;text-align: right;"><a href="#bookmark645" class="s348">N-HiTS, architecture 419</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark647" class="s140">hierarchical interpolation 421</a></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark647" class="s140">input sampling, synchronizing </a><a href="#bookmark648" class="s140">421, </a><a href="#bookmark646" class="s140">422 multi-rate data sampling </a><a href="#bookmark647" class="s140">420 synchronizing, synchronizing </a><a href="#bookmark648" class="s140">421, 422</a></p><p class="s36" style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">NHiTS class in PyTorch Forecasting</p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark648" class="s140">parameters 422</a></p><p class="s36" style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark397" class="s348">No Free Lunch Theorem (NFLT) </a>240 non-linear interpolation</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark73" class="s140">techniques 31</a></p><p class="s36" style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">non-stationarity</p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark266" class="s140">handling, in time series </a><a href="#bookmark267" class="s140">140, 141</a></p><p class="s36" style="padding-top: 1pt;padding-left: 28pt;text-indent: -7pt;line-height: 108%;text-align: left;"><a href="#bookmark32" class="s348">non-stationary time series </a>13 <a href="#bookmark33" class="s140">change in mean over time </a><a href="#bookmark34" class="s140">14, 15 change in variance over time 15</a></p><p class="s36" style="padding-left: 21pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark729" class="s348">Normalized Deviation (ND) </a><a href="#bookmark730" class="s348">472, </a>473 Normalized Root Mean Squared</p><p style="padding-left: 42pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark730" class="s348">Error (NRMSE) 473</a></p><p style="padding-top: 3pt;padding-left: 21pt;text-indent: 0pt;text-align: left;"><a href="#bookmark673" class="s348">normalizing flows 448</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">O</p><p style="padding-top: 6pt;padding-left: 21pt;text-indent: 0pt;text-align: left;"><a href="#bookmark398" class="s348">objective function 242</a></p><p class="s36" style="padding-top: 2pt;padding-left: 21pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark393" class="s348">one-hot encoding </a><a href="#bookmark395" class="s348">234-</a><a href="#bookmark593" class="s348">236, </a><a href="#bookmark360" class="s348">397 optimal weighted ensemble </a><a href="#bookmark361" class="s348">216-</a>218 Optimization in Distributed Learning</p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark332" class="s140">reference link 197</a></p><p class="s36" style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">optimize method</p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark404" class="s140">reference link 248</a></p><p style="padding-top: 1pt;padding-left: 21pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark393" class="s348">ordinal encoding </a><a href="#bookmark395" class="s348">234-</a><a href="#bookmark315" class="s348">236 ordinary least squares (OLS) 173</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s348">outlier 72</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s140">detecting 72</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark134" class="s140">extreme studentized deviate (ESD) </a><a href="#bookmark135" class="s140">73, </a><a href="#bookmark134" class="s140">74 interquartile range (IQR) 73</a></p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark134" class="s140">Isolation Forest 73</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark134" class="s140">seasonal ESD (S-ESD) </a><a href="#bookmark135" class="s140">73, 74</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s140">standard deviation 72</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s140">treating </a><a href="#bookmark135" class="s140">72, </a><a href="#bookmark136" class="s140">74, 75</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark760" class="s348">out-of-sample validation </a><a href="#bookmark446" class="s348">492 output activation functions 278</a></p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark446" class="s140">Softmax 278</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark485" class="s348">output gate 303</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark203" class="s348">overfitting </a><a href="#bookmark204" class="s348">111, 112</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark727" class="s348">over- or under-forecasting 470</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark739" class="s140">bias </a><a href="#bookmark740" class="s140">482- 484</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark740" class="s140">experiment, findings 484</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark434" class="s348">overparameterization 264</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">P</p><p style="padding-top: 6pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark490" class="s348">padding </a><a href="#bookmark491" class="s348">308-311</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark65" class="s348">pandas, datetime manipulations 23</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark67" class="s140">.dt accessor, using </a><a href="#bookmark68" class="s140">25, 26</a></p><p style="padding-top: 1pt;padding-left: 59pt;text-indent: -13pt;text-align: left;"><a href="#bookmark66" class="s140">date columns, converting into DatetimeIndex </a><a href="#bookmark67" class="s140">24, 25</a></p><p style="padding-top: 2pt;padding-left: 59pt;text-indent: -13pt;text-align: left;"><a href="#bookmark66" class="s140">date columns, converting into pd.Timestamp </a><a href="#bookmark67" class="s140">24, 25</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark69" class="s140">date offsets, managing </a><a href="#bookmark70" class="s140">27, 28</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark69" class="s140">date sequences, creating </a><a href="#bookmark70" class="s140">27, 28</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark67" class="s140">datetime properties, using </a><a href="#bookmark68" class="s140">25, 26</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark68" class="s140">indexing 26</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark68" class="s140">slicing 26</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark208" class="s348">paradigm shift </a><a href="#bookmark209" class="s348">116-118</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark479" class="s348">parameter sharing 296</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark445" class="s348">parametrized ReLU 277</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark122" class="s348">partial autocorrelation </a>59 pd.Timestamp</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark66" class="s140">date columns, converting, into </a><a href="#bookmark67" class="s140">24, 25</a></p><p style="padding-top: 5pt;padding-left: 30pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark734" class="s348">Percent Better (PB) </a><a href="#bookmark730" class="s348">477 percent error (PE) 473</a></p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark737" class="s140">for loss curves and complementary pairs 480</a></p><p class="s36" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark324" class="s348">permutation importance </a>187 pmdarima</p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark171" class="s140">reference link 89</a></p><p style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;"><a href="#bookmark670" class="s348">point forecast 445</a></p><p style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;text-align: left;"><a href="#bookmark70" class="s348">point-of-sale (POS) 28</a></p><p style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark550" class="s348">positional encoding </a><a href="#bookmark410" class="s348">364 post hoc interpretation </a><a href="#bookmark179" class="s348">256 power spectral density </a><a href="#bookmark313" class="s348">100 predict function 171</a></p><p style="padding-left: 30pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark275" class="s348">pre-whitening 149</a></p><p style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark408" class="s348">Principal Component Analysis (PCA) </a><a href="#bookmark670" class="s348">253 probabilistic forecasting 445</a></p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark674" class="s140">conformal prediction 449</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark673" class="s140">Monte Carlo dropout 448 normalizing flows </a>448 Probability Density Function</p><p style="padding-left: 51pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark671" class="s140">(PDF) </a><a href="#bookmark672" class="s140">446, 447</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark672" class="s140">quantile functions </a><a href="#bookmark673" class="s140">447, 448</a></p><p style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;"><a href="#bookmark651" class="s348">ProbSparse attention </a><a href="#bookmark652" class="s348">425-427</a></p><p class="s36" style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark329" class="s348">pseudo-residuals </a>193 pystacknet Python library</p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark363" class="s140">reference link 220</a></p><p class="s36" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">PyTorch Forecasting</p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark585" class="s140">reference link </a>387 TimeSeriesDataset, understanding</p><p style="padding-left: 51pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark584" class="s140">from </a><a href="#bookmark585" class="s140">386, 387</a></p><p style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;text-align: left;"><a href="#bookmark491" class="s348">PyTorch implementation, CNN </a><a href="#bookmark492" class="s348">311, 312</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark492" class="s140">bias 312</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark492" class="s140">dilation 312</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark492" class="s140">groups 312</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark491" class="s140">in_channels 311</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark491" class="s140">kernel_size 311</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark491" class="s140">out_channels 311</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark491" class="s140">padding 311</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark491" class="s140">padding_mode 311</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark491" class="s140">stride 311</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark488" class="s348">PyTorch implementation, GRU </a><a href="#bookmark489" class="s348">306, 307</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark488" class="s140">reference link 306</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark485" class="s348">PyTorch implementation, LSTM 303</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark485" class="s140">reference link 303</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s348">PyTorch implementation, RNN 299</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s140">batch_first parameter 299</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s140">bias parameter 299</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s140">bidirectional parameter 299</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s140">dropout parameter 299</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s140">hidden_size parameter 299</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s140">input_size parameter 299</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s140">nonlinearity parameter 299</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s140">num_layers parameter 299</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark481" class="s140">performing </a><a href="#bookmark482" class="s140">299, </a><a href="#bookmark483" class="s140">300, 301</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">PyTorch Lightning documentation</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark513" class="s140">reference link 326</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark510" class="s348">PyTorch Tabular 322</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark507" class="s140">reference link 318</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">R</p><p style="padding-top: 6pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark325" class="s348">Random Forest </a><a href="#bookmark327" class="s348">188-191</a></p><p class="s36" style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">Random Forests(TM) in XGBoost</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark327" class="s140">reference link 191</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark405" class="s348">random partition </a><a href="#bookmark406" class="s348">249, 250</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark400" class="s348">random search </a><a href="#bookmark401" class="s348">244, 245</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark668" class="s348">raw predictions 443</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark707" class="s348">RecJoint 464</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark708" class="s140">forecasting regime 465</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark707" class="s140">training regime 464</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s348">recommendation 110</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark706" class="s348">rectify strategy 463</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark707" class="s140">forecasting regime 464</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark707" class="s140">training regime 464</a></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark479" class="s348">recurrent neural networks (RNNs) </a><a href="#bookmark538" class="s348">296, 348</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark480" class="s140">bi-directional RNNs 298</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark479" class="s140">input sequences, processing </a><a href="#bookmark480" class="s140">296-298 multiple layers, stacking </a><a href="#bookmark481" class="s140">298 PyTorch implementation </a><a href="#bookmark484" class="s140">299-</a>302 used, for one-step-ahead</p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark510" class="s140">forecasting </a><a href="#bookmark516" class="s140">322-333</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark700" class="s348">recursive strategy 457</a></p><p style="padding-top: 1pt;padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark701" class="s140">forecasting regime 458</a></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><a href="#bookmark701" class="s140">training regime 458</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s348">regression 110</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 7pt;line-height: 117%;text-align: left;"><a href="#bookmark206" class="s140">time series, forecasting as </a>114 <a href="#bookmark208" class="s348">regression on time </a><a href="#bookmark204" class="s348">116 regularization 112</a></p><p style="padding-left: 35pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark318" class="s140">geometric perspective </a><a href="#bookmark320" class="s140">177-182</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark317" class="s348">regularized linear regression </a><a href="#bookmark24" class="s348">176 regular time series </a><a href="#bookmark546" class="s348">4 reinforcement learning (RL) </a><a href="#bookmark208" class="s348">360 related time series 116</a></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark732" class="s348">relative error 475</a></p><p class="s36" style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark733" class="s348">Relative Mean Absolute Error (RelMAE) </a>476 Relative Root Mean Squared</p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark733" class="s348">Error (RelRMSE) 476</a></p><p style="padding-top: 3pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark671" class="s348">ReLU function 446</a></p><p style="padding-top: 2pt;padding-left: 49pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark765" class="s348">Repeated Holdout (No Overlap) (Rep-Holdout-O) strategy 497</a></p><p style="padding-top: 2pt;padding-left: 49pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark766" class="s348">Repeated Holdout (No Overlap) with Gaps (Rep-Holdout-O(G)) strategy 498</a></p><p style="padding-top: 2pt;padding-left: 49pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark764" class="s348">repeated holdout (Rep-Holdout) strategy 496</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark486" class="s348">reset gate 304</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark116" class="s348">residual 51</a></p><p style="padding-top: 2pt;padding-left: 28pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark179" class="s348">residual spectral entropy </a><a href="#bookmark177" class="s348">100 residual variability (RV) 97</a></p><p style="padding-left: 35pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark177" class="s140">calculating 97</a></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="#bookmark317" class="s348">ridge regression 176</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s36" style="padding-top: 5pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">RNN model</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark589" class="s140">defining </a><a href="#bookmark590" class="s140">391, 392</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark590" class="s140">initializing 392</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark591" class="s140">training </a><a href="#bookmark592" class="s140">393, 394</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark517" class="s348">RNN-to-fully connected network </a><a href="#bookmark518" class="s348">334-</a><a href="#bookmark519" class="s348">336 RNN-to-RNN </a><a href="#bookmark520" class="s348">337-344</a></p><p style="padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark233" class="s348">rolling window aggregations features </a><a href="#bookmark234" class="s348">127-129</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark730" class="s348">Root Mean Squared Error (RMSE) </a>473 Root Mean Squared Scaled</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark733" class="s348">Error (RMSSE) 476</a></p><p style="padding-top: 3pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark322" class="s348">root node 184</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">S</p><p class="s36" style="padding-top: 6pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;">sampling procedure, global deep learning forecasting models</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark597" class="s140">balancing 402</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark598" class="s140">data distribution, visualizing </a><a href="#bookmark599" class="s140">403, </a>404 dataloader, using with</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark601" class="s140">WeightedRandomSampler </a><a href="#bookmark602" class="s140">406, 407</a></p><p style="padding-top: 2pt;padding-left: 59pt;text-indent: -13pt;text-align: left;"><a href="#bookmark601" class="s140">dataloader, visualizing with WeightedRandomSampler </a><a href="#bookmark602" class="s140">406, 407</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark599" class="s140">tweaking </a><a href="#bookmark600" class="s140">404, 405</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark761" class="s348">sampling strategy </a><a href="#bookmark763" class="s348">493, 495</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark766" class="s140">key points 498</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark541" class="s348">scaled dot product alignment function </a><a href="#bookmark729" class="s348">353 scale dependence 472</a></p><p class="s36" style="padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark733" class="s348">scaled error (SE) </a>476 scale, of time series</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark596" class="s140">using 401</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark398" class="s348">search space 242</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark170" class="s348">Seasonal ARIMA 88</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark119" class="s348">seasonal box plots </a><a href="#bookmark120" class="s348">56, 57</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark116" class="s348">seasonal component </a>51 seasonal_decompose</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark125" class="s140">from statsmodel </a><a href="#bookmark127" class="s140">63-65</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark126" class="s140">parameters 64</a></p><p style="padding-top: 5pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark85" class="s348">seasonal decomposition 47</a></p><p class="s36" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark134" class="s348">seasonal ESD (S-ESD) </a><a href="#bookmark135" class="s348">73, </a>74 seasonality</p><p style="padding-left: 32pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark277" class="s140">correcting 151</a></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark277" class="s140">detecting </a><a href="#bookmark279" class="s140">151-153</a></p><p style="padding-top: 1pt;padding-left: 45pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark127" class="s348">seasonality and trend decomposition using LOESS (STL) </a><a href="#bookmark128" class="s348">65-67</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark127" class="s140">parameters 65</a></p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark165" class="s348">seasonal naïve forecast </a><a href="#bookmark166" class="s348">83, 84</a></p><p class="s36" style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark118" class="s348">seasonal plot </a><a href="#bookmark119" class="s348">55, </a>56 seasonal rolling window</p><p style="padding-left: 45pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark234" class="s348">aggregations </a><a href="#bookmark235" class="s348">129-131</a></p><p style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark517" class="s348">sequence-to-sequence models 334</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark544" class="s140">forecasting with </a><a href="#bookmark546" class="s140">356-360</a></p><p style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark517" class="s140">RNN-to-fully connected network </a><a href="#bookmark518" class="s140">334-</a><a href="#bookmark519" class="s140">336 RNN-to-RNN </a><a href="#bookmark520" class="s140">337-344</a></p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark660" class="s348">Series Decomp block </a><a href="#bookmark122" class="s348">435 series stationary 59</a></p><p style="padding-left: 24pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark330" class="s348">shrinkage 194</a></p><p style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark167" class="s348">simple exponential smoothing (SES) </a><a href="#bookmark356" class="s348">85 simple hill climbing </a><a href="#bookmark357" class="s348">208-210</a></p><p class="s36" style="padding-left: 24pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark358" class="s348">simulated annealing </a><a href="#bookmark359" class="s348">212-</a><a href="#bookmark166" class="s348">215 single exponential smoothing </a><a href="#bookmark510" class="s348">84 single-step-ahead RNNs </a><a href="#bookmark516" class="s348">322-</a>333 single-step forecast baselines</p><p style="padding-left: 32pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark309" class="s140">generating 167</a></p><p class="s36" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark355" class="s348">skimming </a>207 sklearn.tree.DecisionTreeRegressor</p><p style="padding-left: 32pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark323" class="s140">reference link 185</a></p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark448" class="s348">slope 280</a></p><p style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark541" class="s348">softmax function 353</a></p><p style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark671" class="s348">SoftPlus function 446</a></p><p style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark236" class="s348">span 132</a></p><p style="padding-top: 2pt;padding-left: 24pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark593" class="s348">sparse representation </a><a href="#bookmark742" class="s348">397 Spearman’s rank correlation 487</a></p><p style="padding-left: 32pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark741" class="s140">using </a><a href="#bookmark742" class="s140">485-487</a></p><p class="s36" style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">specialized architectures</p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark636" class="s140">need for </a><a href="#bookmark637" class="s140">410, 411</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark179" class="s348">spectral entropy 100</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark322" class="s348">splitting 184</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark730" class="s348">squared error (SE) 473</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark736" class="s140">loss curves and complementary pairs for 479</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark639" class="s348">stack 413</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark362" class="s348">stacked model 219</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark362" class="s348">stacking </a><a href="#bookmark363" class="s348">219, 220</a></p><p class="s36" style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark133" class="s348">standard deviation </a><a href="#bookmark176" class="s348">72, </a><a href="#bookmark33" class="s348">96 standard Gaussian distribution </a>14 standardized code</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark309" class="s140">used, for evaluating machine learning 167</a></p><p style="padding-left: 29pt;text-indent: 7pt;line-height: 117%;text-align: left;"><a href="#bookmark309" class="s140">used, for training machine learning </a>167 <a href="#bookmark662" class="s348">Static Covariate Encoders (SEs) </a><span class="s36">437 static/meta information</span></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark593" class="s140">using 397</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark266" class="s348">stationarity </a><a href="#bookmark32" class="s348">140 stationary time series 13</a></p><p style="padding-left: 29pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark515" class="s348">stochastic gradient descent (SGD) 330</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark451" class="s140">cons 285</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark451" class="s140">pros 285</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark357" class="s348">stochastic hill climbing </a><a href="#bookmark358" class="s348">210-</a><a href="#bookmark658" class="s348">212 Stochastic Processes 433</a></p><p class="s36" style="padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark272" class="s348">stochastic trend </a><a href="#bookmark273" class="s348">146, </a>147 strategies, for combining forecasts</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark353" class="s140">best fit </a><a href="#bookmark354" class="s140">205, 206</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark354" class="s140">measures of central tendency </a><a href="#bookmark356" class="s140">206-</a><a href="#bookmark360" class="s140">208 optimal weighted ensemble </a><a href="#bookmark361" class="s140">216-</a><a href="#bookmark356" class="s140">218 simple hill climbing </a><a href="#bookmark357" class="s140">208-</a><a href="#bookmark358" class="s140">210 simulated annealing </a><a href="#bookmark359" class="s140">212-</a><a href="#bookmark357" class="s140">215 stochastic hill climbing </a><a href="#bookmark358" class="s140">210-212</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">strategies, for improving GMS</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark398" class="s140">hyperparameters, tuning 242</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark392" class="s140">memory, increasing 233</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark405" class="s140">partitioning 249</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark393" class="s140">time series meta-features, using 234</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">strftime</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark66" class="s140">URL 24</a></p><p style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark266" class="s348">strict stationarity 140</a></p><p style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark490" class="s348">stride </a><a href="#bookmark491" class="s348">308-311</a></p><p class="s36" style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark180" class="s348">sum of squared errors (SSE) </a>101 supervised machine learning</p><p style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark202" class="s140">tasks 110</a></p><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark731" class="s348">symmetric error 474</a></p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark738" class="s140">loss curves and complementary pairs for 481</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark731" class="s348">Symmetric Mean Absolute Percent Error (sMAPE) 474</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark731" class="s348">Symmetric Median Absolute Percent Error 474</a></p><p class="s36" style="padding-top: 2pt;padding-left: 24pt;text-indent: -7pt;line-height: 108%;text-align: left;">synthetic time series <a href="#bookmark30" class="s140">autoregressive (AR) signals </a><a href="#bookmark29" class="s140">11 cyclical signals </a><a href="#bookmark30" class="s140">9-11</a></p><p style="padding-left: 24pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark27" class="s140">generating 7</a></p><p style="padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark28" class="s140">red noise 8</a></p><p style="padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark29" class="s140">seasonal signals </a><a href="#bookmark30" class="s140">9-11</a></p><p style="padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark27" class="s140">white noise 7</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 15pt;text-indent: 0pt;text-align: left;">T</p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark507" class="s348">tabular regression </a><a href="#bookmark510" class="s348">318-322</a></p><p style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark480" class="s348">tanh activation 298</a></p><p style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark229" class="s348">target leakage 123</a></p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark230" class="s140">identifying 124</a></p><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark396" class="s348">target mean encoding </a><a href="#bookmark397" class="s348">237-240</a></p><ol id="l194"><ol id="l195"><li><p style="padding-top: 2pt;padding-left: 38pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark408" class="s348">istributed Stochastic Neighbor Embeddings (t-SNE) 253</a></p><p class="s36" style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark519" class="s348">teacher forcing </a>337 Temporal Convolutional</p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a href="#bookmark644" class="s348">Network (TCN) 418</a></p><p style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark208" class="s348">temporal embedding </a><a href="#bookmark228" class="s348">116, </a><a href="#bookmark237" class="s348">122, </a><a href="#bookmark651" class="s348">134, 425</a></p><p style="padding-top: 1pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark237" class="s140">calendar features 134</a></p><p style="padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark239" class="s140">Fourier terms </a><a href="#bookmark240" class="s140">136-138</a></p><p style="padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark238" class="s140">time elapsed 135</a></p><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark663" class="s348">Temporal Fusion Decoder (TD) 438</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark661" class="s348">Temporal Fusion Transformer (TFT) 436</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark667" class="s140">forecasting with 442</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark668" class="s140">interpreting </a><a href="#bookmark669" class="s140">443, 444</a></p><p style="padding-top: 1pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark661" class="s348">Temporal Fusion Transformer (TFT), architecture </a><a href="#bookmark663" class="s348">436-438</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark666" class="s140">Gated Residual Network (GRN) </a><a href="#bookmark663" class="s140">441 Locality Enhancement Seq2Seq layer </a><a href="#bookmark665" class="s140">438 temporal fusion decoder 440</a></p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="#bookmark667" class="s140">Variable Selection Network (VSN) 442</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark647" class="s348">temporal interpolation 421</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark476" class="s348">tensors 293</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">terminologies, time series forecasting</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s140">backtesting 17</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark37" class="s140">endogenous variables 18</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark37" class="s140">exogenous variables 18</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s140">explanatory forecasting 17</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark37" class="s140">forecast combination 18</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s140">forecasting 17</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s140">in-sample 17</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s140">multivariate forecasting 17</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s140">out-sample 17</a></p><p class="s36" style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">test harness</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark161" class="s140">setting up 78</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark171" class="s348">Theta Forecast </a><a href="#bookmark172" class="s348">89, 90</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark171" class="s348">theta lines 89</a></p><p style="padding-top: 2pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark206" class="s348">time delay embedding </a><a href="#bookmark208" class="s348">114-</a><a href="#bookmark228" class="s348">116, </a><a href="#bookmark231" class="s348">122, </a><a href="#bookmark511" class="s348">125, 323</a></p><p style="padding-top: 1pt;padding-left: 59pt;text-indent: -13pt;text-align: left;"><a href="#bookmark235" class="s140">exponentially weighted moving averages (EWMA) </a><a href="#bookmark237" class="s140">131-134</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark231" class="s140">lags/backshift </a><a href="#bookmark232" class="s140">125, </a>126 rolling window aggregations</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark233" class="s140">features </a><a href="#bookmark234" class="s140">127-129</a></p><p style="padding-top: 2pt;padding-left: 59pt;text-indent: -13pt;text-align: left;"><a href="#bookmark234" class="s140">seasonal rolling window aggregations </a><a href="#bookmark235" class="s140">129-131</a></p><p style="padding-top: 3pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark238" class="s348">time elapsed features </a><a href="#bookmark24" class="s348">135 time series 4</a></p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark123" class="s140">decomposing 60</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark176" class="s140">forecastability, assessing of 96</a></p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark266" class="s140">non-stationarity, handling </a><a href="#bookmark267" class="s140">140, 141</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark24" class="s140">types 4</a></p><p class="s36" style="padding-top: 1pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">time series analysis</p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark25" class="s140">causality 5</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark25" class="s140">interpretation 5</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark25" class="s140">time series classification </a><a href="#bookmark24" class="s140">5 time series forecasting 4</a></p><p style="padding-top: 1pt;padding-left: 31pt;text-indent: 0pt;text-align: left;"><a href="#bookmark115" class="s348">time series, components 50</a></p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark116" class="s140">cyclical 51</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark116" class="s140">irregular 51</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark116" class="s140">seasonal 51</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark115" class="s140">trend 50</a></p><p class="s36" style="padding-top: 1pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">time series data</p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" class="s140">formatting 33</a></p><p style="padding-top: 1pt;padding-left: 52pt;text-indent: -13pt;text-align: left;"><a href="#bookmark75" class="s140">half-hourly block-level data (hhblock), converting into 33</a></p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark76" class="s140">regular intervals, enforcing </a><a href="#bookmark77" class="s140">34, 35</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark117" class="s140">visualizing 52</a></p><p style="padding-top: 1pt;padding-left: 31pt;text-indent: 0pt;text-align: left;"><a href="#bookmark62" class="s348">time series dataset 20</a></p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark64" class="s140">data model, preparing </a><a href="#bookmark65" class="s140">22, 23</a></p><p class="s36" style="padding-top: 1pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">TimeSeriesDataset</p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark586" class="s140">dataloader, creating from </a><a href="#bookmark587" class="s140">388, 389</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark587" class="s140">dataloader, working visualization </a><a href="#bookmark588" class="s140">389, 390</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark585" class="s140">initializing 387</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark584" class="s140">parameters </a><a href="#bookmark585" class="s140">386, 387</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark584" class="s140">tasks, automating 386</a></p><p class="s36" style="padding-top: 1pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">time series / date functionality</p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark70" class="s140">reference link 28</a></p><p class="s36" style="padding-top: 1pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">time series, decomposing approach</p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark123" class="s140">deseasonalizing </a><a href="#bookmark124" class="s140">60, 61</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark123" class="s140">detrending 60</a></p><p style="padding-top: 1pt;padding-left: 52pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark407" class="s348">Time Series Feature Extraction Library (tsfel) 251</a></p><p style="padding-top: 2pt;padding-left: 31pt;text-indent: 0pt;text-align: left;"><a href="#bookmark35" class="s348">time series forecasting </a><a href="#bookmark36" class="s348">16, </a><a href="#bookmark163" class="s348">17, 80</a></p><p style="padding-top: 1pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark727" class="s140">aggregate metrics 470</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark206" class="s140">as regression 114</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s140">notation 17</a></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><a href="#bookmark727" class="s140">over- and under-forecasting 470</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark727" class="s140">temporal relevance 470</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s140">terminologies 17</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">time series format</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark77" class="s140">London Smart Meters dataset 35</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">time series meta-features</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark395" class="s140">frequency encoding </a><a href="#bookmark396" class="s140">236, </a><a href="#bookmark397" class="s140">237 LightGBM’s native handling </a><a href="#bookmark398" class="s140">240-</a><a href="#bookmark393" class="s140">242 one-hot encoding </a><a href="#bookmark395" class="s140">234-236</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark393" class="s140">ordinal encoding </a><a href="#bookmark395" class="s140">234-</a><a href="#bookmark396" class="s140">236 target mean encoding </a><a href="#bookmark397" class="s140">237-</a><a href="#bookmark393" class="s140">240 using 234</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">TimeSHAP</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark670" class="s140">reference link 445</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">TimeSynth time series</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark29" class="s140">reference link 9</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">time-varying information</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark592" class="s140">using </a><a href="#bookmark593" class="s140">394-397</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark732" class="s348">Tracking Signal </a>475 traditional programming</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark187" class="s140">versus machine learning 107</a></p><p class="s36" style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">Trainer</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark514" class="s140">reference link 329</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s348">training 110</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark515" class="s348">training step 330</a></p><p class="s36" style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark229" class="s348">train-test contamination </a>123 transform</p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark279" class="s140">deseasonalizing </a><a href="#bookmark280" class="s140">153-155</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark276" class="s140">detrending 150</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark270" class="s140">differencing </a><a href="#bookmark271" class="s140">144, 145</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark438" class="s348">transformations 270</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark546" class="s348">transformers 360</a></p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark557" class="s140">forecasting with </a><a href="#bookmark558" class="s140">373-378</a></p><p style="padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark556" class="s140">in time series </a><a href="#bookmark557" class="s140">372, 373</a></p><p style="padding-top: 1pt;padding-left: 29pt;text-indent: 0pt;text-align: left;"><a href="#bookmark410" class="s348">transparency 256</a></p><p style="padding-top: 2pt;padding-left: 29pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark403" class="s348">Tree Parzen Estimator (TPE) </a><a href="#bookmark115" class="s348">247 trend component </a><a href="#bookmark123" class="s348">50, 60</a></p><p class="s36" style="padding-left: 29pt;text-indent: 0pt;line-height: 11pt;text-align: left;">trends</p><p style="padding-top: 1pt;padding-left: 37pt;text-indent: 0pt;text-align: left;"><a href="#bookmark271" class="s140">correcting 145</a></p><p style="padding-top: 4pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark271" class="s140">detecting 145</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark272" class="s140">deterministic trend </a><a href="#bookmark273" class="s140">146, 147</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark272" class="s140">stochastic trend </a><a href="#bookmark273" class="s140">146, 147</a></p><p style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;"><a href="#bookmark355" class="s348">trimming 207</a></p><p class="s36" style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark167" class="s348">triple exponential smoothing </a><a href="#bookmark168" class="s348">85, </a>86 tsfresh</p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark240" class="s140">reference link 138</a></p><p class="s36" style="padding-top: 1pt;padding-left: 38pt;text-indent: -7pt;line-height: 108%;text-align: left;">types, time series data <a href="#bookmark24" class="s140">irregular time series 4 regular time series 4</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 29pt;text-indent: 0pt;text-align: left;">U</p><p class="s36" style="padding-top: 6pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">uk_bank_holidays.csv file</p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark79" class="s140">mapping 37</a></p><p style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s348">underfitting </a><a href="#bookmark204" class="s348">110-112</a></p><p style="padding-top: 2pt;padding-left: 30pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark651" class="s348">Uniform Input Representation </a><a href="#bookmark266" class="s348">425 unit roots </a><a href="#bookmark269" class="s348">140-143</a></p><p style="padding-left: 12pt;text-indent: 0pt;line-height: 12pt;text-align: center;">Augmented Dickey-Fuller</p><p style="padding-left: 12pt;text-indent: 0pt;line-height: 13pt;text-align: center;"><a href="#bookmark269" class="s140">(ADF) test  </a><a href="#bookmark270" class="s140">143, 144</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark268" class="s140">correcting 142</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark268" class="s140">detecting 142</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark270" class="s140">transform, differencing </a><a href="#bookmark271" class="s140">144, 145</a></p><p style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;"><a href="#bookmark487" class="s348">update gate 305</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 29pt;text-indent: 0pt;text-align: left;">V</p><p class="s36" style="padding-top: 6pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">validation dataset</p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark161" class="s140">creating </a><a href="#bookmark162" class="s140">78, 79</a></p><p class="s36" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark205" class="s348">validation set </a>113 validation strategy</p><p style="padding-left: 38pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark769" class="s140">selecting 501</a></p><p class="s36" style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">validation strategy for datasets</p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark770" class="s140">key points 502</a></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark770" class="s140">with multiple time series 502</a></p><p style="padding-top: 1pt;padding-left: 30pt;text-indent: 0pt;text-align: left;"><a href="#bookmark651" class="s348">value embedding 425</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark546" class="s348">vanilla Transformer model 360</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark555" class="s140">decoder 371</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark552" class="s140">encoder </a><a href="#bookmark553" class="s140">368, 369</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark549" class="s140">multi-headed attention (MHA) </a><a href="#bookmark550" class="s140">363, 364</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark550" class="s140">positional encoding </a><a href="#bookmark551" class="s140">364-367 position-wise feed-forward layer </a><a href="#bookmark547" class="s140">367 self-attention </a><a href="#bookmark548" class="s140">361, 362</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark483" class="s348">vanishing gradients 301</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark662" class="s348">Variable Selection Network (VSN) </a><a href="#bookmark667" class="s348">437, 442</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark437" class="s348">vector 269</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark438" class="s348">vector space 270</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">visualization techniques, time series data</p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark121" class="s140">autocorrelation plot </a><a href="#bookmark123" class="s140">58-60</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark121" class="s140">calendar heatmaps 58</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark117" class="s140">line charts </a><a href="#bookmark118" class="s140">52-55</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark119" class="s140">seasonal box plots </a><a href="#bookmark120" class="s140">56, 57</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark118" class="s140">seasonal plot </a><a href="#bookmark119" class="s140">55, 56</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s347" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">W</p><p style="padding-top: 6pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark644" class="s348">WaveNet 418</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark266" class="s348">weak stationarity </a>140 weather_hourly_darksky.csv file</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark79" class="s140">mapping 37</a></p><p style="padding-top: 1pt;padding-left: 59pt;text-indent: -21pt;line-height: 111%;text-align: left;"><a href="#bookmark729" class="s348">Weighted Average Percent Error (WAPE) </a><a href="#bookmark731" class="s348">472, 474</a></p><p class="s36" style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;line-height: 122%;text-align: left;"><a href="#bookmark729" class="s348">Weighted Mean Absolute Error </a>472 WeightedRandomSampler</p><p style="padding-left: 46pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark601" class="s140">dataloader, using with </a><a href="#bookmark602" class="s140">406, 407</a></p><p style="padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark601" class="s140">dataloader, visualizing with </a><a href="#bookmark602" class="s140">406, 407</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark179" class="s348">Welch method 100</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark280" class="s348">White test 155</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark76" class="s348">wide-format data 34</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark658" class="s348">Wiener-Khinchin theorem 433</a></p><p style="padding-top: 2pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark761" class="s348">window strategy </a><a href="#bookmark762" class="s348">493, 494</a></p><p style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: left;"><a href="#bookmark762" class="s140">key points 494</a></p><p style="padding-top: 1pt;padding-left: 38pt;text-indent: 0pt;text-align: left;"><a href="#bookmark325" class="s348">wisdom of the crowd approach 188</a></p><p class="s347" style="padding-top: 5pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">X</p><p style="padding-top: 6pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"><a href="#bookmark327" class="s348">XGBRFRegressor 191</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><img width="153" height="39" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_915.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a href="http://Packt.com/" class="s349" name="bookmark774">Packt.com</a></p><p class="s350" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;">Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s351" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;">Why subscribe?</p><ul id="l196"><li><p class="s350" style="padding-top: 7pt;padding-left: 55pt;text-indent: -13pt;line-height: 89%;text-align: left;">Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Improve your learning with Skill Plans built especially for you</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Get a free eBook or video every month</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Fully searchable for easy access to vital information</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Copy and paste, print, and bookmark content</p><p class="s350" style="padding-top: 8pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="http://packt.com/" style=" color: #231F20; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank">Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at </a><a href="http://packt.com/" class="s349" target="_blank">packt.com</a><span class="s355"> </span>and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at <span class="s20">customercare@packtpub. com </span>for more details.</p><p class="s355" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: justify;"><a href="http://www.packt.com/" style=" color: #231F20; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank">At </a>www.packt.com<span class="s350">, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s352" style="padding-top: 5pt;padding-left: 93pt;text-indent: 0pt;text-align: left;"><a name="bookmark775">Other Books You May Enjoy</a><a name="bookmark776">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s350" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">If you enjoyed this book, you may be interested in these other books by Packt:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 154pt;text-indent: 0pt;text-align: left;"><span><img width="218" height="270" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_916.jpg"/></span></p><p class="s5" style="padding-top: 11pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Intelligent Document Processing with AWS AI/ML</p><p class="s350" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">Sonali Sahu</p><p class="s350" style="padding-top: 6pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">ISBN: 9781801810562</p><ul id="l197"><li><p class="s350" style="padding-top: 7pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Understand the requirements and challenges in deriving insights from a document</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Explore common stages in the intelligent document processing pipeline</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Discover how AWS AI/ML can successfully automate IDP pipelines</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Find out how to write clean and elegant Python code by leveraging AI</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Get to grips with the concepts and functionalities of AWS AI services</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Explore IDP across industries such as insurance, healthcare, finance, and the public sector</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Determine how to apply business rules in IDP</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 64pt;text-indent: -13pt;text-align: left;">Build, train, and deploy models with serverless architecture for IDP</p><p class="s353" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">Other Books You May Enjoy 523</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_917.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 145pt;text-indent: 0pt;text-align: left;"><span><img width="218" height="270" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_918.jpg"/></span></p><p class="s5" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Practical Deep Learning at Scale with MLflow</p><p class="s350" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Yong Liu</p><p class="s350" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">ISBN: 9781803241333</p></li></ul></li><li><p class="s350" style="padding-top: 7pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Understand MLOps and deep learning life cycle development</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Track deep learning models, code, data, parameters, and metrics</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Build, deploy, and run deep learning model pipelines anywhere</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Run hyperparameter optimization at scale to tune deep learning models</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Build production-grade multi-step deep learning inference pipelines</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Implement scalable deep learning explainability as a service</p></li><li><p class="s350" style="padding-top: 4pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Deploy deep learning batch and streaming inference services</p></li><li><p class="s350" style="padding-top: 5pt;padding-left: 55pt;text-indent: -13pt;text-align: left;">Ship practical NLP solutions from experimentation to production</p></li></ul></li></ol></ol><p class="s353" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">524</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_919.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s351" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Packt is searching for authors like you</p><p class="s355" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;"><a href="http://authors.packtpub.com/" style=" color: #231F20; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank">If you’re interested in becoming an author for Packt, please visit </a><a href="http://authors.packtpub.com/" class="s349" target="_blank">authors.packtpub.com</a> <span class="s350">and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s351" style="padding-left: 37pt;text-indent: 0pt;text-align: justify;">Share Your Thoughts</p><p class="s350" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Now you’ve finished <i>Modern Time Series Forecasting with Python</i><a href="https://packt.link/r/1-803-24680-4" style=" color: #231F20; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;" target="_blank">, we’d love to hear your thoughts! If you purchased the book from Amazon, please </a><a href="https://packt.link/r/1-803-24680-4" class="s349" target="_blank">click here to go straight to the Amazon review page</a><span class="s355"> </span>for this book and share your feedback or leave a review on the site that you purchased it from.</p><p class="s350" style="padding-top: 7pt;padding-left: 37pt;text-indent: 0pt;text-align: justify;">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p><p class="s353" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">525</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="531" height="1" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_920.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s351" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Download a free PDF copy of this book</p><p class="s350" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Thanks for purchasing this book!</p><p class="s350" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 147%;text-align: left;">Do you like to read on the go but are unable to carry your print books everywhere? Is your eBook purchase not compatible with the device of your choice?</p><p class="s350" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.</p><p class="s350" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.</p><p class="s350" style="padding-top: 7pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily</p><p class="s350" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Follow these simple steps to get the benefits:</p><ol id="l198"><li><p class="s350" style="padding-top: 8pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">Scan the QR code or visit the link below</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 174pt;text-indent: 0pt;text-align: left;"><span><img width="138" height="138" alt="image" src="Manu Joseph - Modern Time Series Forecasting with Python_ Explore industry-ready time series forecasting using modern machine learning and deep learnin/Image_921.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s355" style="padding-left: 12pt;text-indent: 0pt;text-align: center;">https://packt.link/free-ebook/9781803246802</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s350" style="padding-left: 55pt;text-indent: -18pt;text-align: left;">Submit your proof of purchase</p></li><li><p class="s350" style="padding-top: 3pt;padding-left: 55pt;text-indent: -18pt;text-align: left;">That’s it! We’ll send your free PDF and other benefits to your email directly</p></li></ol></body></html>
