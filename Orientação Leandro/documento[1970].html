<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>documento[1970]</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; margin:0pt; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 h2 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s3 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s4 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .a, a { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s6 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s7 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s8 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 2pt; }
 .s9 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s10 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s11 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s12 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -4pt; }
 .s13 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -8pt; }
 .s14 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s15 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -3pt; }
 .s16 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 2pt; }
 .s17 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s18 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s19 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s20 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; vertical-align: 8pt; }
 .s21 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s22 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s23 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 8.5pt; vertical-align: 6pt; }
 .s24 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; vertical-align: 8pt; }
 .s25 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s26 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 12pt; vertical-align: 8pt; }
 .s27 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 8.5pt; }
 .s28 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s29 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 2pt; }
 .s30 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s31 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 17pt; }
 .s32 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -8pt; }
 .s33 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s34 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -8pt; }
 .s35 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -2pt; }
 .s36 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s37 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -4pt; }
 .s38 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 4pt; }
 .s39 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -2pt; }
 .s40 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -3pt; }
 .s41 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 5pt; }
 .s42 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 5pt; }
 .s43 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 1pt; }
 .s44 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -3pt; }
 .s45 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s46 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 7pt; }
 .s47 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 2pt; }
 .s48 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 2pt; }
 .s49 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -2pt; }
 .s50 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -5pt; }
 .s51 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -10pt; }
 .s52 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -7pt; }
 .s53 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s54 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -5pt; }
 .s55 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -6pt; }
 .s56 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s57 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -8pt; }
 .s58 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s59 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -5pt; }
 .s60 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 4pt; }
 .s61 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 2pt; }
 .s62 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 10pt; }
 .s63 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 5pt; }
 .s64 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 9pt; }
 .s65 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 7pt; }
 .s66 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 5pt; }
 .s67 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 9pt; }
 .s68 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 11pt; }
 .s69 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 9pt; }
 .s70 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s71 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s72 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s73 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 12pt; }
 .s74 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s75 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -4pt; }
 .s76 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 8pt; }
 .s77 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -6pt; }
 .s78 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 10pt; }
 .s79 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -7pt; }
 .s80 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -10pt; }
 .s81 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s82 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 6pt; }
 .s83 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 8pt; }
 .s84 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9.5pt; }
 .s85 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s86 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10.5pt; }
 .s87 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .s88 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s89 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s90 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -8pt; }
 .s91 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -7pt; }
 .s92 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 3pt; }
 .s93 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s94 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; vertical-align: 8pt; }
 .s95 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8.5pt; vertical-align: 6pt; }
 .s96 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: -3pt; }
 .s97 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 4pt; }
 .s98 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s99 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; vertical-align: 20pt; }
 .s100 { color: black; font-family:"Lucida Sans", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 1pt; }
 .s101 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 1pt; }
 .s102 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s103 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s104 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7.5pt; }
 .s105 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s106 { color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s107 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s108 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 15.5pt; }
 .s109 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s110 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 h4 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8.5pt; }
 .s111 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -4pt; }
 .s112 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; vertical-align: -8pt; }
 .s113 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s114 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s115 { color: #212121; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s116 { color: #212121; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 2; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 1; }
 #l2> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l2> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 li {display: block; }
 #l3 {padding-left: 0pt;counter-reset: d1 3; }
 #l3> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l3> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l4 {padding-left: 0pt;counter-reset: d2 1; }
 #l4> li>*:first-child:before {counter-increment: d2; content: counter(d1, decimal)"."counter(d2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l4> li:first-child>*:first-child:before {counter-increment: d2 0;  }
 li {display: block; }
 #l5 {padding-left: 0pt;counter-reset: e1 4; }
 #l5> li>*:first-child:before {counter-increment: e1; content: counter(e1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l5> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 #l6 {padding-left: 0pt;counter-reset: e2 1; }
 #l6> li>*:first-child:before {counter-increment: e2; content: counter(e1, decimal)"."counter(e2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l6> li:first-child>*:first-child:before {counter-increment: e2 0;  }
 li {display: block; }
 #l7 {padding-left: 0pt;counter-reset: f1 5; }
 #l7> li>*:first-child:before {counter-increment: f1; content: counter(f1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l7> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l8 {padding-left: 0pt;counter-reset: f2 1; }
 #l8> li>*:first-child:before {counter-increment: f2; content: counter(f1, decimal)"."counter(f2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l8> li:first-child>*:first-child:before {counter-increment: f2 0;  }
 li {display: block; }
 #l9 {padding-left: 0pt;counter-reset: g1 6; }
 #l9> li>*:first-child:before {counter-increment: g1; content: counter(g1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l9> li:first-child>*:first-child:before {counter-increment: g1 0;  }
 #l10 {padding-left: 0pt;counter-reset: g2 1; }
 #l10> li>*:first-child:before {counter-increment: g2; content: counter(g1, decimal)"."counter(g2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l10> li:first-child>*:first-child:before {counter-increment: g2 0;  }
 li {display: block; }
 #l11 {padding-left: 0pt;counter-reset: h1 3; }
 #l11> li>*:first-child:before {counter-increment: h1; content: counter(h1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l11> li:first-child>*:first-child:before {counter-increment: h1 0;  }
 #l12 {padding-left: 0pt;counter-reset: h2 1; }
 #l12> li>*:first-child:before {counter-increment: h2; content: counter(h1, decimal)"."counter(h2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l12> li:first-child>*:first-child:before {counter-increment: h2 0;  }
 li {display: block; }
 #l13 {padding-left: 0pt;counter-reset: i1 6; }
 #l13> li>*:first-child:before {counter-increment: i1; content: counter(i1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l13> li:first-child>*:first-child:before {counter-increment: i1 0;  }
 #l14 {padding-left: 0pt;counter-reset: i2 1; }
 #l14> li>*:first-child:before {counter-increment: i2; content: counter(i1, decimal)"."counter(i2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l14> li:first-child>*:first-child:before {counter-increment: i2 0;  }
 li {display: block; }
 #l15 {padding-left: 0pt;counter-reset: j1 1; }
 #l15> li>*:first-child:before {counter-increment: j1; content: counter(j1, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l15> li:first-child>*:first-child:before {counter-increment: j1 0;  }
 #l16 {padding-left: 0pt;counter-reset: j2 1; }
 #l16> li>*:first-child:before {counter-increment: j2; content: counter(j1, decimal)"."counter(j2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l16> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l17 {padding-left: 0pt;counter-reset: j3 1; }
 #l17> li>*:first-child:before {counter-increment: j3; content: counter(j1, decimal)"."counter(j2, decimal)"."counter(j3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l17> li:first-child>*:first-child:before {counter-increment: j3 0;  }
 #l18 {padding-left: 0pt;counter-reset: j2 1; }
 #l18> li>*:first-child:before {counter-increment: j2; content: counter(j1, decimal)"."counter(j2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l18> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l19 {padding-left: 0pt;counter-reset: j3 1; }
 #l19> li>*:first-child:before {counter-increment: j3; content: counter(j1, decimal)"."counter(j2, decimal)"."counter(j3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l19> li:first-child>*:first-child:before {counter-increment: j3 0;  }
 #l20 {padding-left: 0pt;counter-reset: j2 1; }
 #l20> li>*:first-child:before {counter-increment: j2; content: counter(j1, decimal)"."counter(j2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l20> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l21 {padding-left: 0pt;counter-reset: j3 1; }
 #l21> li>*:first-child:before {counter-increment: j3; content: counter(j1, decimal)"."counter(j2, decimal)"."counter(j3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l21> li:first-child>*:first-child:before {counter-increment: j3 0;  }
 #l22 {padding-left: 0pt;counter-reset: j2 1; }
 #l22> li>*:first-child:before {counter-increment: j2; content: counter(j1, decimal)"."counter(j2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l22> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l23 {padding-left: 0pt;counter-reset: j2 1; }
 #l23> li>*:first-child:before {counter-increment: j2; content: counter(j1, decimal)"."counter(j2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l23> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l24 {padding-left: 0pt;counter-reset: j3 1; }
 #l24> li>*:first-child:before {counter-increment: j3; content: counter(j1, decimal)"."counter(j2, decimal)"."counter(j3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l24> li:first-child>*:first-child:before {counter-increment: j3 0;  }
 #l25 {padding-left: 0pt;counter-reset: j3 1; }
 #l25> li>*:first-child:before {counter-increment: j3; content: counter(j1, decimal)"."counter(j2, decimal)"."counter(j3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l25> li:first-child>*:first-child:before {counter-increment: j3 0;  }
 #l26 {padding-left: 0pt;counter-reset: j2 1; }
 #l26> li>*:first-child:before {counter-increment: j2; content: counter(j1, decimal)"."counter(j2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l26> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 li {display: block; }
 #l27 {padding-left: 0pt;counter-reset: k1 1; }
 #l27> li>*:first-child:before {counter-increment: k1; content: counter(k1, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l27> li:first-child>*:first-child:before {counter-increment: k1 0;  }
 #l28 {padding-left: 0pt;counter-reset: k2 1; }
 #l28> li>*:first-child:before {counter-increment: k2; content: counter(k1, decimal)"."counter(k2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l28> li:first-child>*:first-child:before {counter-increment: k2 0;  }
 #l29 {padding-left: 0pt;counter-reset: k3 1; }
 #l29> li>*:first-child:before {counter-increment: k3; content: counter(k1, decimal)"."counter(k2, decimal)"."counter(k3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l29> li:first-child>*:first-child:before {counter-increment: k3 0;  }
 #l30 {padding-left: 0pt;counter-reset: k4 1; }
 #l30> li>*:first-child:before {counter-increment: k4; content: counter(k4, decimal)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l30> li:first-child>*:first-child:before {counter-increment: k4 0;  }
 #l31 {padding-left: 0pt;counter-reset: l1 1; }
 #l31> li>*:first-child:before {counter-increment: l1; content: "("counter(l1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l31> li:first-child>*:first-child:before {counter-increment: l1 0;  }
 #l32 {padding-left: 0pt;counter-reset: k2 1; }
 #l32> li>*:first-child:before {counter-increment: k2; content: counter(k1, decimal)"."counter(k2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l32> li:first-child>*:first-child:before {counter-increment: k2 0;  }
 #l33 {padding-left: 0pt;counter-reset: k3 1; }
 #l33> li>*:first-child:before {counter-increment: k3; content: counter(k1, decimal)"."counter(k2, decimal)"."counter(k3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l33> li:first-child>*:first-child:before {counter-increment: k3 0;  }
 #l34 {padding-left: 0pt;counter-reset: m1 2; }
 #l34> li>*:first-child:before {counter-increment: m1; content: counter(m1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l34> li:first-child>*:first-child:before {counter-increment: m1 0;  }
 #l35 {padding-left: 0pt;counter-reset: m2 7; }
 #l35> li>*:first-child:before {counter-increment: m2; content: counter(m1, decimal)"."counter(m2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l35> li:first-child>*:first-child:before {counter-increment: m2 0;  }
 #l36 {padding-left: 0pt;counter-reset: m3 1; }
 #l36> li>*:first-child:before {counter-increment: m3; content: "("counter(m3, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l36> li:first-child>*:first-child:before {counter-increment: m3 0;  }
 #l37 {padding-left: 0pt;counter-reset: k2 1; }
 #l37> li>*:first-child:before {counter-increment: k2; content: counter(k1, decimal)"."counter(k2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l37> li:first-child>*:first-child:before {counter-increment: k2 0;  }
 #l38 {padding-left: 0pt; }
 #l38> li>*:first-child:before {content: "• "; color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l39 {padding-left: 0pt;counter-reset: o1 1; }
 #l39> li>*:first-child:before {counter-increment: o1; content: "("counter(o1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l39> li:first-child>*:first-child:before {counter-increment: o1 0;  }
 #l40 {padding-left: 0pt;counter-reset: p1 1; }
 #l40> li>*:first-child:before {counter-increment: p1; content: "("counter(p1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l40> li:first-child>*:first-child:before {counter-increment: p1 0;  }
 #l41 {padding-left: 0pt;counter-reset: k3 1; }
 #l41> li>*:first-child:before {counter-increment: k3; content: counter(k1, decimal)"."counter(k2, decimal)"."counter(k3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l41> li:first-child>*:first-child:before {counter-increment: k3 0;  }
 #l42 {padding-left: 0pt;counter-reset: k2 1; }
 #l42> li>*:first-child:before {counter-increment: k2; content: counter(k1, decimal)"."counter(k2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l42> li:first-child>*:first-child:before {counter-increment: k2 0;  }
 #l43 {padding-left: 0pt;counter-reset: q1 4; }
 #l43> li>*:first-child:before {counter-increment: q1; content: counter(q1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l43> li:first-child>*:first-child:before {counter-increment: q1 0;  }
 #l44 {padding-left: 0pt;counter-reset: q2 3; }
 #l44> li>*:first-child:before {counter-increment: q2; content: counter(q1, decimal)"."counter(q2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l44> li:first-child>*:first-child:before {counter-increment: q2 0;  }
 #l45 {padding-left: 0pt;counter-reset: q3 1; }
 #l45> li>*:first-child:before {counter-increment: q3; content: "("counter(q3, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l45> li:first-child>*:first-child:before {counter-increment: q3 0;  }
 #l46 {padding-left: 0pt;counter-reset: r1 4; }
 #l46> li>*:first-child:before {counter-increment: r1; content: counter(r1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l46> li:first-child>*:first-child:before {counter-increment: r1 0;  }
 #l47 {padding-left: 0pt;counter-reset: r2 6; }
 #l47> li>*:first-child:before {counter-increment: r2; content: counter(r1, decimal)"."counter(r2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l47> li:first-child>*:first-child:before {counter-increment: r2 0;  }
 #l48 {padding-left: 0pt;counter-reset: r3 1; }
 #l48> li>*:first-child:before {counter-increment: r3; content: "("counter(r3, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l48> li:first-child>*:first-child:before {counter-increment: r3 0;  }
 #l49 {padding-left: 0pt;counter-reset: s1 4; }
 #l49> li>*:first-child:before {counter-increment: s1; content: counter(s1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l49> li:first-child>*:first-child:before {counter-increment: s1 0;  }
 #l50 {padding-left: 0pt;counter-reset: s2 4; }
 #l50> li>*:first-child:before {counter-increment: s2; content: counter(s1, decimal)"."counter(s2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l50> li:first-child>*:first-child:before {counter-increment: s2 0;  }
 #l51 {padding-left: 0pt;counter-reset: s3 1; }
 #l51> li>*:first-child:before {counter-increment: s3; content: "("counter(s3, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l51> li:first-child>*:first-child:before {counter-increment: s3 0;  }
 #l52 {padding-left: 0pt;counter-reset: t1 3; }
 #l52> li>*:first-child:before {counter-increment: t1; content: "("counter(t1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l52> li:first-child>*:first-child:before {counter-increment: t1 0;  }
 #l53 {padding-left: 0pt;counter-reset: t2 1; }
 #l53> li>*:first-child:before {counter-increment: t2; content: "("counter(t2, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l53> li:first-child>*:first-child:before {counter-increment: t2 0;  }
 #l54 {padding-left: 0pt;counter-reset: u1 3; }
 #l54> li>*:first-child:before {counter-increment: u1; content: "("counter(u1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l54> li:first-child>*:first-child:before {counter-increment: u1 0;  }
 #l55 {padding-left: 0pt;counter-reset: u2 1; }
 #l55> li>*:first-child:before {counter-increment: u2; content: "("counter(u2, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l55> li:first-child>*:first-child:before {counter-increment: u2 0;  }
 #l56 {padding-left: 0pt;counter-reset: k2 1; }
 #l56> li>*:first-child:before {counter-increment: k2; content: counter(k1, decimal)"."counter(k2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l56> li:first-child>*:first-child:before {counter-increment: k2 0;  }
 #l57 {padding-left: 0pt;counter-reset: k3 1; }
 #l57> li>*:first-child:before {counter-increment: k3; content: counter(k1, decimal)"."counter(k2, decimal)"."counter(k3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l57> li:first-child>*:first-child:before {counter-increment: k3 0;  }
 #l58 {padding-left: 0pt; }
 #l58> li>*:first-child:before {content: "• "; color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l59 {padding-left: 0pt;counter-reset: k3 1; }
 #l59> li>*:first-child:before {counter-increment: k3; content: counter(k1, decimal)"."counter(k2, decimal)"."counter(k3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l59> li:first-child>*:first-child:before {counter-increment: k3 0;  }
 #l60 {padding-left: 0pt;counter-reset: w1 5; }
 #l60> li>*:first-child:before {counter-increment: w1; content: counter(w1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l60> li:first-child>*:first-child:before {counter-increment: w1 0;  }
 #l61 {padding-left: 0pt;counter-reset: w2 1; }
 #l61> li>*:first-child:before {counter-increment: w2; content: counter(w1, decimal)"."counter(w2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l61> li:first-child>*:first-child:before {counter-increment: w2 0;  }
 #l62 {padding-left: 0pt; }
 #l62> li>*:first-child:before {content: "• "; color: black; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l63 {padding-left: 0pt;counter-reset: k2 1; }
 #l63> li>*:first-child:before {counter-increment: k2; content: counter(k1, decimal)"."counter(k2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l63> li:first-child>*:first-child:before {counter-increment: k2 0;  }
 #l64 {padding-left: 0pt;counter-reset: x1 2; }
 #l64> li>*:first-child:before {counter-increment: x1; content: counter(x1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l64> li:first-child>*:first-child:before {counter-increment: x1 0;  }
 #l65 {padding-left: 0pt;counter-reset: x2 5; }
 #l65> li>*:first-child:before {counter-increment: x2; content: counter(x1, decimal)"."counter(x2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l65> li:first-child>*:first-child:before {counter-increment: x2 0;  }
 #l66 {padding-left: 0pt;counter-reset: x3 1; }
 #l66> li>*:first-child:before {counter-increment: x3; content: "("counter(x3, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l66> li:first-child>*:first-child:before {counter-increment: x3 0;  }
 #l67 {padding-left: 0pt;counter-reset: y1 3; }
 #l67> li>*:first-child:before {counter-increment: y1; content: "("counter(y1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l67> li:first-child>*:first-child:before {counter-increment: y1 0;  }
 #l68 {padding-left: 0pt;counter-reset: y2 1; }
 #l68> li>*:first-child:before {counter-increment: y2; content: "("counter(y2, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l68> li:first-child>*:first-child:before {counter-increment: y2 0;  }
 #l69 {padding-left: 0pt;counter-reset: z1 6; }
 #l69> li>*:first-child:before {counter-increment: z1; content: counter(z1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l69> li:first-child>*:first-child:before {counter-increment: z1 0;  }
 #l70 {padding-left: 0pt;counter-reset: z2 2; }
 #l70> li>*:first-child:before {counter-increment: z2; content: counter(z1, decimal)"."counter(z2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l70> li:first-child>*:first-child:before {counter-increment: z2 0;  }
 #l71 {padding-left: 0pt;counter-reset: z3 1; }
 #l71> li>*:first-child:before {counter-increment: z3; content: "("counter(z3, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l71> li:first-child>*:first-child:before {counter-increment: z3 0;  }
 #l72 {padding-left: 0pt;counter-reset: c1 3; }
 #l72> li>*:first-child:before {counter-increment: c1; content: "("counter(c1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l72> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l73 {padding-left: 0pt;counter-reset: c2 1; }
 #l73> li>*:first-child:before {counter-increment: c2; content: "("counter(c2, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l73> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l74 {padding-left: 0pt;counter-reset: d1 5; }
 #l74> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l74> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l75 {padding-left: 0pt;counter-reset: d2 1; }
 #l75> li>*:first-child:before {counter-increment: d2; content: counter(d1, decimal)"."counter(d2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l75> li:first-child>*:first-child:before {counter-increment: d2 0;  }
 #l76 {padding-left: 0pt;counter-reset: d3 1; }
 #l76> li>*:first-child:before {counter-increment: d3; content: "("counter(d3, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l76> li:first-child>*:first-child:before {counter-increment: d3 0;  }
 #l77 {padding-left: 0pt;counter-reset: e1 3; }
 #l77> li>*:first-child:before {counter-increment: e1; content: "("counter(e1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l77> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 #l78 {padding-left: 0pt;counter-reset: e2 1; }
 #l78> li>*:first-child:before {counter-increment: e2; content: "("counter(e2, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l78> li:first-child>*:first-child:before {counter-increment: e2 0;  }
 #l79 {padding-left: 0pt;counter-reset: f1 3; }
 #l79> li>*:first-child:before {counter-increment: f1; content: "("counter(f1, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l79> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l80 {padding-left: 0pt;counter-reset: f2 1; }
 #l80> li>*:first-child:before {counter-increment: f2; content: "("counter(f2, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l80> li:first-child>*:first-child:before {counter-increment: f2 0;  }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><span><img width="763" height="887" alt="image" src="documento[1970]/Image_001.jpg"/></span></p><p style="padding-top: 8pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">UNIVERSIDADE FEDERAL DO PARANÁ</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">LUCAS DE AZEVEDO TAKARA</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 12pt;padding-left: 45pt;text-indent: 0pt;line-height: 173%;text-align: center;">DEEP REINFORCEMENT LEARNING APPROACH APPLIED TO AN AUTOMATED ASSET TRADING SYSTEM</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 12pt;padding-left: 215pt;text-indent: 0pt;line-height: 173%;text-align: center;">CURITIBA 2023</p><p style="padding-top: 8pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">LUCAS DE AZEVEDO TAKARA</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 14pt;padding-left: 45pt;text-indent: 0pt;line-height: 173%;text-align: center;">DEEP REINFORCEMENT LEARNING APPROACH APPLIED TO AN AUTOMATED ASSET TRADING SYSTEM</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 13pt;padding-left: 243pt;text-indent: 0pt;text-align: justify;">Dissertação apresentada como requisito parcial à obtenção do grau de Mestre em Engenharia Elétrica, no Programa de Pós-Graduação em Engenharia Elétrica, Setor de Ciências Exatas, da Universidade Federal do Paraná.</p><p class="s1" style="padding-top: 9pt;padding-left: 243pt;text-indent: 0pt;line-height: 191%;text-align: justify;">Área de concentração: <i>Engenharia Elétrica</i>. Orientador: Leandro dos Santos Coelho.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 215pt;text-indent: 0pt;line-height: 173%;text-align: center;">CURITIBA 2023</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 4pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">DADOS INTERNACIONAIS DE CATALOGAÇÃO NA PUBLICAÇÃO (CIP) UNIVERSIDADE FEDERAL DO PARANÁ</p><p class="s1" style="padding-bottom: 3pt;padding-left: 91pt;text-indent: 0pt;line-height: 11pt;text-align: left;">SISTEMA DE BIBLIOTECAS – BIBLIOTECA CIÊNCIA E TECNOLOGIA</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 42pt;text-indent: 0pt;text-align: left;">Takara, Lucas de Azevedo.</p><p class="s1" style="padding-left: 42pt;text-indent: 17pt;text-align: left;">Deep Reinforcement Learning approach applied to an Automated Asset Trading System. / Lucas de Azevedo Takara. – Curitiba, 2023.</p><p class="s1" style="padding-left: 56pt;text-indent: 0pt;text-align: left;">1 recurso on-line : PDF.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 42pt;text-indent: 17pt;text-align: left;">Dissertação (Mestrado) – Universidade Federal do Paraná, Setor de Tecnologia Programa de Pós-Graduação em Engenharia Elétrica.</p><p class="s1" style="padding-left: 59pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Orientador: Prof. Dr. Leandro dos Santos Coelho.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 42pt;text-indent: 17pt;text-align: left;">1. Engenharia elétrica. 2. Deep Reinforcement Learning. 3. Sistema financeiro. 4. Computadores. I. Coelho, Leandro dos Santos. II. Universidade Federal do Paraná. Programa de Pós-Graduação em Engenharia Elétrica. III. Título.</p><p style="padding-left: 65pt;text-indent: 0pt;text-align: left;"/><p class="s1" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">Bibliotecário: Nilson Carlos Vieira Júnior CRB-9/1797</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 163pt;text-indent: 0pt;text-align: left;">TERMO DE APROVAÇÃO</h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;line-height: 163%;text-align: justify;">Os membros da Banca Examinadora designada pelo Colegiado do Programa de Pós-Graduação ENGENHARIA ELÉTRICA da Universidade Federal do Paraná foram convocados para realizar a arguição da Dissertação de Mestrado de <b>LUCAS DE AZEVEDO TAKARA </b>intitulada: <b>DEEP REINFORCEMENT LEARNING APPROACH APPLIED TO AN AUTOMATED ASSET TRADING</b></p><p class="s4" style="padding-left: 7pt;text-indent: 0pt;line-height: 163%;text-align: left;">SYSTEM<span class="s3">, sob orientação do Prof. Dr. LEANDRO DOS SANTOS COELHO, que após terem inquirido o aluno e realizada a avaliação do trabalho, são de parecer pela sua APROVAÇÃO no rito de defesa.</span></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;line-height: 163%;text-align: left;">A outorga do título de mestre está sujeita à homologação pelo colegiado, ao atendimento de todas as indicações e correções solicitadas pela banca e ao pleno atendimento das demandas regimentais do Programa de Pós-Graduação.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Curitiba, 14 de Fevereiro de 2023.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 4pt;padding-left: 45pt;text-indent: 0pt;line-height: 130%;text-align: center;">Ass natura E etrôn ca 15/02/2023 15:41:28.0</p><p class="s3" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">LEANDRO DOS SANTOS COELHO</p><p class="s3" style="padding-top: 2pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">Pres dente da Banca Exam nadora</p><p class="s3" style="padding-top: 4pt;padding-left: 45pt;text-indent: 0pt;line-height: 130%;text-align: center;">Ass natura E etrôn ca 15/02/2023 08:55:05.0</p><p class="s3" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">LUIS HENRIQUE ASSUMPÇÃO LOLIS</p><p class="s3" style="padding-top: 2pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">Ava  ador Interno (UNIVERSIDADE FEDERAL DO PARANÁ)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 4pt;padding-left: 45pt;text-indent: 0pt;line-height: 130%;text-align: center;">Ass natura E etrôn ca 16/02/2023 12:37:06.0</p><p class="s3" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">ANDRÉ ALVES PORTELA SANTOS</p><p class="s3" style="padding-top: 2pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">Ava  ador Externo (CUNEF UNIVERSIDAD)</p><p class="s3" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;line-height: 130%;text-align: center;">Ass natura E etrôn ca 15/02/2023 16:06:29.0 GIDEON VILLAR LEANDRO</p><p class="s3" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">Ava  ador Interno (UNIVERSIDADE FEDERAL DO PARANÁ)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;">A negociação quantitativa consiste em estratégias baseadas em exploração estatística para identificar padrões, criando oportunidades de negociação. O aprendizado por reforço profundo (DRL) alcançou progresso significativo em várias áreas, como jogos, controle e manipulação, permitindo que os computadores executem tarefas complexas de tomada de decisão. Aplicado a finanças, os agentes de negociação DRL podem otimizar suas decisões em diferentes cenários de mercado, gerando uma estratégia rentável por meio de suas experiências anteriores. No entanto, muitas abordagens fornecem recompensas de maneira constante ao agente, devido ao desafio de conduzi-lo a uma política lucrativa fornecendo informações ocasionalmente. Embora prover recompensas a cada iteração do algoritmo propicie a conduzi-lo a uma política rapidamente, tal método faz com que o mesmo tenha dificuldades em convergir a uma política generalizada para diversos de cenários de mercado. Esta dissertação de mestrado propõe um sistema de negociação baseado em DRL que possui como componente principal, uma variante do algoritmo Redes Q Profundas (DQN) chamada de DQN de Negociação Extendido (ETDQN) que é capaz adaptar seu aprendizado para negociar em diversos momentos de comportamento do mercado, recebendo recompensas apenas ao término da negociação. Baseado em aprendizado distributivo e outras extensões independentes propostas pela comunidade DRL, o algoritmo otimiza sua tomada de decisão por meio de experiências amostradas por prioridade, contendo cada uma sub-objetivos dinstintos, auxiliando o agente a alcançar seu objetivo principal de reter o valor máximo de lucros, e também removendo a necessidade de ajustes finos de recompensa. ETDQN aprendeu a negociar em três diferentes sinais de séries temporais financeiras, identificando com sucesso oportunidades de negociação em diferentes cenários de mercado. O algoritmo apresentou um comportamento mais agressivo em relação à volatilidade de seus retornos anuais do que o <i>benchmark </i>DQN de Negociação e teve 1,46 e 7,13 vezes melhor desempenho em relação aos retornos cumulativos diários médios aplicado a dados de mercado históricos da Western Digital Corporation e criptomoeda Cosmos. Além disso, o algoritmo proposto foi 2,14 vezes mais lucrativo do que o segundo <i>benchmark </i>mais bem avaliado aplicado aos dados do fundo negociado em bolsa iShares S&amp;P500, &quot;Compre-e-Segure&quot;.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Palavras-chave: Negociação Quantitativa, Aprendizado por Reforço Profundo, Redes Q- profundas, Ambiente de Negociação com Recompensa Esparsa, Processamento de Sinal Financeiro</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;">Quantitative trading consists of strategies that rely on statistical exploration to identify patterns that turn into trading opportunities. Deep reinforcement learning (DRL) achieved significant progress in several areas, such as gaming, control, and manipulation, enabling computers to perform complex decision-making tasks. Applied to finance, DRL trading agents can optimize their decisions during distinct market scenarios to reach a profitable strategy by learning from previous experiences. However, many approaches provide constant feedback to the agent, due to the complicated reward tuning that is required to guide the algorithm to a lucrative policy by only giving information occasionally. This master’s thesis proposes a DRL-based trading system that has as its main component, a variant of the Deep Q-Network algorithm called Extended Trading DQN (ETDQN) that can be able to adapt its learning to trade across numerous market-behavior moments, receiving feedback only when a trade is over. Based on distributional learning and other independent extensions submitted by the DRL community, the algorithm optimizes its decision-making process by replaying prioritized experiences containing different sub-goals each, assisting the agent to achieve its main objective of retaining the maximum value of profits, as well as removing the need for fine-tuning rewards. ETDQN learned to trade on three different financial time series signals, successfully identifying trading opportunities in different market scenarios. The algorithm showed more aggressive behavior regarding the volatility of its annual returns than the Trading DQN benchmark and had 1.46 and 7.13 times better performance regarding mean daily cumulative returns in Western Digital Corporation and Cosmos cryptocurrency historical market data. In addition, the proposed algorithm was</p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">2.14 times more lucrative than the second best-evaluated benchmark with iShares S&amp;P500</p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">exchange-traded fund data frame, <i>Buy-and-Hold</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Keywords: Quantitative Trading, Deep Reinforcement Learning, Deep Q-Networks, Sparsed- reward Trading Environment, Financial Signal Processing</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark9" class="a">1.1 </a>Research methodology sequence 17</p><ol id="l1"><ol id="l2"><li><p style="padding-top: 14pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Q-table representing states from a trading environment 22</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Q-table update mechanism in a trading environment 23</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Q-table and Deep Q-Network architectures 24</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Deep Q-Network sampling and update mechanism in a trading environment 25</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark35" class="a">Double deep Q-Network sampling and update mechanism in a trading environment</a> 27</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark37" class="a">Example of support vector </a><a href="#bookmark37" class="s6">𝑧𝑖 </a>and its probability distribution 29</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Operations using Bellman operator on Distribution of returns 30</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Neural Network architecture in Categorical DQN 31</p><p style="text-indent: 0pt;line-height: 15pt;text-align: left;"><a href="#bookmark40" class="s7">T</a></p><p style="text-indent: 0pt;text-align: left;"/></li><li><p style="padding-top: 1pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark40" class="a">Computation of the projection of  </a><a href="#bookmark40" class="s8">ˆ</a><a href="#bookmark40" class="s9"> </a><a href="#bookmark40" class="s6">𝑧 𝑗 </a><a href="#bookmark40" class="a">onto the support </a><a href="#bookmark40" class="s6">𝑧𝑖</a><a href="#bookmark40" class="a">, and variables </a><a href="#bookmark40" class="s6">𝑏 𝑗 </a><a href="#bookmark40" class="a">, </a><a href="#bookmark40" class="s6">𝑙</a><a href="#bookmark40" class="a">, </a><a href="#bookmark40">and</a></p><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;"><a href="#bookmark40" class="s6">𝑢 </a>derived from it 32</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Neural network architectures 34</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark44" class="a">Linear noisy layer </a>schematic 36</p></li></ol></ol><ol id="l3"><ol id="l4"><li><p style="padding-top: 14pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark55" class="a">Systematic literature review process </a>overview 41</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark56" class="a">Publications per year </a>of quantitative trading related works 42</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark57" class="a">Year-based publication percentage in the periodical, conference, and pre-print</a> papers 43</p></li><li><p style="padding-top: 3pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark57" class="a">Amount of publications by </a>research databases 43</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Paper publication according to publication types 44</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Co-authorship network of RL-related publications applied to quantitative trading 47</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Co-occurrence network of RL-related publications applied to quantitative trading 48</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark63" class="a">Deep reinforcement learning algorithms applied to reviewed </a>works 49</p></li></ol></ol><ol id="l5"><ol id="l6"><li><p style="padding-top: 14pt;padding-left: 59pt;text-indent: -42pt;text-align: justify;"><a href="#bookmark71" class="a">(a) iShares Core S&amp;P 500 exchange-traded fund price in United States Dollar with outliers from 2010-2022. (b) iShares Core S&amp;P 500 exchange-traded fund outlier-free fom 2010-2022 (c) iShares Core S&amp;P 500 exchange-traded fund </a><a href="#bookmark71">price</a></p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">boxplot. 52</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: justify;"><a href="#bookmark72" class="a">(a) Western Digital price in United States Dollar with outliers from </a><a href="#bookmark72">2010-2022.</a></p><p style="padding-left: 59pt;text-indent: 0pt;text-align: justify;">(b) Western Digital outlier-free fom 2010-2022 (c) Western Digital price boxplot. 53</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark73" class="a">(a) ATOM cryptocurrency price in USDT from 2019-06 to 2021-06. (b) ATOM </a>cryptocurrency price boxplot. 54</p></li><li><p style="padding-top: 3pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark74" class="a">Bar sampling method comparison applied to the iShares S&amp;P500 ETF data frame from 2009-09-28 09:41 to 2009-09-28 10:32: (a) time bars sampled in a </a><a href="#bookmark74">1-minute</a></p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">frequency, (b) Dollar bars sampled every 500K dollars market-value exchange. 55</p></li><li><p style="padding-top: 8pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark74" class="a">Amount of weekly sampled bars generated from iShares Core S&amp;P 500 exchange- traded fund transactions from 2009 to 2022: (a) Weekly sampled dollar bars count, (b) Weekly </a>sampled time bars count. 55</p></li><li><p style="padding-top: 3pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark75" class="a">Amount of weekly sampled bars generated from Western Digital Corporation</a> transactions from 2010 to 2022. 56</p></li><li><p style="padding-top: 3pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark76" class="a">Amount of weekly sampled bars generated from Binance spot ATOM/USDT</a> cryptocurrency pair transactions from 2010 to 2022. 57</p></li><li><p style="padding-top: 3pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark77" class="a">iShares Core S&amp;P 500 exchange-traded fund time series properties from 2009 to</a> 2022. 58</p></li><li><p style="padding-top: 3pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark78" class="a">Western </a>Digital Corporation time series properties from 2010 to 2022. 59</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Binance spot ATOM/USDT cryptocurrency pair from 2019 to 2021 60</p></li></ol></ol><ol id="l7"><ol id="l8"><li><p style="padding-top: 14pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark90" class="a">Trading problem formulated as a Markov </a>decision process 62</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark92" class="a">Adopted Deep Feed-Forward Q-Neural Network architecture in the trading</a> problem 64</p></li><li><p style="padding-top: 3pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark94" class="a">Sparse reward design process for </a>asset trading 66</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Sequential data sampling approach 66</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark96" class="a">Deep reinforcement learning-based automated strategy </a>overview 69</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Deep reinforcement learning-based automated strategy learning process 70</p></li></ol></ol><ol id="l9"><ol id="l10"><li><p style="padding-top: 14pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Daily iShares Core S&amp;P 500 ETF cumulative returns from 2010 to 2022 73</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark106" class="a">Daily Western </a>Digital Corporation cumulative returns from 2010 to 2022 75</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Daily ATOM/USDT cryptocurrency cumulative returns from 2019 to 2022 76</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">iShares S&amp;P500 ETF 6-month rolling Sharpe ratio 79</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark110" class="a">Western </a>Digital Corporation 6-month rolling Sharpe ratio 81</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Binance spot ATOM/USDT cryptocurrency 6-month rolling Sharpe ratio 82</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark114" class="a">iShares S&amp;P500 ETF drawdown </a>from 2009 to 2022 85</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark114" class="a">Western Digital Corporation drawdown </a>from 2010 to 2022 85</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark115" class="a">Binance spot ATOM/USDT cryptocurrency pair drawdown </a>from 2019 to 2022 . 87</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">iShares S&amp;P500 ETF monthly returns from 2009 to 2022 89</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark118" class="a">Western </a>Digital Corporation monthly returns from 2010 to 2022 90</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Binance spot ATOM/USDT monthly returns from 2019 to 2021 92</p></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark27" class="a">2.1 </a>Trading style average time duration 19</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l11"><ol id="l12"><li><p style="padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark59" class="a">Main journals which contain machine learning publications related to algorithmic</a> trading 45</p></li><li><p style="padding-top: 3pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark59" class="a">Main conferences of the reviewed </a>works 45</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark60" class="a">Main pre-print platforms of the reviewed </a>works 46</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark61" class="a">Main performance metrics applied to the reviewed </a>works 47</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark62" class="a">Main markets applied to reviewed </a>works 48</p></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark98" class="a">5.1 </a>Hyperparameters 71</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l13"><ol id="l14"><li><p style="padding-left: 59pt;text-indent: -42pt;text-align: left;">Daily cumulative returns 78</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">6-month rolling Sharpe ratio 83</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Annualized total returns generated per strategies from 2009 to 2022 93</p></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">A2C Advantage Actor Critic</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">A3C Asynchronous Advantage Actor Critic</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">ACM Association for Computing Machinery</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">ADX Average Directional Index</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">AT Algorithmic Trading</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">ATR Average True Range</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">B&amp;H Buy and Hold</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">BIAS Deviation Rate</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">CAM Class Activation Maps</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">COVID-19 Coronavirus disease</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">CNN Convolutional Neural Network</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">DAX Deutscher Aktien Index</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">DDPG Deep Deterministic Policy Gradient</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">DDQN Double Q-learning</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">DELT Departamento de Engenharia Elétrica</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">+DI Negative Directional Indicator</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">-DI Positive Directional Indicator</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">DL Deep Learning</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">DQN Deep Q-Network</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">DRL Deep Reinforcement Learning</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">EMA Exponential Moving Average</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">FOREX Foreign Exchange Market</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">Ibovespa Índice Bovespa</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">KOSPI Korea Composite Stock Price Index</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">LSTM Long Short-Term Memory</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">MA Moving Average</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">MACD  Moving average convergence divergence</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">MDPI Multidisciplinary Digital Publishing Institute</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">MR Mean Reversion with Moving Averages</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">NASDAQ National Association of Securities Dealers Automated Quotations OBV On-balance Volume</p><p style="padding-left: 33pt;text-indent: 0pt;line-height: 14pt;text-align: left;">OHLC Open, High, Low, Close</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;line-height: 131%;text-align: left;">PPGINF Programa de Pós-Graduação em Engenharia Elétrica PPO Proximal Policy Optimization</p><p style="padding-left: 33pt;text-indent: 0pt;line-height: 14pt;text-align: left;">QT Quantitative Trading</p><p style="padding-top: 9pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">RL Reinforcement Learning</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">ROC Rate of Change</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">RRL Recurrent Reinforcement Learning</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">S&amp;H Sell and Hold</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">S&amp;P500 Standard and Poor’s 500</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">SHAP Shapley Additive Explanations</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">SSO Slow Stochastic Oscillator</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">SSRN Social Science Research Network</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">TCN Temporal Convolutional Networks</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">TR Trend-following with moving averages</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">TWAP Time-Weighted Average Price</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">UFPR Universidade Federal do Paraná</p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">VR Volatility Ratio</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 7pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">LIST OF SYMBOLS</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 33pt;text-indent: 0pt;text-align: left;">t <span class="p">Time-step</span></p><p class="s5" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">t<span class="s10">0 </span><span class="p">Initial time-step</span></p><p class="s5" style="padding-top: 3pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">𝑠t <span class="p">State at time </span>t</p><p class="s5" style="padding-top: 2pt;padding-left: 34pt;text-indent: 0pt;line-height: 18pt;text-align: left;">𝑠t<span class="s11">+</span><span class="s10">1 </span><span class="p">State at time </span>t<span class="p">+1</span></p><p class="s5" style="padding-left: 34pt;text-indent: 0pt;line-height: 18pt;text-align: left;">𝑠<span class="s11">′ </span><span class="p">State at time </span>t+1</p><p class="s5" style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑎 <span class="p">Action at time </span>t</p><p class="s5" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑎𝑡 <span class="p">Action at time </span>t</p><p class="s5" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑎<span class="s11">′ </span><span class="p">Action at time </span>t<span class="p">+1</span></p><p class="s5" style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑟t <span class="p">Reward at time </span>t</p><p class="s5" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑟t<span class="s11">+</span><span class="s10">1 </span><span class="p">Reward at time </span>t<span class="p">+1</span></p><p class="s5" style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑟i <span class="p">Reward at iteration </span>i</p><p class="s5" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑟 <span class="p">Reward at iteration </span>i</p><p class="s5" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;line-height: 18pt;text-align: left;">𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s11">)                  </span><span class="p">State-action pair</span></p><p class="s5" style="padding-left: 33pt;text-indent: 0pt;line-height: 18pt;text-align: left;">𝑄𝜋 <span class="s11">(</span>𝑠, 𝑎<span class="s11">)                </span><span class="p">State-action pair under policy </span>𝜋</p><p class="s11" style="padding-left: 33pt;text-indent: 0pt;line-height: 18pt;text-align: left;"><span class="s5">𝑄i</span>+<span class="s10">1 </span>(<span class="s5">𝑠, 𝑎</span>)               <span class="p">State-action pair at iteration </span><span class="s5">i</span></p><p class="s5" style="padding-left: 33pt;text-indent: 0pt;line-height: 18pt;text-align: left;">𝑄<span class="s11">∗ (</span>𝑠, 𝑎<span class="s11">)                 </span><span class="p">Optimal state-action pair</span></p><p class="s5" style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝛼 <span class="p">Learning Rate</span></p><p class="s5" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝛾 <span class="p">Discount factor</span></p><p class="s5" style="padding-top: 4pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">𝜋 <span class="p">Policy adopted by the agent</span></p><p class="s5" style="padding-top: 2pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">𝜋<span class="s11">∗ </span><span class="p">Optimal Policy</span></p><p class="s5" style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝜃i <span class="p">Main Q-Network Weights</span></p><p class="s12" style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;line-height: 83%;text-align: right;">𝜃<span class="s13">i</span><span class="s14">−</span></p><p style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">Target Q-Network Weights</p><p style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">E Expectation</p><p class="s5" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑚𝑎𝑥 <span class="p">Maximum Operator</span></p><p class="s5" style="padding-top: 4pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">𝑅𝑎 <span class="p">Return from asset </span>a</p><p class="s5" style="padding-top: 4pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">𝑅𝑏 <span class="p">Risky-free rate from investment</span></p><p class="s5" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝜎𝑎 <span class="p">Standard deviation of asset</span></p><p class="s5" style="padding-top: 4pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">𝑃t <span class="p">Asset Price at time-step </span>t</p><p class="s5" style="padding-top: 4pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">𝑃t<span class="s15">0 </span><span class="p">Asset Price at initial time-step</span></p><p class="s5" style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑆𝑎 <span class="p">Sharpe Ratio</span></p><p class="s5" style="padding-top: 4pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">𝑀𝐷𝐷 <span class="p">Maximum Drawdown</span></p><p class="s11" style="padding-top: 2pt;padding-left: 33pt;text-indent: 0pt;line-height: 18pt;text-align: left;">∞ <span class="p">Infinity</span></p><p class="s5" style="padding-left: 34pt;text-indent: 0pt;line-height: 18pt;text-align: left;">𝐿i <span class="s11">(</span>𝜃i<span class="s11">) </span><span class="p">Loss function in respect to weights </span>𝜃i <span class="p">at iteration </span>𝑖</p><p class="s5" style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑌 𝐷𝑄𝑁 <span class="p">Deep Q-Network state-action value function target</span></p><p class="s5" style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">𝑌 𝐷𝑜𝑢𝑏𝑙𝑒𝐷𝑄𝑁               <span class="p">Double Deep Q-Network state-action value function target</span></p><p class="s11" style="padding-top: 5pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">T<span class="s16">ˆ</span><span class="s10">                        </span><span class="p">Bellman optimality operator</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 7pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">CONTENTS</h1><ol id="l15"><li><h1 style="padding-top: 30pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">INTRODUCTION 14</h1><ol id="l16"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">OBJECTIVES 15</p><ol id="l17"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">General Objective 15</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Specific Objectives 15</p></li></ol></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">CONTRIBUTIONS 16</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">METHODOLOGY 16</p></li></ol></li><li><h1 style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark27" class="s17">THEORETICAL </a>FOUNDATION 19</h1><ol id="l18"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">TRADING STRATEGIES 19</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark27" class="a">EVALUATION </a>METRICS 19</p><ol id="l19"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Sharpe Ratio 19</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Simple Returns 20</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Logarithmic Returns 20</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Simple Cumulative Returns 20</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Cumulative Logarithmic Returns 21</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark29" class="a">Maximum </a>Drawdown 21</p></li></ol></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Q-LEARNING 21</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">DEEP Q-NETWORKS 24</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">DOUBLE DEEP Q-NETWORKS 26</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark36" class="a">CATEGORICAL </a>DQN 28</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">DUELING NETWORK ARCHITECTURE 33</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark43" class="a">NOISY LINEAR </a>LAYER 35</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark44" class="a">PRIORITIZED EXPERIENCE </a>REPLAY 36</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark45" class="a">HINDSIGHT EXPERIENCE </a>REPLAY 37</p></li></ol></li><li><h1 style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark53" class="s17">RELATED </a>WORKS 39</h1><ol id="l20"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark53" class="a">SYSTEMATIC </a>REVIEW OF THE LITERATURE 39</p><ol id="l21"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Periodical Publications 40</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Conference Publications 42</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Pre-print Publications 44</p></li></ol></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">CO-AUTHORSHIP AND CO-OCCURRENCE NETWORKS 46</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark60" class="a">DATA </a>SYNTHESIS 46</p></li></ol></li><li><h1 style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark69" class="s17">DATA COLLECTION, CLEANING, PRE-PROCESSING AND EX- PLORATORY </a>ANALYSIS 50</h1><ol id="l22"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark69" class="a">DATA </a>COLLECTION 50</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark69" class="a">DATA </a>CLEANING 50</p></li><li><p style="padding-top: 8pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark72" class="a">DATA </a>PRE-PROCESSING 53</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark76" class="a">EXPLORATORY DATA </a>ANALYSIS 57</p></li></ol></li><li><h1 style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">EXPERIMENTAL SETUP AND METHODOLOGY 62</h1><ol id="l23"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark90" class="a">TRADING FORMULATED AS A MARKOV </a>DECISION PROCESS 62</p><ol id="l24"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Trading environment 63</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Agent 64</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Reward 65</p></li></ol></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark93" class="a">BACK-TESTING </a>MODEL PROCESS 65</p><ol id="l25"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">Sequential sampling 65</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark95" class="a">Back-testing </a>process 67</p></li></ol></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">HYPERPARAMETERS 69</p></li></ol></li><li><h1 style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark105" class="s17">PERFORMANCE </a>ANALYSIS 73</h1><ol id="l26"><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">CUMULATIVE RETURNS 73</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark108" class="a">SHARPE </a>RATIO 78</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">DRAWDOWN 84</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark116" class="a">MONTHLY </a>RETURNS 88</p></li><li><p style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;"><a href="#bookmark120" class="a">ANNUAL </a>RETURNS 93</p></li></ol></li><li><h1 style="padding-top: 4pt;padding-left: 59pt;text-indent: -42pt;text-align: left;">CONCLUSION AND FUTURE WORKS 95</h1></li></ol><h1 style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;text-align: left;">REFERENCES 97</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l27"><li><h1 style="padding-left: 31pt;text-indent: -14pt;text-align: left;"><a name="bookmark0">INTRODUCTION</a><a name="bookmark6">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">According to Kissell </a>(2021), quantitative trading is an automatic trading execution of a financial asset that utilizes mathematical functions and models to make trading decisions. Quantitative hedge funds use these algorithms to maximize profits due to their ability to learn from historical data.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Seeking to help traders better understand stock market conditions, earlier technical analysis-based algorithms were built by replicating human trading behavior using traditional technical indicators. However, these systems have failed due to their instability, poor generalization ability, and susceptibility to environmental change.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark125" class="a">Reinforcement Learning (RL) is an emerging sub-field of machine learning. It enables an agent to learn in an interactive environment through trial and error, using feedback from its actions and experiences. The agent aims to improve its policy to acquire better rewards. Deep reinforcement learning (DRL) is a combination of reinforcement learning, and deep learning techniques that achieved significant progress in several areas, such as gaming Mnih et al. </a><a href="#bookmark124" class="a">(2015), and manipulation Levine et al. </a>(2016), enabling computers to perform complex decision-making tasks. DRL also has achieved significant performance in trading tasks seeking to maximize profits by capturing possible trading opportunities.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">Even though there exist publications about DRL-based trading systems in the literature, many trading systems, such as Li et al. </a><a href="#bookmark125" class="a">(2019) and Théate and Ernst </a>(2021), provide feedback to the agent constantly due to the complicated reward tuning that is required to guide the algorithm to a lucrative policy by only giving information occasionally.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Although it may be easier to conduct the agent to a policy quickly, this method misguides the agent to be able to converge to a generalized policy involving multiple market scenarios. Providing feedback to an agent at every iteration may lead it to not exploring the environment efficiently, restraining it from generalizing its decision-making process.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">In addition, financial markets do not produce meaningful information constantly. Therefore the algorithm should be trained so that it receives a contribution only during the end of an event, in this case, at the end of a trade.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark125" class="a">This master dissertation proposes to create a Deep Q-Network (DQN) algorithm variant based on Trading DQN (TDQN), presented by Théate and Ernst </a>(2021), named Extended Trading DQN (ETDQN). This variant can generalize its learning to trade across numerous market-behavior moments, receiving a simple, sparse exponential profit-and-loss reward once the trade ends.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">ETDQN incorporates Trading DQN algorithm extensions, such as Double DQN, to tackle the overestimation problem and extends it by including some DQN improvements combined in Hessel et al. </a><a href="#bookmark124">(2018).</a></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">The DQN-variant adopts distributional learning instead of the standard expectation. Proposed by Bellemare et al. </a><a href="#bookmark125" class="a">(2017), this approach is beneficial since other statistics from the approximated distribution can be learned and optimized. In addition, the algorithm took advantage of both prioritized Schaul et al. </a><a href="#bookmark124" class="a">(2016) and hindsight Andrychowicz et al. </a>(2017) experience replays to sample meaningful experiences, according to prioritization, with different sub-goals each, assisting the model in its primary goal of optimizing its decisions to collect the maximum amount of profits and also removes the need for complex reward tuning.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark125" class="a">To improve learning even further, instead of the standard feed-forward architecture adopted in the standard DQN, ETDQN contains the dueling neural network architecture </a><a href="#bookmark125">Wang</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;text-align: right;"><a href="#bookmark125" class="a" name="bookmark1">et al. (2016)</a><a href="#bookmark124" class="a"> in combination with noisy linear layers Fortunato et al. (2018).</a> This combination assists the agent in performing better exploration and identifying which states are more valuable.<a name="bookmark2">&zwnj;</a><a name="bookmark3">&zwnj;</a><a name="bookmark7">&zwnj;</a></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: right;"><a href="#bookmark124" class="a">Lastly, instead of using the common sampling approach based on regular intervals, this work adopted a different approach proposed by de Prado (2018)</a> that provides the agent intraday data according to market activity. This method provides information to the agent at the same pace that the market processes information. Contrary to standard time sampling, this approach does not oversample information during low market activity or undersample during high activity. Aiming to compare performance, ETDQN is compared against the classical benchmarks</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Buy-and-Hold <span class="p">and </span>Sell-and-Hold<span class="p">. To check if the model performs better than random, a strategy named random action which chooses its actions randomly based on a discrete uniform distribution, is compared with it. Finally, ETDQN is also compared to the DQN variant TDQN. TDQN was implemented according to the same instructions described in the original paper. The performance comparison considers cumulative returns, the 6-month rolling Sharpe ratio, maximum drawdown, and average annual return (AAR).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l28"><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">OBJECTIVES</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">This section aims to describe the general and specific objectives of this master’s dissertation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l29"><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">General Objective</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">This master dissertation proposes an event-driven DRL-based trading system that processes information according to market activity. The algorithm can generate a policy that generalizes across multiple market scenarios, identify trading opportunities, and take advantage of them. Moreover, it should be profitable concerning mean daily cumulative and annual averaged returns, provide a competitive mean Sharpe ratio, and have less risk of ruin than the remaining benchmarks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Specific Objectives</p><p style="padding-top: 8pt;padding-left: 59pt;text-indent: 0pt;text-align: left;">The specific objectives of this dissertation are:</p><ol id="l30"><li><p style="padding-top: 13pt;padding-left: 59pt;text-indent: -14pt;text-align: justify;">Generate a market-valuation driven intraday trading environment with technical indica- tors, time signature, and candlestick bars features. The environment should generate information at the same pace the market reaches a pre-defined market value and provide feedback to the agent only once it finishes a trade.</p></li><li><p style="padding-top: 9pt;padding-left: 59pt;text-indent: -14pt;text-align: justify;">Implement a deep Q-Network (DQN) variant algorithm that extends the Trading DQN benchmark, incorporating Categorical DQN, Noisy-Dueling neural networks, prioritized, and hindsight experience replay extensions that can identify and take advantage of trading opportunities by receiving non-frequent feedback from the environment.</p></li><li><p style="padding-top: 9pt;padding-left: 59pt;text-indent: -14pt;text-align: justify;">Prove that the proposed algorithm performs better than the best-evaluated benchmark in iShares S&amp;P500 ETF, Western Digital Corporation, and ATOM/USDT data frames, considering the metrics: Mean daily cumulative returns, 6-month rolling Sharpe ratio, maximum Drawdown, and Average Annual Return.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li></ol></li><li><p style="padding-top: 6pt;padding-left: 40pt;text-indent: -23pt;text-align: left;"><a name="bookmark4">CONTRIBUTIONS</a><a name="bookmark5">&zwnj;</a><a name="bookmark8">&zwnj;</a></p><p style="padding-top: 12pt;padding-left: 59pt;text-indent: 0pt;text-align: left;">The intended contributions of this dissertation are:</p><ol id="l31"><li><p style="padding-top: 11pt;padding-left: 59pt;text-indent: -19pt;text-align: justify;">Provide an automated intraday trading system that is deployable on fundamental trading markets and is profitable regarding cumulative and annual returns in the long term.</p></li><li><p style="padding-top: 9pt;padding-left: 59pt;text-indent: -19pt;text-align: justify;">Analyze and explore a pre-processing sampling step based on market value that generates information according to market activity, and compare it against the usual time sampling approach.</p></li><li><p style="padding-top: 9pt;padding-left: 59pt;text-indent: -19pt;text-align: justify;">Propose a DQN-variant that can identify and take advantage of trading opportunities by receiving non-frequent feedback from the environment and does not need complicated reward tuning.</p></li><li><p style="padding-top: 9pt;padding-left: 59pt;text-indent: -19pt;text-align: justify;">Obtain evidence that the proposed algorithm leads to a better overall performance in cumulative returns and drawdown against the <i>Buy-and-Hold</i>, <i>Sell-and-Hold</i>, random action, and TDQN benchmark strategies.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">METHODOLOGY</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">According to Lakatos and Marconi </a>(2003), a research methodology procedure is a set of systematic and rational guidelines that aims to firm a foundation for advancing knowledge and facilitating theory development. Research work can be classified as a primary or applied research type. Primary research seeks to expand the existing scientific knowledge base, whether applied research is designed to solve specific practical problems.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark9" class="a">This master dissertation adopts the applied research method to solve a problem using a quantitative approach. Figure </a>1.1 shows the research methodology sequence for the respective work, which defines the milestones aiming to achieve its objective. The following hypothesis was raised: It may be possible to guide a deep reinforcement learning-based trader agent to find trading opportunities more effectively in terms of learning by simulating a sparse-reward trading environment.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">A systematic literature review of deep reinforcement learning approaches applied to trading tasks was conducted. Related works about this topic were collected, analyzed, and discussed to find the current gaps and advances in the literature.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The data collection, cleaning, and pre-processing procedures were performed, followed by an exploratory analysis. The data frames came from two distinct sources: Kibot and Binance spot’s application programming interface. The adopted Interquartile range method removed outliers with a 7-day sliding window approach and was followed by the creation of bars sampled according to market value. Subsequently, the data was used to generate technical indicators and time signature features, embedding this information into the state.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The next step includes the DQN-variant algorithm and the dynamics of the trading environment implementation. It consists of the sparse reward dynamics implementation and the environment behavior regarding the chosen action. The developed algorithm was trained, having each trade position at each time step saved into a log file.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Finally, its performance was evaluated along with four distinct benchmarks: <i>Buy- and-Hold</i>, <i>Sell-and-Hold</i>, random action strategy, and TDQN benchmarks, according to daily cumulative returns, monthly and annual returns, drawdown, and rolling Sharpe ratio metrics.</p><p class="s5" style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Buy-and-hold <span class="p">is a benchmark that the investor buys the asset and holds it for an extended period. The strategy follows the asset’s price evolution throughout time. On the contrary,</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 148pt;text-indent: 0pt;text-align: left;"><span><img width="251" height="770" alt="image" src="documento[1970]/Image_002.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 155pt;text-indent: 0pt;text-align: left;"><a name="bookmark9">Figure 1.1: Research methodology sequence</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Sell-and-Hold <span class="p">is a strategy in which the trader should open a short position (bet against) the asset for a long time. In the random action strategy, the trader would choose decisions randomly according to a uniform probability distribution, and TDQN is a DRL-based algorithm proposed</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark125" class="a">by Théate and Ernst </a>(2021) which implements Double DQN and apply it to a trading environment with time bars and provides dense rewards to the agent. Finally, the conclusion summarizes the results achieved in this dissertation and suggests future research direction.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h1 style="padding-top: 7pt;padding-left: 31pt;text-indent: -14pt;text-align: left;"><a name="bookmark10">THEORETICAL FOUNDATION</a><a name="bookmark11">&zwnj;</a><a name="bookmark12">&zwnj;</a><a name="bookmark13">&zwnj;</a><a name="bookmark27">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">This section reviews some reinforcement learning concepts and Q-learning-based algorithms, as well as trading styles and metrics such as returns, maximum drawdown, and Sharpe ratio, which are necessary to build and evaluate the proposed deep reinforcement learning-based automated trading strategy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l32"><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">TRADING STRATEGIES</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark27" class="a">Widely applied in stock, cryptocurrency, commodity futures, and foreign exchange markets, trading is a process of buying and selling one given financial asset to make a profit. Scalp trading, day trading, swing trading, and position trading are different trading styles that can be explored in strategies. Scalping, or scalp trading, is the most short-term form of trading. This style holds open positions for seconds or minutes at most, aiming to make many quick trades with smaller profit gains. Day trading is a style that consists of opening and exiting positions on the same day, removing the risk of any significant overnight moves. Trades are usually held for hours. Swing trading is a style that typically holds positions for several days, although it might extend for some weeks. Lastly, position trading is long-term focused—this category spans months or even years. Table </a>2.1 shows the average time duration of each trading style.</p><p class="s1" style="padding-top: 11pt;padding-left: 151pt;text-indent: 0pt;text-align: left;">Table 2.1: Trading style average time duration</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="415" height="1" alt="image" src="documento[1970]/Image_003.png"/></span></p><h1 style="padding-bottom: 1pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">Trading Style Trading Time Period</h1><p style="padding-left: 88pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="415" height="1" alt="image" src="documento[1970]/Image_004.png"/></span></p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;">Scalpping From seconds to minutes</p><p style="padding-left: 94pt;text-indent: 0pt;text-align: left;">Day Trading From minutes to hours (one day maximum) Swing Trade From days to weeks</p><p class="s18" style="padding-left: 88pt;text-indent: 0pt;line-height: 14pt;text-align: left;">  Position Trading From months to years                                     </p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">In trading, mainly in future markets, two possible positions are long or short. The first approach aims to open a position of a perpetual future contract at a specific price and profit from it as the price increases. On the other hand, the second approach is to open a position of a perpetual future contract, betting against the asset and making profits from it as the price decreases. Considering the existence of different trading styles, different agents may learn different trading styles.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">EVALUATION METRICS</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">This section describes the evaluation metrics on which the strategy is evaluated. Some essential metrics for trading stocks include the Sharpe ratio, cumulative returns, and Maximum drawdown. The following subsections describe the metrics mentioned above.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l33"><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Sharpe Ratio</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark28" class="a">The Sharpe ratio measures the risk-adjusted return of a financial portfolio. In 1966, Sharpe proposed this risk/reward ratio to capture in a single number. Equation </a>2.1 defines the Sharpe ratio:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 15pt;text-indent: 0pt;text-align: right;"><a name="bookmark14">𝑆𝑎</a><a name="bookmark15">&zwnj;</a><a name="bookmark16">&zwnj;</a><a name="bookmark28">&zwnj;</a></p><p class="s22" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;line-height: 21pt;text-align: left;"><span class="s19">= </span><span class="s20">E</span><span class="s21">[</span>𝑅<span class="s23">𝑎 </span><span class="s24">− </span>𝑅<span class="s23">𝑏</span><span class="s25"> </span><span class="s24">]</span><span class="s11"> </span><span class="s5">. </span><span class="p">(2.1)</span></p><p class="s5" style="padding-left: 35pt;text-indent: 0pt;line-height: 11pt;text-align: left;">𝜎𝑎</p><p style="padding-top: 3pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">The quality of an investment is defined as the expected value E of the asset return <i>𝑅𝑎 </i>subtracted by the risk-free return <i>𝑅𝑏</i>, divided by the standard deviation of the asset <i>𝜎𝑎</i>. This metric is defined by the mean of returns generated in the portfolio divided by the standard deviation of the same feature. The lower the standard deviation, the less risk and the higher the Sharpe ratio; the opposite is true.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Simple Returns</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark28" class="a">Simple returns on an investment are the amount that the investment has gained or lost over time, independent of the amount of time involved. Equation </a>2.2 defines simple returns calculation:</p><p class="s23" style="padding-top: 6pt;padding-left: 196pt;text-indent: 0pt;line-height: 21pt;text-align: left;"><span class="s5">𝑅𝑒𝑡𝑢𝑟𝑛𝑠 </span><span class="s19">= </span><span class="s26">𝑃</span>𝑡<span class="s27">  </span><span class="s24">−</span><span class="s21"> </span><span class="s22">𝑃</span>𝑡<span class="s25"> </span><span class="s5">.                                                    </span><span class="p">(2.2)</span></p><p class="s5" style="padding-left: 45pt;text-indent: 0pt;line-height: 75%;text-align: center;">𝑃𝑡<span class="s15">0</span></p><p style="padding-top: 3pt;padding-left: 45pt;text-indent: 0pt;line-height: 13pt;text-align: center;">It is expressed as a percentage and is calculated by subtracting the asset price <i>𝑃 </i>in time</p><p class="s5" style="padding-left: 14pt;text-indent: 0pt;line-height: 18pt;text-align: center;">𝑡<span class="s10">0 </span><span class="p">(</span>𝑃𝑡<span class="s15">0</span><span class="s28"> </span><span class="p">) from the current price </span>𝑃 <span class="p">at time </span>𝑡 <span class="p">(</span>𝑃𝑡<span class="p">), divided by the asset price in time </span>𝑡<span class="s10">0 </span><span class="p">(</span>𝑃𝑡<span class="s15">0</span><span class="s28"> </span><span class="p">).</span></p></li><li><p style="padding-top: 14pt;padding-left: 49pt;text-indent: -32pt;text-align: left;">Logarithmic Returns</p><p style="padding-top: 7pt;padding-left: 17pt;text-indent: 42pt;line-height: 15pt;text-align: justify;">Logarithmic returns measure the rate of exponential growth. Unlike simple arithmetic returns, which measure the percent of price change for each sub-period, logarithmic returns measure the exponent of its natural growth during the same sub-period mentioned above. They are given by:</p><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝑡</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝑡</p><p style="text-indent: 0pt;text-align: left;"/><p class="s29" style="text-indent: 0pt;line-height: 15pt;text-align: left;">𝑃<span class="s25">𝑡</span><span class="s14">−</span><span class="s30">1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="padding-top: 1pt;padding-left: 175pt;text-indent: 0pt;text-align: left;"><span class="s5">𝑟  </span>= <span class="s10">ln</span><span class="s11">(</span><span class="s10">1 </span><span class="s11">+ </span><span class="s5">𝑅 </span><span class="s11">) </span>= <span class="s10">ln </span><span class="s31">(</span> <span class="s20"> </span><span class="s18"> </span><span class="s22">𝑃</span><span class="s23">𝑡</span><span class="s27">   </span><span class="s25"> </span><span class="s31">)</span> <span class="s5">,                                              </span><span class="p">(2.3)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">(·)</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 10pt;padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">where <span class="s10">ln    </span>is the natural logarithmic function, <i>𝑅𝑡 </i>is the simple period return, <i>𝑃𝑡 </i>is the asset price at time <i>𝑡 </i>and <i>𝑃𝑡  </i><span class="s10">1 </span>is the asset price at <i>𝑡    </i><span class="s10">1</span>. As denoted above in the equation, simple logarithmic returns can be calculated by adding 1 to the simple return and subsequently taking its natural logarithm of it. Another option is to divide the asset price by its predecessor and apply the natural logarithm. This type of return tends to be more normally distributed than simple arithmetic ones.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Simple Cumulative Returns</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">Cumulative Returns, also known as multi-period returns, show the evolution of the investment taking its historical path (many periods) into consideration, unlike simple arithmetic returns, which consider a single one. The <i>𝑘</i>-period cumulative return is defined as the cumulative product of the <i>𝑘</i>-sub period returns calculated by:</p><p class="s11" style="padding-top: 12pt;padding-left: 16pt;text-indent: 113pt;line-height: 113%;text-align: justify;"><span class="s10">1 </span>+ <span class="s5">𝑅𝑡 </span>(<span class="s5">𝑘</span>) <span class="s19">= </span>(<span class="s10">1 </span>+ <span class="s5">𝑅𝑡</span>)(<span class="s10">1 </span>+ <span class="s5">𝑅𝑡</span>−<span class="s10">1</span>) · · · (<span class="s10">1 </span>+ <span class="s5">𝑅𝑡</span>−<span class="s5">𝑘</span>+<span class="s10">1</span>)<span class="s5">,                              </span><span class="p">(2.4) where  </span><span class="s5">𝑘  </span><span class="p">is the number of period returns,  in order to illustrate a 30-day cumulative returns calculation, the following procedure would have to be made:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 130pt;text-indent: 0pt;text-align: left;"><span class="s10">1 </span>+ <span class="s5">𝑅𝑡 </span>(<span class="s10">30</span>) <span class="s19">= </span>(<span class="s10">1 </span>+ <span class="s5">𝑅𝑡</span>)(<span class="s10">1 </span>+ <span class="s5">𝑅𝑡</span>−<span class="s10">1</span>) · · · (<span class="s10">1 </span>+ <span class="s5">𝑅𝑡</span>−<span class="s10">29</span>)<span class="s5">,                              </span><span class="p">(2.5)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="text-indent: 0pt;line-height: 11pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 91%;text-align: justify;"><a name="bookmark17">𝑘 </a><span class="s19">= </span><span class="s10">30 </span><span class="p">and the 30-day cumulative return would be calculated by the cumulative product from the first daily return </span>𝑅𝑡 <span class="p">to the last </span>𝑅𝑡  <span class="s10">29 </span><span class="p">simple return. Mathematically, multi-period returns can be expressed as:</span><a name="bookmark18">&zwnj;</a><a name="bookmark19">&zwnj;</a><a name="bookmark29">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 15pt;padding-left: 15pt;text-indent: 0pt;text-align: right;"><span class="s10">1 </span>+ <span class="s5">𝑅𝑡 </span>(<span class="s5">𝑘</span>) <span class="s19">=</span></p><p class="s25" style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">𝑘<span class="s14">−</span><span class="s30">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;"> </p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">fl <span class="s32">−</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">𝑗 <span class="s33">=</span><span class="s30">0</span></p><p class="s11" style="padding-top: 15pt;text-indent: 0pt;text-align: left;">(<span class="s10">1 </span>+ <span class="s5">𝑅𝑡</span>− <span class="s5">𝑗 </span>)<span class="s5">.                                               </span><span class="p">(2.6)</span></p><p style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">The symbol is the productory from the period return <i>𝑗 </i><span class="s19">= </span><span class="s10">0 </span>to the number of <i>𝑘 </i><span class="s10">1 </span>with each period return summed by 1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Cumulative Logarithmic Returns</p><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">L</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: left;">Cumulative logarithmic returns are more simple to calculate than simple cumulative ones and are given by:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 15pt;padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑟𝑡 <span class="s11">(</span>𝑘<span class="s11">) </span><span class="s19">=</span></p><p class="s25" style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">𝑘<span class="s14">−</span><span class="s30">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">∑︁</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">𝑗 <span class="s33">=</span><span class="s30">0</span></p><p class="s5" style="padding-top: 15pt;text-indent: 0pt;text-align: left;">𝑟𝑡<span class="s11">− </span>𝑗 .                                                       <span class="p">(2.7)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;line-height: 91%;text-align: justify;">The cumulative logarithmic return <i>𝑟𝑡 𝑘 </i>from period <i>𝑘 </i>is the from the return from the initial period <i>𝑗 </i><span class="s19">= </span><span class="s10">0 </span>to <i>𝑘 </i><span class="s10">1</span>.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Unlike simple cumulative returns that require calculating the geometric sum or the cumulative product of the two continuously compounded returns, in order to calculate cumulative logarithmic returns, it is only necessary to sum the two continuously compounded returns.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Maximum Drawdown</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark29" class="a">Maximum Drawdown (MDD) is the biggest peak-to-valley decline of an investment during a specific holding period. Drawdowns can help to determine an investment’s financial loss risk by looking into the past. Equation </a>2.8 defines the Maximum Drawdown:</p><p class="s5" style="padding-top: 9pt;padding-left: 152pt;text-indent: 0pt;line-height: 74%;text-align: left;"><span class="s34">𝑀</span>𝐷𝐷 <span class="s19">= </span><u>𝑃𝑒𝑎𝑘 𝐻𝑖𝑔ℎ </u><span class="s21">−</span><span class="s11"> </span><u>𝑇𝑟𝑜𝑢𝑔ℎ 𝐿𝑜𝑤</u> <span class="s34">.</span>                                      <span class="p">(2.8)</span></p><p class="s5" style="padding-left: 45pt;text-indent: 0pt;line-height: 11pt;text-align: center;">𝑃𝑒𝑎𝑘 𝐻𝑖𝑔ℎ</p><p style="padding-top: 4pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">It is calculated by <i>𝑃𝑒𝑎𝑘 𝐻𝑖𝑔ℎ </i>subtracted by <i>𝑇𝑟𝑜𝑢𝑔ℎ 𝐿𝑜𝑤</i>, divided by <i>𝑃𝑒𝑎𝑘 𝐻𝑖𝑔ℎ</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">Q-LEARNING</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark125" class="a">Q-learning is a value-based, model-free, and off-policy reinforcement learning algorithm introduced by Watkins and Dayan (1992)</a> that seeks to approximate the best action <i>𝑎 </i>to be taken given a current state <i>𝑠</i>.  The algorithm estimates a state-action value function <i>𝑄𝜋  𝑠, 𝑎  </i>, which represents how good it is for an agent to perform a particular action in a state under a given policy</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">𝜋<a href="#bookmark29" class="a">. Equation </a><span class="p">2.9 shows the respective state-action value function:</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 13pt;padding-left: 59pt;text-indent: 53pt;line-height: 18pt;text-align: left;">𝑄𝜋<span class="p">(</span>𝑠, 𝑎<span class="s11">) </span><span class="s19">= </span><span class="p">E</span><span class="s11">[</span>𝑟𝑡 <span class="s11">+ </span>𝛾𝑟𝑡<span class="s11">+</span><span class="s10">1 </span><span class="s11">+ </span>𝛾<span class="s10">2</span>𝑟𝑡<span class="s11">+</span><span class="s10">2 </span><span class="s11">+ </span>...<span class="s11">|</span>𝑠𝑡 <span class="s19">= </span>𝑠, 𝑎𝑡 <span class="s19">= </span>𝑎, 𝜋<span class="s11">]</span>.                         <span class="p">(2.9) Denominated also as Q-function, </span>𝑄𝜋<span class="p">(</span>𝑠, 𝑎   <span class="p">is calculated by taking the sum of rewards</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( | )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">𝑟  <span class="p">discounted by </span>𝛾 <span class="p">at each time step </span>𝑡<span class="p">, achievable under a given behavior policy </span>𝜋  <span class="s19">= </span>𝑃  𝑎 𝑠  <span class="p">, after making an observation </span>𝑠𝑡 <span class="p">and taking action </span>𝑎𝑡 <span class="p">at a </span>𝑡 <span class="p">time step.  The standard Q-learning algorithm often deals with problems in which the state and action spaces are small enough to approximate value functions to be represented as tables, also known as Q-tables. These contain the representation of every observation originated by the environment and every possible action</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark30" class="a" name="bookmark30">to be taken. With these features combined, the table represents all possible combinations of each state-action pair’s ability to reach the agent’s ultimate objective. Figure </a>2.1 shows an example of a Q-table applied to the trading problem having price and the relative strength index (RSI) as features.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 120pt;text-indent: 0pt;text-align: left;"><span><img width="327" height="225" alt="image" src="documento[1970]/Image_005.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 111pt;text-indent: 0pt;text-align: left;">Figure 2.1: Q-table representing states from a trading environment</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 17pt;text-indent: 42pt;line-height: 14pt;text-align: justify;"><a href="#bookmark30" class="a">Suppose that the agent is at the state where RSI and Price features are respectively equal to 30 and 2.5. If the model chooses the hold action, the Q-table shown in Figure 2.1</a><span class="p"> states that its action value is 3. In other words, this particular state-action pair is more beneficial to reach the agent’s goal than the buy action, which is -3. However, this Q-table shows that in this particular state </span>𝑅𝑆𝐼 <span class="s19">= </span><span class="s10">30 </span><span class="p">and </span>𝑃𝑟𝑖𝑐𝑒 <span class="s19">= </span><span class="s10">2</span>.<span class="s10">0</span><span class="p">, the best action to be taken according to the model objective is to sell, which its action value is 8. In addition, the last row of the Q-table shows that more states can be included, extending the trading environment and state-action pairs. Lastly, suppose the agent chooses its actions according to the Epsilon-Greedy policy, which selects the action with the highest estimated state-action pair during its interaction with the environment. In that case, it learns to avoid taking specific actions into the state in the future due to its low </span>Q-value<span class="p">.</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )+</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 16pt;text-indent: 42pt;line-height: 14pt;text-align: justify;">In reinforcement learning, the agent actively iterates with the environment through actions until a desired state is reached or a maximum number of steps are expired. This series of steps is defined as an episode. In the beginning, Q-learning starts all possible state-action pairs evaluated at 0 and begins learning from the environment.  In order to learn through iteration, action values must be estimated. Some methods, such as Monte-Carlo learning, only estimate how good an action is when the episode finishes and only then is the Q-table updated. Instead of waiting for an entire episode to learn, temporal difference methods can estimate <i>𝑄     𝑣𝑎𝑙𝑢𝑒𝑠 </i><a href="#bookmark30" class="a">at every iteration. These methods update the Q-table at the exact moment the episode is unfolding. At the core of Q-learning lies the Bellman equation shown in equation 2.10,</a> serving as the target to estimate the action value given the information in the tuple   <i>𝑆𝑡𝑎𝑡𝑒t, 𝐴𝑐𝑡𝑖𝑜𝑛t, 𝑅𝑒𝑤𝑎𝑟 𝑑t, 𝑆𝑡𝑎𝑡𝑒t  </i><span class="s10">1 </span><a href="#bookmark30" class="a">. The main idea in Q-learning is that the state-action pairs can be iteratively approximated using equation </a><a href="#bookmark30">2.10.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s35" style="text-indent: 0pt;line-height: 69%;text-align: left;">𝑎<span class="s36">′</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-top: 4pt;padding-left: 59pt;text-indent: 96pt;text-align: left;"><span class="s5">𝑄t</span>+<span class="s10">1 </span>(<span class="s5">𝑠, 𝑎</span>) <span class="s19">= </span><span class="p">E</span>[<span class="s5">rt </span>+ <span class="s5">𝛾𝑚𝑎𝑥𝑄𝑡 </span>(<span class="s5">𝑠</span>′<span class="s5">, 𝑎</span>′)]<span class="s5">.                                     </span><span class="p">(2.10)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">+</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 9pt;padding-left: 17pt;text-indent: 42pt;line-height: 88%;text-align: justify;">The mentioned equation states that the new <i>Q-value </i>yielded from being at state <i>𝑠 </i>and performing action <i>𝑎 </i>at time step <i>𝑡    </i><span class="s10">1 </span>is equals to the immediate reward <i>rt </i>plus the best <i>Q-value </i>possible from the following state <i>𝑠</i><span class="s11">′</span>, multiplied by the discount factor <i>𝛾  </i>which controls the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark31" class="a" name="bookmark31">contribution of past rewards, and E is the expectation symbol. This component is also called the target. Equation </a>2.2 shows the component as mentioned above, a formula adapted to be recursive and shaped to update Q-table action values.</p><p class="s36" style="text-indent: 0pt;line-height: 69%;text-align: left;">′<span class="s35">𝑎</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-top: 12pt;padding-left: 59pt;text-indent: 45pt;text-align: left;"><span class="s5">𝑄t</span>+<span class="s10">1 </span>(<span class="s5">𝑠, 𝑎</span>) <span class="s19">= </span><span class="s5">𝑄t </span>(<span class="s5">𝑠, 𝑎</span>) + <span class="s5">𝛼</span>[<span class="s5">rt </span>+ <span class="s5">𝛾𝑚𝑎𝑥𝑄t </span>(<span class="s5">𝑠</span>′<span class="s5">, 𝑎</span>′) − <span class="s5">𝑄t </span>(<span class="s5">𝑠, 𝑎</span>)]<span class="s5">.                    </span><span class="p">(2.11)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">∈ [ ]</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;line-height: 94%;text-align: justify;"><a href="#bookmark31" class="a">Figure </a><span class="p">2.2 shows an example of </span>Q-value <a href="#bookmark31" class="a">update by using equation </a><a href="#bookmark30" class="a">2.11. Where it is composed of two main components: temporal difference target defined on equation </a><span class="p">2.10, which is used to update the Q-table given a new experience tuple, as well as </span>𝑄 𝑠, 𝑎 <span class="p">element that is the current estimation of the action value. The hyper-parameter </span>𝛼 <span class="s10">0</span>, <span class="s10">1 </span><span class="p">is the learning rate. It controls how much a new estimated action value impacts the current estimate. If </span>𝛼 <span class="s19">= </span><span class="s10">0</span><span class="p">, the model will not learn, and the estimated action pair is not changed. On the other hand, if </span>𝛼 <span class="s19">= </span><span class="s10">1</span><span class="p">, the old action value is completely discarded.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: left;"><span><img width="594" height="346" alt="image" src="documento[1970]/Image_006.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">Figure 2.2: Q-table update mechanism in a trading environment</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark30" class="a">By simulating an update of the Q-table presented in Figure 2.1,</a><span class="p"> the learning process has already been started; hence the Q-table is not 0. Suppose the agent receives the initial observation of </span>𝑅𝑆𝐼  <span class="s19">= </span><span class="s10">36 </span><span class="p">and </span>𝑃𝑟𝑖𝑐𝑒  <span class="s19">= </span><span class="s10">2</span>.<span class="s10">0 </span><span class="p">from the environment.  Then, the same acts accordingly to its greedy policy and chooses to buy since it is the action with the highest value. In the following, the environment reacts to the agent’s choice. It returns the immediate reward calculated by the objective function defined during the implementation, which yields </span>𝑟𝑡  <span class="s19">= </span><span class="s10">4 </span><span class="p">and the next state</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 88%;text-align: justify;">𝑅𝑆𝐼  <span class="s19">= </span><span class="s10">15 </span><span class="p">and </span>𝑃𝑟𝑖𝑐𝑒  <span class="s19">= </span><span class="s10">1</span>.<span class="s10">0</span><span class="p">.  As soon as the learner receives the previous information, it can update the </span>Q-value <a href="#bookmark31" class="a">using equation 2.2.</a><span class="p"> Having the current estimated action value of </span>𝑄𝑡  𝑠, 𝑎   <span class="s19">= </span><span class="s10">5</span><span class="p">, at the moment the next state </span>𝑆<span class="s11">′ </span><span class="p">is presented, the agent will not take the following action based on the previous policy but will choose the best action value from all the possible ones.   In this case, the best action value in the next state </span>𝑆<span class="s11">′ </span><span class="p">is 9 (hold), in mathematical representation,</span></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">𝑚𝑎𝑥𝑄t 𝑠<span class="s11">′</span>, 𝑎<span class="s11">′ </span><span class="s19">= </span><span class="s10">9</span><span class="p">. Finally, assuming that the discount and learning rates are 0.99 and 0.1, the</span></p><p class="s35" style="padding-left: 24pt;text-indent: 0pt;line-height: 43%;text-align: left;">𝑎<span class="s36">′</span></p><p class="s10" style="padding-left: 17pt;text-indent: 0pt;line-height: 17pt;text-align: justify;"><span class="p">new </span><span class="s5">Q-value </span><span class="p">estimation is given by: </span><span class="s5">𝑄 </span><span class="s11">(</span><span class="s5">𝑠, 𝑎</span><span class="s11">) </span><span class="s19">= </span>5 <span class="s11">+ </span>0<span class="s5">.</span>1<span class="s11">[</span>4 <span class="s11">+ </span>0<span class="s5">.</span>99 <span class="s11">∗ </span>9 <span class="s11">− </span>5<span class="s11">] </span><span class="s19">= </span>5<span class="s5">.</span>791<span class="p">. As iterations</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">→</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a name="bookmark20">go by, such action values tend to converge to the optimal action-value function </a><i>𝑄𝑡       𝑄𝑡</i>* as<a name="bookmark32">&zwnj;</a></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">→ ∞</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">𝑡 <span class="p">. The optimal state-action value function measures the maximum state-action function overall policies and is given by</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) ( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 4pt;padding-left: 16pt;text-indent: 157pt;text-align: justify;">𝑄<span class="p">*</span><span class="s11">(</span>𝑠, 𝑎<span class="s11">) </span><span class="s19">= </span><span class="p">E</span><span class="s11">[</span>𝑚𝑎𝑥𝑄𝜋 <span class="s11">(</span>𝑠, 𝑎<span class="s11">)]</span>,                                           <span class="p">(2.12) where </span>𝑄<span class="p">*  </span>𝑠, 𝑎  <span class="p">is the optimal action-value function, E the expectation symbol, </span><span class="s10">max </span><span class="p">the maximum operator, and </span>𝑄𝜋  𝑠, 𝑎   <span class="p">the action-value function following the </span>𝜋 <span class="p">policy.  </span>𝑄<span class="p">*  </span>𝑠, 𝑎   <span class="p">equals the action-value function that returns the highest expected action-value pairs under </span>𝜋 <span class="p">policy.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">DEEP Q-NETWORKS</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">Deep Q-Network (DQN) is an algorithm that aims to approximate a state-action value function in a Q-Learning framework using a deep neural network. Introduced by Mnih et al. </a><a href="#bookmark32" class="a">(2013), it was the first deep learning model to successfully learn to control policies directly from high-dimensional sensory input using reinforcement learning. Figure </a>2.3 shows a standard Q-table and a Deep Q-network.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 77pt;text-indent: 0pt;text-align: left;"><span><img width="415" height="366" alt="image" src="documento[1970]/Image_007.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 134pt;text-indent: 0pt;text-align: left;">Figure 2.3: Q-table and Deep Q-Network architectures</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">In order to overcome the limitation of small state and action spaces in Q-tables, DQN incorporated a nonlinear neural network function approximator with weights <i>𝜃 </i>called Q-networks. The main features introduced to the algorithm in order to stabilize learning were Fixed Q-targets and Experience Replay. Q-learning updates its estimated action values based on another estimated value, hence this mechanism may potentially lead to harmful correlations. DQN solves this issue by using a target neural network, which contains fixed Q-targets. This network has the same</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a name="bookmark33">architecture as the main Q-network, except it keeps its </a><i>𝜃</i><span class="s37">i</span><span class="s38">−</span><span class="s14"> </span>weight parameters fixed, and after <i>𝜏 </i>steps, they are updated by copying the main Q-network weights <i>𝜃𝑖</i><a href="#bookmark33" class="a">. Equation 2.13</a> represents the main Q-network forward pass.</p><p class="s36" style="text-indent: 0pt;line-height: 69%;text-align: left;">′<span class="s35">𝑎</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) ∼ ( ) ( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 8pt;padding-left: 16pt;text-indent: 74pt;line-height: 24pt;text-align: left;">𝐿𝑖 <span class="s11">(</span>𝜃𝑖<span class="s11">) </span><span class="s19">= </span><span class="p">E</span><span class="s11">(</span>𝑠,𝑎,𝑟,𝑠<span class="s36">′</span><span class="s39">)</span><span class="s14">∼</span><span class="s25">𝑈 </span><span class="s14">(</span><span class="s25">𝐷</span><span class="s14">) </span><span class="s11">[(</span>r <span class="s11">+ </span>𝛾𝑚𝑎𝑥𝑄 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′</span><span class="s10">; </span>𝜃<span class="s40">i</span><span class="s41">−</span><span class="s11">) − </span>𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃𝑖<span class="s11">))]</span><span class="s42">2</span>,               <span class="p">(2.13) where the subscript of the expected value operator   </span>𝑠, 𝑎, 𝑟, 𝑠<span class="s11">′     </span>𝑈  𝐷   <span class="p">means that the   </span>𝑠, 𝑎, 𝑟, 𝑠<span class="s11">′</span></p><p class="s36" style="text-indent: 0pt;line-height: 69%;text-align: left;">′<span class="s35">𝑎</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 88%;text-align: justify;"><span class="p">experience tuples are uniformly drawn from the experience replay buffer, E is the expectation symbol, </span>r <span class="s11">+ </span>𝛾𝑚𝑎𝑥𝑄 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′</span><span class="s10">; </span>𝜃<span class="s37">i</span><span class="s38">−</span><span class="s11">) </span><span class="p">is the temporal difference updated action value target originated</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;"><span class="p">from the target neural network, and </span>𝑄  𝑠, 𝑎<span class="s10">; </span>𝜃𝑖   <span class="p">is the action value originated from the main Q-network, parameterized by weights </span>𝜃𝑖<a href="#bookmark33" class="a">. Figure 2.4</a><span class="p"> shows the learning mechanism of the DQN algorithm.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 33pt;text-indent: 0pt;text-align: left;"><span><img width="578" height="412" alt="image" src="documento[1970]/Image_008.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">Figure 2.4: Deep Q-Network sampling and update mechanism in a trading environment</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The learning phase starts by uniformly sampling a mini-batch of experiences tuples from the experience replay buffer and feeding them to both networks. Then, the Q-network weights <i>𝜃𝑖 </i>are copied to the target network, and its <i>𝜃</i><span class="s37">i</span><span class="s38">− </span>weights are kept fixed for <i>𝜏 </i>steps, having also its output</p><p class="s5" style="padding-left: 12pt;text-indent: 0pt;line-height: 6pt;text-align: center;"><span class="p">set to as target </span>𝑌 𝐷𝑄𝑁  <span class="s19">= </span>r <span class="s11">+ </span>𝛾𝑚𝑎𝑥𝑄 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′</span><span class="s10">; </span>𝜃<span class="s11">−) </span><span class="p">to the main Q-network. In sequence, the predicted</span></p><p class="s25" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;line-height: 9pt;text-align: right;">𝑖</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 15pt;text-align: left;"><span class="p">action value </span>𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s10">;</span></p><p class="s35" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 11pt;text-align: left;">𝑎<span class="s36">′ </span><span class="s43">i</span></p><p class="s5" style="text-indent: 0pt;line-height: 13pt;text-align: left;">𝜃𝑖<span class="s11">) </span><span class="p">produced by the main network is obtained, and its loss is calculated by</span></p><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;">taking the squared difference between the components above.  Finally, differentiating the loss function with respect to the Q-network weights <i>𝜃𝑖</i><a href="#bookmark34" class="a">, reaches Equation 2.14,</a> which results in the backward pass at each iteration <i>𝑖 </i>of the main Q-network.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝐷𝑄𝑁</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 10pt;padding-left: 16pt;text-indent: 87pt;text-align: left;"><a name="bookmark21"><span class="s11">∇</span></a>𝜃<span class="s44">𝑖</span><span class="s45"> </span>𝐿𝑖 <span class="s11">(</span>𝜃𝑖<span class="s11">) </span><span class="s19">= </span><span class="p">E</span>𝑠,𝑎,𝑟,𝑠<span class="s36">′ </span><span class="s11">[</span>𝛼<span class="s11">(</span>𝑌<span class="s37">𝑖</span><span class="s25">           </span><span class="s11">− </span>𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃𝑖<span class="s11">))∇</span>𝜃<span class="s44">𝑖</span><span class="s45"> </span>𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃𝑖<span class="s11">)]</span>,                    <span class="p">(2.14)</span><a name="bookmark34">&zwnj;</a></p><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝑖</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 4pt;padding-left: 16pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><span class="p">where the rate of weights </span>𝜃𝑖 <span class="p">change is the difference of the temporal difference updated action value target </span>𝑌 𝐷𝑄𝑁 <span class="p">and the action value </span>𝑄  𝑠, 𝑎<span class="s10">; </span>𝜃𝑖   <span class="p">produced by the prediction neural network multiplied by the gradient of the previous component in respect to weights </span>𝜃𝑖<span class="p">.</span></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">In addition to incorporating Fixed-Q targets, an experience replay mechanism was also introduced to the algorithm. Initially published by Lin </a><a href="#bookmark125" class="a">(1992), it is a fixed-size memory buffer that stores the most recent experiences gathered by the agent. It enables online reinforcement learning agents to reuse them multiple times instead of immediately disposing of them. This technique stabilizes learning by breaking the temporal correlations by randomly sampling experiences and not presenting them sequentially. It also brings benefits to learning due to the reuse of rare experiences multiple times. Experience replay is typically implemented so that once a new experience is collected, the buffer discards the oldest and stores the new one in its place. Regarding its sampling strategy, the fixed-size buffer has many variants that explore distinct distributions. Some other variants include Schaul et al. </a><a href="#bookmark124" class="a">(2016), which proposes the Prioritized Experience Replay to sample transitions accordingly to a non-uniform distribution, benefiting experiences with more significant temporal difference error, and Andrychowicz et al. </a>(2017), which presents the Hindsight Experience Replay as an alternative for complicated reward engineering in challenging environments with sparse or binary rewards. The main idea of this variant is to repeat each episode aiming for a different goal other than the previous one that the agent was attempting to achieve.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">DOUBLE DEEP Q-NETWORKS</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;line-height: 14pt;text-align: justify;"><a href="#bookmark124" class="a">In both standard Q-learning and DQN algorithms, the max operator, in their respective temporal difference targets, utilize the same weights to select and evaluate an action. Thus, this approach makes the agent more likely to select overestimated values under certain conditions, such as when training starts and Q-values are still evolving, resulting in overoptimistic value estimates. Aiming to reduce the overestimation problem, Hasselt et al. </a>(2016) proposed to implement the original Double Q-learning to deep reinforcement learning proposing a solution that decomposes the max operator in the target function into action selection and action evaluation. Two value functions are learned by assigning each experience randomly to update one of the two value functions, such that there are two weights, <i>𝜃 </i>and <i>𝜃</i><span class="s46">′ </span><a href="#bookmark34" class="a">. Seeking to make the minimal possible modifications to DQN towards Double Q-learning, the author proposed to use the main Q-network in DQN architecture to evaluate the greedy policy and the target network to estimate the values. Even though both networks are not entirely decoupled, they can be used without introducing additional networks. Equation </a>2.15 shows the temporal difference target update for Double DQN.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 3pt;padding-left: 141pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑌 𝐷𝑜𝑢𝑏𝑙𝑒𝐷𝑄𝑁  <span class="s19">= </span>r <span class="s11">+ </span>𝛾𝑚𝑎𝑥𝑄 <span class="s11">((</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′</span><span class="s10">; </span>𝜃𝑖<span class="s11">)</span><span class="s10">; </span>𝜃<span class="s40">i</span><span class="s41">−</span><span class="s11">)</span>.                                <span class="p">(2.15)</span></p><p class="s43" style="padding-top: 1pt;padding-left: 147pt;text-indent: 0pt;text-align: left;">i <span class="s35">𝑎</span><span class="s36">′</span></p><p style="padding-top: 5pt;padding-left: 17pt;text-indent: 42pt;line-height: 86%;text-align: justify;">The selection of the action, in the max operator, is due to the online weights <i>𝜃𝑖 </i>and made by the main Q-network. However, the second set of weights <i>𝜃</i><span class="s37">i</span><span class="s38">− </span>are used to evaluate this policy. The target network is still updated every <i>𝜏 </i>steps by copying the Q-network weights</p><p class="s25" style="padding-top: 1pt;text-indent: 0pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">𝜃𝑖<a href="#bookmark33" class="a">.  Equations 2.13,</a><a href="#bookmark34" class="a"> 2.14</a><span class="p"> still apply to Double DQN, but having its targets </span>𝑌 𝐷𝑄𝑁  <span class="p">replaced by</span></p><p class="s25" style="padding-top: 1pt;text-indent: 0pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 3pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;">𝑌 𝐷𝑜𝑢𝑏𝑙𝑒𝐷𝑄𝑁 <a href="#bookmark35" class="a">. Figure 2.5</a><span class="p"> shows the learning mechanism of the Double DQN algorithm.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="624" height="346" alt="image" src="documento[1970]/Image_009.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 54pt;text-indent: 0pt;text-align: left;"><a name="bookmark35">Figure 2.5: Double deep Q-Network sampling and update mechanism in a trading environment</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The learning phase starts by uniformly sampling experience tuples stored in the experience replay buffer. The total amount of sampled tuples is equivalent to the chosen batch size. Then, the next state array serves as input to the main Q-network and the <i>𝑎𝑟𝑔𝑚𝑎𝑥 </i>operator is applied to retrieve the best action predicted from the network. In the following, the next state is also fed to the target network, however the <i>Q-value </i>retrieved is regarding the position of the previous action generated by the main Q-network, in other words, the action is generated by the main Q-network and evaluated by the target network. In case that the trade is over, the temporal difference target is given by the reward, else it is given by</p><p class="s36" style="text-indent: 0pt;line-height: 69%;text-align: left;">′<span class="s35">𝑎</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 11pt;padding-left: 16pt;text-indent: 125pt;text-align: left;">𝑌 𝐷𝑜𝑢𝑏𝑙𝑒𝐷𝑄𝑁  <span class="s19">= </span>r <span class="s11">+ </span>𝛾𝑚𝑎𝑥𝑄 <span class="s11">((</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′</span><span class="s10">; </span>𝜃𝑖<span class="s11">)</span><span class="s10">; </span>𝜃<span class="s40">i</span><span class="s41">−</span><span class="s11">)</span>,                                <span class="p">(2.16)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">(( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><span class="p">where </span>𝑟 <span class="p">is the reward, </span>𝛾 <span class="p">is the discount rate, </span>𝑚𝑎𝑥𝑄 𝑠<span class="s11">′</span>, 𝑎<span class="s11">′</span><span class="s10">; </span>𝜃𝑖 <span class="s10">; </span>𝜃<span class="s37">i</span><span class="s38">− </span><span class="p">is the </span>Q-value <span class="p">in which the action is generated by the main Q-network having the target network as evaluator. Lastly, the current state is fed to the main Q-network to output the </span>Q-value <span class="p">function, in respect to the current</span></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">state and the loss is given by</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s25" style="padding-top: 6pt;padding-left: 119pt;text-indent: 0pt;text-align: left;"><span class="s29">𝐿</span>𝑖 <span class="s47">(</span><span class="s5">𝜃</span>𝑖 <span class="s47">)</span><span class="s11"> </span><span class="s19">= </span><span class="p">E</span><span class="s14">(</span>𝑠,𝑎,𝑟,𝑠<span class="s48">′</span><span class="s14">)∼</span>𝑈 <span class="s14">(</span>𝐷<span class="s14">) </span><span class="s47">[</span><span class="s5">𝑌</span></p><p class="s25" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">𝐷𝑜𝑢𝑏𝑙𝑒𝐷𝑄𝑁</p><p class="s5" style="padding-top: 6pt;padding-left: 1pt;text-indent: 0pt;text-align: left;"><span class="s11">− </span>𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃𝑖<span class="s11">)]</span></p><p class="s42" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">2<span class="s5">, </span><span class="p">(2.17)</span></p><p style="padding-top: 4pt;padding-left: 16pt;text-indent: 0pt;line-height: 13pt;text-align: left;">where <i>𝜃𝑖 </i>is the weights of the main Q-network, <i>𝑌 𝐷𝑜𝑢𝑏𝑙𝑒𝐷𝑄𝑁 </i>is the target, and the action value</p><p class="s5" style="padding-left: 16pt;text-indent: 0pt;line-height: 18pt;text-align: left;">𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃𝑖<span class="s11">) </span><span class="p">is produced by the main neural network.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-top: 6pt;padding-left: 40pt;text-indent: -23pt;text-align: left;"><a name="bookmark22">CATEGORICAL DQN</a><a name="bookmark36">&zwnj;</a></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">Bellemare et al. </a>(2017) proposed to explore the distribution of the random values received by an RL agent instead of the typical approach that models the expectation of these values using the Bellman expectation equation for state-action value function <i>𝑄 𝑠, 𝑎 </i>which is written as</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s11">) </span><span class="s19">= </span><span class="p">E</span>𝜋 <span class="s11">[</span>𝑅<span class="s11">(</span>𝑠, 𝑎<span class="s11">) + </span>𝛾𝑄 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′)]</span>,                                      <span class="p">(2.18)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><span class="p">where </span>𝑅 𝑠, 𝑎 <span class="p">is the immediate reward provided in state </span>𝑠 <span class="p">under the action </span>𝑎<span class="p">, </span>𝛾 <span class="p">is the discount factor, </span>𝑄 𝑠<span class="s11">′</span>, 𝑎<span class="s11">′ </span><span class="p">is the state-action value in the next state </span>𝑠<span class="s11">′ </span><span class="p">taking action </span>𝑎<span class="s11">′</span><span class="p">, E is the expectation symbol. The authors remove the expectations inside Bellman’s equation and consider instead the full distribution of the random variable </span>𝑍𝜋<span class="p">.</span></p><p class="s5" style="padding-top: 13pt;padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑍 <span class="s11">(</span>𝑠, 𝑎<span class="s11">) </span><span class="s19">=</span>𝐷 𝑅<span class="s11">(</span>𝑠, 𝑎<span class="s11">) + </span>𝛾𝑍 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′)</span>,                                          <span class="p">(2.19)</span></p><p class="s5" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 92%;text-align: justify;"><span class="p">where R(s,a) is the immediate reward provided in state </span>𝑠 <span class="p">under the action </span>𝑎<span class="p">, </span>𝛾 <span class="p">is the discount factor, </span>𝑍 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′) </span><span class="p">is the distribution of the the random variable </span>𝑍 <span class="p">in next state </span>𝑠<span class="s11">′ </span><span class="p">and taking action</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;line-height: 88%;text-align: justify;">𝑎<span class="s11">′</span><span class="p">. </span><span class="s19">=</span>𝐷 <span class="p">states that </span>𝑍  𝑠, 𝑎   <span class="p">is equivalent to a distribution. This distribution is adopted as a mapping from state-action pairs to distributions over returns. It is called the value distribution.</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">∈</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">Based on a distributional approach, Bellemare et al. (2017)</a> proposed an algorithm to approximate distribution probability masses placed on a discrete support vector <i>𝑧 </i>parameterized by <i>𝑁𝑎𝑡𝑜𝑚𝑠     </i>N<span class="s11">+</span>. The discrete distribution’s atoms may be seen as the &quot;canonical returns&quot; of it. They are consecutive, non-overlapping intervals with evenly spaced values in <i>𝑧</i>.  The discrete support <i>𝑧𝑖 </i>is given by</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 4pt;padding-left: 206pt;text-indent: 0pt;text-align: left;">𝑧𝑖 <span class="s19">= </span>𝑉𝑚𝑖𝑛 <span class="s11">+ </span>𝑖<span class="s19">Δ</span>𝑧,                                                      <span class="p">(2.20)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">∈</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;">where <i>𝑉𝑚𝑖𝑛     </i>R is the minimum (starting) value of the support vector <i>𝑧𝑖 </i>whose values are evenly spaced, <i>𝑖 </i>is the position of vector <i>𝑧𝑖</i>. <span class="s19">Δ</span><i>𝑧 </i>is the minimum value that is added to each position of vector <i>𝑧 </i>defined by</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s19" style="padding-top: 1pt;padding-left: 199pt;text-indent: 0pt;line-height: 20pt;text-align: left;">Δ<span class="s5">𝑧 </span>= <span class="s26">𝑉</span><span class="s23">𝑚</span><span class="s27">𝑎𝑥  </span><span class="s24">−</span><span class="s11"> </span><span class="s22">𝑉</span><span class="s23">𝑚</span><span class="s27">𝑖𝑛</span><span class="s25"> </span><span class="s5">,                                                   </span><span class="p">(2.21)</span></p><p class="s29" style="padding-left: 230pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑁<span class="s25">𝑎𝑡𝑜𝑚𝑠  </span><span class="s47">−</span><span class="s11"> </span><span class="s10">1</span></p><p style="padding-left: 16pt;text-indent: 0pt;line-height: 16pt;text-align: left;">where <i>𝑁𝑎𝑡𝑜𝑚𝑠 </i>is the number of atoms, <i>𝑉𝑚𝑎𝑥 </i><span class="s11">∈ </span>R is the maximum value of the discrete support and</p><p class="s5" style="padding-left: 16pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑉𝑚𝑖𝑛 <span class="p">is described by equation above. In addition, the atom probabilities </span>𝑝𝑖 <span class="s11">(</span>𝑠, 𝑎<span class="s11">) </span><span class="p">of a distribution</span></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 16pt;text-align: left;">𝑍𝜋 <span class="s11">(</span>𝑠, 𝑎<span class="s11">) </span><span class="p">can be computed as</span></p><p class="s11" style="padding-top: 2pt;text-indent: 0pt;text-align: left;">( ) <span class="s49">L </span><span class="s50">− </span><span class="s51">𝑒 </span><span class="s52">𝑗 </span><span class="p">(2.22)</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s12" style="padding-top: 8pt;padding-left: 261pt;text-indent: 0pt;line-height: 58%;text-align: left;">𝑒<span class="s25">𝑧</span><span class="s53">𝑖</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="58" height="1" alt="image" src="documento[1970]/Image_010.png"/></span></p><p class="s5" style="padding-left: 45pt;text-indent: 0pt;line-height: 73%;text-align: center;">𝑝𝑖   𝑠, 𝑎    <span class="s19">=     </span><span class="s54">𝑁   </span><span class="s30">1    </span><span class="s55">𝑧 </span>,</p><p class="s25" style="padding-left: 256pt;text-indent: 0pt;line-height: 11pt;text-align: left;">𝑗 <span class="s33">=</span><span class="s30">0</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">− ( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">where <i>𝑒 </i>is the Euler’s number, <i>𝑧 </i>is the support vector that holds the &quot;canonical&quot; values of the distribution.   <i>𝑁  </i>is the number of atoms, <i>𝑖 </i>is the position of the atom that the probability is calculated in the vector <i>𝑧</i>, and  <i>𝑗  </i>is an evenly-space variable that sums all-atom values from position <span class="s10">0 </span>to <i>𝑁𝑎𝑡𝑜𝑚𝑠     </i><span class="s10">1</span>.  <i>𝑝𝑖  𝑠, 𝑎  </i>. It is known as the softmax function. This function takes the vector <i>𝑧 </i><a href="#bookmark37" class="a">and normalizes it into a probability distribution consisting of probabilities proportional to the exponentials of the input numbers. Figure 2.6</a> shows an illustrative example of generating the support vector <i>𝑧 </i>and calculating the probability of each atom.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Suppose the number of atoms <i>𝑁𝑎𝑡𝑜𝑚𝑠  </i>is 10.  The support vector <i>𝑧𝑖  </i>will contain ten positions. In addition, the example adopted the vector’s boundaries with the minimum starting</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 110pt;text-indent: 0pt;text-align: left;"><span><img width="307" height="289" alt="image" src="documento[1970]/Image_011.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 5pt;padding-left: 100pt;text-indent: 0pt;text-align: left;"><a name="bookmark37">Figure 2.6: Example of support vector </a><i>𝑧</i><span class="s56">𝑖 </span>and its probability distribution</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;">number <i>𝑉𝑚𝑖𝑛 </i>of -5 and the maximum final number <i>𝑉𝑚𝑎𝑥 </i>of +5. To fill the remaining positions, first, a minimum variation of vector <i>𝑧 </i>(<span class="s19">Δ</span><i>𝑧</i>) is calculated by:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s21" style="padding-top: 9pt;padding-left: 159pt;text-indent: 0pt;line-height: 34%;text-align: left;"><span class="s57">Δ</span><span class="s5">𝑧 </span><span class="s19">= </span><span class="s22">𝑉𝑚𝑎𝑥 </span>−<span class="s11"> </span><span class="s22">𝑉𝑚𝑖𝑛 </span><span class="s58">5 </span>− (−<span class="s58">5</span>)</p><p class="s10" style="padding-top: 12pt;padding-left: 3pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s19">= </span>1<span class="s5">.</span>11<span class="s5">,</span></p><p class="s11" style="padding-left: 43pt;text-indent: 0pt;line-height: 18pt;text-align: center;"><span class="s5">𝑁 </span>− <span class="s10">1      10 </span>− <span class="s10">1</span></p><p class="s5" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;"><span class="p">where Adopting </span><span class="s19">Δ</span>𝑧  <span class="s19">=  </span><span class="s10">1</span>.<span class="s10">11</span><span class="p">,  the remaining positions are calculated by multiplying </span><span class="s19">Δ</span>𝑧  <span class="p">with its respective position and sum the minimum adopted value </span>𝑉𝑚𝑖𝑛<a href="#bookmark36" class="a">, according to Equation </a><span class="p">2.20. To illustrate this computation, the first and second positions of the support vector are written, respectively</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 45pt;text-indent: 0pt;line-height: 18pt;text-align: center;"><span class="s5">𝑧</span>1 <span class="s19">= </span><span class="s11">−</span>5 <span class="s11">+ (</span>1 <span class="s11">× </span>1<span class="s5">.</span>11<span class="s11">) </span><span class="s19">= </span><span class="s11">−</span>3<span class="s5">.</span>89<span class="s5">,</span></p><p class="s10" style="padding-left: 45pt;text-indent: 0pt;line-height: 18pt;text-align: center;"><span class="s5">𝑧</span>2 <span class="s19">= </span><span class="s11">−</span>5 <span class="s11">+ (</span>2 <span class="s11">× </span>1<span class="s5">.</span>11<span class="s11">) </span><span class="s19">= </span><span class="s11">−</span>2<span class="s5">.</span>78<span class="s5">.</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 9pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;">In order to calculate the probabilities <i>𝑝𝑖  𝑠, 𝑎   </i>for each atom of the vector <i>𝑧𝑖</i><a href="#bookmark36" class="a">, the Softmax function in Equation 2.22</a> was applied to it.  In the upper part of the equation, each atom on positions <i>𝑖 </i>was taken and served as the exponent of Euler’s number. The value of each atom on positions <i>𝑖 </i>was added to the lower part.  To illustrate how the function is applied, the probabilities of the value in positions four and five are given by:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 2pt;text-indent: 0pt;text-align: left;">( ) <span class="s49">L</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 15pt;padding-left: 117pt;text-indent: 0pt;line-height: 80%;text-align: left;">𝑝<span class="s10">4 </span>𝑠, 𝑎 <span class="s19">= </span><span class="s59">9</span></p><p class="s25" style="padding-left: 15pt;text-indent: 0pt;line-height: 11pt;text-align: right;">𝑗 <span class="s33">=</span><span class="s30">0</span></p><p class="s30" style="padding-top: 4pt;padding-left: 18pt;text-indent: 0pt;line-height: 10pt;text-align: left;">0<span class="s25">.</span>56</p><p style="text-indent: 0pt;text-align: left;"><span><img width="125" height="1" alt="image" src="documento[1970]/Image_012.png"/></span></p><p class="s14" style="text-indent: 0pt;line-height: 65%;text-align: left;">−<span class="s12">𝑒</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s10" style="padding-left: 72pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s19">= </span>0<span class="s5">.</span>00259 <span class="s19">= </span>0<span class="s5">.</span>259%<span class="s5">,</span></p><p class="s5" style="text-indent: 0pt;line-height: 14pt;text-align: left;">𝑒<span class="s60">𝑧 </span><span class="s61">𝑗 </span><span class="s19">= </span><span class="s10">221</span>.<span class="s10">242</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 2pt;text-indent: 0pt;text-align: left;">( ) <span class="s49">L</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 15pt;padding-left: 116pt;text-indent: 0pt;line-height: 80%;text-align: left;">𝑝<span class="s10">5 </span>𝑠, 𝑎 <span class="s19">= </span><span class="s59">9</span></p><p class="s25" style="padding-left: 15pt;text-indent: 0pt;line-height: 11pt;text-align: right;">𝑗 <span class="s33">=</span><span class="s30">0</span></p><p class="s30" style="padding-top: 4pt;padding-left: 10pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s12">𝑒</span>0<span class="s25">.</span>55</p><p style="text-indent: 0pt;text-align: left;"><span><img width="125" height="1" alt="image" src="documento[1970]/Image_013.png"/></span></p><p class="s10" style="padding-left: 72pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s19">= </span>0<span class="s5">.</span>00787 <span class="s19">= </span>0<span class="s5">.</span>788%<span class="s5">.</span></p><p class="s5" style="text-indent: 0pt;line-height: 14pt;text-align: left;">𝑒<span class="s60">𝑧 </span><span class="s61">𝑗 </span><span class="s19">= </span><span class="s10">221</span>.<span class="s10">242</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s25" style="text-indent: 0pt;line-height: 17pt;text-align: left;"><span class="s47">(</span><span class="s11">     )</span>𝑡<span class="s14">+</span><span class="s30">1</span><span class="s35">𝑡</span>   <span class="s30">1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;line-height: 82%;text-align: justify;"><a href="#bookmark36" class="a" name="bookmark38">The main idea of Categorical DQN is that the return distributions satisfy equation </a><span class="p">2.19. Suppose a given state </span>𝑠 <span class="p">and action </span>𝑎<span class="p">, the distribution of returns under the optimal policy </span>𝜋<span class="s11">∗ </span><span class="p">should match the target distribution, defined by taking the distribution for the next state </span>𝑠<span class="s11">′ </span><span class="p">and action </span>𝑎<span class="s11">∗ </span><span class="s19">= </span>𝜋<span class="s11">∗ </span>𝑠 <a href="#bookmark40" class="a">. Figure </a><span class="p">2.9 shows the effects of reward, discount factor, and projection</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 91%;text-align: left;">step in th<span class="s62">+</span>e returns distribution.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 35pt;text-indent: 0pt;text-align: left;"><span><img width="492" height="331" alt="image" src="documento[1970]/Image_014.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">Figure 2.7: Operations using Bellman operator on Distribution of returns</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">T</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">𝑃𝜋 𝑍 𝑠<span class="s11">′</span>, 𝑎<span class="s11">′ </span><span class="p">is the next state distribution under policy </span>𝜋<span class="p">, </span>𝛾 <span class="p">is the discount factor, </span>𝑅 <span class="p">is the immediate reward, </span><span class="s19">Φ </span><span class="p">is a L2-projection of the target distribution onto the support vector </span>𝑧𝑖 <span class="p">and is the distributional Bellman optimality operator. By increasing the </span>𝛾 <span class="p">discount factor, the</span></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">distribution shrinks towards 0, and the probability masses are concentrated towards the center, increasing the probability in a certain range. In addition, adding rewards shifts the distribution in the x-axis. Lastly, the projected Bellman update step <span class="s19">Φ </span>distribution is shown in the last Figure. This projection may be used as a target to calculate the loss when updating the Bellman equation. To adapt the variant Bellman update to the DQN architecture for a given experience</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 16pt;text-align: justify;"><span class="p">tuple </span><span class="s11">(</span>𝑠, 𝑎, 𝑟, 𝑠<span class="s11">′)</span><span class="p">, first, the </span>Q-value <span class="p">for the next state </span>𝑄 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′) </span><span class="p">is calculated:</span></p><p class="s5" style="text-indent: 0pt;line-height: 18pt;text-align: left;">𝑄 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′) </span><span class="s19">=</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="text-indent: 0pt;line-height: 18pt;text-align: left;">𝑧𝑖 𝑝𝑖 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">′)</span>.                                             <span class="p">(2.23)</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="padding-top: 7pt;padding-left: 44pt;text-indent: 0pt;text-align: center;">𝑁<span class="s49">∑︁</span><span class="s14">−</span><span class="s30">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s25" style="text-indent: 0pt;line-height: 12pt;text-align: left;">𝑖<span class="s33">=</span><span class="s30">0</span></p><p style="padding-left: 236pt;text-indent: 0pt;line-height: 12pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 12pt;text-align: left;">𝑖<span class="s33">=</span><span class="s30">0</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">L</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 81%;text-align: justify;">Where the <i>𝑄 𝑣𝑎𝑙𝑢𝑒 </i>in the next state <i>𝑠</i><span class="s11">′ </span>is the sum <span class="s63">𝑁</span><span class="s14">−</span><span class="s30">1 </span>of the inner products of the distribution in respect to the greedy action with its probability distribution vector <i>𝑝𝑖 𝑠</i><span class="s11">′</span><i>, 𝑎</i><span class="s11">′ </span>from 0 to the number of atoms <i>𝑁 </i><span class="s10">1</span><a href="#bookmark39" class="a">. Figure </a>2.8 shows an illustration of feeding a state in the neural network to retrieve the best distribution regarding the greedy action <i>𝑎</i><span class="s11">∗</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><span><img width="585" height="266" alt="image" src="documento[1970]/Image_015.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 122pt;text-indent: 0pt;text-align: left;"><a name="bookmark39">Figure 2.8: Neural Network architecture in Categorical DQN</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;line-height: 78%;text-align: justify;"><span class="p">Suppose the array </span>𝑍 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗) </span><a href="#bookmark39" class="a">selected in Figure 2.8</a><span class="p"> was in respect to the greedy action distribution.  </span>𝑍 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗) </span><span class="p">can be denoted </span>𝑧𝑖<span class="p">, and the </span>Q-value <span class="p">of that distribution would be calculated as the inner product of the support vector </span>𝑧𝑖 <span class="p">and </span>𝑝𝑖 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗) </span><span class="p">written as:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 15pt;padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑄 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗) </span><span class="s19">=</span></p><p class="s30" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">9</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">∑︁</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">𝑖<span class="s33">=</span><span class="s30">0</span></p><p class="s5" style="padding-top: 15pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">𝑧𝑖 𝑝𝑖 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗</span></p><p class="s10" style="padding-top: 15pt;text-indent: 0pt;text-align: left;"><span class="s11">) </span><span class="s19">= </span>0<span class="s5">.</span>68<span class="s5">.</span></p><p style="padding-top: 8pt;padding-left: 59pt;text-indent: 0pt;line-height: 10pt;text-align: left;">To calculate the loss of the neural network, a new projected vector support <i>𝑧 𝑗 </i>is created</p><p style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="a">with evenly spaced values, according to equations 2.20 and </a>2.21. three additional <i>𝑏 𝑗 </i>, <i>𝑙</i>, and <i>𝑢 </i>vector variables respectively written:</p><p class="s11" style="padding-left: 3pt;text-indent: 0pt;line-height: 21pt;text-align: left;">T<span class="s16">ˆ</span><span class="s10"> </span><span class="s5">𝑧 𝑗 </span><span class="p">is computed as well as</span></p><p class="s43" style="text-indent: 0pt;line-height: 10pt;text-align: left;">𝑉<span class="s45">𝑚𝑖𝑛</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 9pt;padding-left: 192pt;text-indent: 0pt;text-align: left;"><span class="s11">T</span><span class="s16">ˆ</span><span class="s10"> </span>𝑧 𝑗  <span class="s19">= </span><span class="s64"> </span>𝑟 <span class="s11">+ </span>𝛾𝑧 𝑗 <span class="s64"> </span><span class="s65">𝑉</span><span class="s66">𝑚</span><span class="s45">𝑎𝑥  </span>,                                                 <span class="p">(2.24)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑏 𝑗</p><p class="s5" style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;line-height: 25pt;text-align: left;"><span class="s19">= </span><span class="s67">T</span><span class="s68">ˆ</span><span class="s10"> </span><span class="s69">𝑧</span> <span class="s65">𝑗</span><span class="s25">  </span><span class="s67">−</span><span class="s11"> </span>𝑉<span class="s65">𝑚</span><span class="s25">𝑖𝑛 </span>,                                                    <span class="p">(2.25)</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="72" height="1" alt="image" src="documento[1970]/Image_016.png"/></span></p><p class="s19" style="padding-left: 34pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Δ<span class="s5">𝑧</span></p><p class="s5" style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">𝑙 <span class="s19">= </span><span class="s11">⌊</span>𝑏 𝑗 <span class="s11">⌋</span>, <span class="p">(2.26)</span></p><p class="s5" style="padding-top: 16pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">𝑢 <span class="s19">= </span><span class="s11">⌈</span>𝑏 𝑗 <span class="s11">⌉</span>. <span class="p">(2.27)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">T</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 15pt;text-align: left;">𝑗 <span class="s47">T </span>𝑗</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;line-height: 79%;text-align: justify;">Variable <i>𝑏 </i>contains real value index positions which each value of <span class="s16">ˆ </span><i>𝑧 </i>is closest in respect to the support <i>𝑧 𝑗 </i>and is defined by the projected support <span class="s16">ˆ </span><i>𝑧 𝑗 </i>subtracted by the minimum value of</p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">the distribution <i>𝑉𝑚𝑖𝑛 </i>divided by <span class="s19">Δ</span><i>𝑧 </i>which are defined above. The lower variable <i>𝑙 </i>is the floor of the variable <i>𝑏 𝑗 </i>.  The floor of a number is the greatest integer less than or equal to <i>𝑏 𝑗 </i>.  On the other hand, the array variable <i>𝑢 </i>is the ceiling of <i>𝑏 𝑗 </i>. The ceiling of a variable is the least integer greater than or equal to <i>𝑏 𝑗 </i><a href="#bookmark40" class="a">. Figure 2.9</a> illustrates the calculation of the abovementioned variables.</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 42pt;line-height: 83%;text-align: justify;"><span class="p">Suppose that the distribution </span>𝑍  𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗  </span><a href="#bookmark39" class="a">was taken from Figure </a><span class="p">2.8.  To calculate the neural network’s loss, a new support vector must be created. In addition, suppose that the first array </span>𝑧𝑖  <span class="p">is the created support vector.   The result of the projection of </span><span class="s11">T</span><span class="s16">ˆ</span><span class="s10"> </span>𝑧𝑖  <span class="p">onto the support</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span><img width="425" height="280" alt="image" src="documento[1970]/Image_017.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 4pt;padding-left: 22pt;text-indent: 0pt;text-align: left;"><a name="bookmark40">Figure 2.9: Computation of the projection of </a><span class="s70">T</span><span class="s71">ˆ</span><span class="s72"> </span><i>𝑧 </i><span class="s56">𝑗 </span>onto the support <i>𝑧</i><span class="s56">𝑖</span>, and variables <i>𝑏 </i><span class="s56">𝑗 </span>, <i>𝑙</i>, and <i>𝑢 </i>derived from it</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝑧𝑖 <a href="#bookmark39" class="a">is shown </a><span class="p">2.24 in the second array. The immediate reward given from the environment</span></p><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">            </p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 21pt;text-align: left;">T<span class="s16">ˆ</span><span class="s10"> </span><span class="s5">𝑧 𝑗  </span><span class="s19">=   </span><span class="s5">𝑟 </span>+ <span class="s5">𝛾𝑧 𝑗  </span><span class="s65">𝑉</span><span class="s66">𝑚</span><span class="s45">𝑎𝑥  </span><span class="s19">= </span><span class="s10">4 </span>+ [(<span class="s10">0</span><span class="s5">.</span><span class="s10">99 </span>× <span class="s5">𝑧 𝑗 </span>)]<span class="s38">+</span><span class="s30">5 </span><span class="p">. The bounds of the projection must be in </span>[<span class="s5">𝑉𝑚𝑖𝑛, 𝑉𝑚𝑎𝑥</span>]<span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 16pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><i>𝑟𝑡 </i>was adopted as <span class="s10">4 </span>and the discount factor <i>𝛾 </i>was set to <span class="s10">0</span><i>.</i><span class="s10">99</span>, the calculation done was</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">T</p><p style="text-indent: 0pt;text-align: left;"/><p class="s43" style="text-indent: 0pt;line-height: 10pt;text-align: left;">𝑉<span class="s45">𝑚𝑖𝑛</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s14" style="text-indent: 0pt;line-height: 13pt;text-align: left;">−<span class="s30">5</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 0pt;line-height: 79%;text-align: justify;">Following the example, they should be within the [-5,+5] range. It can be observed that the last four positions of <span class="s16">ˆ </span><i>𝑧 𝑗 </i>contained samples greater than <i>𝑉𝑚𝑎𝑥</i>; hence they were clipped and</p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">replaced by the maximum value. In the following, the variable <i>𝑏 𝑗 </i>is calculated using Equation</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">T</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">T <span class="s5">𝑖</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;line-height: 74%;text-align: justify;"><a href="#bookmark39" class="a">2.25</a><span class="p"> based on the previous  </span><span class="s16">ˆ</span><span class="s10"> </span>𝑧  <span class="p">array with </span><span class="s19">Δ</span>𝑧 <span class="s19">= </span><span class="s10">1</span>.<span class="s10">11</span><span class="p">.  This variable computes the real-valued index positions in which each value of  </span><span class="s16">ˆ</span><span class="s10"> </span>𝑧𝑖 <span class="p">is closest in respect to the support </span>𝑧𝑖<span class="p">. Lastly, </span>𝑙 <span class="p">and </span>𝑢</p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;">variables are calculated. The distribution <i>𝑙 </i>and <i>𝑢 </i>compute the integer neighboring indexes from</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) ( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) ( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">𝑏𝑖<span class="p">. </span>𝑙 <span class="p">contains the greatest integers less than or equal to its respective position on </span>𝑏 𝑗 <span class="p">, and </span>𝑢 <span class="p">holds the least integers greater than or equal to its respective position on </span>𝑏 𝑗 <span class="p">.  Lastly, </span>𝑚 <span class="p">is described as the projected distribution of </span>𝑍  𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗ </span><span class="p">.  The vector  </span>𝑝𝑖  𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗  </span><span class="p">holds the probability masses</span></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><span class="p">in respect to </span>𝑍 𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗ </span><span class="p">. The previous variables </span>𝑢<span class="p">, </span>𝑙 <span class="p">and </span>𝑏 𝑗 <span class="p">shifts the probability </span>𝑝𝑖 𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗ </span><span class="p">and</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">distributes to its neighbors. The projected distribution <i>𝑚 </i>is used as the target for the network.</p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark40" class="a">Equations 2.28 and </a>2.29 define the probability mass distribution.</p><p class="s5" style="padding-top: 13pt;padding-left: 175pt;text-indent: 0pt;text-align: left;">𝑚𝑙 <span class="s19">= </span>𝑚𝑙 <span class="s11">+ </span>𝑝𝑖 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗)(</span>𝑢 <span class="s11">− </span>𝑏𝑖<span class="s11">)</span>,                                           <span class="p">(2.28)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) T</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 17pt;padding-left: 16pt;text-indent: 157pt;text-align: justify;">𝑚𝑢 <span class="s19">= </span>𝑚𝑢 <span class="s11">+ </span>𝑝𝑖 <span class="s11">(</span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗)(</span>𝑏𝑖 <span class="s11">− </span>𝑙<span class="s11">)</span>,                                           <span class="p">(2.29) where </span>𝑝 𝑗  𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗  </span><span class="p">is the probability vector of  </span><span class="s16">ˆ</span><span class="s10"> </span>𝑧 𝑗 <span class="p">, </span>𝑢<span class="p">, </span>𝑏𝑖 <span class="p">are the same as the variables mentioned above and variable </span>𝑚 <span class="p">is the variable that holds probability masses function of the distribution values.</span></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Lastly, Cross-entropy is used as the loss function. It is a measure of the difference between two probability distributions. The rest follows the same architecture as the standard DQN algorithm. Cross-entropy is defined as</p><p class="s5" style="padding-top: 2pt;padding-bottom: 1pt;padding-left: 176pt;text-indent: 0pt;text-align: left;">𝐿 <span class="s11">(</span>𝜃<span class="s11">) </span><span class="s19">= </span><span class="s11">− </span><span class="s73">∑︁</span><span class="s19"> </span>𝑚𝑖<span class="p">log </span>𝑝𝑖 <span class="s11">(</span>𝑠, 𝑎<span class="s11">)</span>,                                            <span class="p">(2.30)</span></p><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝑖</p><p style="padding-left: 229pt;text-indent: 0pt;line-height: 9pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">L</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) ( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a name="bookmark23">where it is the sum defined as      of the multiplication of </a><i>𝑚𝑖 </i>the projected distribution vector of probability masses and  <i>𝑝𝑖  𝑠, 𝑎   </i>the probability vector of the distribution <i>𝑍  𝑠, 𝑎   </i>which is generated by inputting the state <i>𝑠 </i>taking action <i>𝑎</i>, the base of the used logarithm (log) was the Euler’s number <i>𝑒</i>.<a name="bookmark41">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l34"><ol id="l35"><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">DUELING NETWORK ARCHITECTURE</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark125" class="a">The dueling neural network architecture was proposed by Wang et al. </a>(2016). The authors’ intuition behind this architecture is that some states are more valuable than others; it is unnecessary to estimate the value of each action choice for non-important conditions. They illustrate this insight using the Atari2600’s Enduro racing game as an example. During the race, it is only worth knowing whether to move left or right when a collision is imminent. In other states, the agent should know which action to take, but in many other states, the choice of action has no repercussions on what happens. The dueling network architecture was proposed to learn which states are (or are not) valuable without understanding the effect of each action for each state. The architecture consists of two linear stream layers (fully connected) such that each has the capability of providing separate estimates of the value and advantage functions. Lastly, both streams are combined to produce a single <i>Q-value </i>for each action. To construct the architecture</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 92%;text-align: justify;"><span class="p">mentioned above. The state-value and advantage function concepts have to be explained. The state-value function </span>𝑉 <span class="s11">(</span>𝑠<span class="s11">) </span><span class="p">is given by:</span></p><p class="s5" style="padding-top: 10pt;padding-left: 16pt;text-indent: 120pt;line-height: 18pt;text-align: right;">𝑉𝜋<span class="p">(</span>𝑠<span class="s11">) </span><span class="s19">= </span><span class="p">E</span><span class="s11">[</span>𝑟𝑡 <span class="s11">+ </span>𝛾𝑟𝑡<span class="s11">+</span><span class="s10">1 </span><span class="s11">+ </span>𝛾<span class="s10">2</span>𝑟𝑡<span class="s11">+</span><span class="s10">2 </span><span class="s11">+ </span>...<span class="s11">|</span>𝑠𝑡 <span class="s19">= </span>𝑠, 𝜋<span class="s11">]</span>,                               <span class="p">(2.31) where E is the expectation symbol, </span>𝑟𝑡  <span class="p">is the reward provided to the agent at time step </span>𝑡<span class="p">, </span>𝑠𝑡  <span class="p">is</span></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">the state at time step t, <i>𝜋 </i>is the policy that the agent follows and <i>𝛾 </i>is the discount factor. This function is the expected return when it starts in state <i>𝑠 </i>and follows <i>𝜋 </i><a href="#bookmark29" class="a">after that. In other words, it measures how good it is to be in a particular state. This is useful because the agent can know if his current state is valuable or worthless regarding reaching his final goal. Usually, this function is confused with the state-action value function (Equation </a><a href="#bookmark41" class="a">2.9). However, as mentioned above, Equation </a>2.31 measures how good a state is compared to the others and follows the policy <i>𝜋</i><a href="#bookmark29" class="a">. It only provides information about the value of the state. Equation </a>2.9 measures how good an action is in a certain state and follows the policy <i>𝜋 </i>after that. It provides information about the quality of the action and measures the value of choosing a particular action when in this state; hence the name <i>Q-value</i>. The advantage function measures how much an action is a good or bad decision given a certain state and is given by:</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 12pt;padding-left: 16pt;text-indent: 150pt;line-height: 18pt;text-align: right;">𝐴𝜋 <span class="s11">(</span>𝑠, 𝑎<span class="s11">) </span><span class="s19">= </span><span class="p">E</span><span class="s11">[</span>𝑄𝜋 <span class="s11">(</span>𝑠, 𝑎<span class="s11">) − </span>𝑉𝜋 <span class="s11">(</span>𝑠<span class="s11">)]</span>,                                        <span class="p">(2.32) where E is the expectation symbol, </span>𝑄𝜋  𝑠, 𝑎   <span class="p">is the state-action value function under the policy</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">𝜋 <span class="p">and </span>𝑉𝜋 𝑠 <a href="#bookmark42" class="a">is the state-value function. The advantage function interpretation measures the advantage of selecting an action from a state. Figure </a><span class="p">2.10 shows the standard adopted feed-forward architecture commonly used in deep reinforcement learning and the dueling neural network architecture.</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) ( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark42" class="a">Figure </a>2.10(b) shows the dueling neural network architecture. The architecture shares a common module at the beginning composed of linear layers followed by rectified linear unit activation functions (RELUs). The network is separated into two streams to represent the state-value <i>𝑉 𝑠 </i>and advantage <i>𝐴 𝑠, 𝑎 </i>functions. At the end of the architecture, both streams are combined by an aggregating layer followed by a softmax activation function to produce the</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 16pt;text-align: justify;"><span class="p">probabilities over the estimated </span>𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s11">) </span><span class="p">state-action value function. On the other hand, Figure</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 117pt;text-indent: 0pt;text-align: left;"><span><img width="337" height="186" alt="image" src="documento[1970]/Image_018.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l36"><li><p class="s74" style="padding-left: 107pt;text-indent: -10pt;text-align: left;"><a name="bookmark42">Typical feed-forward neural network architecture adopted in deep reinforcement learning</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 33pt;text-indent: 0pt;text-align: left;"><span><img width="566" height="223" alt="image" src="documento[1970]/Image_019.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s74" style="padding-left: 170pt;text-indent: -11pt;text-align: left;">Dueling feed-forward neural network architecture</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 160pt;text-indent: 0pt;text-align: left;">Figure 2.10: Neural network architectures</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">2.10(a) shows the typical single-stream architecture adopted during the classical DQN algorithm. It consists of linear layers, followed by the RELU activation functions. A softmax activation function in the last linear layer normalizes the network’s output to a probability distribution over the state-action value function.</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><a href="#bookmark42" class="a">According to Figure </a>2.10(b), consider one stream of linear layers output a scalar</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) | | ( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">𝑉 𝑠<span class="s10">; </span>𝜃<span class="s10">; </span>𝛽 <span class="p">, and the remaining stream output a </span>𝐴 <span class="p">-scalar vector </span>𝐴 𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼 <span class="p">. </span>𝜃 <span class="p">denotes the parameters of the linear layers shared by both streams at the beginning of the network. </span>𝛼 <span class="p">and </span>𝛽 <a href="#bookmark41" class="a">are the parameters of the two streams of linear layers. To generate the state-action value function, the definition of the advantage given by Equation </a><span class="p">2.32 can be applied, resulting in the following</span></p><p class="s5" style="padding-top: 12pt;padding-left: 59pt;text-indent: 80pt;text-align: left;">𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼, 𝛽<span class="s11">) </span><span class="s19">= </span>𝑉 <span class="s11">(</span>𝑠<span class="s10">; </span>𝜃<span class="s10">; </span>𝛽<span class="s11">) + </span>𝐴<span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼<span class="s11">)</span>.                                <span class="p">(2.33)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) ( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 4pt;padding-left: 16pt;text-indent: 42pt;line-height: 85%;text-align: left;"><span class="p">However, since </span>𝑄 𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼, 𝛽 <a href="#bookmark43" class="a">is just a parameterized estimate of the actual state-action value function, Equation </a><span class="p">2.35 cannot fully recover </span>𝑉 𝑠<span class="s10">; </span>𝜃<span class="s10">; </span>𝛽 <span class="p">and </span>𝐴 𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼 <span class="p">by providing the</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 16pt;text-indent: 0pt;line-height: 15pt;text-align: left;">𝑄 𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼, 𝛽 <span class="p">function. To solve the previous issue, the authors force the advantage function</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">estimator to have zero advantage at the chosen action according to</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) ( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="text-indent: 0pt;line-height: 12pt;text-align: left;">𝑎<span class="s48">′</span><span class="s36"> </span><span class="s14">∈| </span>𝐴<span class="s14">|</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 2pt;padding-left: 17pt;text-indent: 68pt;line-height: 27pt;text-align: left;"><a name="bookmark24">𝑄 </a><span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼, 𝛽<span class="s11">) </span><span class="s19">= </span>𝑉 <span class="s11">(</span>𝑠<span class="s10">; </span>𝜃<span class="s10">; </span>𝛽<span class="s11">) + ( </span>𝐴<span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼<span class="s11">) − </span><span class="s10">max </span>𝐴<span class="s11">(</span>𝑠, 𝑎<span class="s11">′</span><span class="s10">; </span>𝜃, 𝛼<span class="s11">))</span>,              <span class="p">(2.34) forcing the advantage function estimator to be 0 in the best-evaluated action </span>𝑎<span class="s11">′</span><span class="p">, we have the</span><a name="bookmark43">&zwnj;</a></p><p class="s5" style="padding-left: 16pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Q-value <span class="p">function equals to the state-value function (</span>𝑄 𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼, 𝛽 <span class="s19">= </span>𝑉 𝑠<span class="s10">; </span>𝜃<span class="s10">; </span>𝛽 <span class="p">). This way, one</span></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">stream can measure the estimation of the value function while the other produces an estimate of the advantage function. Finally, the authors replace the <i>𝑚𝑎𝑥 </i>operator with an average by adding an alternative module. The following final equation was adopted to calculate the Q-function separating the advantage and state value functions</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">| <span class="s5">𝐴</span>|</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">𝑄 <span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼, 𝛽<span class="s11">) </span><span class="s19">= </span>𝑉 <span class="s11">(</span>𝑠<span class="s10">; </span>𝜃<span class="s10">; </span>𝛽<span class="s11">) + ( </span>𝐴<span class="s11">(</span>𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼<span class="s11">) − </span><u> </u><u> </u><span class="s58">1 </span><span class="s10"> </span><span class="s73">∑︁</span><span class="s19"> </span>𝐴<span class="s11">(</span>𝑠, 𝑎<span class="s11">′</span><span class="s10">; </span>𝜃, 𝛼<span class="s11">))</span>,            <span class="p">(2.35)</span></p><p class="s35" style="text-indent: 0pt;line-height: 69%;text-align: left;">𝑎<span class="s36">′</span></p><p style="padding-left: 324pt;text-indent: 0pt;line-height: 12pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 7pt;padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><span class="p">where </span>𝑠 <span class="p">is a state </span>𝑠<span class="p">, </span>𝑎 <span class="p">is an action </span>𝑎<span class="p">, </span>𝑉  𝑠<span class="s10">; </span>𝜃<span class="s10">; </span>𝛽   <span class="p">is the state-value function in state </span>𝑠 <span class="p">with respect to the parameters </span>𝜃 <span class="p">from the first shared linear layer of the network and </span>𝛽 <span class="p">are the parameters from the value-stream only.  </span>𝐴  𝑠, 𝑎<span class="s10">; </span>𝜃, 𝛼   <span class="p">is the advantage function of the action </span>𝑎 <span class="p">in state </span>𝑠 <span class="p">with respect to the parameters </span>𝜃 <span class="p">and </span>𝛼<span class="p">, which are the parameters from the Advantage stream branch</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="1" alt="image" src="documento[1970]/Image_020.png"/></span></p><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝐴</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 19%;text-align: left;"><span class="p">of the network.  </span><span class="s75">|</span><span class="s14"> </span><span class="s42">1</span><span class="s30"> </span><span class="s75">|</span><span class="s14"> </span><span class="s76">L</span><span class="s35">𝑎</span><span class="s36">′  </span>𝐴<span class="s11">(</span>𝑠, 𝑎<span class="s11">′</span><span class="s10">; </span>𝜃, 𝛼<span class="s11">) </span><span class="p">is the average of the advantage values pro</span><span class="s77">L</span><span class="p">duced in state</span></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑠 <span class="p">taking the best evaluated action </span>𝑎<span class="s11">′ </span><span class="p">with respect to the parameters </span>𝜃 <span class="p">and </span>𝛼<span class="p">. represents a</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">| |</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;line-height: 90%;text-align: justify;">sum of multiple terms; in this case, all advantage values are produced in state <i>𝑠 </i>concerning the best-evaluated action <i>𝑎</i><span class="s11">′</span>. Lastly, <i>𝐴 </i>is the absolute value of the number of evaluated advantage values.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">NOISY LINEAR LAYER</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;">In a typical reinforcement learning approach, the agent explores or exploits through a <i>𝜖 </i>-greedy heuristic. The hyperparameter <i>𝜖 </i>decreases throughout the training. If <i>𝜖 </i>&lt; random number, the agent chooses its action randomly, and if <i>𝜖 </i><a href="#bookmark124" class="a">&gt; random number, the agent acts &quot;greedily&quot; by choosing its activity according to its policy. Fortunato et al. </a>(2018) proposed to replace the conventional exploration <i>𝜖 </i>-greedy heuristic for a noisy linear layer that adds parametric noise to its weights where learned perturbations of the network weights are used to drive exploration. The authors show that the induced stochasticity of the agent’s policy can be used for enhanced investigation. The feature’s main idea is that a single change to the weights vector may influence a consistent state-dependent shift in policy over multiple time steps. A noise distribution samples the perturbations, and variance is a parameter that can be considered as the energy of the injected noise. These variance parameters are learned using gradients from the reinforcement learning loss function alongside the other parameters of the agent. Suppose a linear layer from a neural network with <i>𝑝 </i>inputs and <i>𝑞 </i>outputs given by</p><p class="s5" style="padding-top: 11pt;padding-left: 216pt;text-indent: 0pt;text-align: left;">𝑦 <span class="s19">= </span>𝑤𝑥 <span class="s11">+ </span>𝑏, <span class="p">(2.36)</span></p><p class="s5" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 78%;text-align: left;"><span class="p">where </span>𝑥 <span class="s11">∈ </span><span class="p">R</span>𝑝 <span class="p">is the layer’s input, </span>𝑤 <span class="s11">∈ </span><span class="p">R</span>𝑞<span class="s11">×</span>𝑝 <span class="p">is the neural network’s weight matrix, and neural network’s </span>𝑏 <span class="s11">∈ </span><span class="p">R</span>𝑏 <span class="p">is the bias. Its corresponding noisy linear layer is defined by</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">∈ ∈ ∈</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">∈ ∈ ∈</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 9pt;padding-left: 16pt;text-indent: 135pt;line-height: 21pt;text-align: left;">𝑦 <span class="s19">= </span><span class="s11">( </span>𝜇𝑤 <span class="s11">+ </span>𝜎𝑤 <span class="s11">⊙ </span>𝜀𝑤<span class="s11">)</span>𝑥 <span class="s11">+ </span>𝜇𝑏 <span class="s11">+ </span>𝜎𝑏 <span class="s11">⊙ </span>𝜀𝑏, <span class="p">(2.37) where </span>𝜇𝑤 <span class="p">R</span>𝑞<span class="s11">×</span>𝑝 <span class="p">and </span>𝜎𝑤 <span class="p">R</span>𝑞<span class="s11">×</span>𝑝 <span class="p">are the learnable parameters from the weight matrix, </span>𝜇𝑏 <span class="p">R</span>𝑞</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><span class="p">and </span>𝜎𝑏 <span class="p">R</span>𝑞 <span class="p">are learnable parameters for the neural network’s bias. </span>𝜀𝑤 <span class="p">R</span>𝑞<span class="s11">×</span>𝑝 <span class="p">and </span>𝜀𝑏 <span class="p">R</span>𝑞 <span class="p">are</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">noise random variables from the weight matrix and bias respectively. The authors propose two</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝑖, 𝑗</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a name="bookmark25">distribution approaches to generate the noisy random variables for the parameters </a><i>𝜀𝑤 </i>and <i>𝜀𝑏</i>: Independent Gaussian noise and Factorized Gaussian noise. The first option uses an independent Gaussian noise entry per weight, and the second uses an independent noise per output and another independent noise per input. This master dissertation uses the factorized Gaussian noise approach to reduce the compute time of random number generation. <i>𝑝 </i>unit Gaussian variables <i>𝜀𝑖 </i>for noise of the inputs and q unit Gaussian variables <i>𝜀 𝑗 </i>for noise of the outputs can be used to factorize<a name="bookmark44">&zwnj;</a></p><p class="s5" style="text-indent: 0pt;line-height: 12pt;text-align: left;">𝜀</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="padding-left: 15pt;text-indent: 0pt;line-height: 8pt;text-align: right;">𝑤</p><p class="s25" style="padding-left: 15pt;text-indent: 0pt;line-height: 9pt;text-align: right;">𝑖, 𝑗</p><p class="s5" style="padding-left: 2pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><span class="p">(</span>𝑝 <span class="s11">+ </span>𝑞 <span class="p">unit Gaussian variables in total). Each </span>𝜀𝑤</p><p style="padding-left: 4pt;text-indent: 0pt;line-height: 14pt;text-align: left;">and <i>𝜀 𝑗 </i>units can be defined as</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="text-indent: 0pt;line-height: 12pt;text-align: left;">𝜀</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;line-height: 9pt;text-align: right;">𝑤</p><p class="s25" style="padding-left: 15pt;text-indent: 0pt;line-height: 9pt;text-align: right;">𝑖, 𝑗</p><p class="s5" style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><span class="s19">=  </span>𝑓 <span class="s11">(</span>𝜀𝑖<span class="s11">) </span>𝑓 <span class="s11">(</span>𝜀 𝑗 <span class="s11">)</span>,                                                    <span class="p">(2.38)</span></p><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝑗</p><p style="text-indent: 0pt;text-align: left;"/><p class="s78" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">√<span class="s19">︁</span><span class="s11">(  )                                                       (  )         (  )   |  |</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 16pt;text-indent: 0pt;text-align: left;">𝜀𝑏 <span class="s19">= </span>𝑓 <span class="s11">(</span>𝜀 𝑗 <span class="s11">)</span>, <span class="p">(2.39)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 352pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="17" height="1" alt="image" src="documento[1970]/Image_021.png"/></span></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;"><span class="p">where </span>𝑓 𝑥 <span class="p">is a real-valued function and adopted as </span>𝑓 𝑥 <span class="s19">= </span>𝑠𝑔𝑛 𝑥 𝑥 <span class="p">, </span>𝜀 <span class="p">corresponds to the Gaussian variables for to create noise for the inputs and </span>𝜀 𝑗 <a href="#bookmark44" class="a">the Gaussian variables to apply noise to the outputs. Figure </a><span class="p">2.11 shows the schematic of the noisy linear layer.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span><img width="229" height="215" alt="image" src="documento[1970]/Image_022.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 160pt;text-indent: 0pt;text-align: left;">Figure 2.11: Linear noisy layer schematic</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">∈</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">⊙</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;">where <i>𝑥 </i>R<i>𝑝 </i>is the layer’s input, <i>𝜀 </i>is a vector of zero-mean noise with fixed statistics. <i>𝜀𝑤 </i>are the Gaussian random variables to generate noise for the weights matrix, <i>𝜀𝑏 </i>are the Gaussian random variables to generate noise for the network bias. is the element-wise multiplication, <i>𝜇𝑤 </i>and</p><p class="s5" style="padding-left: 16pt;text-indent: 0pt;text-align: justify;">𝜎𝑤<span class="p">is the mean and standard deviation of the distribution regarding the weights matrix and </span>𝜇𝑏 <span class="p">and </span>𝜎𝑏<span class="p">is the mean and standard deviation of the distribution regarding the bias. </span>𝑤 <span class="p">is the matrix weights and </span>𝑏 <span class="p">the bias of the neural network and </span>𝑦 <span class="p">the output.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">PRIORITIZED EXPERIENCE REPLAY</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark125" class="a">Prioritized experience replay was proposed to enhance the use of replay memory for learning. Schaul et al. </a>(2016) proposed to modify the sampling mechanism of the standard DQN algorithm, which samples transitions uniformly at random. The main idea of the technique is to prioritize transitions that maximally reduce the global loss in its current state that improves on uniform random replay. The authors argue that one idealized criterion to prioritize experiences could be the amount the agent can learn from experiences in their current state, in other words, the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;"><a name="bookmark26">expected learning progress. However, this measure is not directly accessible. Attempting to access this measure indirectly, the authors adopt the magnitude of the transition’s temporal-difference error (TD-error) </a><i>𝛿 </i>that stipulates how unexpected the transition is. Specifically, how far the value is its next-step bootstrap estimate. This particular criterion is helpful for DQN since the algorithm already calculates TD error and updates the parameters in proportion to <i>𝛿</i>. The authors’ first attempt is to apply a ’greed TD error prioritization’ to Q-learning. The algorithm stores the last TD error along with each experience in the replay memory. The transition with the most significant absolute TD error is replayed from the buffer, and the Q-learning updates the Q-function proportionally to the TD error. When new transitions arrive without a known TD error, they have maximum priority to ensure they are visited once. However, transitions with low TD error on the first visit may only be replayed for a short time, making the algorithm more prone to overfitting since high error transitions are replayed more frequently, resulting in a lack of diversity. A stochastic sampling method was proposed to overcome this issue. It alternates between pure greedy prioritization and uniform random sampling. The probability of sampling a transition <i>𝑖 </i>is given by<a name="bookmark45">&zwnj;</a></p><p class="s12" style="padding-top: 7pt;padding-left: 45pt;text-indent: 0pt;line-height: 4pt;text-align: center;">𝑝<span class="s25">𝛼</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="41" height="1" alt="image" src="documento[1970]/Image_023.png"/></span></p><p class="s5" style="padding-top: 4pt;padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑃<span class="s11">(</span>𝑖<span class="s11">) </span><span class="s19">= L</span></p><p class="s25" style="padding-top: 4pt;padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">𝑖</p><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝑘</p><p style="text-indent: 0pt;text-align: left;"/><p class="s79" style="padding-top: 1pt;text-indent: 0pt;line-height: 69%;text-align: left;">𝑘 <span class="s12">𝑝</span><span class="s25">𝛼</span></p><p class="s5" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">, <span class="p">(2.40)</span></p><p class="s25" style="text-indent: 0pt;line-height: 9pt;text-align: left;">𝑘</p><p style="text-indent: 0pt;text-align: left;"/><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">L</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 17pt;text-indent: 0pt;line-height: 85%;text-align: left;">where <i>𝑝𝑖  &gt; </i><span class="s10">0 </span>corresponds to the priority of experience <i>𝑖</i>.  <i>𝛼 </i>how much prioritization is used. Suppose <i>𝛼 </i><span class="s19">= </span><span class="s10">0</span>, the probability of sampling the transition corresponds to the uniform case.     <i>𝑘 𝑝𝛼</i></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;">is the sum of all experiences priorities in the <i>𝑘 </i>mini-batch.</p><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;">The authors first considered a direct variant adopting the proportional prioritization</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">| | +</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )<span class="s5">𝑖</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">𝑝𝑖 <span class="s19">= </span>𝛿𝑖 𝜖 <span class="p">, where </span>𝜖 <span class="p">is a positive constant that prevents the edge-case of transitions not being revisited once their error is zero. The second variant is an indirect, rank-based prioritization</span></p><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;">.</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;line-height: 32%;text-align: left;"><i>𝑝  </i><span class="s19">= </span><u>    </u><span class="s58">1   </span><span class="s10"> </span>, where <i>𝑟𝑎𝑛𝑘  𝑖   </i>is the rank of transition <i>𝑖 </i>when the replay memory is sorted according <span class="s80">to</span> <span class="s11">|</span><i>𝛿</i><span class="s11">|</span><span class="s25">𝑟𝑎𝑛𝑘 </span><span class="s14">(</span><span class="s25">𝑖</span><span class="s14">)</span></p><p style="padding-top: 3pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;">The estimation of the expected value with stochastic updates assumes that they are done corresponding to the same distribution as its expectation. However, Prioritized replay introduces bias because it changes this distribution and the solution to which the estimates will converge. To tackle this problem, the authors correct this bias by using importance-sampling (IS) weights given by</p><p class="s58" style="padding-top: 3pt;padding-left: 45pt;text-indent: 0pt;line-height: 16pt;text-align: center;"><span class="s64">(</span><span class="s81"> </span>1<span class="s10">    </span>   1 <span class="s10"> </span><span class="s64">)</span><span class="s19"> </span><span class="s82">𝛽</span></p><p class="s5" style="padding-top: 1pt;padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑤𝑖 <span class="s19">=</span></p><p class="s5" style="padding-left: 8pt;text-indent: 0pt;line-height: 26pt;text-align: left;">𝑁  <span class="s83">×</span><span class="s11"> </span>𝑃<span class="s11">(</span>𝑖<span class="s11">)</span></p><p class="s5" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">, <span class="p">(2.41)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">where <i>𝑃  𝑖   </i>is the probability of sampling a transition <i>𝑖</i>, <i>𝛽 </i>is a compensation factor. The authors define a schedule on  <i>𝛽 </i>exponent that reaches one only at the end of the learning exploiting the flexibility of annealing the amount of importance-sampling correction over time.   <i>𝛽  </i>is linearly annealed from its initial value <i>𝛽</i><span class="s10">0 </span>to 1, which fully compensates for the non-uniform probabilities <i>𝑃  𝑖  </i>.  It is worth mentioning that by increasing both <i>𝛼 </i>and <i>𝛽 </i>simultaneously, the prioritization sampling gets more aggressive while correcting for it more strongly.  <i>𝑁 </i>is the size of the mini-batch which stores experiences.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 46pt;text-indent: -29pt;text-align: left;">HINDSIGHT EXPERIENCE REPLAY</p></li></ol></ol><p class="s5" style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><span class="p">In  reinforcement  learning,  the  agent’s  goal  is  formalized  regarding  reward  signal feedback generated from the environment to the agent.  More precisely, the agent’s purpose in  deep  reinforcement  learning  is  to  maximize  the  total  reward  it  receives  by  constructing a  single  function  approximator </span>𝑉 <span class="s11">(</span>𝑠<span class="s10">; </span>𝜃<span class="s11">) </span><span class="p">that  estimates  the  long-term  reward  from  any  state</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">∈ ∈</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">|| + ||</p><p style="text-indent: 0pt;text-align: left;"/><p class="s39" style="text-indent: 0pt;line-height: 76%;text-align: left;">+ + <span class="s11">||</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">||</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">𝑠<span class="p">,  using parameters  </span>𝜃<a href="#bookmark125" class="a">.   Extending this approach,  Schaul et al. (2015)</a><span class="p"> introduced universal value function approximators (UVFAs) </span>𝑉  𝑠, 𝑔, 𝜃   <span class="p">that generalizes over states  </span>𝑠  <span class="p">and goals </span>𝑔 <span class="p">concerning </span>𝜃 <a href="#bookmark124" class="a">parameters. The authors show how to apply UVFAs for supervised learning and RL, demonstrating that a UVFA can generalize to previously unseen goals.  A sparse reward refers to a reward function that is zero in most of its domain and only gives feedback occasionally. If the environment has a sparse reward function, the agent won’t get any feedback about whether the instantaneous actions it takes are good or bad.   Based on the UVFAs mentioned above, Andrychowicz et al. (2017)</a><span class="p"> proposed the hindsight experience replay (HER) to deal with sparse rewards. The main idea of HER is to replay each episode with a different goal than the one the agent was trying to achieve, generalizing not just over states </span>𝑠     𝑆 <span class="p">but also over goals </span>𝑔     𝐺<span class="p">. In the algorithm’s simplest form, the goal </span>𝑔<span class="p">, achieved in the final state of the episode, is concatenated with the current state </span>𝑠𝑡 <span class="p">(</span>𝑠𝑡   𝑔<span class="p">) and serves as input to the agent. Then, the action </span>𝑎𝑡 <span class="p">is executed, and it observes the next state </span>𝑠𝑡  <span class="s10">1</span><span class="p">, and the next state is also concatenated with the goal </span>𝑔 <span class="p">(</span>𝑠𝑡  <span class="s10">1  </span>𝑔<span class="p">) and stored as an experience tuple in the replay buffer (</span>𝑠𝑡   𝑔, 𝑎𝑡, 𝑟𝑡, 𝑠𝑡  <span class="s10">1  </span>𝑔<span class="p">).  The agent can also concatenate additional goals. Lastly, the authors explored different strategies for choosing goals with HER. So far, the only goals used for replay were the ones corresponding to the final state of the environment. This strategy is called final. Apart from it, the sampling also can consider the future, episode, and random strategies.  The first strategy replays with </span>𝑘 <span class="p">random states, which come from future states from the same episode as the transition being replayed.  The episode strategy replays </span>𝑘 <span class="p">random states from the same episode as the replayed transition.  Lastly, the random approach replays with </span>𝑘 <span class="p">random states encountered during the training procedure. These strategies have a hyper-parameter </span>𝑘<span class="p">, which controls the ratio of HER data to data from normal experience replay in the replay buffer.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h1 style="padding-top: 7pt;padding-left: 31pt;text-indent: -14pt;text-align: left;"><a name="bookmark46">RELATED WORKS</a><a name="bookmark47">&zwnj;</a><a name="bookmark53">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l37"><li><p style="padding-top: 11pt;padding-left: 40pt;text-indent: -23pt;text-align: left;">SYSTEMATIC REVIEW OF THE LITERATURE</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">This section performs a Systematic Review of the Literature (SLR) about reinforcement learning applied to trading tasks. Aiming to have an overview of the existing literature on the topic, reinforcement learning, deep learning, and deep reinforcement learning papers related to the previously mentioned topic were collected, analyzed, and discussed. In addition, the following literature review explored Scopus, Web of Science, and Google Scholar academic research databases, seeking to retrieve current relevant information about the topic, limiting its search space from 2017 to 2023.</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 59pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">Adopting SRL<i>𝑚𝑒𝑡 ℎ𝑜𝑑𝑜𝑙𝑜𝑔𝑦𝑠𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡𝑔𝑢𝑖𝑑𝑒𝑙𝑖𝑛𝑒𝑠, 𝑡 ℎ𝑒𝑆𝐿𝑅𝑤𝑎𝑠𝑑𝑖𝑣𝑖𝑑𝑒𝑑𝑖𝑛𝑡𝑜𝑎 𝑝𝑟𝑖𝑚𝑎𝑟 𝑦𝑡 ℎ𝑟 𝑒𝑒</i></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) (</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 16pt;text-align: justify;">𝑠𝑡𝑎𝑔𝑒 𝑝𝑟𝑜𝑐𝑒𝑠𝑠 <span class="s10">: </span>𝑝𝑙𝑎𝑛𝑛𝑖𝑛𝑔  𝑑𝑒 𝑓 𝑖𝑛𝑖𝑡𝑖𝑜𝑛𝑜 𝑓 𝑡 ℎ𝑒𝑟 𝑒𝑠𝑒𝑎𝑟𝑐ℎ𝑞𝑢𝑒𝑠𝑡𝑖𝑜𝑛𝑠  , 𝑐𝑜𝑛𝑑𝑢𝑐𝑡𝑖𝑜𝑛  𝑖𝑛𝑐𝑙𝑢𝑠𝑖𝑜𝑛𝑎𝑛𝑑𝑒𝑥𝑐𝑙𝑢𝑠𝑖𝑜𝑛𝑐𝑟</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">This systematic literature review addressed the following research questions:</p><ul id="l38"><li><p style="padding-top: 10pt;padding-left: 59pt;text-indent: -10pt;text-align: left;">RQ1. What are the main reinforcement learning methods applied to quantitative trading?</p></li><li><p style="padding-top: 7pt;padding-left: 59pt;text-indent: -10pt;text-align: left;">RQ2. What are the main performance metrics used on these algorithms?</p></li><li><p style="padding-top: 7pt;padding-left: 59pt;text-indent: -10pt;text-align: left;">RQ3. What are the main markets to which the models have been applied?</p></li></ul><p style="padding-top: 11pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">Once the research questions were established, the planning stage was completed and followed by conduction. In this step, the following inclusion criteria were adopted:</p><ol id="l39"><li><p style="padding-top: 11pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">Primary Studies Publications;</p></li><li><p style="padding-top: 10pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">Published within the last five years;</p></li><li><p style="padding-top: 10pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">English Language Publications;</p></li><li><p style="padding-top: 10pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">Journal, conference, and pre-print papers;</p></li><li><p style="padding-top: 10pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">Publications relevant to deep learning, reinforcement learning, and deep reinforcement learning topics related to quantitative trading tasks.</p></li></ol><p style="padding-top: 11pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">Pre-prints were also included in this review, seeking to achieve more papers. Only English publications were selected due to the belief that most relevant studies would only be published in English.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The exclusion criteria were set to identify publications, which were eventually removed from the research study. The following criteria are adopted:</p><ol id="l40"><li><p style="padding-top: 11pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">Secondary Studies Publications;</p></li><li><p style="padding-top: 10pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">Technical reports;</p></li><li><p style="padding-top: 10pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">Non-English language paper publications;</p></li><li><p style="padding-top: 10pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">Publications that do not approach reinforcement learning or deep reinforcement learning methods related to quantitative trading tasks;</p></li><li><p style="padding-top: 9pt;padding-left: 59pt;text-indent: -19pt;text-align: left;">Non-downloadable publications on their respective research database.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 59pt;text-indent: 0pt;text-align: left;"><a name="bookmark48">In sequence, through capes</a><i>𝑐𝑖𝑡, 𝑏𝑜𝑡 ℎ𝑆𝑐𝑜 𝑝𝑢𝑠𝑎𝑛𝑑𝑊 𝑒𝑏𝑜 𝑓 𝑆𝑐𝑖𝑒𝑛𝑐𝑒𝑟 𝑒𝑠𝑒𝑎𝑟𝑐ℎ𝑑𝑎𝑡𝑎𝑏𝑎𝑠𝑒𝑠𝑤𝑒𝑟 𝑒𝑎𝑐𝑐𝑒𝑠𝑠𝑒</i><a name="bookmark54">&zwnj;</a></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">𝑏𝑎𝑠𝑒𝑑𝑠𝑒𝑎𝑟𝑐ℎ𝑞𝑢𝑒𝑟 𝑦𝑤𝑎𝑠 𝑝𝑢𝑡𝑖𝑛𝑡𝑜𝑡 ℎ𝑒𝑖𝑟𝑟 𝑒𝑠 𝑝𝑒𝑐𝑡𝑖𝑣𝑒𝑎𝑑𝑣𝑎𝑛𝑐𝑒𝑑𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑠𝑒𝑎𝑟𝑐ℎ𝑡𝑎𝑏.𝑇 ℎ𝑒𝑠𝑒𝑑𝑎𝑡𝑎𝑏𝑎𝑠𝑒𝑠𝑐𝑜𝑛𝑡𝑎𝑖</p><p style="padding-top: 7pt;padding-left: 21pt;text-indent: 0pt;text-align: center;">(&quot;Finance Trading&quot; OR &quot;Quantitative Trading&quot; OR &quot;Algorithmic Trading&quot; OR &quot;Trading&quot; OR &quot;Strategy Trading&quot;)) AND ((&quot;Reinforcement Learning&quot; OR &quot;Deep Learning&quot; OR &quot;Deep Reinforcement Learning&quot;)</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark55" class="a">In the first stage, all fields were included (no date range or language limitations). A total of number 808 publications was retrieved. After that, papers were selected accordingly to the inclusion criteria, and the search space was constrained to article title, abstract, and keywords only. A total number of 376 publications were selected. Then, the selected papers from stage 2 were evaluated accordingly to the exclusion criteria. A total of 1 paper was not English written, 47 were not published between 2017-2022, and 42 were duplicated. Analyzing their respective title, 44 were off the review’s scope. Some of them included deep reinforcement learning methods; however, their application was not in trading tasks. In the course of stage 4, papers were selected and removed based on their respective title, abstracts, and conclusion analysis, resulting in 188 papers. Lastly, papers were excluded accordingly to their complete analysis. 167 publications were selected. Figure </a>3.1 shows an overview of the systematic literature review methodology pipeline, containing the number of publications at each stage.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Adopting kitchenham2007guidelines for performing data synthesis, general information such as title, authors, and source, as well as specific ones, such as performance metrics and benchmark algorithms, were retrieved.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark56" class="a">Figure </a><a href="#bookmark57" class="a">3.2 shows the amount of RL-related publications applied to trading tasks per year between 2017 - 2022. 93 journal papers, 65 conferences, and 9 pre-print publications were selected, having the majority published in 2021. The bar chart indicates an ascending slope during the years, meaning the following year has a more significant publication amount than its precedent. Exceptionally in 2022, it is impossible to confirm this fact because the review has been conducted during its course. As previously mentioned, containing the more significant amount of publications on it, 43.0% of journals, 32.3% conferences, and 66.7% pre-print publications were published in 2021. Figure </a>3.3 shows the year-based publication percentage in periodicals, conferences, and pre-prints, respectively.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark57" class="a">Figure </a><a href="#bookmark58" class="a">3.4 shows the number of RL-related publications retrieved by research databases separately: Elsevier, Institute of Electrical and Electronics Engineers (IEEE), Xplore, Springer, Hindawi, and ArXiv, among others. Elsevier research database had the most significant contribution with 39 papers, followed by IEEE Xplore and Multidisciplinary Digital Publishing Institute (MDPI), having 30 and 26 publications, respectively. Figure </a>3.5 shows the total amount of RL-related works by publication types, concluding that 55.7% are periodical, 38.9% conference, and 5.4% pre-print publications.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l41"><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Periodical Publications</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark59" class="a">Table </a>3.1 shows the prominent periodical publications responsible for most works on reinforcement learning related to trading tasks. Both had an absolute frequency of 9 publications and a 9.47% frequency percentage each. IEEE Access had the most significant number of publications achieving ten works and an 11% frequency percentage, followed by Lecture Notes in Computer Science and Expert Systems with Applications.</p><p class="s5" style="padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><span class="p">DRL</span><span class="s10">1</span>𝑖𝑛𝑡𝑟𝑜𝑑𝑢𝑐𝑒𝑑𝑎𝑛𝑜𝑣𝑒𝑙𝑟 𝑒𝑐𝑢𝑟𝑟 𝑒𝑛𝑡𝑑𝑒𝑒 𝑝𝑛𝑒𝑢𝑟𝑎𝑙𝑛𝑒𝑡𝑤𝑜𝑟 𝑘 𝑓 𝑜𝑟𝑟 𝑒𝑎𝑙<span class="s11">−</span>𝑡𝑖𝑚𝑒 𝑓 𝑖𝑛𝑎𝑛𝑐𝑖𝑎𝑙𝑠𝑖𝑔𝑛𝑎𝑙𝑟 𝑒 𝑝𝑟 𝑒𝑠𝑒</p><p class="s5" style="padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><span class="p">DRL</span><span class="s10">2</span>𝑒𝑥𝑡𝑒𝑛𝑑𝑒𝑑𝑏𝑜𝑡 ℎ𝑣𝑎𝑙𝑢𝑒<span class="s11">−</span>𝑏𝑎𝑠𝑒𝑑𝐷𝑄𝑁 𝑎𝑛𝑑𝑎𝑐𝑡𝑜𝑟<span class="s11">−</span>𝑐𝑟𝑖𝑡𝑖𝑐 𝐴<span class="s10">3</span>𝐶𝑑𝑒𝑒 𝑝𝑟 𝑒𝑖𝑛 𝑓 𝑜𝑟𝑐𝑒𝑚𝑒𝑛𝑡𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑎𝑙𝑔𝑜</p><p class="s5" style="padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: right;">𝑡𝑒𝑟𝑚𝑚𝑒𝑚𝑜𝑟 𝑦 <span class="s11">(</span>𝐿𝑆𝑇 𝑀<span class="s11">)</span>𝑚𝑜𝑑𝑢𝑙𝑒𝑡𝑜𝑐𝑎 𝑝𝑡𝑢𝑟 𝑒𝑡𝑒𝑚 𝑝𝑜𝑟𝑎𝑙 𝑝𝑎𝑡𝑡𝑒𝑟𝑛𝑠𝑏𝑎𝑠𝑒𝑑𝑜𝑛𝑚𝑎𝑟 𝑘 𝑒𝑡𝑜𝑏𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛𝑠.𝑆𝑡𝑎𝑐𝑘 𝑒𝑑𝑑𝑒𝑛𝑜</p><p class="s5" style="padding-left: 15pt;text-indent: 0pt;line-height: 16pt;text-align: right;">𝑒𝑚𝑏𝑒𝑑𝑑𝑒𝑑𝑎𝑐𝑡𝑖𝑜𝑛𝑠 𝑝𝑎𝑐𝑒𝑖𝑠 𝑝𝑟 𝑒𝑠𝑒𝑛𝑡𝑒𝑑, 𝑎𝑙𝑙𝑜𝑤𝑖𝑛𝑔𝑡 ℎ𝑒𝑎𝑔𝑒𝑛𝑡𝑡𝑜𝑙𝑒𝑎𝑟𝑛𝑡𝑜𝑐𝑜𝑛𝑡𝑟𝑜𝑙 𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑠<span class="s11">(</span>ℎ𝑜𝑙𝑑𝑖𝑛𝑔𝑚𝑜𝑟 𝑒 𝑝𝑜𝑠𝑖𝑡𝑖</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="630" height="726" alt="image" src="documento[1970]/Image_024.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 129pt;text-indent: 0pt;text-align: left;"><a name="bookmark55">Figure 3.1: Systematic literature review process overview</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 17pt;text-align: left;">DRL<span class="s10">3 </span><i>𝑝𝑟𝑜 𝑝𝑜𝑠𝑒𝑑𝑎𝑐𝑜𝑢 𝑝𝑙𝑒𝑜 𝑓 𝑎𝑑𝑎 𝑝𝑡𝑖𝑣𝑒𝑠𝑡𝑜𝑐𝑘𝑡𝑟𝑎𝑑𝑖𝑛𝑔𝑠𝑡𝑟𝑎𝑡𝑒𝑔𝑖𝑒𝑠, 𝑖𝑛𝑐𝑜𝑟 𝑝𝑜𝑟𝑎𝑡𝑖𝑛𝑔𝑔𝑎𝑡𝑒𝑑𝑟 𝑒𝑐𝑢𝑟𝑟 𝑒𝑛𝑡𝑢</i></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">− −</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑏𝑎𝑙𝑎𝑛𝑐𝑒𝑣𝑜𝑙𝑢𝑚𝑒𝑖𝑛𝑑𝑖𝑐𝑎𝑡𝑜𝑟 𝑠, 𝑡 ℎ𝑒𝑎𝑔𝑒𝑛𝑡𝑠𝑎𝑟 𝑒𝑐𝑜𝑚 𝑝𝑎𝑟 𝑒𝑑𝑡𝑜𝑡 ℎ𝑒𝑡𝑢𝑟𝑡𝑙𝑒𝑡𝑟𝑎𝑑𝑖𝑛𝑔𝑎𝑛𝑑𝑡 ℎ𝑒𝑠𝑡𝑎𝑡𝑒    𝑜 𝑓</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">𝑡 ℎ𝑒   𝑎𝑟𝑡𝑑𝑖𝑟 𝑒𝑐𝑡𝑟 𝑒𝑖𝑛 𝑓 𝑜𝑟𝑐𝑒𝑚𝑒𝑛𝑡𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑠𝑡𝑟𝑎𝑡𝑒𝑔𝑖𝑒𝑠.𝑇 ℎ𝑒 𝑝𝑟 𝑒𝑣𝑖𝑜𝑢𝑠𝑐𝑜𝑚 𝑝𝑎𝑟𝑖𝑠𝑜𝑛𝑠ℎ𝑜𝑤𝑠𝑎𝑛𝑖𝑛𝑐𝑟 𝑒𝑎𝑠𝑒𝑖𝑛𝑠𝑡𝑎𝑏𝑖𝑙𝑖</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">𝑏𝑎𝑠𝑒𝑑𝑠𝑡𝑟𝑎𝑡𝑒𝑔𝑖𝑒𝑠.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 56pt;text-indent: 0pt;text-align: left;"><span><img width="465" height="349" alt="image" src="documento[1970]/Image_025.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 105pt;text-indent: 0pt;text-align: left;"><a name="bookmark49">Figure 3.2: Publications per year of quantitative trading related works</a><a name="bookmark56">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">− −</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 59pt;text-indent: 0pt;line-height: 17pt;text-align: left;">DRL<span class="s10">5</span><i>𝑖𝑛𝑡𝑟𝑜𝑑𝑢𝑐𝑒𝑑𝑎𝐷𝑄𝑁          𝑖𝑛𝑠 𝑝𝑖𝑟 𝑒𝑑𝑎𝑙𝑔𝑜𝑟𝑖𝑡 ℎ𝑚𝑑𝑒𝑛𝑜𝑚𝑖𝑛𝑎𝑡𝑒𝑑𝑇𝑟𝑎𝑑𝑖𝑛𝑔𝐷𝑒𝑒 𝑝𝑄</i></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑁 𝑒𝑡𝑤𝑜𝑟 𝑘  𝑇 𝐷𝑄𝑁  𝑡 ℎ𝑎𝑡𝑐𝑜𝑛𝑡𝑎𝑖𝑛𝑠𝑎𝑚𝑜𝑑𝑖 𝑓 𝑖𝑒𝑑𝑣𝑒𝑟 𝑠𝑖𝑜𝑛𝑜 𝑓 𝑡 ℎ𝑒𝑠𝑡𝑎𝑛𝑑𝑎𝑟 𝑑𝐷𝑄𝑁, 𝑖𝑛𝑐𝑙𝑢𝑑𝑖𝑛𝑔𝑡 ℎ𝑒𝐷𝑜𝑢𝑏𝑙𝑒𝐷𝑄𝑁𝑎𝑙𝑔</p><p class="s5" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝑓 𝑜𝑟𝑤𝑎𝑟 𝑑𝑛𝑒𝑡𝑤𝑜𝑟 𝑘 𝑎𝑟𝑐ℎ𝑖𝑡𝑒𝑐𝑡𝑢𝑟 𝑒, 𝐴𝑑𝑎𝑚𝑜 𝑝𝑡𝑖𝑚𝑖𝑧𝑒𝑟, 𝑎𝑛𝑑𝑡 ℎ𝑒𝑢𝑠𝑒𝑜 𝑓 𝐻𝑢𝑏𝑒𝑟𝑙𝑜𝑠𝑠.𝑇𝑟𝑎𝑖𝑛𝑖𝑛𝑔𝑖𝑛𝑑𝑎𝑖𝑙 𝑦ℎ𝑖𝑠𝑡𝑜𝑟𝑖𝑐𝑎𝑙𝑑</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">− −</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 59pt;text-indent: 0pt;line-height: 15pt;text-align: left;">DRL<span class="s10">4 </span><i>𝑝𝑟 𝑒𝑠𝑒𝑛𝑡𝑒𝑑𝑎𝑚𝑢𝑙𝑡𝑖                𝐷𝑄𝑁𝑎𝑙𝑔𝑜𝑟𝑖𝑡 ℎ𝑚𝑡ℎ𝑎𝑡𝑖𝑠𝑎 𝑝 𝑝𝑙𝑖𝑒𝑑𝑡𝑜𝑠𝑒𝑣𝑒𝑟𝑎𝑙𝑟 𝑒𝑎𝑙</i></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 15pt;text-align: left;">𝑤𝑜𝑟𝑙𝑑𝑡𝑟𝑎𝑑𝑖𝑛𝑔𝑠𝑐𝑒𝑛𝑎𝑟𝑖𝑜𝑠, 𝑠𝑢𝑐ℎ𝑎𝑠𝑡 ℎ𝑒𝑆<span class="s10">&amp;</span>𝑃<span class="s10">500 </span>𝑓 𝑢𝑡𝑢𝑟 𝑒𝑚𝑎𝑟 𝑘 𝑒𝑡𝑎𝑛𝑑𝑡 ℎ𝑒𝐽.𝑃.𝑀𝑜𝑟𝑔𝑎𝑛.𝑇 ℎ𝑒𝑎𝑢𝑡 ℎ𝑜𝑟𝑖𝑛𝑐𝑙𝑢𝑑𝑒𝑠𝑎 𝑝</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑝𝑟𝑜𝑐𝑒𝑠𝑠𝑖𝑛𝑔𝑠𝑡𝑒 𝑝𝑏𝑎𝑠𝑒𝑑𝑜𝑛𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑛𝑔𝑚𝑒𝑡𝑎   𝑓 𝑒𝑎𝑡𝑢𝑟 𝑒𝑠𝑎 𝑓 𝑡𝑒𝑟𝑐𝑜𝑛𝑣𝑒𝑟𝑡𝑖𝑛𝑔 𝑝𝑟𝑖𝑐𝑒𝑡𝑖𝑚𝑒𝑠𝑒𝑟𝑖𝑒𝑠𝑖𝑛𝑡𝑜𝐺𝑟𝑎𝑚𝑖𝑎𝑛𝐴𝑛</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">𝑏𝑎𝑠𝑒𝑑𝑚𝑒𝑡𝑎   𝑙𝑒𝑎𝑟𝑛𝑒𝑟 𝑝𝑟𝑜𝑐𝑒𝑠𝑠𝑒𝑠𝑡 ℎ𝑒𝑠𝑖𝑔𝑛𝑎𝑙𝑠 𝑝𝑟𝑜𝑣𝑖𝑑𝑒𝑑𝑏𝑦𝐶 𝑁 𝑁𝑡𝑜𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒 𝑓 𝑖𝑛𝑎𝑙𝑡𝑟𝑎𝑑𝑖𝑛𝑔𝑑𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝑠.𝑆ℎ𝑎𝑟 𝑝𝑒</p><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;">multi<i>𝑑 𝑞𝑛𝑢𝑠𝑒𝑑𝑒𝑛𝑠𝑒𝑚𝑏𝑙𝑖𝑛𝑔𝑡𝑒𝑐ℎ𝑛𝑖𝑞𝑢𝑒𝑠𝑜𝑛𝐷𝑄𝑁 𝑎𝑔𝑒𝑛𝑡𝑠𝑡𝑜 𝑝𝑒𝑟 𝑓 𝑜𝑟𝑚𝑖𝑛𝑡𝑟𝑎𝑑𝑎𝑦𝑠𝑡𝑜𝑐𝑘𝑚𝑎𝑟 𝑘 𝑒𝑡𝑡𝑟𝑎𝑑𝑖𝑛𝑔</i></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 16pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝑟 𝑒𝑠𝑜𝑙𝑢𝑡𝑖𝑜𝑛 𝑓 𝑒𝑎𝑡𝑢𝑟 𝑒𝑖𝑠𝑎𝑙𝑠𝑜 𝑝𝑟𝑜 𝑝𝑜𝑠𝑒𝑑𝑡𝑜𝑐𝑎 𝑝𝑡𝑢𝑟 𝑒𝑑𝑖 𝑓 𝑓 𝑒𝑟 𝑒𝑛𝑡𝑑𝑎𝑡𝑎   𝑝𝑟𝑖𝑐𝑒 𝑝𝑎𝑡𝑡𝑒𝑟𝑛𝑠𝑎𝑡𝑚𝑢𝑙𝑡𝑖 𝑝𝑙𝑒𝑡𝑖𝑚𝑒 𝑓 𝑟𝑎𝑚𝑒𝑠.𝑇</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 16pt;text-align: left;">explainable<span class="s10">2</span><i>𝑠𝑢𝑔𝑔𝑒𝑠𝑡𝑒𝑑𝑎𝑛𝑒𝑥 𝑝𝑙𝑎𝑖𝑛𝑎𝑏𝑙𝑒𝑟 𝑒𝑖𝑛 𝑓 𝑜𝑟𝑐𝑒𝑚𝑒𝑛𝑡𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔 𝑓 𝑟𝑎𝑚𝑒𝑤𝑜𝑟 𝑘 𝑓 𝑜𝑟 𝑝𝑜𝑟𝑡 𝑓 𝑜𝑙𝑖𝑜𝑚𝑎𝑛𝑎</i></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑏𝑎𝑠𝑒𝑑𝑅𝐿𝑚𝑜𝑑𝑒𝑙.𝐼𝑛𝑎𝑑𝑑𝑖𝑡𝑖𝑜𝑛, 𝑡 ℎ𝑒 𝑓 𝑖𝑛𝑎𝑙𝑎𝑐𝑐𝑢𝑚𝑢𝑙𝑎𝑡𝑖𝑣𝑒 𝑝𝑜𝑟𝑡 𝑓 𝑜𝑙𝑖𝑜𝑣𝑎𝑙𝑢𝑒, 𝑆ℎ𝑎𝑟 𝑝𝑒𝑟𝑎𝑡𝑖𝑜, 𝑎𝑛𝑑𝑚𝑎𝑥𝑖𝑚𝑢𝑚𝑑𝑟𝑎𝑤</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Aiming to incorporate sentiment analysis to understand the sentiment of the news,</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><span class="p">DRL</span><span class="s10">6</span>𝑖𝑛𝑐𝑙𝑢𝑑𝑒𝑑𝑚𝑢𝑙𝑡𝑖𝑚𝑜𝑑𝑎𝑙𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔, 𝑐𝑜𝑚𝑏𝑖𝑛𝑖𝑛𝑔𝑑𝑖 𝑓 𝑓 𝑒𝑟 𝑒𝑛𝑡𝑚𝑜𝑑𝑎𝑙𝑖𝑡𝑖𝑒𝑠𝑜 𝑓 𝑑𝑎𝑡𝑎𝑡𝑜𝑒𝑛ℎ𝑎𝑛𝑐𝑒𝑡 ℎ𝑒𝑚𝑜𝑑𝑒𝑙<span class="s11">′</span>𝑠 𝑝𝑒𝑟</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 15pt;text-align: left;">sentiment<i>𝑛𝑒𝑤𝑠 𝑝𝑟𝑜 𝑝𝑜𝑠𝑒𝑑𝑎 𝑓 𝑟𝑎𝑚𝑒𝑤𝑜𝑟 𝑘𝑡 ℎ𝑎𝑡𝑖𝑛𝑐𝑙𝑢𝑑𝑒𝑠 𝑓 𝑜𝑢𝑟 𝐷𝑅𝐿𝑎𝑙𝑔𝑜𝑟𝑖𝑡 ℎ𝑚𝑠               </i><span class="s10">:</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝐴𝑑𝑣𝑎𝑛𝑡𝑎𝑔𝑒 𝐴𝑐𝑡𝑜𝑟   𝐶𝑟𝑖𝑡𝑖𝑐, 𝑃𝑟𝑜𝑥𝑖𝑚𝑎𝑙 𝑃𝑜𝑙𝑖𝑐𝑦𝑂 𝑝𝑡𝑖𝑚𝑖𝑧𝑎𝑡𝑖𝑜𝑛, 𝐷𝑒𝑒 𝑝𝐷𝑒𝑡𝑒𝑟𝑚𝑖𝑛𝑖𝑠𝑡𝑖𝑐𝑃𝑜𝑙𝑖𝑐𝑦𝐺𝑟𝑎𝑑𝑖𝑒𝑛𝑡, 𝑎𝑛𝑑𝐷𝑒</p><p class="s5" style="padding-left: 15pt;text-indent: 0pt;line-height: 15pt;text-align: right;">𝐿𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑢𝑠𝑒𝑠ℎ𝑖𝑠𝑡𝑜𝑟𝑖𝑐𝑎𝑙𝑠𝑡𝑜𝑐𝑘 𝑎𝑛𝑑𝑇 𝑤𝑖𝑡𝑡𝑒𝑟𝑚𝑎𝑟 𝑘 𝑒𝑡𝑠𝑒𝑛𝑡𝑖𝑚𝑒𝑛𝑡𝑑𝑎𝑡𝑎𝑐𝑜𝑛𝑠𝑖𝑠𝑡𝑖𝑛𝑔𝑜 𝑓 𝐷𝑜𝑤𝐽𝑜𝑛𝑒𝑠𝑎𝑛𝑑𝑆<span class="s10">&amp;</span>𝑃<span class="s10">500</span>.𝑇</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 15pt;text-indent: 0pt;line-height: 15pt;text-align: right;"><span class="p">Lastly, sentiment</span>𝑛𝑒𝑤𝑠<span class="s10">2</span>𝑎𝑙𝑠𝑜𝑢𝑠𝑒𝑠𝑎𝑠𝑒𝑛𝑡𝑖𝑚𝑒𝑛𝑡   𝑎𝑤𝑎𝑟 𝑒𝑎 𝑝 𝑝𝑟𝑜𝑎𝑐ℎ𝑎𝑠𝑎𝑛𝑒𝑥𝑡𝑒𝑛𝑠𝑖𝑜𝑛𝑡𝑜𝑡 ℎ𝑒𝑎𝑑𝑎 𝑝𝑡𝑖𝑣𝑒𝐷 𝐷</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑣𝑎𝑙𝑢𝑒 𝑝𝑒𝑟 𝑒 𝑝𝑜𝑐ℎ, 𝑢𝑠𝑖𝑛𝑔𝑑𝑖 𝑓 𝑓 𝑒𝑟 𝑒𝑛𝑡𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑟𝑎𝑡𝑒𝑠𝑑𝑒 𝑝𝑒𝑛𝑑𝑖𝑛𝑔𝑜𝑛𝑤ℎ𝑒𝑡 ℎ𝑒𝑟𝑡 ℎ𝑒 𝑝𝑟 𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑒𝑟𝑟𝑜𝑟𝑤𝑎𝑠 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒</p><p class="s5" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">𝑡𝑢𝑛𝑒𝑑𝑡𝑜𝑚𝑎𝑘 𝑒𝑖𝑡𝑙𝑒𝑎𝑟𝑛𝑎𝑏𝑜𝑢𝑡𝑡 ℎ𝑒𝑚𝑎𝑟 𝑘 𝑒𝑡𝑠𝑒𝑛𝑡𝑖𝑚𝑒𝑛𝑡𝑒 𝑓 𝑓 𝑒𝑐𝑡𝑖𝑣𝑒𝑙 𝑦.𝑇 𝑤𝑜𝑐𝑜𝑚 𝑝𝑜𝑛𝑒𝑛𝑡𝑠 𝑓 𝑜𝑟𝑚𝑢𝑙𝑎𝑡𝑒𝑡 ℎ𝑒𝑜𝑏 𝑗 𝑒𝑐𝑡𝑖𝑣𝑒</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">𝑅𝑒𝑤𝑎𝑟 𝑑𝑏𝑦𝑡 ℎ𝑒𝑐ℎ𝑎𝑛𝑔𝑒𝑖𝑛𝑝𝑜𝑟𝑡 𝑓 𝑜𝑙𝑖𝑜𝑣𝑎𝑙𝑢𝑒 𝑓 𝑟𝑜𝑚𝑡 ℎ𝑒 𝑝𝑟 𝑒𝑣𝑖𝑜𝑢𝑠𝑑𝑎𝑦𝑎𝑛𝑑𝑚𝑎𝑟 𝑘 𝑒𝑡𝑠𝑒𝑛𝑡𝑖𝑚𝑒𝑛𝑡𝑐𝑜𝑛 𝑓 𝑖𝑑𝑒𝑛𝑐𝑒𝑠𝑐𝑜𝑟 𝑒.𝑇</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-top: 6pt;padding-left: 49pt;text-indent: -32pt;text-align: left;">Conference Publications</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: left;"><a href="#bookmark59" class="a">Table </a>3.2 shows the leading conferences responsible for most publications on reinforce- ment learning related to trading tasks. The International Joint Conference on Neural Networks</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="610" height="230" alt="image" src="documento[1970]/Image_026.png"/></span></p><p class="s74" style="padding-top: 5pt;padding-left: 76pt;text-indent: 0pt;text-align: left;"><a name="bookmark57">(a) Periodical publications (b) Conference publications</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 140pt;text-indent: 0pt;text-align: left;"><span><img width="307" height="230" alt="image" src="documento[1970]/Image_027.jpg"/></span></p><p class="s74" style="padding-top: 5pt;padding-left: 202pt;text-indent: 0pt;text-align: left;">(c) Pre-print publications</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 51pt;text-indent: 0pt;text-align: left;">Figure 3.3: Year-based publication percentage in the periodical, conference, and pre-print papers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 24pt;text-indent: 0pt;text-align: left;"><span><img width="585" height="232" alt="image" src="documento[1970]/Image_028.jpg"/></span></p><p class="s1" style="padding-top: 7pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">Figure 3.4:  Amount of publications by research databases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 13pt;text-indent: 0pt;text-align: center;">had the more significant number of publications with an absolute frequency of 4 publications,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="365" height="274" alt="image" src="documento[1970]/Image_029.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 123pt;text-indent: 0pt;text-align: left;"><a name="bookmark50">Figure 3.5: Paper publication according to publication types</a><a name="bookmark58">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;">representing 6% of the total conference publications, followed by Association for Computing Machinery (ACM) International Conference proceedings with three published works and also having 5% of the total conference publications.</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">DRL<span class="s10">7</span><i>𝑠𝑢𝑔𝑔𝑒𝑠𝑡𝑒𝑑𝑐𝑜𝑚𝑏𝑖𝑛𝑖𝑛𝑔𝑎𝐷 𝐷𝑃𝐺𝑎𝑔𝑒𝑛𝑡𝑤𝑖𝑡 ℎ𝑎𝐶𝑁 𝑁𝑛𝑒𝑢𝑟𝑎𝑙𝑛𝑒𝑡𝑤𝑜𝑟 𝑘𝑡𝑜 𝑝𝑒𝑟 𝑓 𝑜𝑟𝑚 𝑝𝑜𝑟𝑡 𝑓 𝑜𝑙𝑖𝑜𝑚𝑎</i></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 59pt;text-indent: 0pt;line-height: 15pt;text-align: justify;">Combining recurrent reinforcement learning and LSTMs, DRL<span class="s10">8</span><i>𝑖𝑚 𝑝𝑙𝑒𝑚𝑒𝑛𝑡𝑎𝑚𝑢𝑙𝑡𝑖</i></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑜𝑏 𝑗 𝑒𝑐𝑡𝑖𝑣𝑒𝑠𝑡𝑟𝑢𝑐𝑡𝑢𝑟 𝑒𝑡 ℎ𝑎𝑡𝑠𝑒 𝑝𝑎𝑟𝑎𝑡𝑒𝑙 𝑦𝑚𝑒𝑎𝑠𝑢𝑟 𝑒𝑠 𝑝𝑟𝑜 𝑓 𝑖𝑡𝑎𝑛𝑑𝑟𝑖𝑠𝑘 .𝑆ℎ𝑎𝑟 𝑝𝑒𝑟𝑎𝑡𝑖𝑜, 𝑎𝑛𝑛𝑢𝑎𝑙 𝑝𝑟𝑜 𝑓 𝑖𝑡, 𝑎𝑛𝑑𝑡𝑜𝑡𝑎𝑙𝑡𝑟𝑎</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">− − −</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝑏𝑎𝑠𝑒𝑑𝑓 𝑢𝑡𝑢𝑟 𝑒𝑠𝑐𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝑠𝑡𝑟𝑎𝑑𝑒𝑑𝑖𝑛𝐶 ℎ𝑖𝑛𝑎𝑠𝑡𝑜𝑐𝑘   𝐼 𝐹, 𝑠𝑡𝑜𝑐𝑘   𝐼 𝐻, 𝑎𝑛𝑑𝑠𝑡𝑜𝑐𝑘   𝐼𝐶𝑐𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝑠, 𝑤ℎ𝑖𝑐ℎ𝑎𝑟 𝑒𝑡 ℎ𝑟 𝑒𝑒𝑖</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 15pt;text-align: left;">𝑏𝑎𝑠𝑒𝑑𝑓 𝑢𝑡𝑢𝑟 𝑒𝑠𝑐𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝑠𝑡𝑟𝑎𝑑𝑒𝑑𝑖𝑛𝐶 ℎ𝑖𝑛𝑎𝑤𝑖𝑡 ℎ<span class="s10">1   </span>𝑚𝑖𝑛𝑐𝑙𝑜𝑠𝑒 𝑝𝑟𝑖𝑐𝑒𝑠𝑎𝑠𝑖𝑛𝑝𝑢𝑡𝑑𝑎𝑡𝑎.</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">DRL<span class="s10">9 </span><i>𝑝𝑟 𝑒𝑠𝑒𝑛𝑡𝑒𝑑𝑎𝑑𝑒𝑒 𝑝𝑚𝑢𝑙𝑡𝑖   𝑚𝑜𝑑𝑎𝑙𝑟 𝑒𝑖𝑛 𝑓 𝑜𝑟𝑐𝑒𝑚𝑒𝑛𝑡𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔 𝑝𝑜𝑙𝑖𝑐𝑦, 𝑐𝑜𝑚𝑏𝑖𝑛𝑖𝑛𝑔𝑏𝑜𝑡 ℎ𝐶𝑁 𝑁 𝑎𝑛𝑑𝐿</i></p><p class="s5" style="padding-left: 59pt;text-indent: -42pt;line-height: 91%;text-align: left;">𝑏𝑎𝑠𝑒𝑑𝑎𝑔𝑒𝑛𝑡.𝑇 ℎ𝑒𝑠𝑡𝑟𝑎𝑡𝑒𝑔𝑦𝑚𝑜𝑑𝑒𝑙𝑖𝑠𝑡𝑟𝑎𝑖𝑛𝑒𝑑𝑜𝑛<span class="s10">256</span>𝑠𝑡𝑜𝑐𝑘 𝑠𝑙𝑖𝑠𝑡𝑒𝑑𝑎𝑡𝐾𝑂𝑆𝑃𝐼, 𝑢𝑠𝑖𝑛𝑔𝑑𝑎𝑖𝑙 𝑦𝑂𝐻 𝐿𝐶 𝑝𝑟𝑖𝑐𝑒𝑠, 𝑡𝑟𝑎𝑑𝑖𝑛 <span class="p">DRL</span><span class="s10">10</span>𝑖𝑛𝑣𝑒𝑠𝑡𝑖𝑔𝑎𝑡𝑒𝑑𝑡 ℎ𝑒 𝑝𝑜𝑡𝑒𝑛𝑡𝑖𝑎𝑙𝑜 𝑓 𝐷𝐷𝑃𝐺𝑎 𝑝 𝑝𝑙𝑖𝑒𝑑𝑡𝑜<span class="s10">13</span>𝑑𝑖 𝑓 𝑓 𝑒𝑟 𝑒𝑛𝑡𝑠𝑡𝑜𝑐𝑘 𝑠𝑙𝑖𝑠𝑡𝑒𝑑𝑜𝑛𝑡 ℎ𝑒𝐵𝑟𝑎𝑧𝑖𝑙𝑖𝑎 <span class="p">10.1145/3383455.3422540 explored ensemble methods, training an agent to obtain an</span></p><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;">ensemble trading strategy using three actor-critic algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and DDPG. Inheriting and integrating its best features, the strategy robustly adjusts to market situations. Applied to the Dow Jones 30 constituent stocks, the agent’s performance is evaluated with the cumulative, annualized return, volatility, Sharpe ratio, and maximum drawdown performance metrics. In the end, the strategy is compared to the Dow Jones Industrial Average (DJIA) and min-variance portfolio allocation benchmarks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Pre-print Publications</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark60" class="a">Table </a>3.3 shows the leading platforms responsible for pre-print publications on reinforce- ment learning related to quantitative trading tasks. ArXiv repository had the more significant number of publications with an absolute frequency of 5, representing 55.56% of the total pre-print publications.</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 15pt;text-align: justify;">DRL<span class="s10">11</span><i>𝑟 𝑒𝑣𝑖𝑒𝑤𝑠𝑡 ℎ𝑒 𝑝𝑟𝑜𝑔𝑟 𝑒𝑠𝑠𝑚𝑎𝑑𝑒𝑠𝑜 𝑓 𝑎𝑟𝑤𝑖𝑡 ℎ𝑑𝑒𝑒 𝑝𝑟 𝑒𝑖𝑛 𝑓 𝑜𝑟𝑐𝑒𝑚𝑒𝑛𝑡𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑖𝑛𝑡 ℎ𝑒𝑠𝑢𝑏𝑑𝑜𝑚𝑎𝑖𝑛𝑜 𝑓</i></p><p class="s5" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝑓 𝑟 𝑒𝑞𝑢𝑒𝑛𝑐𝑦𝑞𝑢𝑎𝑛𝑡𝑖𝑡𝑎𝑡𝑖𝑣𝑒𝑠𝑡𝑜𝑐𝑘𝑡𝑟𝑎𝑑𝑖𝑛𝑔.</p><p class="s5" style="padding-left: 59pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><span class="p">DRL</span><span class="s10">12</span>𝑝𝑟 𝑒𝑠𝑒𝑛𝑡𝑎𝑚𝑒𝑡𝑎 𝑓 𝑖𝑛𝑎𝑛𝑐𝑒𝑟 𝑒𝑖𝑛 𝑓 𝑜𝑟𝑐𝑒𝑚𝑒𝑛𝑡𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔<span class="s11">(</span>𝐹𝑖𝑛𝑅𝐿<span class="s11">−</span>𝑀𝑒𝑡𝑎<span class="s11">) </span>𝑓 𝑟𝑎𝑚𝑒𝑤𝑜𝑟 𝑘𝑡 ℎ𝑎𝑡𝑏𝑢𝑖𝑙𝑑𝑠𝑎</p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑑𝑟𝑖𝑣𝑒𝑛 𝑓 𝑖𝑛𝑎𝑛𝑐𝑖𝑎𝑙𝑟 𝑒𝑖𝑛 𝑓 𝑜𝑟𝑐𝑒𝑚𝑒𝑛𝑡𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔.𝑇 ℎ𝑒𝑡𝑜𝑜𝑙𝑠𝑒 𝑝𝑎𝑟𝑎𝑡𝑒𝑠 𝑓 𝑖𝑛𝑎𝑛𝑐𝑖𝑎𝑙𝑑𝑎𝑡𝑎 𝑝𝑟𝑜𝑐𝑒𝑠𝑠𝑖𝑛𝑔 𝑓 𝑟𝑜𝑚𝐷 𝑅𝐿<span class="s11">−</span></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 16pt;text-align: left;">𝑏𝑎𝑠𝑒𝑑𝑠𝑡𝑟𝑎𝑡𝑒𝑔𝑦<span class="s11">′</span>𝑠𝑑𝑒𝑠𝑖𝑔𝑛𝑝𝑖 𝑝𝑒𝑙𝑖𝑛𝑒𝑎𝑛𝑑𝑝𝑟𝑜𝑣𝑖𝑑𝑒𝑠𝑜 𝑝𝑒𝑛<span class="s11">−</span>𝑠𝑜𝑢𝑟𝑐𝑒𝑑𝑎𝑡𝑎𝑒𝑛𝑔𝑖𝑛𝑒𝑒𝑟𝑖𝑛𝑔𝑡𝑜𝑜𝑙𝑠 𝑓 𝑜𝑟 𝑏𝑖𝑔 𝑓 𝑖𝑛𝑎𝑛𝑐𝑖𝑎𝑙𝑑𝑎𝑡</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="0" alt="image" src="documento[1970]/Image_030.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="0" alt="image" src="documento[1970]/Image_031.png"/></span></p><p class="s1" style="padding-left: 11pt;text-indent: 0pt;text-align: center;"><a name="bookmark59">Table 3.1: Main journals which contain machine learning publications related to algorithmic trading</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:43.016pt" cellspacing="0"><tr style="height:14pt"><td style="width:210pt"><p class="s84" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Journal</p></td><td style="width:92pt"><p class="s84" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Absolute Frequency</p></td><td style="width:100pt"><p class="s84" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Percentage Frequency</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">IEEE Access</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">10</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">11%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Expert Systems with Applications</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">9</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">9%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Lecture Notes in Computer Science</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">9</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">9%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Applied Sciences (Switzerland)</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">4</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">4%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Knowledge-Based Systems</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">4</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">4%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Pattern Recognition</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">4</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">4%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Mathematics</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Applied Intelligence</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Advances in Neural Information Processing Systems</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Neural Computing and Applications</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">3%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Complexity</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Quantitative Finance</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Journal of Marine Science and Engineering</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Electronics (Switzerland)</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Information Fusion</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Sun SITE Central Europe Workshop Proceedings</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">2%</p></td></tr><tr style="height:12pt"><td style="width:210pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Others (containing a single paper each)</p></td><td style="width:92pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">29</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">31%</p></td></tr><tr style="height:14pt"><td style="width:210pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s84" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Total</p></td><td style="width:92pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s84" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">93</p></td><td style="width:100pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s84" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">100%</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="529" height="0" alt="image" src="documento[1970]/Image_032.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="587" height="0" alt="image" src="documento[1970]/Image_033.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="587" height="0" alt="image" src="documento[1970]/Image_034.png"/></span></p><p class="s1" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">Table 3.2:  Main conferences of the reviewed works</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:21.7418pt" cellspacing="0"><tr style="height:14pt"><td style="width:229pt"><p class="s84" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Conference</p></td><td style="width:115pt"><p class="s84" style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Absolute Frequency</p></td><td style="width:100pt"><p class="s84" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Percentage Frequency</p></td></tr><tr style="height:18pt"><td style="width:229pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">International Joint Conference on Neural Networks</p></td><td style="width:115pt"><p class="s85" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">4</p></td><td style="width:100pt"><p class="s85" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">6%</p></td></tr><tr style="height:18pt"><td style="width:229pt"><p class="s85" style="padding-top: 6pt;padding-left: 4pt;text-indent: 0pt;line-height: 11pt;text-align: left;">ACM International Conference</p></td><td style="width:115pt"><p class="s85" style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: left;">3</p></td><td style="width:100pt"><p class="s85" style="padding-top: 6pt;padding-left: 4pt;text-indent: 0pt;line-height: 11pt;text-align: left;">5%</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s86" style="padding-top: 6pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">IEEE 4th Advanced Information Management,</p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:18pt"><td style="width:238pt"><p class="s85" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Discovery and Data Mining</p></td><td style="width:70pt"><p class="s85" style="padding-left: 20pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2</p></td><td style="width:136pt"><p class="s85" style="padding-left: 40pt;text-indent: 0pt;line-height: 7pt;text-align: left;">3%</p></td></tr><tr style="height:24pt"><td style="width:238pt"><p class="s85" style="padding-top: 6pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">1st ACM International Conference on AI in Finance</p></td><td style="width:70pt"><p class="s85" style="padding-top: 6pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">2</p></td><td style="width:136pt"><p class="s85" style="padding-top: 6pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">3%</p></td></tr><tr style="height:24pt"><td style="width:238pt"><p class="s85" style="padding-top: 6pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">International Joint Conference on Artificial Intelligence</p></td><td style="width:70pt"><p class="s85" style="padding-top: 6pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">2</p></td><td style="width:136pt"><p class="s85" style="padding-top: 6pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">3%</p></td></tr><tr style="height:23pt"><td style="width:238pt"><p class="s85" style="padding-top: 6pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Others (containing a single paper each)</p></td><td style="width:70pt"><p class="s85" style="padding-top: 6pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">50</p></td><td style="width:136pt"><p class="s85" style="padding-top: 6pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">77%</p></td></tr><tr style="height:20pt"><td style="width:238pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s84" style="padding-top: 6pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Total</p></td><td style="width:70pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s84" style="padding-top: 6pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">65</p></td><td style="width:136pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s84" style="padding-top: 6pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">100%</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s86" style="padding-left: 26pt;text-indent: 0pt;line-height: 212%;text-align: left;">Communicates, Electronic and Automation Control Conference Special Interest Group on Knowledge</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s86" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">2 3%</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="587" height="0" alt="image" src="documento[1970]/Image_035.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 4pt;padding-left: 59pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><a name="bookmark51"><span class="p">DRL</span></a><span class="s10">13</span>𝑖𝑛𝑡𝑟𝑜𝑑𝑢𝑐𝑒𝑠𝑡 ℎ𝑒𝑟 𝑒𝑠 𝑝𝑒𝑐𝑡𝑖𝑣𝑒𝑣𝑎𝑙𝑢𝑒  <span class="s11">− </span>𝑏𝑎𝑠𝑒𝑑𝑎 𝑝 𝑝𝑟𝑜𝑎𝑐ℎ𝐷 𝐷𝑄𝑁𝑎𝑛𝑑𝑡ℎ𝑒 𝑝𝑜𝑙𝑖𝑐𝑦  <span class="s11">−</span><a name="bookmark52">&zwnj;</a><a name="bookmark60">&zwnj;</a></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 16pt;text-align: left;">𝑏𝑎𝑠𝑒𝑑𝑎 𝑝 𝑝𝑟𝑜𝑎𝑐ℎ𝑃𝑃𝑂, 𝑎 𝑝 𝑝𝑙𝑖𝑒𝑑𝑡𝑜<span class="s10">4</span>𝑑𝑖 𝑓 𝑓 𝑒𝑟 𝑒𝑛𝑡𝑒𝑛𝑣𝑖𝑟𝑜𝑛𝑚𝑒𝑛𝑡𝑠.𝑇 ℎ𝑒𝑇𝑊 𝐴𝑃<span class="s11">(</span>𝑇𝑖𝑚𝑒<span class="s11">−</span>𝑊𝑒𝑖𝑔ℎ𝑡𝑒𝑑𝐴𝑣𝑒𝑟𝑎𝑔𝑒𝑃𝑟𝑖𝑐𝑒</p><p class="s1" style="padding-top: 7pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">Table 3.3:  Main pre-print platforms of the reviewed works</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="547" height="0" alt="image" src="documento[1970]/Image_036.png"/></span></p><h3 style="padding-left: 45pt;text-indent: 0pt;text-align: center;">Preprint                                                           Absolute Frequency    Percentage Frequency</h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="547" height="0" alt="image" src="documento[1970]/Image_037.png"/></span></p><p class="s87" style="padding-left: 45pt;text-indent: 0pt;text-align: left;">arXiv.org 5 55.56%</p><p class="s87" style="padding-top: 9pt;padding-left: 45pt;text-indent: 0pt;text-align: left;">Social Science Research Network – SSRN 4 44.44%</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="547" height="0" alt="image" src="documento[1970]/Image_038.png"/></span></p><h3 style="padding-bottom: 1pt;padding-left: 45pt;text-indent: 0pt;text-align: left;">Total 9 100%</h3><p style="padding-left: 39pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="536" height="0" alt="image" src="documento[1970]/Image_039.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><p style="padding-top: 6pt;padding-left: 40pt;text-indent: -23pt;text-align: left;">CO-AUTHORSHIP AND CO-OCCURRENCE NETWORKS</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;text-align: left;">VOSviewer software was used to provide an overview of the data synthesis. Built by  vos<i>𝑣𝑖𝑒𝑤𝑒𝑟, 𝑡 ℎ𝑒𝑡𝑜𝑜𝑙𝑎𝑙𝑙𝑜𝑤𝑠𝑏𝑖𝑏𝑙𝑖𝑜𝑚𝑒𝑡𝑟𝑖𝑐𝑛𝑒𝑡𝑤𝑜𝑟 𝑘𝑣𝑖𝑠𝑢𝑎𝑙𝑖𝑧𝑎𝑡𝑖𝑜𝑛𝑎𝑛𝑑𝑐𝑜𝑛𝑠𝑡𝑟𝑢𝑐𝑡𝑖𝑜𝑛𝑏𝑎𝑠𝑒𝑑𝑜𝑛𝑏𝑖𝑏𝑙𝑖𝑜𝑔𝑟𝑎 𝑝ℎ𝑖𝑐𝑐</i></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">− − −</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: left;">𝑎𝑢𝑡 ℎ𝑜𝑟 𝑠ℎ𝑖 𝑝, 𝑐𝑜   𝑐𝑖𝑡𝑎𝑡𝑖𝑜𝑛, 𝑎𝑛𝑑𝑐𝑜   𝑜𝑐𝑐𝑢𝑟𝑟 𝑒𝑛𝑐𝑒𝑎𝑚𝑜𝑛𝑔𝑠𝑖𝑚𝑖𝑙𝑎𝑟 𝑘 𝑒𝑦𝑤𝑜𝑟 𝑑𝑠.𝐼𝑛𝑜𝑟 𝑑𝑒𝑟𝑡𝑜𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒𝑡 ℎ𝑒𝑐𝑜</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">𝑎𝑢𝑡 ℎ𝑜𝑟 𝑠ℎ𝑖 𝑝𝑎𝑛𝑑𝑐𝑜   𝑜𝑐𝑐𝑢𝑟𝑟 𝑒𝑛𝑐𝑒𝑛𝑒𝑡𝑤𝑜𝑟 𝑘 𝑠, 𝑡 ℎ𝑒𝑖𝑛 𝑓 𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛𝑜 𝑓 𝑡 ℎ𝑒𝑠𝑒𝑙𝑒𝑐𝑡𝑒𝑑𝑝𝑟 𝑒𝑣𝑖𝑜𝑢𝑠𝑠𝑒𝑙𝑒𝑐𝑡𝑒𝑑𝑝𝑎 𝑝𝑒𝑟 𝑠𝑤𝑎𝑠</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark61" class="a">Figure </a><a href="#bookmark56" class="a">3.6 shows the generated co-authorship network. Through its visualization, it is possible to analyze how researchers are related according to the amount of co-authored papers and the specific dates that they published their shared work. Writers had to publish at least one paper together and contain at least two documents. Of the 392 authors, only 28 met the threshold, distributed into 5 clusters. Confirming the information from Figure </a>3.2, it is observable that the highest amount of publications was in 2021. Moreover, the most significant cluster, regarding the number of publications, was composed of Yong Zhang and Xuanzhe Liu, four publications each, and Wen Zhang, with two publications.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark62" class="a">Figure </a>3.7 shows the keyword co-occurrence networks. By setting a minimum number of 3 keyword occurrences, from the total amount of 444 keywords, only 19 met the threshold. For each, the total strength of the co-occurrence links with other keywords is calculated, and the ones with the most significant total link are selected. VosViewer calculated the total amount of 4 clusters. It is possible to observe that Machine Learning and Deep Learning keywords were the most significant clusters. Both had 25 occurrences and a total link strength of 42 and 37, respectively. In addition, neural networks, algorithmic trading, artificial intelligence, finance, reinforcement learning, and explainable AI also contained a significant number of occurrences, each with 10, 9, 7, 7, and 6, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">DATA SYNTHESIS</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark63" class="a">Figure </a>3.8 shows the deep reinforcement learning algorithms on which the reviewed works were based. A total amount of 7 papers (18.4%) were built based on the policy-based PPO algorithm, 11 papers (28.9%) on the value-based DQN algorithm, 16 papers (42.1%) on the actor-critic DDPG algorithm, followed by the A2C algorithm containing two papers (5.3%), and the A3C and RRL algorithms, containing one paper each (2.6%).</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark61" class="a">Table </a>3.4 shows the most common performance metrics used for performance mea- surement purposes by the previously mentioned algorithms. A total amount of 5 papers (6.5%) claimed to use the annualized returns, eight papers (10.4%) used maximum drawdown, ten papers used Sharpe ratio (13%), and three papers (3.9%) used standard deviation to measure performance. Total profits were also used in the same proportion as the previously mentioned metric.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><span><img width="618" height="391" alt="image" src="documento[1970]/Image_040.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 45pt;text-indent: 0pt;text-align: center;"><a name="bookmark61">Figure 3.6: Co-authorship network of RL-related publications applied to quantitative trading</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">Table 3.4:   Main performance metrics applied to the reviewed works</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="519" height="1" alt="image" src="documento[1970]/Image_041.png"/></span></p><h1 style="padding-bottom: 1pt;padding-left: 55pt;text-indent: 0pt;text-align: left;">Performance Metrics Absolute Frequency Percentage Frequency</h1><p style="padding-left: 49pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="519" height="1" alt="image" src="documento[1970]/Image_042.png"/></span></p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;">Sharpe Ratio 10 13%</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;">Maximum Drawdown 8 10.4%</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;">Annualized Return 5 6.5%</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;">Standard Deviation 3 3.9%</p><p style="padding-left: 55pt;text-indent: 0pt;text-align: left;">Total Profits 3 3.9%</p><p style="padding-bottom: 1pt;padding-left: 55pt;text-indent: 0pt;text-align: left;">Others (containing one each) 48 62.3%</p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="519" height="1" alt="image" src="documento[1970]/Image_043.png"/></span></p><h1 style="padding-bottom: 1pt;padding-left: 55pt;text-indent: 0pt;text-align: left;">Total 77 100%</h1><p style="padding-left: 49pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="519" height="1" alt="image" src="documento[1970]/Image_044.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark62" class="a">Table </a>3.5 shows the most common markets to which the previously mentioned works were applied. S&amp;P500 had the absolute frequency of 7 papers, having its application on 14.3% of the RL-related works. Cryptocurrency, Dow Jones, and National Association of Securities Dealers Automated Quotations (NASDAQ) had a total amount of 3 papers (6.1%) each. Stock-IF, Stock-IC, and foreign exchange market (FOREX) had two papers (4.1%) each.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The previously formulated research questions are answered, interpreting the presented data synthesized information.</p><p style="padding-left: 59pt;text-indent: 0pt;text-align: justify;">RQ1: What are the main reinforcement learning methods applied to quantitative trading? Through the systematic literature review, it was possible to identify that the actor-</p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">critic deep deterministic policy gradient method was the main deep reinforcement learning algorithm applied to quantitative trading tasks, having 42.1% of total citations on the RL-related papers. Deep Q-Network and Proximal Policy Optimization had 28.9% and 18.4%, respectively, and consequently, achieved the second and third positions. Advantage Actor-Critic (5.3%),</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 22pt;text-indent: 0pt;text-align: left;"><span><img width="611" height="372" alt="image" src="documento[1970]/Image_045.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="436" height="1" alt="image" src="documento[1970]/Image_046.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="436" height="1" alt="image" src="documento[1970]/Image_047.png"/></span></p><p class="s1" style="padding-top: 6pt;padding-left: 140pt;text-indent: -81pt;line-height: 208%;text-align: left;"><a name="bookmark62">Figure 3.7: Co-occurrence network of RL-related publications applied to quantitative trading Table 3.5: Main markets applied to reviewed works</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:80.599pt" cellspacing="0"><tr style="height:46pt"><td style="width:88pt"><p class="s88" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Markets</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s89" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Cryptocurrency</p></td><td style="width:114pt"><p class="s88" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Absolute Frequency</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">3</p></td><td style="width:124pt"><p class="s88" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Percentage Frequency</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">6.1%</p></td></tr><tr style="height:15pt"><td style="width:88pt"><p class="s89" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Dow Jones</p></td><td style="width:114pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">3</p></td><td style="width:124pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">6.1%</p></td></tr><tr style="height:15pt"><td style="width:88pt"><p class="s89" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;">FOREX</p></td><td style="width:114pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">2</p></td><td style="width:124pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">4.1%</p></td></tr><tr style="height:15pt"><td style="width:88pt"><p class="s89" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;">NASDAQ</p></td><td style="width:114pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">3</p></td><td style="width:124pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">6.1%</p></td></tr><tr style="height:15pt"><td style="width:88pt"><p class="s89" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;">S&amp;P500</p></td><td style="width:114pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">7</p></td><td style="width:124pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">14.3%</p></td></tr><tr style="height:15pt"><td style="width:88pt"><p class="s89" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Stock-IC</p></td><td style="width:114pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">2</p></td><td style="width:124pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">4.1%</p></td></tr><tr style="height:15pt"><td style="width:88pt"><p class="s89" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Stock-IF</p></td><td style="width:114pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">4</p></td><td style="width:124pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">8.2%</p></td></tr><tr style="height:15pt"><td style="width:88pt"><p class="s89" style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Others</p></td><td style="width:114pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">25</p></td><td style="width:124pt"><p class="s89" style="padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;">51%</p></td></tr><tr style="height:17pt"><td style="width:88pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s88" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Total</p></td><td style="width:114pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s88" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">49</p></td><td style="width:124pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s88" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">100%</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="436" height="1" alt="image" src="documento[1970]/Image_048.png"/></span></p><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;">Asynchronous Advantage Actor-Critic (2.6%), and Recurrent Reinforcement Learning (2.6%) were also claimed to be implemented in some reviewed works.</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">RQ2: What are the main performance metrics used on these algorithms?</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark61" class="a">According to Table </a>3.4, the Sharpe ratio was the primary metric applied to performance evaluation. 13% of all RL-related reviewed publications claimed to have used such a metric. In addition, maximum drawdown and annualized return were used, each containing 10.4% and 6.5%, respectively. Finally, total profits and standard deviation were applied and cited in 3.9% of all RL-related reviewed works.</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">RQ3: What are the main markets in which the models have been applied to?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 99pt;text-indent: 0pt;text-align: left;"><span><img width="350" height="420" alt="image" src="documento[1970]/Image_049.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 87pt;text-indent: 0pt;text-align: left;"><a name="bookmark63">Figure 3.8: Deep reinforcement learning algorithms applied to reviewed works</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark62" class="a">As stated in Table </a>3.5, S&amp;P500 was the primary market for the reviewed agents that had been applied to. 14.3% of the RL-related reviewed publications have used such market to apply their algorithms, followed by the Stock-IF with 8.2%, Cryptocurrency, Dow Jones, and NASDAQ markets that respectively had 6.1% each, and lastly, FOREX and Stock-IC markets with 4.1%.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h1 style="padding-top: 7pt;padding-left: 30pt;text-indent: -13pt;text-align: left;"><a name="bookmark64">DATA COLLECTION, CLEANING, PRE-PROCESSING AND EXPLORATORY ANALYSIS</a><a name="bookmark65">&zwnj;</a><a name="bookmark66">&zwnj;</a><a name="bookmark69">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">This chapter describes the adopted data gathering, cleaning, and pre-processing process. First, the data-gathering process is defined, then the outliers removal process is explained, and finally, exploratory analysis is conducted. Aiming to clarify what the algorithm should learn, the data was processed in a way that could reflect good statistical properties, such as returns closer to a normal distribution, according to the Jarque-Bera test and the absence of outliers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l42"><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">DATA COLLECTION</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">Time bars are the most popular technique introduced in finance to transform information from a non-uniform frequency to a regular sampled series. Despite its popularity, this type of bar over-samples information during low-activity market periods and the other way around. In addition, these bars are more prone to undesired statistical properties, such as heteroscedasticity and non-normality of returns (de Prado </a><a href="#bookmark124">(2018)).</a></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark124" class="a">This dissertation adopts dollar bars to achieve returns closer to independent and identically distributed Gaussian distribution. The generation of these bars is according to every time a pre-defined market value has been exchanged. According to Ané and Geman </a>(2000), the cumulative number of trades makes the returns closer to a normal distribution, although outliers are usually consistent. In many exchanges, the order book accumulates bids and offers without matching them for some time, and once the auction is concluded, a large trade is published at the clearing price for an excessive amount. In order to build the bars mentioned above, transaction data from the iShares Core S&amp;P 500 exchange-traded fund and Western Digital Corporation were downloaded from the Kibot website, which offered free tick with bid/ask intraday data. These features were discarded, adopting prices and shares amount only. The original downloaded iShares Core S&amp;P 500 exchange-traded fund data frame contained 10,460,165 transactions corresponding to the periods of 2009-09-28 09:30:00 / 2022-12-02 16:00:00. Western Digital</p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Corporation included 69,242,555 transactions from 2009-09-28 09:41:53 to 2022-09-21 16:00:00. Finally, the Cosmos / United States Dollar Tether (ATOM/USDT) cryptocurrency pair market data was collected via Binance spot application programming interface comprehending the period of 2019-06-05 16:35:28.905 / 2021-11-26 12:11:26.995, the data frame had 30,535,634 transactions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">DATA CLEANING</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;line-height: 14pt;text-align: justify;">The Interquartile method identifies and removes outliers to clean the data collected in the previous section according to a sliding window approach. In this procedure, the entire data frame is rearranged in ascending order and divided into four parts (quarters). The first quartile (<i>𝑄</i><span class="s10">1</span>) is the middle value between the first data sample and the median from the dataframe, the median is the second quartile (<i>𝑄</i><span class="s10">2</span>), and the third quartile (<i>𝑄</i><span class="s10">3</span>) is the middle value between the median and the last term. Given a dataset with <i>𝑁 </i>observations arranged in ascending order, the quartiles are calculated by:</p><p class="s10" style="padding-left: 34pt;text-indent: 0pt;line-height: 8pt;text-align: center;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="1" alt="image" src="documento[1970]/Image_050.png"/></span></p><p class="s11" style="padding-left: 199pt;text-indent: 0pt;line-height: 66%;text-align: left;"><span class="s5">𝑄</span><span class="s10">1 </span><span class="s19">= </span>(<span class="s5">𝑁 </span>+ <span class="s10">1</span>) × <span class="s90">4 </span><span class="s5">, </span><span class="p">(4.1)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 32pt;text-indent: 0pt;line-height: 13pt;text-align: center;"><a name="bookmark70">1</a></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="1" alt="image" src="documento[1970]/Image_051.png"/></span></p><p class="s11" style="padding-left: 198pt;text-indent: 0pt;line-height: 66%;text-align: left;"><span class="s5">𝑄</span><span class="s10">2 </span><span class="s19">= </span>(<span class="s5">𝑁 </span>+ <span class="s10">1</span>) × <span class="s90">2 </span><span class="s5">, </span><span class="p">(4.2)</span></p><p class="s10" style="padding-top: 6pt;padding-left: 34pt;text-indent: 0pt;line-height: 13pt;text-align: center;">3</p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="1" alt="image" src="documento[1970]/Image_052.png"/></span></p><p class="s11" style="padding-left: 198pt;text-indent: 0pt;line-height: 66%;text-align: left;"><span class="s5">𝑄</span><span class="s10">3 </span><span class="s19">= </span>(<span class="s5">𝑁 </span>+ <span class="s10">1</span>) × <span class="s90">4 </span><span class="s5">. </span><span class="p">(4.3)</span></p><p style="padding-top: 2pt;padding-left: 59pt;text-indent: 0pt;text-align: left;">In the following, the interquartile range (IQR) is calculated by:</p><p class="s5" style="padding-top: 10pt;padding-left: 204pt;text-indent: 0pt;text-align: left;">𝐼𝑄 𝑅 <span class="s19">= </span>𝑄<span class="s10">3 </span><span class="s11">− </span>𝑄<span class="s10">1</span>.                                                       <span class="p">(4.4)</span></p><p style="padding-top: 9pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a href="#bookmark70" class="a">Followed by the lower and upper boundaries, which are defined by Equations 4.5 and </a>4.6, respectively.</p><p class="s5" style="padding-left: 15pt;text-indent: 0pt;line-height: 16pt;text-align: right;">𝐿𝑜𝑤𝑒𝑟 <span class="s19">= </span>𝑄<span class="s10">1 </span><span class="s11">− (</span><span class="s10">1</span>.<span class="s10">5 </span><span class="s11">× </span>𝐼𝑄 𝑅<span class="s11">)</span>,                                             <span class="p">(4.5)</span></p><p class="s5" style="padding-top: 2pt;padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑈 𝑝 𝑝𝑒𝑟 <span class="s19">= </span>𝑄<span class="s10">3 </span><span class="s11">+ (</span><span class="s10">1</span>.<span class="s10">5 </span><span class="s11">× </span>𝐼𝑄 𝑅<span class="s11">)</span>,                                             <span class="p">(4.6)</span></p><p style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;line-height: 91%;text-align: justify;">where the Lower boundary <i>𝐿𝑜𝑤𝑒𝑟 </i>is <i>𝑄</i><span class="s10">1 </span>subtracted from the product of 1.5 and the interquartile range IQR and the Upper boundary <i>𝑈 𝑝 𝑝𝑒𝑟 </i>is the sum of <i>𝑄</i><span class="s10">3 </span>with the product of the interquartile range and 1.5.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark71" class="a">In this master dissertation, the IQR method was applied with a sliding window approach. Every time a state is assembled, the data from the last seven days concerning the current time is stored in an array, and the IQR method is applied to remove outliers. For illustration purposes, the outlier values were saved in an array and analyzed. Figure </a>4.1 shows the original price behavior of the iShares Core S&amp;P 500 ETF data frame before the outlier removal and its boxplot and after it.</p><p class="s74" style="padding-top: 6pt;padding-left: 114pt;text-indent: 0pt;text-align: left;"><a name="bookmark71">(a) (b)</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 165pt;text-indent: 0pt;text-align: left;"><span><img width="217" height="182" alt="image" src="documento[1970]/Image_053.jpg"/></span></p><p class="s74" style="padding-top: 3pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">(c)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Figure 4.1: (a) iShares Core S&amp;P 500 exchange-traded fund price in United States Dollar with outliers from 2010-2022. (b) iShares Core S&amp;P 500 exchange-traded fund outlier-free fom 2010-2022 (c) iShares Core S&amp;P 500 exchange-traded fund price boxplot.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark71" class="a">Figures 4.1(a) and 4.1(b) show iShares Core S&amp;P 500 ETF price evolution throughout time. Figure 4.1(a) was generated from the original data frame that contained outliers. There is a clear outlier presence in 2010, when a sudden price drop occurs towards zero. Figure </a>4.1(c) shows the amount and location of the removed outliers. A single 163.43 USD-valued outlier was detected by the upper boundary and removed, in contrast to the lower border, which caught a total amount of 115. The total amount of 116 transactions was considered outliers and were removed from the data frame, resulting in a final number of 10,460,049 transactions.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark72" class="a">With respect to the Western Digital Corporation data frame, the same IQR method was applied to remove outliers. Figure </a>4.2 shows the price evolution from 2010 to 2022 with and without outliers and its boxplot.</p><p class="s74" style="padding-top: 6pt;padding-left: 114pt;text-indent: 0pt;text-align: left;"><a name="bookmark67">(a) (b)</a><a name="bookmark72">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 157pt;text-indent: 0pt;text-align: left;"><span><img width="241" height="195" alt="image" src="documento[1970]/Image_054.jpg"/></span></p><p class="s74" style="padding-top: 8pt;padding-left: 45pt;text-indent: 0pt;text-align: center;">(c)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">Figure 4.2: (a) Western Digital price in United States Dollar with outliers from 2010-2022. (b) Western Digital outlier-free fom 2010-2022 (c) Western Digital price boxplot.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark72" class="a">In Figure 4.2(a), the total amount of 4805 outliers are located between 2014 and 2015. Comparing both Figures 4.2(b) and 4.2(a), it is possible to observe that the second plot was clipped in the y-axis, due to the presence of outliers greater than 109.83 USD. Lastly, Figure </a>4.2(c) shows the boxplot of the Western Digital transactions price data frame, containing the</p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">47.68 USD median value and the outliers mentioned above the upper boundary. The plot confirms that the outliers are concentrated between 110 and 120 USD and are not dispersed much.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark73" class="a">Figure 4.3(a) shows the price evolution of the ATOM/USDT cryptocurrency pair on the Binance spot exchange during the periods of 2019-06 and 2021-11. The price started at 4.755 USDT in June 2019 and has a 650% increase in just a couple of years. Unlike the previously analyzed assets, cryptocurrencies are known to be highly volatile. Figure </a>4.3(b) shows the price feature box plot indicating the median of 33.0 USDT. Even though there are outliers located below the lower boundary, the decision not to remove them was taken due to the assumption that since cryptocurrencies are extremely volatile, these outliers may not be incorrect data. In addition to it, since cryptocurrency markets work with no interruptions, it is improbable that the order book accumulates bids and offers without matching them for some time.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l43"><ol id="l44"><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: justify;">DATA PRE-PROCESSING</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">This section concerns the pre-processing step that is performed after the outlier removal from the last section. In every state assemble, transaction data collected is manipulated and</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 24pt;text-indent: 0pt;text-align: left;"><span><img width="251" height="188" alt="image" src="documento[1970]/Image_055.jpg"/></span>	<span><img width="233" height="187" alt="image" src="documento[1970]/Image_056.jpg"/></span></p><ol id="l45"><li><p class="s74" style="padding-top: 7pt;padding-left: 356pt;text-indent: -235pt;text-align: left;"><a name="bookmark73">(b)</a></p></li></ol></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Figure 4.3: (a) ATOM cryptocurrency price in USDT from 2019-06 to 2021-06. (b) ATOM cryptocurrency price boxplot.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;">transformed into dollar bars. In addition to it, its statistical properties are evaluated and analyzed. Aiming to analyze its properties in this work, dollar bars were stored in an array and saved into a data frame during every state assemble.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Contrasting time bars, dollar bars are not sampled in a constant pre-defined frequency, but every time a pre-defined amount of market value has been exchanged. In order to generate time bars from transactions, a constant pre-defined frequency must be chosen to be the interval in which they are generated. The open price is equivalent to the first transaction price during that period; the high is equivalent to the transaction that achieved the highest price compared to the remaining transactions. The same opposed logic applies to the low. The close price is equivalent to the last transaction price, and finally, volume is equivalent to the sum of the amount of exchanged shares during this interval.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark74" class="a">Figure </a>4.4 compares time and dollar bars. Transaction data from the iShares S&amp;P500 ETF, corresponding to the interval of 2009-09-28 09:41- 2009-09-28 10:32, were used to create both. Time bars were sampled in a 1-minute frequency, whereas dollar bars were sampled every time market reached 500K exchanged dollars. It was observable that during this period, the total amount of 52 and 21 time and dollar bars were sampled, respectively. What can be concluded is that during this period, the total amount of exchanged value of 500K USD was achieved 21 times. Noticeably, time bars undersample information during 9:41 and 9:51, in an ascending price movement, lacking show strength. In contrast, it is observable that dollar bars do this by generating a couple of solid green candles. It is also noticeable that between 9:51 and 10:00, time bars oversample information, indicating a price fall. In contrast, dollar bars show that this pullback is not strong due to the exchanged value; hence goes sideways and generates weak dollar bars. This type of bar assists the agent in understanding the relationship between market value and price movements.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark74" class="a">Regarding the dollar bar generation, the defined amount to sample a dollar bars was 500K United States Dollars (USD) in iShares Core S&amp;P 500 ETF data frame. Transactions are aggregated until the exchanged amount market value reaches the threshold above. The open price is equivalent to the first transaction price from the group; the high is equal to the maximum price compared to all transactions that belong to the group. The opposed logic applies to low. Close price equals the last transaction price of the aggregated transactions, and volume is the sum of all group transactions. Figure </a>4.5 shows the time and dollar bars count histogram throughout the weeks generated from iShares Core S&amp;P 500 ETF applying the previously mentioned procedures.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span><img width="264" height="198" alt="image" src="documento[1970]/Image_057.jpg"/></span>	<span><img width="279" height="206" alt="image" src="documento[1970]/Image_058.jpg"/></span></p><p class="s74" style="padding-top: 1pt;padding-left: 121pt;text-indent: 0pt;text-align: left;"><a name="bookmark74">(a) (b)</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Figure 4.4: Bar sampling method comparison applied to the iShares S&amp;P500 ETF data frame from 2009-09-28 09:41 to 2009-09-28 10:32: (a) time bars sampled in a 1-minute frequency, (b) Dollar bars sampled every 500K dollars market-value exchange.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="259" height="178" alt="image" src="documento[1970]/Image_059.jpg"/></span>	<span><img width="259" height="178" alt="image" src="documento[1970]/Image_060.jpg"/></span></p><p class="s74" style="padding-top: 8pt;padding-left: 121pt;text-indent: 0pt;text-align: left;">(a) (b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">Figure 4.5: Amount of weekly sampled bars generated from iShares Core S&amp;P 500 exchange-traded fund transactions from 2009 to 2022: (a) Weekly sampled dollar bars count, (b) Weekly sampled time bars count.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark74" class="a">Regarding the sampled amount, 398,362 dollar bars were generated against 1,081,690 time bars. Some observations regarding their distributions are made by comparing both count plots. In Figure 4.5(a), the number of sampled bars per year is increasing, even though some exceptions exist. During 2012, 2017, 2019, and 2021, the amount of generated dollar bars was 23.2%, 14.7%, 59.64%, and 24.87%, respectively, lower than in the previous year. In addition, 2018 had the highest exchanged market value compared to the remaining years, resulting in 56691 dollar bars. On the other hand, 2009 had the lowest exchanged market value having 2902 bars only. This fact may have occurred due to missing data since the data frame started at 2009-09-28 09:30:00. The distribution in Figure 4.5(b) is uniform since time bars are sampled in a constant frequency but with some exceptions. Factors that may prevent the sampling from being perfectly uniform are the peculiarities of each year, like holidays. Contrasting the distribution in Figure </a>4.5(a), the second distribution provides market information uniformly, independently of market activity. With 1 minute as the defined sampling frequency, 2016 had the most significant amount of bars concerning the remaining years containing 91511 bars.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">The market value defined to sample a dollar bar in Western Digital Corporation data was 1,000,000 USD. Since this asset holds more transactions than the previous one, it contains</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a name="bookmark75">higher liquidity; hence the defined threshold had to be greater than the previously chosen. Figure</a></p><ol id="l46"><ol id="l47"><li><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;">shows the histogram of sampled dollar and time bars count per week generated from Western Digital Corporation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span><img width="255" height="156" alt="image" src="documento[1970]/Image_061.jpg"/></span>	<span><img width="259" height="156" alt="image" src="documento[1970]/Image_062.jpg"/></span></p><ol id="l48"><li><p class="s74" style="padding-top: 7pt;padding-left: 77pt;text-indent: -10pt;text-align: left;">Weekly sampled dollar bars count (b) Weekly sampled time bars count</p></li></ol></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">Figure 4.6: Amount of weekly sampled bars generated from Western Digital Corporation transactions from 2010 to 2022.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 43pt;text-align: justify;"><a href="#bookmark75" class="a">A total amount of 550,598 dollar bars were generated against 1,305,281. In Figure </a><a href="#bookmark74" class="a">4.6(a), it observable that differently from the distribution shown in Figure </a><a href="#bookmark75" class="a">4.5(a), the number of samples does not continuously increase throughout the years. By sampling 36,563 bars in 2010, the distribution kept increasing until it reached its peak in 2017. A total of 65,196 bars was generated this year, followed by a continuous reduction until 2022. It is possible to affirm that the amount of bars is 78.31% higher during its peak in 2017 compared to 2022. Regarding the mean value, an average number of 42,353 bars are generated per week per year. Figure </a>4.6(b) shows the distribution of the produced time bars. Similarly to the iShares S&amp;P500 ETF data, 1 minute was adopted as the predefined frequency for sampling a bar and is close to a uniform distribution. With an expected 100,406 sampling amount of bars per week per year, the distribution peaked in 2020 and its lowest value in 2010 compared to the remaining years, with 110,186 and 88,450 bars, respectively. In addition, the distribution contemplates 24.57% more bars during 2020 compared to its bottom in 2010.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark74" class="a">Finally, the defined market value amount to sample a dollar bar in the ATOM/USDT data frame was 7.5K USDT. Since the cryptocurrency does not contain higher liquidity than the remaining analyzed data frames, it is feasible that the threshold was lower than the previously studied assets. Figure </a>4.5 shows the histogram of dollar and time bars count throughout the weeks generated from Binance spot ATOM/USDT cryptocurrency pair concerning the 2019-04-29 04:15:31 / 2021-11-26 12:11:26 period.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark76" class="a">A total number of 359,786 dollar bars were generated against 1,197,149. It is observable that the amount of time bars is 232.74% higher than dollar ones. The cryptocurrency asset has as much data as the previous data frames because its market operates uninterruptibly. In Figure 4.7(a), it is noticeable that the amount of the generated dollar bars grows exponentially during these three years. In 2019, 2020, and 2021, 1537, 5820, and 352429 dollars were sampled yearly; hence 2021 had 350,892 more sampled bars than 2019. This fact may be due to the cryptocurrency market growth in recent years. In addition, Figure </a>4.7(b) shows the distribution of the generated time bars. Since the collected transactions started in April 2019 and ended in December 2021, it is possible to affirm that 2019 has incomplete data; hence it cannot be analyzed. It may be possible to conclude that if the data concerning 2019 were complete, the distribution would be uniform. The year 2021 sampled the highest amount of time bars and 2019 the smallest; each generated 475,388 and 259,249, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="265" height="156" alt="image" src="documento[1970]/Image_063.jpg"/></span>	<span><img width="265" height="156" alt="image" src="documento[1970]/Image_064.jpg"/></span></p><p class="s74" style="padding-top: 7pt;padding-left: 66pt;text-indent: 0pt;text-align: left;"><a name="bookmark68">(a) Weekly sampled dollar bars count (b) Weekly sample time bars count</a><a name="bookmark76">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">Figure 4.7: Amount of weekly sampled bars generated from Binance spot ATOM/USDT cryptocurrency pair transactions from 2010 to 2022.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s19" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">√︄<span class="s91">L</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 16pt;text-indent: 42pt;text-align: left;"><a href="#bookmark76" class="a">In order to measure the count stability, the standard deviation, given by Equation </a>4.7, was adopted as the dispersion measure to compare both distributions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 225pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="94" height="1" alt="image" src="documento[1970]/Image_065.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 15pt;text-indent: 0pt;line-height: 7pt;text-align: right;">𝜎 <span class="s19">=</span></p><p class="s25" style="padding-top: 1pt;padding-left: 23pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑁</p><p class="s25" style="padding-left: 22pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑖<span class="s33">=</span><span class="s30">1</span></p><p class="s11" style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;line-height: 18pt;text-align: left;">(<span class="s5">𝑥𝑖 </span>− <span class="s5">𝜇</span>)<span class="s92">2</span></p><p style="padding-top: 12pt;padding-left: 152pt;text-indent: 0pt;line-height: 7pt;text-align: left;">(4.7)</p><p style="text-indent: 0pt;text-align: left;"><span><img width="91" height="1" alt="image" src="documento[1970]/Image_066.png"/></span></p><p class="s5" style="padding-left: 45pt;text-indent: 0pt;line-height: 18pt;text-align: center;">𝑁  <span class="s11">− </span><span class="s10">1</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 16pt;text-indent: 42pt;line-height: 14pt;text-align: justify;">The standard deviation is a measure of the dispersion of a population. The sum of each value from the population <i>𝑥𝑖</i>, from <i>𝑖  </i><span class="s19">= </span><span class="s10">1 </span>to N, is given by the sum of each value subtracted from its mean <i>𝜇</i>.  This subtraction is then squared and divided by the <i>𝑁     </i><span class="s10">1 </span>total population size. Finally, the squared root of this value is taken. Regarding the interpretation of the standard deviation, a couple of insights can be deduced: a high <i>𝜎 </i>value means that the data points that belong to the population are dispersed; in other words, the data points are generally far from the mean. On the other hand, a low standard deviation means that the values are typically clustered close to the mean.</p><p class="s5" style="padding-top: 1pt;padding-left: 17pt;text-indent: 42pt;line-height: 91%;text-align: justify;"><span class="p">Concerning the iShares Core S&amp;P 500 ETF data frame, the standard deviation of the sampled dollar and time bar counts were calculated and were respectively </span>𝜎 <span class="s19">= </span><span class="s10">413</span>.<span class="s10">39 </span><span class="p">and</span></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 15pt;text-align: justify;">𝜎 <span class="s19">= </span><span class="s10">218</span>.<span class="s10">97</span><span class="p">. These values indicate that the time bars have more stable counts than dollar ones and</span></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;"><span class="p">are closer to the 77263-valued mean. Regarding the second data frame, the standard deviation of the previously mentioned bars was </span>𝜎 <span class="s19">= </span><span class="s10">368</span>.<span class="s10">24 </span><span class="p">and </span>𝜎 <span class="s19">= </span><span class="s10">271</span>.<span class="s10">97</span><span class="p">.  Similarly, time bars had the most stable counts. Lastly, dollar and time bars in ATOM/USDT cryptocurrency were respectively 6826.89 and 1375.11. Time bars are expected to be more stable than the dollar type since the first type is sampled in a constant frequency.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l49"><ol id="l50"><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">EXPLORATORY DATA ANALYSIS</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark77" class="a">Finally, time series properties from the close price feature are discussed for each data frame. Figure </a>4.8 shows monthly returns, distribution/plot of dollar bar returns, and daily volatility generated from the iShares Core S&amp;P 500 ETF dollar bars.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark77" class="a">Figure 4.8(a) shows the monthly returns heatmap generated from the close price between 2009-2022. In March 2020, the exchange-traded fund had the lowest return having a -15.5% dump, and its highest in November during the same year with a 13% pump. Months with 0% return are those whose data was not collected. As shown in Figure </a>4.8(b), returns are stationary, and a few spikes are detected on both sides of the data frame: In 2010, the ETF had its best return of 12.25% and its worst in 2020, having an -11.13% dump due to the Coronavirus pandemic</p><ol id="l51"><li><p class="s74" style="padding-top: 7pt;padding-left: 89pt;text-indent: -10pt;text-align: left;"><a name="bookmark77">Monthly returns heat map (b) Simple logarithmic returns</a></p></li></ol></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 34pt;text-indent: 0pt;text-align: left;"><span><img width="256" height="201" alt="image" src="documento[1970]/Image_067.jpg"/></span>	<span><img width="243" height="186" alt="image" src="documento[1970]/Image_068.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l52"><li><p class="s74" style="padding-left: 68pt;text-indent: -10pt;text-align: left;">Simple logarithmic returns distribution (d) Daily volatility</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 47pt;text-indent: 0pt;text-align: left;">Figure 4.8: iShares Core S&amp;P 500 exchange-traded fund time series properties from 2009 to 2022.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark77" class="a">(COVID-19). It is also worth mentioning a fluctuation during 2016 in which 11.2% and -8.2% returns are present. In addition, the returns have a standard deviation of 0.11%, a variance of 0.012%, and a 0.00025% mean. Finally, in Figure </a>4.8(c), the distribution of the logarithmic returns is shown. It can be observed that most density of the variable ranges between -5% and 5% and contains few dispersed returns on both left and right sides. On the left side of the distribution, returns are found up to -13%, and on the right side, up to 14%. Most of the values are clustered around the mean of the returns.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark77" class="a">Volatility is the statistical measure of the dispersion of returns for a given asset during a period. This measure provides information about the price fluctuation around its mean in percentage. The standard deviation of returns calculates daily volatility. Figure 4.8(d) shows the daily volatility of the iShares S&amp;P500 ETF, having rolling standard deviations in a 7-bar window. Good-day trading opportunities are usually related to volatile moments. According to Figure </a><a href="#bookmark78" class="a">4.8(d), three main volatility spikes were detected in 2010 (6.32%), 2016 (5.44%), and 2020 (4.3%). Moments of medium volatility, such as in 2012 and 2020, are also present in the ETF. Lastly, it is also worth mentioning that volatility only provides information that the price fluctuates around its mean, but it does not tell what side the price fluctuates. By analyzing the plot, iShares Core S&amp;P 500 ETF does not have high volatility. Figure </a>4.9 shows time series properties generated from the Western Union Corporation dollar bars.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark78" class="a">Similarly to iShares Core S&amp;P 500 ETF, Figure </a>4.9(a) shows the heat map plot informing monthly returns generated by Western Digital shares during 2010-2022. The lowest -27.8%</p><ol id="l53"><li><p class="s74" style="padding-top: 7pt;padding-left: 89pt;text-indent: -10pt;text-align: left;"><a name="bookmark78">Monthly returns heat map (b) Simple logarithmic returns</a></p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 32pt;text-indent: 0pt;text-align: left;"><span><img width="260" height="178" alt="image" src="documento[1970]/Image_069.jpg"/></span>	<span><img width="243" height="186" alt="image" src="documento[1970]/Image_070.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l54"><li><p class="s74" style="padding-top: 5pt;padding-left: 68pt;text-indent: -10pt;text-align: left;">Simple logarithmic returns distribution (d) Daily volatility</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 79pt;text-indent: 0pt;text-align: left;">Figure 4.9: Western Digital Corporation time series properties from 2010 to 2022.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark78" class="a">monthly return sits in May 2019. On the other hand, the asset’s 32% highest return was in July 2012. Comparing Figure </a><a href="#bookmark77" class="a">4.9(a) to </a><a href="#bookmark78" class="a">4.8(a), Western Digital’s monthly returns have greater amplitude than the previous data frame. This fact can be confirmed in Figure 4.9(b), which shows the logarithmic returns plot. 15% both-sided returns are present during the years of 2020 and 2021, as well as -6.2% and -5.9% variations in 2022 and 2016 respectively. The logarithmic returns contain a mean of -0.000035%, a standard deviation of 0.21%, and a variance of 0.0441%. Analyzing the distribution in Figure </a>4.9(c), it can be asserted that most of the returns are also clustered around its mean and concentrated between -10% and +10%. A few 15% variations can also be spotted on both sides of the distribution.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: right;"><a href="#bookmark78" class="a">Lastly, Figure </a><a href="#bookmark79" class="a">4.9(d) shows the daily volatility of the asset. Compared to iShares Core S&amp;P 500 ETF, Western Digital’s close price volatility is more often. The highest volatile moments were during 2020-2022, with both variations close to 6%. The COVID-19 pandemic and the United States of America inflation increase events may explain these spikes. Finally, Figure </a>4.10 shows time series properties generated from the Binance spot ATOM/USDT cryptocurrency pair.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark79" class="a">Figure 4.10(a) shows the monthly returns generated by ATOM/USDT cryptocurrency dollar bars from April 2019 and November 2021. The cryptocurrency pair had its -45.2% lowest and 137% highest returns during August 2019 and February 2021, respectively. It is also worth mentioning some variations, such as 91.6% and 88.5% in August 2020 and 2021. In addition, some significant -45.1% and -45.2% negative returns are spotted during March 2020 and August 2019. Figure </a>4.10(b) shows the logarithmic returns plot generated in the previously mentioned</p><ol id="l55"><li><p class="s74" style="padding-top: 7pt;padding-left: 89pt;text-indent: -10pt;text-align: left;"><a name="bookmark79">Monthly returns heat map (b) Simple logarithmic returns</a></p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 34pt;text-indent: 0pt;text-align: left;"><span><img width="256" height="201" alt="image" src="documento[1970]/Image_071.jpg"/></span>	<span><img width="248" height="186" alt="image" src="documento[1970]/Image_072.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-left: 58pt;text-indent: 0pt;text-align: left;">(c) Simple logarithmic returns distribution (d) Daily volatility</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 82pt;text-indent: 0pt;text-align: left;">Figure 4.10: Binance spot ATOM/USDT cryptocurrency pair from 2019 to 2021</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark79" class="a">period. According to it, the highest spike occurred in 2021 during a -24% drop, followed by a recovery. A few -15% approximately spikes can be detected in April 2020, as well as some 10% variations in the last quarter of 2019. Figure 4.10(c) shows the distribution of the logarithmic returns shown in the previously analyzed plot. Differently from the previous data frames, ATOM/USDT logarithmic returns distribution has a -4.01 skewness, which tends to negative values. The distribution has a mean of 0.001%, a standard deviation of 0.6%, and a variance of 0.35%. Compared to the previously analyzed data frames, the asset return’s dispersion is greater than the remaining. Figure </a>4.10(d) shows the volatility plot of the data frame. According to the rolling volatility, this is the most volatile asset analyzed in this master dissertation, having its maximum volatility moment evaluated at 25%. Some other 6% to 12% volatile moments are also present in April 2020. Regarding the overall trend, volatility increases during the second semester of 2019 and starts to reduce from 2021 onwards. Additionally, ATOM/USDT had a 12% volatility moment during the COVID-19 event, as well as its 25% most volatile moment in May 2021. After these events, the asset’s volatility follows an overall downward trend until the rest of the data frame, with a few spikes ranging between 2% and 4%.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">In order to check the goodness of dollar bars properties, the Jarque-Bera test of normality was conducted in both time and dollar bars to compare which one have the lowest test statistic. The Jarque-Bera (<span class="s93">JB</span>) test evaluates the goodness-of-fit whether the samples, in this case, logarithmic returns generated from the dollar and time bars, have the its skewness and kurtosis measures close enough to a normal distribution. <span class="s93">JB </span>test is defined by:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;line-height: 16pt;text-align: left;">6</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-left: 247pt;text-indent: -67pt;line-height: 44%;text-align: left;"><a name="bookmark80"><span class="s93">JB </span></a><span class="s19">= </span><span class="s26">𝑛</span><span class="s5"> </span><span class="s31">(</span><span class="s5">𝑆</span><span class="s10">2 </span>+ <span class="s94">1</span><span class="s10"> </span>(<span class="s5">𝐾 </span>− <span class="s10">3</span>)<span class="s42">2</span><span class="s31">) </span><span class="s5">, </span><span class="p">(4.8) </span><span class="s10">4</span></p><p class="s11" style="padding-top: 5pt;text-indent: 0pt;text-align: left;"><span class="s76">L</span><span class="s63">𝑁</span><span class="s25">  </span>( <span class="s5">𝑋𝑖</span><span class="s19">=</span><span class="s10">1 </span>− <span class="s5">𝜇</span>)<span class="s42">3</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">where <i>𝑛 </i>is the number of data samples originated from the variable; <i>𝑆 </i><a href="#bookmark80" class="a">is the skewness of the distribution, which is a measure of the asymmetry of it, given by Equation </a><a href="#bookmark80">4.9.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑆 <span class="s19">=</span></p><p class="s25" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">𝑖</p><p style="text-indent: 0pt;text-align: left;"><span><img width="99" height="1" alt="image" src="documento[1970]/Image_073.png"/></span></p><p class="s11" style="padding-left: 9pt;text-indent: 0pt;line-height: 18pt;text-align: left;">(<span class="s5">𝑁 </span>− <span class="s10">1</span>) ∗ <span class="s5">𝜎</span></p><p class="s59" style="padding-top: 6pt;text-indent: 0pt;text-align: left;">3 <span class="s5">, </span><span class="p">(4.9)</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">−</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">where <i>𝑆 </i>is the sum from <i>𝑖 </i><span class="s19">= </span><span class="s10">1 </span>to <i>𝑁 </i>of the difference between every sample <i>𝑋 </i>and the population mean <i>𝜇 </i>to third power, divided by the product of the total number of the population <i>𝑁 </i><span class="s10">1 </span>and the standard deviation to the power of three. In addition to it, <i>𝐾 </i>is the kurtosis which measures if the distribution contains a specified form of tail.</p><p class="s5" style="padding-top: 6pt;padding-left: 223pt;text-indent: 0pt;line-height: 18pt;text-align: left;">𝐾 <span class="s19">= </span><u>𝜇</u><span class="s95">4</span><span class="s30"> </span>, <span class="p">(4.10)</span></p><p class="s96" style="padding-left: 45pt;text-indent: 0pt;line-height: 53%;text-align: center;">𝜎<span class="s30">4</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">×</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">×</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">× ×</p><p style="text-indent: 0pt;text-align: left;"/><p class="s10" style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;line-height: 89%;text-align: justify;"><span class="p">where </span><span class="s5">𝜇</span>4 <span class="p">is the fourth central moment of the distribution and </span><span class="s5">𝜎</span>4 <span class="p">is the standard deviation to the fourth power. Dollar bars returns had </span>4<span class="s5">.</span>71 10<span class="s97">10 </span><span class="p">samples that were not according to a normal distribution, against </span>6<span class="s5">.</span>4 10<span class="s97">12 </span><span class="p">from time bars concerning the iShares Core S&amp;P500 ETF. In addition to it, Western Union dollar bars returns were 2.28 times more normally distributed compared to time bars, each one of them had </span>1<span class="s5">.</span>85 10<span class="s97">9 </span><span class="p">and </span>4<span class="s5">.</span>22 10<span class="s97">9 </span><span class="p">respectively. Lastly, as an exception due to the chosen threshold, ATOM/USDT time bars were 110 times more normally distributed than dollars’, each one of them had </span>4<span class="s5">.</span>79 <span class="s11">× </span>10<span class="s97">9 </span><span class="p">and </span>3<span class="s5">.</span>78 <span class="s11">× </span>10<span class="s97">7 </span><span class="p">respectively.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h1 style="padding-top: 7pt;padding-left: 31pt;text-indent: -14pt;text-align: left;"><a name="bookmark81">EXPERIMENTAL SETUP AND METHODOLOGY</a><a name="bookmark82">&zwnj;</a><a name="bookmark90">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">This chapter describes the proposed deep reinforcement learning-based strategy im- plementation. The following sections detail the approach to formulating trading as a Markov decision process (MDP) and implementing the market trading environment. In addition to the description of the reward design process and the back-testing methodology.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l56"><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">TRADING FORMULATED AS A MARKOV DECISION PROCESS</p><p style="padding-top: 12pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark90" class="a">Markov decision process is a time-discrete stochastic control framework that provides a mathematical foundation to model decision-making problems. In MDPs, a system is in some given state and moves forward to another state based on the agent’s decisions (decision maker). These interact continuously. The agent selects actions, and the environment responds to them by presenting new situations to the agent, as well as rewards which consist of numerical values that the decision maker seeks to maximize over time through its actions. Figure </a>5.1 shows the schematic of the proposed MDP to model an asset trading market. The environment is formulated as a set of states containing information such as technical indicators, time signatures, and candlestick dollar bars. The trader agent can interact with it by choosing between buy, hold, and sell actions. The agent’s goal is to control the system so that some performance criterion is maximized, in this case, profits.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 92pt;text-indent: 0pt;text-align: left;"><span><img width="472" height="363" alt="image" src="documento[1970]/Image_074.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 104pt;text-indent: 0pt;text-align: left;">Figure 5.1: Trading problem formulated as a Markov decision process</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l57"><li><p style="padding-top: 6pt;padding-left: 49pt;text-indent: -32pt;text-align: left;"><a name="bookmark83">Trading environment</a><a name="bookmark91">&zwnj;</a></p><p class="s5" style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;line-height: 14pt;text-align: justify;"><span class="p">In reinforcement learning, the environment is defined as the entity comprising everything outside the agent. More specifically, the agent and environment interact at each of a sequence of discrete time steps </span>𝑡 <span class="s19">= </span><span class="s10">1</span>, <span class="s10">2</span>, <span class="s10">3</span>, ... <span class="p">At each time step </span>𝑡<span class="p">, the agent receives some representation of the environment’s state </span>𝑠𝑡  𝜖  𝑆 <span class="p">where </span>𝑆 <span class="p">is the set of all possible states.  In this dissertation, the adopted environment sets each time step representing every time the market value reaches a predefined value.</span></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The state of a system provides information that helps the agent to take action; hence this is the input. In order to predict the action better than random, the features in the state must be at least weakly predictable and stationary (have a stable mean). The market environment used the following input features:</p><ul id="l58"><li><p style="padding-top: 10pt;padding-left: 59pt;text-indent: -10pt;text-align: left;">Candlestick dollar bars</p></li><li><p style="padding-top: 7pt;padding-left: 59pt;text-indent: -9pt;text-align: left;">Time signature</p></li><li><p style="padding-top: 7pt;padding-left: 59pt;text-indent: -9pt;text-align: left;">Technical indicators</p></li></ul><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">In order to construct the state, tick data, which is the record of transactions and prices of an asset, is grouped until it reaches a pre-defined market value and a dollar bar is generated as cited in Chapter 4. In the following, the current date time from the generated bar is collected, and outliers are removed according to the IQR method, taking into consideration the last seven days according to the date. Once the data is cleaned, the state can be assembled. According to the current date time, the state searches for the previous <i>𝐿𝑂𝑂𝐾 𝐵 𝐴𝐶𝐾 </i>amount of data samples of every feature and appends them it. Every feature is normalized to prevent a large-valued variable from dominating the entire neural network.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Time signature features such as the current time of the day and week were also included in the state. These features may have a significant impact on profitability. Adding a particular time of the week may provide additional information that may not be apparent from a conventional numerical timestamp. These could be events that occur in regular weekly intervals, such as Federal Open Market Committee Meetings. Aiming to calculate the current time of the day, the</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">×</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">hour and minute corresponding the current index position is retrieved and multiplied by <span class="s10">24 60</span></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">to map that into float number and then normalization is applied. To calculate the day of week, the corresponding day of the current index position is retrieved and transformed into an integer number and it is divided by 6 to normalize it.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">Finally, the following technical indicators were also included to construct the state vector. Relative Strength Index was used with in 14-day period basis on the closing prices. It is a Momentum Oscillator that is used with the intention to to measure strength or weakness of the asset. This indicator ranges on a scale from 0 to 100, with high and low levels marked at 70 and 30 representing that above the 70 level, the asset is overbought and in 30 oversold. In addition to it, Balance of Power was also included in the state. It determines the pressure of the buyers and sellers by looking at how strongly the price has changed. Aiming to identify trends, the Aroon Oscillator served also as input tracking the highs and lows for the last 25 periods. This oscillator measures the strength of the current trend and providing information about its persistence. It also ranges on a scale from 0 to 100. The higher the number indicator’s value, stronger the trend. Lastly, Moving Average Convergence/Divergence indicator was used to trade trends. It is calculated by subtracting the value of a faster period Exponential Moving Average from a slower one. It does not oscillate at some range. When this indicator crosses above zero is considered bullish, while crossing below zero is bearish.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-top: 6pt;padding-left: 49pt;text-indent: -32pt;text-align: left;"><a name="bookmark84">Agent</a><a name="bookmark92">&zwnj;</a></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><span class="p">The agent is the entity that makes decisions based on rewards and punishments, seeking to maximize the total reward in a particular environment. This work implemented the agent using two deep feed-forward artificial neural networks. The first neural network, also mentioned as the main Q-network, is used for getting the probability mass function </span>𝑝𝜃<span class="s44">𝑖</span><span class="s98">− </span>𝑠, 𝑎 <span class="p">, given the</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">current state <i>𝑠 </i>and action <i>𝑎</i>, the best action <i>𝑎</i><span class="s11">∗ </span>from the next state <i>𝑠</i><span class="s11">′</span>. The second neural network,</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 81%;text-align: justify;"><span class="p">named target Q-network, generates the probability mass function given the next state </span>𝑠<span class="s11">′</span><span class="p">, using the action caused by the main Q-network </span>𝑝𝜃<span class="s44">𝑖 </span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗ </span><span class="p">. Using the Bellman equation in the projected distribution of </span>𝑝𝜃<span class="s44">𝑖 </span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗ </span><span class="p">, a probability mass function is generated to calculate the target values for the current state </span>𝑠<span class="p">. Since the two function approximators have seen different samples, it is not</span></p><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark92" class="a">likely to overestimate the same action. Figure </a>5.2 shows both neural network inputs and outputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 26pt;text-indent: 0pt;text-align: left;"><span><img width="612" height="247" alt="image" src="documento[1970]/Image_075.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">Figure 5.2: Adopted Deep Feed-Forward Q-Neural Network architecture in the trading problem</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 5pt;text-indent: 0pt;text-align: left;"><span class="s99">f</span>𝑓 <span class="s11">(</span>𝑥<span class="s11">)</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Both deep neural networks’ architectures are composed of input, hidden, and output layers. The input layer size corresponds to the size of the state <i>𝑠 </i>concatenated with goal <i>𝑔</i>. It comprises a standard linear layer, performing linear operations to the input data. Next, it is followed by a rectified linear unit (RELU) activation function, which is defined as</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 6pt;padding-left: 15pt;text-indent: 0pt;line-height: 68%;text-align: right;"><span class="s57">= </span>𝑥 <span class="p">if </span>𝑥 &gt; <span class="s10">0</span>,</p><p class="s10" style="padding-left: 15pt;text-indent: 0pt;line-height: 13pt;text-align: right;">0 <span class="p">otherwise</span><span class="s5">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 147pt;text-indent: 0pt;text-align: left;">(5.1)</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">×</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;">where  <i>𝑓  𝑥   </i>is the output of the function, <i>𝑥 </i>is the input. This function will output the input directly if it is positive; otherwise, it will output zero.  It has become the default activation function for many types of neural networks. The hidden layer adopts the Dueling network architecture, consisting of two noisy linear stream layers that each provide separate estimates of the value and advantage functions.  The value stream outputs the state value function <i>𝑉𝜋  𝑠, 𝑔   </i>with the dimension of  <i>𝑁𝑎𝑡𝑜𝑚𝑠</i>,  which are the &quot;canonical returns&quot; of the distribution.   The advantage stream outputs the advantage of the actions in the state <i>𝑠 </i>with dimensions <i>𝑁𝑎𝑐𝑡𝑖𝑜𝑛𝑠     𝑁𝑎𝑡𝑜𝑚𝑠</i>. The Noisy linear layers substitute the standard <i>𝜖 </i>-greedy into the choice between exploration and exploitation. It injects factorized Gaussian noise to perturb the network’s weights, leading</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a name="bookmark85">to better exploration. Lastly, advantage and value streams are combined and followed by the Softmax activation function, which transforms values into probabilities. The output layer consists of the array of 3 probability masses functions, one for each action buy, sell, and hold. Each probability mass function shows the probability that each return would have to be chosen in a</a><a name="bookmark86">&zwnj;</a><a name="bookmark87">&zwnj;</a><a name="bookmark93">&zwnj;</a></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 16pt;text-align: justify;"><span class="p">certain action. The output layer has a dimension of </span>𝑁𝑎𝑐𝑡𝑖𝑜𝑛 <span class="s11">× </span>𝑁𝑎𝑡𝑜𝑚𝑠<span class="p">.</span></p></li><li><p style="padding-top: 14pt;padding-left: 49pt;text-indent: -32pt;text-align: left;">Reward</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;line-height: 14pt;text-align: justify;"><a href="#bookmark125" class="a">Reinforcement learning does not rely on detailed instructional information. According to Russell and Norcvig (2016),</a> the success of a reinforcement learning application strongly depends on how well the reward signal frames the goal of the application’s designer and how well the signal assesses progress in reaching that goal. Every reward function has pros and cons, and it is up to the designer who decides which is the best for them. In this master’s dissertation, the exponential profit and loss were adopted as the reward function. It is calculated by subtracting the current price (<i>𝑝𝑡</i>), taking into consideration the transaction cost <i>𝑡𝑐 </i>involved when exiting the position, subtracted by the entry price (<i>𝑝</i><span class="s10">0</span>), also taking in consideration <i>𝑡𝑐 </i>in respect to starting the trade.  All variables mentioned above are divided by the entry price (<i>𝑝</i><span class="s10">0</span>), also taking into consideration <i>𝑡𝑐 </i>and multiplied by the respective <i>𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛 </i><a href="#bookmark93" class="a">taken in the trade (1 for long and -1 for short). Finally, the natural exponent is taken of the respective value. Equation 5.2</a> shows the reward function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="80" height="1" alt="image" src="documento[1970]/Image_076.png"/></span></p><p class="s5" style="padding-top: 10pt;padding-left: 15pt;text-indent: 0pt;text-align: right;">𝑅<span class="s11">(</span>𝑠, 𝑎, 𝑔<span class="s11">) </span><span class="s19">= </span>𝑒</p><p class="s45" style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><span class="s100">𝑝</span>𝑡 <span class="s101">(</span><span class="s28">1</span><span class="s36">−</span>𝑡𝑐 <span class="s101">)</span><span class="s36"> − </span>𝑝<span class="s102">0</span><span class="s28"> </span><span class="s101">(</span><span class="s28">1</span><span class="s36">+</span>𝑡𝑐 <span class="s101">)</span><span class="s36"> </span><span class="s75">×</span><span class="s25">𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛</span></p><p style="padding-top: 12pt;padding-left: 120pt;text-indent: 0pt;text-align: left;">(5.2)</p><p class="s36" style="text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s100">𝑝</span><span class="s45">  </span>(<span class="s28">1</span>+<span class="s45">𝑡  </span>)<span class="s45">𝑐</span><span class="s102">0</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 4pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark94" class="a">The Exponential profit and loss were chosen as the reward function based on the assumption that the agent should receive a greater reward for greater returns. At each time step, the agent will predict whether the trade is over or not. The first option will consider the current position, and the agent will provide the reward. On the other hand, if the trade has yet to be over, the agent will take action and will not receive any reward. Figure </a>5.3 shows the process mentioned above.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">BACK-TESTING MODEL PROCESS</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">During the back-testing process, the model walks through the data frame multiple times to learn and improve trading. An episode is defined once the agent starts a trade by buying an asset (action 1) and exits when a sell signal is received later (action -1). In addition, a sequence can also be considered an episode once the agent starts the trade by selling an asset (action -1) and exits when a buy signal is received (action 1).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l59"><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Sequential sampling</p><p style="padding-top: 8pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark94" class="a">Train-test splits and k-fold cross-validation are statistical methods often adopted to evaluate performance in machine learning models. However, these approaches do not work in the case of time series data because they ignore the temporal components inherent in the problem. In the time dimension of observations, it is impossible to split data randomly into groups. Instead, data must be split concerning the temporal order in which values were observed. In time series forecasting, this evaluation of models on historical data is called backtesting. The proposed framework was evaluated according to the sequential sampling approach. The model steps through the dataset sequentially, running episode after episode, improving itself along the way. Figure </a>5.4 is described to illustrate sequential sampling.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 122pt;text-indent: 0pt;text-align: left;"><span><img width="329" height="368" alt="image" src="documento[1970]/Image_077.gif"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><a name="bookmark94">Figure 5.3: Sparse reward design process for asset trading</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 33pt;text-indent: 0pt;text-align: left;"><span><img width="542" height="326" alt="image" src="documento[1970]/Image_078.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 150pt;text-indent: 0pt;text-align: left;">Figure 5.4: Sequential data sampling approach</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">In this master dissertation, trade and episode are considered synonyms. The first trade starts at 2008-01-01 11:00 AM, finishes at 2008-05-01 4:00 PM, and is classified as the 1st episode. Instead of random sampling, the following trade (episode) will start at the data sample:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;"><a name="bookmark88">2008-05-01 at 4:05 PM. The 2nd episode starts at 2008-05-01 4:05 PM and lasts until 2009-01-05 6:25 AM. Subsequently, Trade 3 (3rd episode) will start at 2009-01-05 6:30 AM and ends at 2009-05-14 8:15 AM. It is possible to infer that a new trade must begin on 2009-05-14 at 8:20 AM. This mechanism goes until the end of the data frame.</a><a name="bookmark95">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 49pt;text-indent: -32pt;text-align: left;">Back-testing process</p><p style="padding-top: 8pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark96" class="a">This subsection describes the backtesting process of training the agent to identify trading opportunities. The proposed algorithm contains a couple of steps: the sampling and learning phases. The sampling phase consists in integrating with the environment, collecting experience tuples, and sampling them to the learning phase according to priority. On the other hand, the learning phases consist in feeding the batch of the sampled experience tuples and calculating the target and the loss function. Lastly, the algorithm performs the backward pass to transmit what it has learned from the loss function to its neural network weights. This way, the algorithm can approximate a function that maximizes the goals defined by the designer. Figure </a>5.5 shows an overview of the process above described of backtesting implementation.</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( || )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( || )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">The application starts by instantiating the market environment, the main and target Q-artificial neural networks, and the hindsight-prioritized experience replay buffer. A new episode starts, and a state is generated for each time step. It is a combination of market features observed at a certain time-step <i>𝑡 </i>presented to the agent. The state combines multi-resolution dollar bars, technical indicators, and time signature features. The first feature is regarding the asset’s price and is sampled every moment the market value reaches a predefined threshold. Technical indicators were also calculated from the price feature and concatenated to the state. Lastly, the current day of the week is served as time signature features to capture an event that may occur in weekly intervals. In the following, a goal is generated according to a normal distribution. It is concatenated with the state, composing the final tensor <i>𝑠 𝑔 </i>that serves as input to the main Q-network. Since this is the start of the algorithm, it can’t get the terminal state and put it as the goal; hence it is randomly generated. Then, the tensor <i>𝑠 𝑔 </i>serves as input to the main Q-network and takes action <i>𝑎 </i>according to the policy <i>𝜋</i>. Once the environment receives action <i>𝑎</i>,</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( || )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 83%;text-align: justify;"><span class="p">it generates a next state </span>𝑠<span class="s11">′</span><span class="p">, an immediate reward </span>𝑟<span class="p">, and whether the trade is over. The algorithm concatenates the chosen goal </span>𝑔 <span class="p">with the following state </span>𝑠<span class="s11">′ </span><span class="p">forming the tensor </span>𝑠<span class="s11">′ </span>𝑔 <span class="p">and stores it as an experience tuple in the hindsight-prioritized experience replay. This experience tuple</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">||</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( || )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">|| ||</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;line-height: 84%;text-align: justify;"><span class="p">contains the current state </span>𝑠 <span class="p">the agent is in concatenated with the chosen goal </span>𝑔   𝑠  𝑔  <span class="p">, the taken action </span>𝑎<span class="p">, the immediate reward received </span>𝑟<span class="p">, the next state </span>𝑠<span class="s11">′ </span><span class="p">received concatenated with the goal </span>𝑔 <span class="p">(</span>𝑠<span class="s11">′  </span>𝑔<span class="p">) and if the trade is over or not.  In the following, if the designer wishes to set additional goals to the hindsight-prioritized experience replay to create a broader perception of subgoals to the agent, a new goal </span>𝑔<span class="s11">′ </span><span class="p">is selected according to a predefined rule and concatenated with the current state </span>𝑠 <span class="p">(</span>𝑠  𝑔<span class="s11">′</span><span class="p">) and next state (</span>𝑠<span class="s11">′  </span>𝑔<span class="s11">′</span><span class="p">) and stored as experience tuple in the replay buffer. The accumulated experiences at their initial moment have a maximum priority of </span><span class="s10">1</span>.<span class="s10">0 </span><span class="p">since they</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark45" class="a">should be sampled at least once by the replay buffer. Prioritizing experiences introduces bias due to the sampling proportion corresponding to the temporal difference error. The algorithm uses importance-sampling weights (Equation </a>2.41) that fully compensate for the non-uniform probabilities <i>𝑃 𝑖 </i>if <i>𝛽 </i><span class="s19">= </span><span class="s10">1 </span>to tackle this problem. The hyper-parameter <i>𝛽 </i>starts at <span class="s10">0</span><i>.</i><span class="s10">4 </span>and is</p><p class="s10" style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><span class="p">linearly increased during every time step and reaches </span>1<span class="s5">.</span>0 <span class="p">at the end of training. The learning</span></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark45" class="a">phase only begins when the experience replay contains a number of tuples greater than the defined batch size to train the neural network. If it does not, the algorithm repeats the same previous steps until it reaches enough experience. On the other hand, if the memory size is greater than the batch size, the algorithm gets experience tuples indexes according to their priority using Equation </a>(2.40). It is a stochastic sampling method that alternates between pure greedy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;">prioritization and uniform random sampling. In addition to it, the probability weights <i>𝑃  𝑖   </i><a href="#bookmark45" class="a">are corrected and computed using 2.41</a> to compensate for the non-uniform probabilities <i>𝑃  𝑖   </i>saving the correct weights in the <i>𝑤𝑖 </i>array. In the following, the learning phase starts. This phase consists in calculating the target for the distributions generated by the network and consequently its loss (<i>𝐿  𝜃𝑖  </i>). This phase will be covered in further detail in the following subsection. After calculating</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( ) ( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">𝐿  𝜃𝑖  <span class="p">, an element-wise multiplication is performed between </span>𝑤𝑖 <span class="p">and </span>𝐿  𝜃𝑖  <span class="p">, and the neural network performs the backward pass to adjust its weights. The learned experience’s priorities are updated by summing the proportion of their temporal difference error with a minimum value of </span>𝜖 <span class="p">to make priorities </span>&gt; <span class="s10">0</span><span class="p">. Lastly, if the trade did not finish, the algorithm repeats all steps above until it is. Once the trade ends, the target Q-network performs a hard copy update from the main Q-network</span></p><p style="padding-left: 16pt;text-indent: 0pt;line-height: 13pt;text-align: left;">weights.</p><p style="padding-top: 1pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark97" class="a">Figure </a>5.6 shows the overall learning phase that includes taking the sampled batches generated from the sampling phase, calculate the target distribution and the loss of the neural network.</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">||</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">||</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><span class="p">The learning process of the algorithm starts by sampling a batch of tuples based on priority from the replay buffer. The batch size was 1 for illustration purposes. In the following, the next state concatenated with the goal (</span>𝑠<span class="s11">′ </span>𝑔<span class="p">) serves as input to the main Q-network. The first linear</span></p><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><span class="p">input layer of the network processes </span>𝑠<span class="s11">′ </span>𝑔 <span class="p">and gets split into two branches: the value and advantage</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">streams. This division benefits learning since the network learns to estimate the distribution of</p><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;">values only for important conditions; hence it learns what states and goals are more valuable. It is important in sparsed-reward environments since feedback only occurs occasionally. Each branch is composed of noisy linear layers, which add parametric noise to the network weights used to drive exploration. The layer contains a hyperparameter called <i>𝜎</i>, the equivalent initial standard deviation for creating the perturbation. The higher this hyperparameter is, the more energy the injected noise perturbs the weights. The value and advantage streams are combined again and followed by a softmax layer to produce the final probability distributions, one for each action. Then, for each distribution, each atom is multiplied with its respective probabilities to generate</p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 88%;text-align: justify;">Q-values, one for each distribution. The greedy action <i>𝑎</i><span class="s11">∗ </span>is the distribution that returned the highest Q-value. This action is stored, and the same procedure of predicting the probabilities of</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">||</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-left: 17pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><span class="p">each action from (</span>𝑠<span class="s11">′ </span>𝑔<span class="p">) occurs in the target network. However, the target network only evaluates the action. Instead of generating Q-values and selecting the greedy action, the greedy action generated by the main Q-Network stored is retrieved, and its probability distribution is collected in the target network. This probability distribution is denominated </span>𝑝𝜃<span class="s36">− </span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗ </span><span class="p">. To perform the</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">Bellman update, a support <i>𝑧𝑖 </i>array is created. It contains the same number of atoms that the</p><p class="s25" style="text-indent: 0pt;line-height: 15pt;text-align: left;">𝜃 <span class="s48">− </span><span class="s47">( ) </span>𝑚𝑖𝑛</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">T</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 2pt;padding-left: 17pt;text-indent: 0pt;line-height: 69%;text-align: justify;">𝑝 𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗ </span><span class="p">distribution. However, its values are evenly spaced, ranging from minimum </span>𝑉 <span class="p">and maximum </span>𝑉𝑚𝑎𝑥 <span class="p">values. In the following, the Bellman update </span><span class="s16">ˆ </span>𝑧 𝑗 <span class="p">is performed concerning the</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 15pt;text-align: justify;">support <i>𝑧𝑖</i>. <i>𝑧𝑖 </i>is multiplied by the discount factor <i>𝛾 </i>and summed to the immediate reward <i>𝑟</i>. Since adding <i>𝑟 </i>dislocates the distribution in the x-axis, the update may be beyond the set [<i>𝑉𝑚𝑖𝑛</i>,<i>𝑉𝑚𝑎𝑥</i>]</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">T T</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">intervals. To tackle this issue, <span class="s16">ˆ </span><i>𝑧 𝑗 </i>is clipped into the previous range. To project <span class="s16">ˆ </span><i>𝑧 𝑗 </i>closest to</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">T</p><p style="text-indent: 0pt;text-align: left;"/><p class="s5" style="padding-top: 2pt;padding-left: 16pt;text-indent: 0pt;line-height: 86%;text-align: justify;"><span class="p">the support </span>𝑧𝑖<span class="p">, the </span>𝑏𝑖 <span class="p">array computes the closest real-valued index positions, which each value of  </span><span class="s16">ˆ</span><span class="s10"> </span>𝑧 𝑗 <span class="p">is closer in respect to </span>𝑧𝑖 <span class="p">and from that its integer lower and upper neighboring indexes. The probabilities of </span>𝑝𝜃<span class="s36">−   </span>𝑠<span class="s11">′</span>, 𝑎<span class="s11">∗  </span><span class="p">are distributed and saved to the projected distribution </span>𝑚<span class="p">. In the following, the main Q-network predicts the distributions from the current state concatenated</span></p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">( )</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">||</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;">with the goal tensor (<i>𝑠  𝑔</i>), and the distribution corresponding to the action <i>𝑎 </i>in the experience serves as the chosen action. This distribution is denominated <i>𝑝𝜃  𝑠, 𝑎  </i>. Finally, the loss between the project distribution <i>𝑚 </i>and <i>𝑝𝜃  𝑠, 𝑎   </i>is calculated concerning the cross-entropy loss function, which measures the difference between two probability distributions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span><img width="623" height="732" alt="image" src="documento[1970]/Image_079.jpg"/></span></p><p class="s1" style="padding-top: 9pt;padding-left: 91pt;text-indent: 0pt;text-align: left;"><a name="bookmark89">Figure 5.5: Deep reinforcement learning-based automated strategy overview</a><a name="bookmark96">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">HYPERPARAMETERS</p><p style="padding-top: 12pt;padding-left: 59pt;text-indent: 0pt;text-align: left;">This section describes the set of hyperparameters necessary to tune the model. Table</p><ol id="l60"><ol id="l61"><li><p style="padding-left: 34pt;text-indent: -18pt;text-align: left;">shows the chosen values for each.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l62"><li><h1 style="padding-top: 5pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;"><a name="bookmark97">Learning Rate</a><span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;">Determines the step size at each iteration while moving toward a minimum of a loss function.</p></li><li><h1 style="padding-top: 7pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Loss Optimizer<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Optimizer algorithm to reduce loss.</p></li><li><h1 style="padding-top: 8pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Loss Function<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Maps the distance between the current output of the algorithm and the expected output.</p></li><li><h1 style="padding-top: 8pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Activation Function<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Function that adds non-linearity to the neural network</p></li><li><h1 style="padding-top: 8pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Number of Actions<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Number of discrete actions that agent can take. Buy, sell, and hold.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;text-align: left;"><span><img width="590" height="577" alt="image" src="documento[1970]/Image_080.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 77pt;text-indent: 0pt;text-align: left;">Figure 5.6: Deep reinforcement learning-based automated strategy learning process</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><h1 style="padding-top: 5pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;"><a name="bookmark98">Lookback</a><span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Number of previous timesteps that are used in order to predict the subsequent timestep.</p></li><li><h1 style="padding-top: 8pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Batch Size<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Number of samples that are propagated through the neural networks.</p></li><li><h1 style="padding-top: 8pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Discount Rate<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Reflects both time value and risk compensation.</p></li><li><h1 style="padding-top: 8pt;padding-left: 59pt;text-indent: -10pt;line-height: 15pt;text-align: left;">Memory Size<span class="p">:</span></h1><p class="s5" style="padding-left: 59pt;text-indent: 0pt;line-height: 18pt;text-align: left;"><span class="p">Maximum amount of </span>&lt; 𝑠 <span class="s11">|| </span>𝑔, 𝑎, 𝑟, 𝑠<span class="s46">′</span><span class="s36">  </span><span class="s11">|| </span>𝑔 &gt; <span class="p">tuples that the replay buffer can store.</span></p></li><li><h1 style="padding-top: 5pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Additional Goals<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Number of Goals to add to the Hindsight Experience Replay</p></li><li><h1 style="padding-top: 8pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Noisy layer initial standard deviation (<i>𝜎</i><span class="s10">0</span>)<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;">Coming from Factorized Gaussian linear layer. Initial standard deviation to inject noisy into the neural network’s weights.</p></li><li><h1 style="padding-top: 7pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Prioritization of experiences<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Amount of prioritization of experience tuples.</p></li><li><h1 style="padding-top: 8pt;padding-left: 59pt;text-indent: -10pt;line-height: 17pt;text-align: left;">Distributional Atom (<i>𝑁𝑎𝑡𝑜𝑚𝑠</i>)<span class="p">:</span></h1><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Number of &quot;canonical values&quot; generated from the distribution.</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">/</p><p style="text-indent: 0pt;text-align: left;"/></li><li><h1 style="padding-top: 9pt;padding-left: 59pt;text-indent: -10pt;text-align: left;">Distributional minimum and maximum values (<i>𝑉𝑚𝑖𝑛  𝑉𝑚𝑎𝑥</i>)<span class="p">: Maximum and minimum value of the generated distribution.</span></h1></li></ul></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 45pt;text-indent: 0pt;text-align: center;">Table 5.1:  Hyperparameters</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:19.1025pt" cellspacing="0"><tr style="height:10pt"><td style="width:143pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="2"><p class="s103" style="padding-left: 39pt;text-indent: 0pt;line-height: 8pt;text-align: left;">2*<b>Hyperparameter</b></p></td><td style="width:310pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" colspan="3"><p class="s104" style="padding-left: 143pt;padding-right: 143pt;text-indent: 0pt;line-height: 8pt;text-align: center;">Assets</p></td></tr><tr style="height:9pt"><td style="width:100pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s104" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">iShares Cores S&amp;P500 ETF</p></td><td style="width:103pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s104" style="padding-left: 4pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Western Digital Corporation</p></td><td style="width:107pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s104" style="padding-left: 4pt;text-indent: 0pt;line-height: 8pt;text-align: left;">ATOMUSDT Cryptocurrency</p></td></tr><tr style="height:8pt"><td style="width:143pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Learning Rate</p></td><td style="width:100pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="15"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.0000625</p><p class="s103" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">Adam</p><p class="s103" style="padding-left: 3pt;padding-right: 23pt;text-indent: 0pt;line-height: 109%;text-align: left;">Cross-entropy Rectified Linear Unit 20</p><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 9pt;text-align: left;">8</p><p class="s103" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">0.99</p><p class="s103" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">500</p><p class="s103" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">5</p><p class="s103" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">0.5</p><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.5</p><p class="s106" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s103">0.4 </span><span class="s105">→−  </span>1<span class="s107">.</span>0</p><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">51</p><p class="s103" style="padding-left: 3pt;padding-right: 47pt;text-indent: 0pt;line-height: 9pt;text-align: left;">[-20, 100] 500K USD</p></td><td style="width:103pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="15"><p class="s103" style="padding-left: 4pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.0000625</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">Adam</p><p class="s103" style="padding-left: 4pt;padding-right: 33pt;text-indent: 0pt;line-height: 109%;text-align: left;">Cross-entropy Rectified Linear Unit 20</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;line-height: 9pt;text-align: left;">16</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">0.99</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">500</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">3</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">0.8</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.5</p><p class="s106" style="padding-left: 4pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s103">0.4 </span><span class="s105">→−  </span>1<span class="s107">.</span>0</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;line-height: 8pt;text-align: left;">51</p><p class="s103" style="padding-left: 4pt;padding-right: 57pt;text-indent: 0pt;line-height: 9pt;text-align: left;">[-20, 100] 1M USD</p></td><td style="width:107pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="15"><p class="s103" style="padding-left: 4pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.00000425</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">Adam</p><p class="s103" style="padding-left: 4pt;padding-right: 30pt;text-indent: 0pt;line-height: 109%;text-align: left;">Cross-entropy Rectified Linear Unit 20</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;line-height: 9pt;text-align: left;">8</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">0.99</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">600</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">4</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;text-align: left;">0.3</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.5</p><p class="s106" style="padding-left: 4pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s103">0.4 </span><span class="s105">→−  </span>1<span class="s107">.</span>0</p><p class="s103" style="padding-left: 4pt;text-indent: 0pt;line-height: 8pt;text-align: left;">51</p><p class="s103" style="padding-left: 4pt;padding-right: 57pt;text-indent: 0pt;line-height: 9pt;text-align: left;">[-20, 100] 7.5K USD</p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Loss Optimizer</p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Loss Function</p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Activation Function</p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Lookback</p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Batch Size</p></td></tr><tr style="height:12pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Discount Rate (<i>𝛾</i><span class="s105">)</span></p></td></tr><tr style="height:7pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 5pt;text-align: left;">Memory Size</p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Additional Goals</p></td></tr><tr style="height:12pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Initial Standard Deviation (<i>𝜎</i><span class="s106">0</span><span class="s105">)</span></p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Experiences prioritization (<i>𝛼</i><span class="s105">)</span></p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Prioritization bias compensation (<i>𝛽</i><span class="s105">)</span></p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Distributional atoms (N<i>𝑎𝑡𝑜𝑚𝑠</i><span class="s105">)</span></p></td></tr><tr style="height:9pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s107" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s103">Distributional min/max values (V</span>𝑚𝑖𝑛<span class="s105">/</span>𝑉𝑚𝑎𝑥<span class="s105">)</span></p></td></tr><tr style="height:8pt"><td style="width:143pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s103" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Market value sampling threshold</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 9pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><span class="p">At the first attempt, the learning rate was set to the standard rate indicated for the Adam optimizer, which is </span>0<span class="s5">.</span>001<span class="p">. However, the algorithm presented a poor performance by not being able to converge to a good policy. Then, according to DRL similar papers, the adopted learning rate of </span>0<span class="s5">.</span>0000625 <span class="p">demonstrated an improvement. Initially, this rate was adopted to optimize the stochastic gradient descent, but it showed promising results in Adam. In the network architecture, RELUs were adopted as activation functions except for the last layer, which contains the Softmax function. RELUs were chosen instead of other types because they avoid nonlinear activation functions and reduce complexity. Lookback was set to 20. This parameter was highly sensible to</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">tune. The model showed a higher variance during the training when the lookback parameter was greater than 60. The same fact occured in the batch size. The higher both parameters, the more variance in the results. The opposite is also valid. The discount rate was set to 0.99 to make the agent seek to learn as long-term as possible. The memory size was set to 500. The memory size did not have to be expressive due to the prioritization feature implemented, allowing the agent to keep the most important transitions. To sample sub-goals to make the agent learn as much as possible from alternative outcomes, additional goals were fed to the agent. Each data frame had distinct amounts, having the iShares Cores S&amp;P500 ETF the maximum amount of 5 different goals. To perform better exploration, the initial standard deviation set to perturb the noise of the neural network was 0.5, 0.8, and 0.3, respectively. In most volatile assets, exploring much is not a good idea due to the possibility of a loss. The more volatile the asset, the lower <i>𝜎</i><span class="s10">0</span>. The prioritization exponent, which sets how much prioritization to set in transitions, was set to</p><p class="s11" style="text-indent: 0pt;line-height: 15pt;text-align: left;">[− ] [ ]</p><p style="text-indent: 0pt;text-align: left;"/><p class="s10" style="padding-top: 1pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><span class="p">0.5. The value was set according to the original implemented paper. The same applies to the </span><span class="s5">𝛽 </span><span class="p">parameter that compensates for the non-uniform distribution sampling which is linearly increased at every time step from </span>0<span class="s5">.</span>4 <span class="p">to </span>1<span class="s5">.</span>0<span class="p">. This hyperparameter was set to 0.6. Lastly, the distribution’s number of &quot;canonical returns&quot; was 51 as the original implementation of the categorical DQN algorithm. However, the minimum and maximum values that the distribution can have were set to -20 and 100. Some other values such as </span>10<span class="s5">, </span>10 <span class="p">and </span>0<span class="s5">, </span>200 <span class="p">were tried. However, the best results were achieved with the pair mentioned above.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h1 style="padding-top: 7pt;padding-left: 31pt;text-indent: -14pt;text-align: left;"><a name="bookmark99">PERFORMANCE ANALYSIS</a><a name="bookmark100">&zwnj;</a><a name="bookmark105">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 16pt;text-indent: 42pt;line-height: 14pt;text-align: justify;">This chapter presents the results of the conducted experiments. In order to evaluate and compare performance, cumulative returns, Sharpe ratio, drawdown, monthly and annual returns were calculated for every benchmark and agent’s strategy, followed by an analysis contemplating each of them. Transaction fees (<i>𝑡𝑐</i>) were taken into consideration. The adopted <i>𝑡𝑐 </i>in ATOM/USDT crytocurrency was <span class="s10">0</span><i>.</i><span class="s10">03% </span>and <span class="s10">0</span><i>.</i><span class="s10">18% </span>for the remaining.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l63"><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">CUMULATIVE RETURNS</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark105" class="a">In this section, each strategy’s cumulative product of the daily percentage change was calculated and analyzed. The cumulative returns metrics show the historical evolution of the portfolio value and not only its final evaluation. Figure </a>6.1 shows the cumulative returns regarding the abovementioned strategies applied to the iShares S&amp;P500 ETF data frame.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="528" height="403" alt="image" src="documento[1970]/Image_081.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 76pt;text-indent: 0pt;text-align: left;">Figure 6.1: Daily iShares Core S&amp;P 500 ETF cumulative returns from 2010 to 2022</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark105" class="a">As indicated by Figure </a>6.1, the ETDQN agent outperformed all benchmarks regarding cumulative returns concerning the minimum, mean, and maximum value. The strategy had a mean of 3.86, reaching a maximum portfolio value of 8.61 and a minimum of 1.0 between 2010 and 2022. The policy did not lose the invested amount at any moment during the 12 years.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark77" class="a">Starting in 2010, the model had a good performance, outperforming every benchmark. It formed a local peak in April and struggled to learn a good policy until 2016. According to Figure </a>4.8(d), a 6% volatility spike occurs in the asset, and the agent can take advantage of it, overtaking the lead in performance. The strategy keeps increasing the profits compared to the remaining onwards. In March 2020, once the Coronavirus pandemic (COVID-19) happened, the model achieved its best performing moment, having an outstanding 164% profit, reaching 8.61 cumulative returns. Followed by a -6.9% drop in April, the agent keeps increasing its profits and ends its portfolio evaluated at 7.60.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">TDQN benchmark had a mean of 2.64, a maximum of 4.92, a minimum of 0.95. Even though the strategy is increasing between 2010 and 2013, still does not perform better than the previous strategy. However, in 2013 the TDQN outperforms the agent strategy following an up-trend reaching 2.63 cumulative returns. Unfortunately, differently from the agent strategy is not able to identify and take advantage from the 2019 and 2020 dumps. From 2021 onwards, the strategy cumulative returns keeps increasing achieving its maximum value.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The <i>Buy-and-Hold </i>benchmark had a mean of 2.06, a maximum of 3.01, and a minimum of 0.91 portfolio value. The strategy ranges between 1.0 to 1.43 cumulative returns until the end of 2011, when the asset starts an up-trend that remains active until 2016, Having the second position concerning performance. In the following, the strategy has a short-term downtrend having -2.2% and -5.3% drops, respectively, in December 2015 and January 2016. In addition, the approach formed a local bottom in December 2018 with a -10.1% dump, followed by a recovery process; the asset was affected by the COVID-19 pandemic event during March 2020, having its worst month with a -15.7% dump. The portfolio assumes an up-trend and ends the backtest evaluated at 2.61, recovering from the drop in a v-shaped way. Contrasting the previous strategy.</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">The <i>Sell-and-Hold </i>benchmark had a poor performance with a mean of 0.23 of the initial</p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">invested amount. This strategy assumes the worst position regarding performance. A maximum portfolio value of 1.05 was achieved, as well as a minimum of 0.23. Visually analyzing the strategy, a down-trend pattern in the long term is observable. Some positive spikes during 2012, 2019, and 2020 are detected, even though a reversion and continuation of the down-trend follow them.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark106" class="a">Lastly, the Random Action strategy achieved the third position regarding performance with a mean of 0.96, a maximum of 1.19, and a minimum of 0.58. It achieved the third position regarding mean performance. During the 2010 - 2016 period, the strategy does not have a considerable variation in performance. In the following years, it assumes a slight down-trend finishing the backtest and having a 0.89 portfolio value. Figure </a>6.2 shows the cumulative returns from the same strategies applied to the Western Digital Corporation from 2010 to 2022.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 30pt;text-indent: 0pt;text-align: left;"><span><img width="524" height="382" alt="image" src="documento[1970]/Image_082.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 74pt;text-indent: 0pt;text-align: left;"><a name="bookmark106">Figure 6.2: Daily Western Digital Corporation cumulative returns from 2010 to 2022</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Similarly to the previous data frame, ETDQN outperformed all benchmarks. In 2012, the strategy was evaluated at 0.93, achieving its worst period. From 2012 onwards, the policy stood out from the other benchmarks and doubled its initial investment. The strategy stabilized between 3.0 and 3.5 during 2014/mid-2015. At the end of 2015, it increased its value five times during a drop in the asset. From 2016 to 2018, the asset moves sideways, so the strategy. In 2018, the asset assumed a down-trend again; The policy increased its value by eight times, followed by a drop of 1.5. In the following, the model recovers its value during the COVID-19 event and has its highest 164% pump during March. After the event, the policy assumes a slightly sloped downtrend and finishes the backtest assuming an up-trend evaluated at 51.94. In this strategy, the mean portfolio value was evaluated at 17.12, meaning that the investment would have increased to the above amount.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">TDQN benchmark had a minimum value of 0.87, a maximum of 6.12, a mean of 2.40. The strategy had the second best position regarding performance. Between 2010 and 2013, this strategy is able to outperform the previous policy, achieving a peak of 3.1. However, from 2013 onwards the strategy is not able to generalize well staying on second position until the end of the backtest. Between the years of 2013 to the half-2014, the strategy’s follows a downtrend, followed by a recovery in 2015 reaching its highest 6.12 evaluation in the end of 2018. Unfortunately, from 2019 onwards, the model is not able to generalize well, failing on taking advantage of the COVID-19 event related loss. It reaches its 0.87 lowest value on 2022.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">With respect to the <i>Buy-and-Hold </i>benchmark, the strategy had a minimum evaluation of 0.78 during 2012, a maximum of 3.86 in 2015, and a mean of 2.13, achieving second place</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a name="bookmark107">regarding performance. The policy oscillated between 1.0 and 1.5 from 2010 until 2014. An uptrend started and went up to 2015 when the asset had its best-evaluated value of 3.86. In the following, the strategy loses all profits earned from the previous trend and goes back to 1.2 in the most significant dump of the asset of -78% to its maximum value. The same pattern repeats from 2016 to 2019, but with a lower margin. The asset had a -58% dump during the COVID-19 event, followed by a recovery in 2021. The asset finishes the backtest evaluated in 1.21.</a></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Contrasting the <i>Buy-and-Hold </i>strategy, <i>Sell-and-Hold </i>benchmark had achieved the fifth and last position regarding performance against the remaining benchmarks. The policy’s evaluation is a minimum value of 0.05 in 2022, a maximum of 1.06 in 2010, and a mean of 0.23. During the 2010-2015 interval, as opposed to the previous strategy, a strong downtrend was predominant until the mid of 2015. From 2015-2016, the strategy has one of its best performance moments recovering its portfolio with an 11.2% pump. Followed by the asset value increase in 2016, the <i>Sell-and-Hold </i>benchmark policy is penalized again, reverting its profits and dropping</p><p style="padding-left: 17pt;text-indent: -2pt;text-align: justify;">-21.2% while it traded in 2018 with a 28% profit. The strategy keeps decreasing its value until reaching 0.05 of the initial investment.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark107" class="a">Lastly, the Random Action strategy had a fourth place with a minimum value of 0.45. The strategy assumed a downtrend from 2010 to 2019. It assumes an uptrend during the next three years, followed by another downtrend. Figure </a>6.3 shows the cumulative returns from the same strategies applied to the Binance spot cryptocurrency pair from 2019 to 2022.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: left;"><span><img width="552" height="411" alt="image" src="documento[1970]/Image_083.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 6pt;padding-left: 70pt;text-indent: 0pt;text-align: left;">Figure 6.3: Daily ATOM/USDT cryptocurrency cumulative returns from 2019 to 2022</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 59pt;text-indent: 0pt;text-align: justify;">The agent strategy outperformed all benchmarks regarding the Binance spot ATOM-</p><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;">/USDT data frame. The evaluated policy metrics were a minimum value of 0.95 in August 2019 and a maximum value of 37.62 in November 2021. With a portfolio value performance mean of 12.08, the strategy starts with fewer bars than the remaining datasets and, throughout the backtest course, has its amount increased. During September 2019 and July 2020, the strategy increased its initial investment by three and then decreased performance. During this period, the asset price significantly increases quickly and gets equivalent to the agents’ performance. Fortunately, in July 2021, the agent is smart enough to capture the asset’s upward trend and outperforms the <i>Buy-and-Hold </i>benchmark by opening short positions during the price fall.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">TDQN had a minimum of 0.26, a maximum value of 6.12, and a mean of 2.40 cumulative returns. Between the intervals from 2019-07 to 2020-03, the agent struggles to be competitive, reaching its lowest value of 0.26 in March 2020. Fortunately, the policy is able to learn from the experiences and follows an up-trend from April 2020 on wards outperforming the <i>Buy-and-Hold </i>strategy in the end of June 2020 and <i>Sell-and-Hold </i>benchmark in July. The strategy reaches its maximum peak value of 6.12 during May 2021, surpassing even the previous strategy at the moment. Unfortunately, differently from the above mentioned policy, the agent is not able to generalize well at the moment and has a worse performance than <i>Buy-and-Hold </i>benchmark, assuming a downtrend until the end of the backtest.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">The <i>Buy-and-Hold </i>benchmark achieved second place in performance, being evaluated at 0.29 during its minimum value in August 2019 and at 7.67 during its maximum performance in August 2021. The strategy had a 5.65 cumulative return mean regarding performance. Between July 2019 and April 2020, the strategy had its worst performance having its minimum evaluated performance value within this interval. In contrast to this period, the cryptocurrency market’s Bullrun starts, and the asset multiplies its value by 8, recovering its losses and generating profits. From April 2021 onwards, the strategy moves sideways, followed by a -33.9% drop during May 2021. The strategy forms a local bottom followed by another 52% pump between July and October, stabilizing its value at 6.84.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The <i>Sell-and-Hold </i>benchmark strategy had the fourth position regarding cumulative returns contrasting the previous strategy. In addition, the policy had its worst performance equivalent to its mean of 0.01. If the investor tried to allocate all capital into a short position, he would have lost all investment. During the start of the backtest, the strategy had its best performance moment of 2.22, outperforming even the ETDQN. However, the strategy assumes a downtrend and does not recover from it.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Lastly, the Random Action strategy achieved third place regarding performance. This policy achieved its worst moment of 0.01 accumulated returns in October 2021 and its best performance moment at the start of the backtest. This policy had a better position than the previous strategy due to its best performance between January and April of 2021, even though both assume downtrends during the entire backtest.</p><p style="padding-left: 16pt;text-indent: 43pt;text-align: justify;"><a href="#bookmark108" class="a">In order to summarize all information regarding cumulative returns, Table </a>6.1 presents the minimum, mean and maximum accumulated return values from ETDQN, <i>Buy-and-Hold</i>, <i>Sell-and-Hold </i>benchmark, and Random Action strategies in respect to the iShares S&amp;P 500 ETF, Western Digital Corporation and Binance spot ATOM/USDT cryptocurrency data frames.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s108" style="padding-top: 3pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Assets</p><p style="text-indent: 0pt;text-align: left;"/><p class="s1" style="padding-top: 6pt;padding-left: 171pt;text-indent: 0pt;text-align: left;"><a name="bookmark101">Table 6.1: Daily cumulative returns</a><a name="bookmark108">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:19.8558pt" cellspacing="0"><tr style="height:27pt"><td style="width:477pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" colspan="6"><p class="s109" style="padding-left: 212pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2*<b>Strategy Cumulative Returns</b></p><p class="s109" style="padding-left: 290pt;text-indent: 0pt;text-align: left;">Minimum Mean Maximum</p></td></tr><tr style="height:35pt"><td style="width:38pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">14*</p></td><td style="width:164pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">4*<b>iShares S&amp;P500 ETF</b></p></td><td style="width:78pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">ETDQN</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s110" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">1.0</p></td><td style="width:70pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s110" style="padding-right: 22pt;text-indent: 0pt;text-align: right;">3.86</p></td><td style="width:68pt;border-top-style:solid;border-top-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s110" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">8.61</p></td></tr><tr style="height:19pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">TDQN</p></td><td style="width:59pt"><p class="s109" style="padding-top: 6pt;padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.95</p></td><td style="width:70pt"><p class="s109" style="padding-top: 6pt;padding-right: 22pt;text-indent: 0pt;line-height: 12pt;text-align: right;">2.64</p></td><td style="width:68pt;border-right-style:solid;border-right-width:1pt"><p class="s109" style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;line-height: 12pt;text-align: left;">4.92</p></td></tr><tr style="height:14pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Buy and Hold</p></td><td style="width:59pt"><p class="s109" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.91</p></td><td style="width:70pt"><p class="s109" style="padding-right: 22pt;text-indent: 0pt;line-height: 12pt;text-align: right;">2.06</p></td><td style="width:68pt;border-right-style:solid;border-right-width:1pt"><p class="s109" style="padding-left: 16pt;text-indent: 0pt;line-height: 12pt;text-align: left;">3.01</p></td></tr><tr style="height:14pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Sell and Hold</p></td><td style="width:59pt"><p class="s109" style="padding-left: 11pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.23</p></td><td style="width:70pt"><p class="s109" style="padding-right: 22pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.42</p></td><td style="width:68pt;border-right-style:solid;border-right-width:1pt"><p class="s109" style="padding-left: 16pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.05</p></td></tr><tr style="height:15pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="2"><p class="s109" style="padding-top: 13pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">4*<b>Western Digital Corporation</b></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Random</p></td><td style="width:59pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">0.58</p></td><td style="width:70pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s109" style="padding-right: 22pt;text-indent: 0pt;text-align: right;">0.96</p></td><td style="width:68pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s109" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">1.19</p></td></tr><tr style="height:25pt"><td style="width:78pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">ETDQN</p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">TDQN</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="4"><p class="s110" style="padding-left: 11pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.98</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">0.87</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">0.78</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">0.05</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">0.45</p></td><td style="width:70pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="4"><p class="s110" style="padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: left;">17.12</p><p class="s109" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">2.40</p><p class="s109" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">2.13</p><p class="s109" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">0.23</p><p class="s109" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">0.88</p></td><td style="width:68pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="4"><p class="s110" style="padding-left: 16pt;text-indent: 0pt;line-height: 11pt;text-align: left;">75.48</p><p class="s109" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">6.12</p><p class="s109" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">3.86</p><p class="s109" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">1.06</p><p class="s109" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">1.23</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Buy and Hold</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Sell and Hold</p></td></tr><tr style="height:15pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="2"><p class="s109" style="padding-top: 13pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">4*<b>ATOMUSDT Cryptocurrency</b></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Random</p></td></tr><tr style="height:25pt"><td style="width:78pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">ETDQN</p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">TDQN</p></td><td style="width:59pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="4"><p class="s110" style="padding-left: 11pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.95</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">0.26</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">0.29</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">0.01</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">0.01</p></td><td style="width:70pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="4"><p class="s110" style="padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: left;">12.08</p><p class="s109" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">0.86</p><p class="s109" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">5.65</p><p class="s109" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">0.01</p><p class="s109" style="padding-left: 28pt;text-indent: 0pt;text-align: left;">0.03</p></td><td style="width:68pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="4"><p class="s110" style="padding-left: 16pt;text-indent: 0pt;line-height: 11pt;text-align: left;">37.62</p><p class="s109" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">7.64</p><p class="s109" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">7.67</p><p class="s109" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">2.22</p><p class="s109" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">1.0</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Buy and Hold</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Sell and Hold</p></td></tr><tr style="height:15pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Random</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 43pt;text-align: justify;">Regarding the first evaluated iShares S&amp;P500 ETF data frame, the ETDQN had 1.46, 1.87, 8.57, 9 and 3.75 higher mean performance compared to TDQN, <i>Buy-and-Hold</i>, <i>Sell-and- Hold </i>benchmark, and Random Action strategy, respectively, hence achieving the first position of performance.</p><p style="padding-left: 16pt;text-indent: 43pt;text-align: justify;">The same policy achieved 7.13, 8.04, 19.45, and 74.43 higher performance than the TDQN, <i>Buy-and-Hold</i>, Random Action, and <i>Sell-and-Hold </i>benchmark strategies applied to the Western Digital Corporation data frame.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Lastly, the abovementioned strategy reached 14.04, 2.14, 402.66, and 1208 greater accumulated returns than the TDQN, <i>Buy-and-Hold</i>, Random Action, and <i>Sell-and-Hold </i>benchmark strategies. This fact exposes that the algorithm has identified patterns within the dollar bars and can profit from opportunities.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">It is also worth mentioning that if the investor would allocate capital into the Random Action and <i>Sell-and-Hold </i><a href="#bookmark108" class="a">benchmark strategies in the Western Digital Corporation data frame, he would have lost almost his entire capital. This fact is also actual when applied to the same strategies in the ATOM/USDT cryptocurrency data frame. According to Table </a>6.1, the capital allocated to the ETDQN would be multiplied by 3.86 in the iShares S&amp;P500 ETF data frame, 17.12 if applied to the Western Digital Corporation and 12.08 if applied to Binance spot ATOM/USDT.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">SHARPE RATIO</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark109" class="a">This section calculated and analyzed the Sharpe ratio metric for each data frame and benchmark. The metric adopted a 126-day window value to analyze the ratio on a 6-month rolling basis. The estimated number of days in a month was 21 since only business days were considered. Figure </a>6.4 shows the aforementioned rolling metric calculated in the iShares S&amp;P500 ETF data frame for TDQN-based, <i>Buy-and-Hold</i>, <i>Sell-and-Hold </i>benchmark, and Random Action benchmarks as well as for ETDQN.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="261" height="188" alt="image" src="documento[1970]/Image_084.jpg"/></span>	<span><img width="261" height="188" alt="image" src="documento[1970]/Image_085.jpg"/></span></p><p class="s74" style="padding-top: 8pt;padding-left: 106pt;text-indent: 0pt;text-align: left;"><a name="bookmark109">(a) ETDQN (b) Buy-and-Hold benchmark</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="261" height="188" alt="image" src="documento[1970]/Image_086.jpg"/></span>	<span><img width="257" height="186" alt="image" src="documento[1970]/Image_087.jpg"/></span></p><p class="s74" style="padding-top: 8pt;padding-left: 79pt;text-indent: 0pt;text-align: left;">(c) Sell-and-Hold benchmark (d) Random Action benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 136pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="186" alt="image" src="documento[1970]/Image_088.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-top: 5pt;padding-left: 208pt;text-indent: 0pt;text-align: left;">(e) TDQN benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 119pt;text-indent: 0pt;text-align: left;">Figure 6.4: iShares S&amp;P500 ETF 6-month rolling Sharpe ratio</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">ETDQN was evaluated during its worst-performing moment with a minimum Sharpe ratio of -3.26 in 2021. The agent also achieved a 6.07 ratio in 2019 during its best performance and a mean of 1.04. At backtest start, the policy oscillates its Sharpe ratio between the -2.5 /</p><ol id="l64"><ol id="l65"><li><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">range, followed by a breakout in 2015. In the following, the strategy achieves a 5.2 ratio performing an interesting risky-reward relation. In 2019, the strategy kept increasing the ratio and achieved its best relation with 6.07 by taking advantage of the COVID-19 event dump and opening a short position. Unfortunately, this period is followed by the worst performance moment</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;">in 2021, achieving a -3.26 ratio. Luckily, the strategy recovers from this loss in a v-shape way and reverts to a mean value. This policy has the first position regarding the mean metric against the benchmarks.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">TDQN benchmark was evaluated with a minimum of -2.66, a mean of 1.03 and a maximum of 4.56 6-month rolling Sharpe ratio. Its best risk-reward moment occurs in 2013, going from -1.24 evaluation to 4.56. During this period this strategy outperforms all of remaining. In addition, its worst risk-reward moment happens in 2020 during the start of COVID-19 pandemic. During this period, the Sharpe ratio has a dump from 3.62 to -2.66. The strategy achieves the second place regarding risk-opportunity relation according to the mean.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">The <i>Buy-and-Hold </i>strategy benchmark was evaluated with a minimum value of -2.97 ratio during its worst risk-reward moment in 2019 and its best moment with a 4.6 ratio in 2017. The strategy’s Sharpe ratio oscillates between 3.0 and -2.5, with few exceptions during 2010 and 2016, and has a 0.76 mean. The policy achieves its best risk-return measure in the following year, even though a 150% dump occurs subsequently. This policy achieved third place regarding risk-opportunity relation according to the mean.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The sell-and-hold policy achieved the last position regarding risk-return relations, reaching a -4.6 Sharpe ratio during its worst moment in 2018 and 2.97 in 2019 during the COVID-19 event. It is also worth mentioning that since this strategy is the reverse of the previously analyzed one, it achieves its best moment of risk-return relation once the previous strategy reaches its worst scenario.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: right;"><a href="#bookmark110" class="a">Finally, the Random Action strategy achieved the fourth position regarding risk-return relation. This policy had its worst moment Sharpe ratio evaluated at -2.95 in 2021, its best rolling ratio estimated at 2.81 in the same year, and a 0.0 mean. Figure </a>6.5 shows the 6-month rolling Sharpe ratio calculated in the Western Digital Corporation data frame regarding all benchmarks. The proposed DRL-based strategy had achieved the best position compared to the benchmarks regarding the mean Sharpe ratio of 1.0. In addition, during 2020, the policy had its worst ratio of -4.64, followed by a recovery in a v-shaped way. Its best risk-return relation was achieved during 2018 when the model assumed a strong uptrend. As expected at the beginning of the backtest, the agent struggled to learn an excellent risk-return relation when in 2010, its ratio had a 200% pump. The ratio oscillates between 6.0 and -2.0, except during 2020 when its highest drop occurs from its maximum value of 6.57 to its lowest; hence, this is the strategy’s</p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">most volatile Sharpe ratio moment.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">TDQN had the third position regarding the Sharpe ratio. Although this policy and the buy-and-hold benchmark achieved the same 0.3 mean value, the maximum Sharpe ratio value was used as tie-breaking criteria; In addition, the strategy had a minimum of -3.43 and maximum of 2.76 6-month rolling Sharpe ratio. This policy has its worst risk-reward moment during March 2014 having a dump from 1.38 Sharpe ratio to -3.43. On the other hand, its best risk-reward moment occurred during July 2016. The ratio had a v-shaped pump from -2.13 to 2.76.</p><p class="s5" style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">Buy-and-Hold <span class="p">benchmark had the second position regarding the Sharpe ratio evaluation with a 0.3 mean. In 2019, the policy had the worst-performing moment when the Sharpe ratio dropped from 1.5 to its lowest value of -4.55. During the beginning of the backtest, the strategy achieved its best ratio of 3.51, followed by an -112% drop. The moments with the highest variation of the metric were during 2016 and 2019, with both of them recovering from it in a v-shape way.</span></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">Contrasting the previous strategy, <i>Sell-and-Hold </i>benchmark had its best and worst evaluated Sharpe ratios as opposed to the <i>Buy-and-Hold </i>strategy. Evaluated in -0.3 Sharpe ratio, the strategy occupied the last position regarding risk-return relation, even though the Random Action strategy was a mean evaluated in 0.13. Its best 4.02-performing moment evaluation was</p><ol id="l66"><li><p class="s74" style="padding-top: 7pt;padding-left: 117pt;text-indent: -10pt;text-align: left;"><a name="bookmark110">ETDQN (b) Buy-and-Hold benchmark</a></p></li></ol></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="261" height="188" alt="image" src="documento[1970]/Image_089.jpg"/></span>	<span><img width="257" height="186" alt="image" src="documento[1970]/Image_090.jpg"/></span></p><ol id="l67"><li><p class="s74" style="padding-top: 8pt;padding-left: 89pt;text-indent: -10pt;text-align: left;">Sell-and-Hold benchmark (d) Random Action benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 136pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="187" alt="image" src="documento[1970]/Image_091.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-top: 5pt;padding-left: 208pt;text-indent: 0pt;text-align: left;">(e) TDQN benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 105pt;text-indent: 0pt;text-align: left;">Figure 6.5: Western Digital Corporation 6-month rolling Sharpe ratio</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">during the COVID-19 event in 2019. On the other hand, the policy suffered from the asset price recovery from the episode above and had its worst risk-return rate of -3.62.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark111" class="a">Lastly, the Random Action strategy was evaluated with a mean Sharpe Ratio of 0.13 and occupied the fourth place regarding risk-return. Figure </a>6.6 shows the 6-month rolling Sharpe ratio calculated in the Binance spot ATOM/USDT data frame regarding all benchmarks.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The <i>Buy-and-Hold </i>benchmark had the best position regarding the mean Sharpe ratio, even though ETDQN achieved a more excellent maximum Sharpe ratio value. With respect to</p><ol id="l68"><li><p class="s74" style="padding-top: 7pt;padding-left: 117pt;text-indent: -10pt;text-align: left;"><a name="bookmark111">ETDQN (b) Buy-and-Hold benchmark</a></p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="261" height="187" alt="image" src="documento[1970]/Image_092.jpg"/></span>	<span><img width="257" height="190" alt="image" src="documento[1970]/Image_093.jpg"/></span></p><p class="s74" style="padding-top: 6pt;padding-left: 79pt;text-indent: 0pt;text-align: left;">(c) Sell-and-Hold benchmark (d) Random Action benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 134pt;text-indent: 0pt;text-align: left;"><span><img width="261" height="189" alt="image" src="documento[1970]/Image_094.jpg"/></span></p><p class="s74" style="padding-top: 7pt;padding-left: 208pt;text-indent: 0pt;text-align: left;">(e) TDQN benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 74pt;text-indent: 0pt;text-align: left;">Figure 6.6: Binance spot ATOM/USDT cryptocurrency 6-month rolling Sharpe ratio</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">the strategy above, the Sharpe ratio decreases from October 2020 to July 2021 until the model successfully captures an uptrend and the ratio reaches its maximum value of 7.3 during October 2021. As previously mentioned, this strategy achieved the second position regarding the mean Sharpe Ratio of 1.19.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">TDQN achieved the third place regarding the Sharpe ratio performance. This strategy was evaluated with a minimum value of -3.18, a maximum value of 3.88, and mean value of</p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">0.86. The strategy had an increasing Sharpe ratio evaluation between October 2019 and July 2020. The strategy has its best risk-reward relation moment during August 2020. From October</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;"><a name="bookmark112">onwards, the Sharpe ratio assumes a downtrend. It reaches its lowest value in October 2021. It is worth mentioned that while DRL-based strategy is having its best risk-reward moment during the previous mentioned date, this strategy is having its worst value of -3.18.</a></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Switching the analysis to the <i>Buy-and-Hold</i>, the policy started the backtest having its minimum evaluated ratio of -1.55 in October 2020 and a maximum of 3.65 in March 2021. In addition, the strategy had a mean of 1.19, hence it achieved the first position regarding the mean. It is observable that the period between October 2020 and March 2021 contain the most volatile period of the asset regarding the Sharpe ratio. It drops from 2.91 to -0.89, followed by a recovery in a v-shaped way achieving its best risk-reward ratio of 3.65.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Alternatively, to the previous strategy, the sell-and-hold benchmark had the worst mean risky-return relation of -1.19; hence was placed in the last position. The strategy achieves its highest ratio evaluation of 1.55 at the start of the backtest and decreases its value onwards, achieving the lowest value of -3.65.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Lastly, the Random Action strategy obtained the third position compared to the remaining policy with a -0.82 mean Sharpe ratio. The maximum value of a 2.44 ratio was achieved in July 2020 during the start of the backtest. Its most significant drop having its ratio evaluated at -4.66, happens during April.</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">In order to summarize all information regarding the 6-month rolling Sharpe ratio, Table</p><ol id="l69"><ol id="l70"><li><p style="padding-left: 16pt;text-indent: 0pt;text-align: justify;">presents the minimum, mean and maximum rolling Sharpe ratio values from ETDQN, TDQN, <i>Buy-and-Hold</i>, <i>Sell-and-Hold </i>benchmark, and Random Action strategies in respect to the iShares S&amp;P 500 ETF, Western Digital Corporation and Binance spot ATOMUSDT cryptocurrency data frames.</p><p class="s108" style="padding-top: 3pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">Assets</p><p style="text-indent: 0pt;text-align: left;"/><p class="s1" style="padding-top: 9pt;padding-left: 163pt;text-indent: 0pt;text-align: left;">Table 6.2: 6-month rolling Sharpe ratio</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:19.8558pt" cellspacing="0"><tr style="height:27pt"><td style="width:477pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" colspan="6"><p class="s109" style="padding-left: 212pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2*<b>Strategy Rolling Sharpe Ratio</b></p><p class="s109" style="padding-left: 290pt;text-indent: 0pt;text-align: left;">Minimum Mean Maximum</p></td></tr><tr style="height:35pt"><td style="width:38pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">14*</p></td><td style="width:164pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">4*<b>iShares S&amp;P500 ETF</b></p></td><td style="width:78pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">ETDQN</p></td><td style="width:61pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="5"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-3.26</p><p class="s110" style="padding-top: 12pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">-2.66</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-2.97</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-4.6</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-2.95</p></td><td style="width:67pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s110" style="padding-left: 27pt;text-indent: 0pt;text-align: left;">1.04</p></td><td style="width:69pt;border-top-style:solid;border-top-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s110" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">6.07</p></td></tr><tr style="height:19pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">TDQN</p></td><td style="width:67pt"><p class="s109" style="padding-top: 5pt;padding-left: 27pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1.03</p></td><td style="width:69pt;border-right-style:solid;border-right-width:1pt"><p class="s109" style="padding-top: 5pt;padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: left;">4.56</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Buy and Hold</p></td><td style="width:67pt"><p class="s109" style="padding-left: 27pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.76</p></td><td style="width:69pt;border-right-style:solid;border-right-width:1pt"><p class="s109" style="padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: left;">4.6</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Sell and Hold</p></td><td style="width:67pt"><p class="s109" style="padding-left: 27pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-0.76</p></td><td style="width:69pt;border-right-style:solid;border-right-width:1pt"><p class="s109" style="padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2.97</p></td></tr><tr style="height:15pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="2"><p class="s109" style="padding-top: 13pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">4*<b>Western Digital Corporation</b></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Random</p></td><td style="width:67pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s109" style="padding-left: 27pt;text-indent: 0pt;text-align: left;">0.0</p></td><td style="width:69pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s109" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">2.81</p></td></tr><tr style="height:25pt"><td style="width:78pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">ETDQN</p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">TDQN</p></td><td style="width:61pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="4"><p class="s109" style="padding-left: 11pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-4.64</p><p class="s110" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-3.43</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-4.55</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-3.51</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-3.62</p></td><td style="width:67pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="4"><p class="s110" style="padding-left: 26pt;padding-right: 20pt;text-indent: 0pt;line-height: 11pt;text-align: center;">1.00</p><p class="s109" style="padding-left: 20pt;padding-right: 20pt;text-indent: 0pt;text-align: center;">0.3</p><p class="s109" style="padding-left: 20pt;padding-right: 20pt;text-indent: 0pt;text-align: center;">0.3</p><p class="s109" style="padding-left: 24pt;padding-right: 20pt;text-indent: 0pt;text-align: center;">-0.3</p><p class="s109" style="padding-left: 26pt;padding-right: 20pt;text-indent: 0pt;text-align: center;">0.13</p></td><td style="width:69pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="4"><p class="s110" style="padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: left;">6.57</p><p class="s109" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">2.76</p><p class="s109" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">3.51</p><p class="s109" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">4.55</p><p class="s109" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">4.02</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Buy and Hold</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Sell and Hold</p></td></tr><tr style="height:15pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="2"><p class="s109" style="padding-top: 13pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">4*<b>ATOMUSDT Cryptocurrency</b></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Random</p></td></tr><tr style="height:25pt"><td style="width:78pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">ETDQN</p><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">TDQN</p></td><td style="width:61pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="4"><p class="s109" style="padding-left: 11pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-2.24</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-3.18</p><p class="s110" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-1.55</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-3.65</p><p class="s109" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">-4.66</p></td><td style="width:67pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="4"><p class="s109" style="padding-left: 27pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1.02</p><p class="s109" style="padding-left: 27pt;text-indent: 0pt;text-align: left;">0.86</p><p class="s110" style="padding-left: 27pt;text-indent: 0pt;text-align: left;">1.19</p><p class="s109" style="padding-left: 27pt;text-indent: 0pt;text-align: left;">-1.19</p><p class="s109" style="padding-left: 27pt;text-indent: 0pt;text-align: left;">-0.82</p></td><td style="width:69pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="4"><p class="s110" style="padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: left;">7.3</p><p class="s109" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">3.88</p><p class="s109" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">3.65</p><p class="s109" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">1.55</p><p class="s109" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">2.44</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Buy and Hold</p></td></tr><tr style="height:13pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Sell and Hold</p></td></tr><tr style="height:15pt"><td style="width:38pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:164pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s109" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Random</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">It is possible to assert that the ETDQN had 1.36, 1.01, 2.74, and 1.4 greater mean Sharpe ratios having <i>Buy-and-Hold</i>, TDQN, <i>Sell-and-Hold </i>benchmark, and Random Action strategies as a basis in iShares S&amp;P500 ETF. The policy achieved the first position regarding this metric. In addition, the same policy had 3.3, 3.3, 6.6, and 7.7 better risk-return relations regarding Western Digital Corporation. In contrast to the previous data frames, the strategy generated by the agent</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a name="bookmark102">was not in the top position regarding the mean Sharpe ratio in ATOM/USDT, even though its maximum value was more significant than any of the benchmarks. This fact may be due to insufficient data generated at the beginning of the data frame and the asset’s volatility.</a><a name="bookmark113">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">DRAWDOWN</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark114" class="a">This section calculated and analyzed the drawdown metric for each data frame and bench- mark. Figure </a>6.7 shows the drawdown underwater plots calculated in the iShares S&amp;P500 ETF data frame for TDQN, <i>Buy-and-Hold</i>, <i>Sell-and-Hold </i>benchmark, Random Action benchmarks, as well as for the ETDQN policy.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The proposed DRL-based strategy suffered from a maximum drawdown of -24.40% in 2021. As previously mentioned, this is the year the strategy’s performance dropped significantly for the Sharpe ratio. A few spikes in 2010, 2012, and 2015 were also present during the backtest, having respectively -11.2%, -16%, and 14.82% drawdowns. Analyzing the plot, it is observable that an average of -10% drawdowns are frequent in the strategy. Lastly, the period between 2016 and 2019 presents minimal declines.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">TDQN had a maximum drawdown of -35.1% during 2020. This dump is related to the COVID-19 event. The agent was not able to capture this decline and opened a long position, hence had its greatest drawdown. Some spikes of -12.4%, 23.1% and 13.9% in 2016, 2017, and 2022, respectively are also observable in the agent strategy.</p><p class="s5" style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Buy-and-Hold <span class="p">strategy had a maximum decline of -36.7% in March 2020. Some significant -22.3%, -20.05% spikes are also present during 2012 and 2019, respectively. An average value of -12.72% decline is expected following this policy.</span></p><p class="s5" style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Sell-and-Hold <span class="p">benchmark strategy the maximum decline value of -78.00%. It is observable that the strategy accumulates its losses throughout the backtest and does to improve in the long run, only in a few moments, such as during 2020, contrasting a previous couple of analyzed strategies.</span></p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">Lastly, the Random Action strategy had a maximum drawdown of -47.5% during 2022. A significant 25% were spotted in 2012. By analyzing each strategy’s maximum drawdowns, it is possible to affirm that the ETDQN’s maximum decline was 12.3%, 53.6%, and 23.1% smaller compared to <i>Buy-and-Hold</i>, <i>Sell-and-Hold </i><a href="#bookmark114" class="a">benchmark, and Random Action benchmarks applied to the iShares S&amp;P500 ETF. Figure </a>6.8 shows the calculated drawdowns in the Western Digital Corporation data frame regarding the previously analyzed strategies.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="261" height="191" alt="image" src="documento[1970]/Image_095.jpg"/></span>	<span><img width="261" height="191" alt="image" src="documento[1970]/Image_096.jpg"/></span></p><ol id="l71"><li><p class="s74" style="padding-top: 7pt;padding-left: 117pt;text-indent: -10pt;text-align: left;"><a name="bookmark114">ETDQN (b) Buy-and-Hold benchmark</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="261" height="191" alt="image" src="documento[1970]/Image_097.jpg"/></span>	<span><img width="255" height="215" alt="image" src="documento[1970]/Image_098.jpg"/></span></p><ol id="l72"><li><p class="s74" style="padding-top: 7pt;padding-left: 89pt;text-indent: -10pt;text-align: left;">Sell-and-Hold benchmark (d) Random Action benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 139pt;text-indent: 0pt;text-align: left;"><span><img width="256" height="199" alt="image" src="documento[1970]/Image_099.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s74" style="padding-top: 5pt;padding-left: 208pt;text-indent: 0pt;text-align: left;">(e) TDQN benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="262" height="148" alt="image" src="documento[1970]/Image_100.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="262" height="148" alt="image" src="documento[1970]/Image_101.jpg"/></span></p><p class="s1" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">Figure 6.7: iShares S&amp;P500 ETF drawdown from 2009 to 2022</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">ETDQN had a -56.60% maximum decline during the start of 2020 and a -50.23% drawdown during the start of 2020. Even though these losses were reverted, this strategy gives insights aimed at a more aggressive trading style since the investor would have lost more than half of his investment value for a certain period.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The TDQN-based strategy had a maximum drawdown of -83.5% in 2022. During the period from 2010 to 2012, the strategy has drawdowns pretty similar to the proposed DRL-based strategy. However, between 2014 and 2015, the policy achives a significant drawdown of 64.3%, followed by a recovery. Its worst moment concerning drawdowns, happens during the period between 2019 and 2022. During the COVID-19 event, it is not able to capture the event and has a</p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">-73.2% drawdown. In addition, in 2022 it has its maximum -83.5% drawdown.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: right;">Regarding the <i>Buy-and-Hold </i>strategy, a maximum drawdown of -74.50% is spotted at the start of 2020. This decline matches with the DRL-based drawdown of -50.23; hence the previous strategy had a smaller drawdown during the same event. Some -65% spikes are also present in this strategy during 2016 and 2019, as well as a more diminutive -40% during 2012. Similarly, as in the previous data frame, <i>Sell-and-Hold </i>benchmark benchmark had accumulated declines in the entire backtest, resulting in a -95% maximum drawdown and losing</p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">almost its entire invested amount for 5%.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Finally, the Random Action strategy had the maximum decline of -65.40% in 2020. Some significant declines in 2011, 2017, and 2022 are also present, each having -42%, -32%, and 58% drawdowns, respectively.</p><p style="padding-left: 59pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">In summary, it is possible to conclude that even though ETDQN had an expressive</p><p style="padding-left: 16pt;text-indent: -1pt;text-align: justify;"><a href="#bookmark115" class="a">-56.60% maximum decline, the policy still had the lowest drawdown applied to the Western Digital Corporation data frame. Figure </a>6.9 shows the calculated drawdowns in the Binance spot ATOM/USDT cryptocurrency data frame regarding the previously analyzed strategies.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span><img width="255" height="223" alt="image" src="documento[1970]/Image_102.jpg"/></span>	<span><img width="255" height="223" alt="image" src="documento[1970]/Image_103.jpg"/></span></p><ol id="l73"><li><p class="s74" style="padding-top: 7pt;padding-left: 117pt;text-indent: -10pt;text-align: left;"><a name="bookmark115">ETDQN (b) Buy-and-Hold benchmark</a></p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="260" height="223" alt="image" src="documento[1970]/Image_104.jpg"/></span>	<span><img width="261" height="207" alt="image" src="documento[1970]/Image_105.jpg"/></span></p><p class="s74" style="padding-top: 5pt;padding-left: 79pt;text-indent: 0pt;text-align: left;">(c) Sell-and-Hold benchmark (d) Random Action benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 139pt;text-indent: 0pt;text-align: left;"><span><img width="256" height="207" alt="image" src="documento[1970]/Image_106.jpg"/></span></p><p class="s74" style="padding-top: 5pt;padding-left: 227pt;text-indent: 0pt;text-align: left;">(e) TDQN</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 63pt;text-indent: 0pt;text-align: left;">Figure 6.9: Binance spot ATOM/USDT cryptocurrency pair drawdown from 2019 to 2022</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">ATOM/USDT cryptocurrency pair had the most significant drawdowns since it was the most volatile asset. ETDQN had a maximum decline of expressively -76.70% in July 2021. The Strategy accumulates declines between July 2020 during one year and achieves its maximum</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: left;"><a name="bookmark103">decline as mentioned above, followed by an impressive reversion of the total loss in the following month.</a><a name="bookmark116">&zwnj;</a></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">The TDQN-based benchmark had a significant maximum drawdown of -90.9% during October 2021. This event is explained due to the failure of the agent to be able to capture the uptrend of the market, opposing to the previous strategy. In addition, between the period of July 2019 and July 2020, the strategy had drawdowns up to 72.3%, followed by a recovery. In July 2020 and July 2021, the policy oscillates having a significant spike of 50% during February. Finally, between July 2021 and October 2021, the model struggles to capture the trend and has its maximum drawdown of -90.9%.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">In addition, the <i>Buy-and-Hold </i>benchmark also had an expressive -74.00% maximum decline during April 2020. The Strategy does not accumulate drawdowns as the previous Strategy. Some other -65% spikes are present during October 2020 and July 2021.</p><p class="s5" style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Sell-and-Hold <span class="p">benchmark strategy had the maximum drawdown of -99.9%; hence if the investor had opened a short position in July 2020 until October 2021, he would have lost all his investment.</span></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Lastly, the Random Action strategy had a maximum decline of -94%. This Strategy had a behavior similar to the previously analyzed.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">It is possible to conclude that the ETDQN achieved the second position regarding having the lowest maximum drawdown. The same Strategy had 17.3%, -3.65%, 30.25%, and 22.55% more stable drawdowns compared to the TDQN-based, <i>Buy-and-Hold</i>, <i>Sell-and-Hold </i>benchmark and Random Action Strategies.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><p style="padding-left: 40pt;text-indent: -23pt;text-align: left;">MONTHLY RETURNS</p></li></ol></ol><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark117" class="a">The generated monthly returns by each benchmark were analyzed to give the reader an idea of how much the strategy would profit monthly. Figure </a>6.10 shows monthly returns heatmaps from each benchmark and the developed algorithm strategy.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark117" class="a">As shown in Figure </a>6.10(a), ETDQN did not profit much in 2009 due to the lack of data. In the following year, the strategy profit had eight months of positive returns having its best performance in March with a 10.1% return. In August 2011, meanwhile, the TDQN-based, <i>Buy-and-Hold</i>, <i>Buy-and-Hold</i>, and Random Action benchmarks had -0.6%, 6.9%, 5.9%, and 5.3% returns, ETDQN had 15.9%, followed by another 7.1% positive return. Unfortunately, the strategy delivers both negative returns in the next couple of months.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">From 2012 to 2017, the profits generated by the agent did not have a high amplitude ranging between -2.5 to 5, hence having a slight positive asymmetry. A few exceptions are present on both sides, for example, the 7.1% profit during September 2016 and the -4.9% loss in August 2015.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">In 2018, the ETDQN again outperformed all benchmarks concerning February, October, and December, generating 7.5%, 9.7%, and 10.5% returns. At the exact moment, TDQN generates</p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">-5.3%, -7.6%, -10.2% returns, <i>Buy-and-Hold </i>strategy -5.3%, -5.3$, and -10.1%, <i>Buy-and-Hold</i></p><ol id="l74"><ol id="l75"><li><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">%, 5.3%, and 10.6% and the Random Action strategy -3.8%, 2.6%, and 5.1% respectively. It is observable that a dump in the asset price must have occurred during this period.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Even though <i>Buy-and-Hold </i>also performed well in these price dumps, it cannot maintain its earnings like ETDQN. In the following year, 2019, the agent-based strategy outperformed all benchmarks once again in March, having an 11.2% return.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">In 2020, the proposed algorithm achieved its best performance compared to all bench- marks taking advantage of the COVID-19 pandemic event price drop. The model can subsequently generate 14.6%, 54.7%, 10.2%, and 4.9% returns. During 2021, the model strategy had profits</p><ol id="l76"><li><p class="s74" style="padding-top: 7pt;padding-left: 117pt;text-indent: -10pt;text-align: left;"><a name="bookmark117">ETDQN (b) Buy-and-Hold benchmark</a></p></li></ol></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="187" alt="image" src="documento[1970]/Image_107.jpg"/></span>	<span><img width="257" height="187" alt="image" src="documento[1970]/Image_108.jpg"/></span></p><ol id="l77"><li><p class="s74" style="padding-top: 8pt;padding-left: 89pt;text-indent: -10pt;text-align: left;">Sell-and-Hold benchmark (d) Random Action benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 136pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="187" alt="image" src="documento[1970]/Image_109.jpg"/></span></p><p class="s74" style="padding-top: 8pt;padding-left: 208pt;text-indent: 0pt;text-align: left;">(e) TDQN benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 103pt;text-indent: 0pt;text-align: left;">Figure 6.10: iShares S&amp;P500 ETF monthly returns from 2009 to 2022</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">between 3.5 and 4.4%, but in 2022, the agent gets volatile again, having returns ranging between</p><p style="padding-left: 17pt;text-indent: -2pt;text-align: justify;"><a href="#bookmark118" class="a">-7.3% to 10.6%. Figure </a>6.11 shows monthly returns generated from each benchmark applied to the Western Digital Corporation data frame.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Comparing monthly returns generated from every strategy, it is observable that the ETDQN monthly returns heat map contains more uniform colors. With a few exceptions, the returns are within a specific range.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">During 2010, the strategy only provided positive returns. In the following year, it is possible to visually positive returns, such as 5%, 14.8%, 4.1%, 14.7%, and 10%, respectively,</p><ol id="l78"><li><p class="s74" style="padding-top: 7pt;padding-left: 117pt;text-indent: -10pt;text-align: left;"><a name="bookmark118">ETDQN (b) Buy-and-Hold benchmark</a></p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="187" alt="image" src="documento[1970]/Image_110.jpg"/></span>	<span><img width="257" height="187" alt="image" src="documento[1970]/Image_111.jpg"/></span></p><ol id="l79"><li><p class="s74" style="padding-top: 8pt;padding-left: 89pt;text-indent: -10pt;text-align: left;">Sell-and-Hold benchmark (d) Random Action benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 136pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="187" alt="image" src="documento[1970]/Image_112.jpg"/></span></p><p class="s74" style="padding-top: 8pt;padding-left: 227pt;text-indent: 0pt;text-align: left;">(e) TDQN</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 89pt;text-indent: 0pt;text-align: left;">Figure 6.11: Western Digital Corporation monthly returns from 2010 to 2022</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">generated in January, March, April, August, and October. Unfortunately, a few expressive -4.6%,</p><p style="padding-left: 16pt;text-indent: -1pt;text-align: justify;">-5.3%, -15.9%, and -11.7% negative returns are spotted in August, September, November, and December. It is also worth mentioning that the backtest is still in an early phase; hence the model weights are not fully adjusted. In 2012, a negative -12.3% return only is placed in December, and some expressive 13.6%, 36%, 9,6%, 13.2%, and 10.3% positive ones are generated during March, May, June, October, and November.</p><p style="padding-left: 16pt;text-indent: 43pt;text-align: justify;">In the following year, 2013, returns are more neural. The policy generated positive 6.5%, 12.3%, and 8.3% returns during February, June, and July, as well as some -5.2%, 5.1% declines</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 0pt;text-align: justify;">in January and September, but still leaving the strategy in a positive balance. During 2014, the most expressive return generated was -13.3% in October, even though some 7.7% and 6.3% positive ones during April and March counterbalanced the equation and ended up closing the year in a positive balance. Succeeding the previous year with a good margin regarding positive returns, 2015 resulted in a -12.8% negative return in January.</p><p style="padding-left: 16pt;text-indent: 43pt;text-align: justify;">However, in the following months of February, May, August, September, October, and December, expressive 8.8%, 6.6%, 23.1%, 18.4%, 23.8%, and 10.5% positive returns were generated by the policy. During 2016, the strategy generated during January, March, April, September, and December expressive 29,4%, 9.1%, 10.7%, 10.4%, and 9,6% positive returns during 2016, even though -6.8% and -7.8% negative returns are spotted. Negative returns predominated the following year, 2017. Expressive returns such as -10.8%, -14.1%, and -9.3% were generated during January, March, and September, and only 11.4% expressive positive returns were generated. Outperforming the previous year, 2018 generated the expressive 17.1%, 9.3%, 11.5%, 10.5%, 14.1%, and 19.8% predominating positive returns. During 2019, the positive/negative return ratio is balanced with expressive positive returns such as 13.4%, 7.9%, 18.8%, and -21.7%, -13%, -10.4%. The year 2020 was marked by the COVID-19 pandemic, drastically affecting the global economy.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">While <i>Buy-and-Hold</i>, <i>Sell-and-Hold </i>benchmark, and Random Action benchmarks generated -25.3%, 12.6%, -18.8% returns in March, the ETDQN agent was smart enough to profit from the drop, resulting in outstanding 164% returns in one month. In the following year, the strategy provides negative -13.7%, -14%, -15.9% returns, even though some positive 10.1%, 9.1%, and 8.8% balance the strategies’ performance.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark118" class="a">Finally, during 2022, the strategy generates expressive 37.6%, 26.5%, and 23.7% returns. However, some -22.9%, -13.8%, and -the policy generates 11.8% negative returns in May, July, and November. Figure </a>6.11 shows monthly returns generated from each benchmark applied to the Binance spot ATOM/USDT data frame.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="187" alt="image" src="documento[1970]/Image_113.jpg"/></span>	<span><img width="257" height="187" alt="image" src="documento[1970]/Image_114.jpg"/></span></p><ol id="l80"><li><p class="s74" style="padding-top: 8pt;padding-left: 117pt;text-indent: -10pt;text-align: left;"><a name="bookmark119">ETDQN (b) Buy-and-Hold benchmark</a></p></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="187" alt="image" src="documento[1970]/Image_115.jpg"/></span>	<span><img width="257" height="187" alt="image" src="documento[1970]/Image_116.jpg"/></span></p><p class="s74" style="padding-top: 8pt;padding-left: 79pt;text-indent: 0pt;text-align: left;">(c) Sell-and-Hold benchmark (d) Random Action benchmark</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 136pt;text-indent: 0pt;text-align: left;"><span><img width="257" height="187" alt="image" src="documento[1970]/Image_117.jpg"/></span></p><p class="s74" style="padding-top: 8pt;padding-left: 227pt;text-indent: 0pt;text-align: left;">(e) TDQN</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-left: 90pt;text-indent: 0pt;text-align: left;">Figure 6.12: Binance spot ATOM/USDT monthly returns from 2019 to 2021</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 59pt;text-indent: 0pt;text-align: justify;">Unlike the previous assets, it is impossible to perform a deeper analysis since ATOM-</p><p style="padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark119" class="a">/USDT data frame contains only three years of data. Comparing the color overview generated by the heat map in Figure </a>6.12(a), the map shows a positive-sided returns tendency.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">In 2019, TDQN outperformed all strategies. However, during the cryptocurrency bull run in 2020, B&amp;H and TDQN beat the proposed algorithm. ETDQN outperformed all the previous benchmarks again in 2021, generating returns such as 72.4%, 163%, 116%, and 73.4%, subsequently in August, September, October, and November, concentrating expressive returns at the year’s end.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a name="bookmark104">Regarding the S&amp;H benchmark, ETDQN outperforms it every year. S&amp;H generated positive returns of 30.7% and 41.9% in 2019, but unfortunately, it also holds more expressive negative -33% -25.8% and -19.9% returns.</a><a name="bookmark120">&zwnj;</a></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Lastly, the proposed algorithm also outperformed the Random Action benchmark. During 2019, the policy had significant negative return amounts, such as -56.5%, -39.8%, and</p><p style="padding-left: 17pt;text-indent: -2pt;text-align: justify;">-21.8% in September, July, and June, respectively. The policy achieved its worst-performing month the following year, generating significant -40.3%, -36.3%, and 29.4% negative returns. In 2021, the strategy generated a distinguished 66.7% return and some positive 52.6%, 33%, and 34.5% profits. Unfortunately, the policy cannot keep its gains and finishes the year with negative returns.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;">6.5 ANNUAL RETURNS</p><p style="padding-top: 12pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;"><a href="#bookmark117" class="a">In this section, annualized returns generated by each benchmark were analyzed in order to give the reader a better overview of each strategy’s performance in a annual basis. Table </a>6.10 shows annualized returns from each benchmark, as well as the developed algorithm strategy.</p><h4 style="padding-top: 2pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">YEAR</h4><p style="text-indent: 0pt;text-align: left;"/><p class="s1" style="padding-top: 12pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">Table 6.3: Annualized total returns generated per strategies from 2009 to 2022</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:18.838pt" cellspacing="0"><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" colspan="2" rowspan="2"><p class="s112" style="padding-left: 3pt;text-indent: 0pt;line-height: 60%;text-align: left;"><span class="s111">2* </span>S <span class="s113">ASSET / </span>Y</p><p class="s113" style="padding-left: 16pt;text-indent: 0pt;line-height: 4pt;text-align: left;">TRATEG</p></td><td style="width:134pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" colspan="5"><p class="s113" style="padding-top: 3pt;padding-left: 37pt;text-indent: 0pt;text-align: left;">iShares SP500 ETF</p></td><td style="width:137pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" colspan="5"><p class="s113" style="padding-top: 3pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">Western Digital</p></td><td style="width:138pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" colspan="5"><p class="s113" style="padding-top: 3pt;padding-left: 47pt;padding-right: 47pt;text-indent: 0pt;text-align: center;">ATOMUSDT</p></td></tr><tr style="height:9pt"><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">B&amp;H</p></td><td style="width:25pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">S&amp;H</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Random</p></td><td style="width:24pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">Agent</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">TDQN</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">B&amp;H</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">S&amp;H</p></td><td style="width:31pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Random</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Agent</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">TDQN</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">B&amp;H</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">S&amp;H</p></td><td style="width:31pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Random</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Agent</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">TDQN</p></td></tr><tr style="height:20pt"><td style="width:24pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">14*</p></td><td style="width:31pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-top: 8pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">2009</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">-0.56</p></td><td style="width:25pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">0.56</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">0.31</p></td><td style="width:24pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 2pt;padding-right: 7pt;text-indent: 0pt;text-align: center;">0.56</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-top: 8pt;padding-right: 7pt;text-indent: 0pt;text-align: center;">3.04</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:31pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:26pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:31pt;border-top-style:solid;border-top-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt" rowspan="14"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p><p class="s114" style="padding-top: 7pt;padding-left: 3pt;padding-right: 17pt;text-indent: 0pt;line-height: 108%;text-align: justify;">X X X X X X X X X</p><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">253.45</p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">27.73</p><p class="s113" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">646.56</p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:27pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt" rowspan="14"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p><p class="s114" style="padding-top: 7pt;padding-left: 3pt;padding-right: 18pt;text-indent: 0pt;line-height: 108%;text-align: justify;">X X X X X X X X X</p><p class="s115" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">-63.81</p><p class="s116" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">348.30</p><p class="s115" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">-53.57</p><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td></tr><tr style="height:12pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2010</p></td><td style="width:26pt"><p class="s114" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">13.37</p></td><td style="width:25pt"><p class="s114" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-14.6</p></td><td style="width:32pt"><p class="s114" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-0.31</p></td><td style="width:24pt"><p class="s113" style="padding-top: 3pt;padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">28.15</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-top: 3pt;padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">22.77</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">13.84</p></td><td style="width:26pt"><p class="s114" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-14.09</p></td><td style="width:31pt"><p class="s114" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-9.42</p></td><td style="width:27pt"><p class="s114" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">8.72</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-top: 3pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">29.71</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-top: 4pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:26pt"><p class="s114" style="padding-top: 4pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:31pt"><p class="s114" style="padding-top: 4pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2011</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-2.74</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-2.28</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-8.25</p></td><td style="width:24pt"><p class="s114" style="padding-left: 2pt;padding-right: 7pt;text-indent: 0pt;line-height: 7pt;text-align: center;">6.75</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">13.20</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-9.35</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-14.73</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-7.41</p></td><td style="width:27pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">3.38</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">67.36</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2012</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">14.59</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-14.31</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">6.91</p></td><td style="width:24pt"><p class="s114" style="padding-left: 2pt;padding-right: 7pt;text-indent: 0pt;line-height: 7pt;text-align: center;">3.45</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">10.47</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">35.89</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-38.16</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">27.78</p></td><td style="width:27pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">138.02</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">6.53</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2013</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">28.47</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-23.07</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-3.55</p></td><td style="width:24pt"><p class="s113" style="padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">-<span class="s114">4.36</span></p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">29.05</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">99.52</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-53.81</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-10.16</p></td><td style="width:27pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">13.35</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s115" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-47.08</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2014</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">9.94</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-9.99</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">10.58</p></td><td style="width:24pt"><p class="s113" style="padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">-<span class="s114">3.22</span></p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">12.18</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">33.48</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-29.34</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-13.26</p></td><td style="width:27pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">11.7</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">6.23</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2015</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-5.34</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">3.19</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">5.58</p></td><td style="width:24pt"><p class="s114" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">11.85</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">14.24</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-45.52</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">63.67</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-1.03</p></td><td style="width:27pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">84.94</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">61.74</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2016</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">13.94</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-13.9</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0.54</p></td><td style="width:24pt"><p class="s113" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">35.49</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">15.03</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">10.61</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-29.82</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-8.13</p></td><td style="width:27pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">41.73</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">120.43</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2017</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">12.99</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-12.00</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-1.67</p></td><td style="width:24pt"><p class="s114" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">10.95</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">19.22</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">17.62</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-23.50</p></td><td style="width:31pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">20.03</p></td><td style="width:27pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-<span class="s114">4.21</span></p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-12.26</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2018</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-11.66</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">10.61</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2.11</p></td><td style="width:24pt"><p class="s113" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">45.87</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">-17.20</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-53.58</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">88.03</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-23.73</p></td><td style="width:27pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">187.24</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">39.17</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">X</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2019</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">28.59</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-23.51</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0.56</p></td><td style="width:24pt"><p class="s114" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">11.11</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">29.42</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">71.93</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-54.00</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">18.59</p></td><td style="width:27pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-18.66</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-58.77</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-15.97</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-33.33</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-81.8</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2020</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-1.16</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-10.27</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-40.8</p></td><td style="width:24pt"><p class="s113" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">68.75</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-right: 7pt;text-indent: 0pt;line-height: 7pt;text-align: center;">1.05</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-13.55</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-26.93</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">1.20</p></td><td style="width:27pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">140.85</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-22.57</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">30.28</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-79.65</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-82.57</p></td></tr><tr style="height:9pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2021</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">22.37</p></td><td style="width:25pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-19.65</p></td><td style="width:32pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-4.85</p></td><td style="width:24pt"><p class="s114" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">19.55</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s113" style="padding-left: 2pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">20.51</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">19.67</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-31.63</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-25.94</p></td><td style="width:27pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-26.8</p></td><td style="width:27pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">10.25</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">462.68</p></td><td style="width:26pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-98.14</p></td><td style="width:31pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">-46.3</p></td></tr><tr style="height:10pt"><td style="width:24pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">2022</p></td><td style="width:26pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">-11.12</p></td><td style="width:25pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">9.70</p></td><td style="width:32pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">7.54</p></td><td style="width:24pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 2pt;padding-right: 7pt;text-indent: 0pt;text-align: center;">4.14</p></td><td style="width:27pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 2pt;padding-right: 7pt;text-indent: 0pt;text-align: center;">-0.27</p></td><td style="width:26pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">-43.9</p></td><td style="width:26pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s113" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">44.65</p></td><td style="width:31pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">6.60</p></td><td style="width:27pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">34.29</p></td><td style="width:27pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">-30.63</p></td><td style="width:27pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:26pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td><td style="width:31pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s114" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">X</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="0" height="12" alt="image" src="documento[1970]/Image_118.png"/></span></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Concerning the iShares S&amp;P500 ETF, TDQN had the best performance with 3.04% returns in 2009, followed by the DRL-based strategy with 0.56%. On the other hand, the DRL-based strategy was the most lucrative in 2010, beating TDQN by 5.98%. In the next five years, TDQN outperforms all benchmarks, and from 2016 to 2021, these two strategies keep alternating in the top performance position. Finally, in 2022 the Sell-and-Hold takes the best-performing position.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">Even though TDQN had the first position for nine years and the DRL-based strategy for four years only, the DRL-based strategy performed better in respect to the average annual return (AAR) metric, which in this case represents historical arithmetic average return between the period of 2009 and 2022. The policy had an AAR of 17.07%, against 12.34% produced by TDQN. The DRL-based strategy showed more aggressive behavior, delivering higher and more volatile returns with a 66.14% higher standard deviation than TDQN’s.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">In the Western Digital Corporation data frame, the DRL-based strategy took the third position regarding performance, having a difference of 20.99% and 5.12% returns from TDQN and Buy-and-Hold benchmarks in 2010. During the following year, the strategy took the second position staying behind TDQN with a 63.68%. In 2012, the DRL-based strategy outperformed the remaining benchmarks, achieving a 138.02% return.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 16pt;text-indent: 42pt;text-align: justify;">In 2016, TDQN stayed in first position, causing approximately triple returns compared to the DRL-based strategy. However, the following year, it returned to the second position, losing to the Buy-and-Hold benchmark, which generated a 99.52%. In 2014, the agent finished the year with approximately half of the returns produced by the Buy-and-Hold benchmark, but it took back the lead, generating 84.94% return in 2015.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">From 2017 onwards, TDQN cannot generalize well and produces few significant returns, such as in 2018 and 2021. During 2018 and 2020, the DRL-based strategy can take profit from both dumps and delivers its best 187.24% and 140.85% returns. The second one can be explained due to the COVID-19 pandemic. Lastly, the Buy-and-Hold strategy assumes the first performance evaluation position in 2019 and 2021, producing 71.93% and 19.67%, respectively, and in 2022 Sell-and-Hold outperforms the DRL-based strategy with a difference of 10.36%.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">Even though both the Buy-and-Hold benchmark and the DRL-based strategy had the same number of times that achieved the first position concerning annual returns, the DRL-based strategy had an AAR from the year 2010 to 2022 of 47.27%, against 10.52%. In addition, the TDQN presented the AAR from 2010 to 2022 of 13.09%, being positioned in the second position regarding annual returns. Once again, the DRL-based strategy showed aggressive behavior with the highest standard deviation of 65.93, compared to 48.44 and 43.14 from TDQN and Buy-and-Hold benchmarks, respectively. According to the analysis, despite its high volatility, the model shows that the model tends to produce positive returns with high volatility, which is the reason that it achieved the best performance compared to all benchmarks.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">In ATOM/USDT cryptocurrency data frame, the DRL-based strategy outperformed all benchmarks during 2019 and 2021, generating 253.45% and 646.56% annual returns. However, the strategy had the second position in 2020, losing to the TDQN, which produced a 348.30% return. The DRL-based strategy had the best-performance regarding AAR from 2019 to 2021 with 309.25%.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">In addition, the strategy again showed the most aggressive and volatile behavior, having 33.24% and 18.61% higher standard deviation than the TDQN and Buy-and-Hold benchmarks, respectively. By analyzing which position each benchmark achieved during every year concerning the annual returns, it is possible to affirm that the DRL-based strategy always had the first or second position, even though the other benchmarks alternate between first, third, and last places.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h1 style="padding-top: 7pt;padding-left: 31pt;text-indent: -14pt;text-align: justify;"><a name="bookmark121">CONCLUSION AND FUTURE WORKS</a><a name="bookmark122">&zwnj;</a></h1></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 17pt;text-indent: 42pt;text-align: justify;">This master dissertation proposed a DRL-based trading system that processes information according to market activity. The proposed system’s main component is a DQN variant that extends the Trading DQN benchmark by incorporating enhancements that generate a policy that generalizes across multiple market scenarios, identifies trading opportunities, and takes advantage of them. ETDQN optimizes its decisions by receiving non-frequent feedback from the environment, and replaying prioritized experiences containing different sub-goals each, which assists in its primary goal of retaining the maximum amount of profits. It is observable that, as opposed to the TDQN benchmark, the proposed algorithm could successfully generalize its policy and identify main events related to higher volatility, such as the COVID-19 dump during March 2020 and the ATOM/USDT pump during July 2021, and take profits from it, without the need of complex reward tuning.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">Exponential profit and loss was the adopted reward function based on the assumption that the agent could notice an exponential growth rate during a particular time. A trading environment was created containing candlestick dollar bars and RSI, MACD, Momentum, and Aroon oscillator technical indicators generated from it. In addition, it also included time signatures of the current hour and day of the week. Historical intraday tick data from iShares Core S&amp;P 500 ETF, Western Digital Corporation, and ATOM/USDT assets were collected and pre-processed according to market value to generate the previously mentioned bars. They do not undersample information during high market activity nor over-sample information during low market activity. iShares Core S&amp;P 500 ETF generated 398,362 dollar bars using a 500K USD threshold, Western Digital Corporation developed 550,598 bars using a 1M USD threshold, and ATOM/USDT produced 359,786 intraday dollar bars using a 7.5K USDT threshold.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Concerning mean cumulative returns, ETDQN proved to be approximately 1.46 and 7.13 times more lucrative compared to the second-best evaluated benchmark in the iShares S&amp;P500 ETF and Western Digital Corporation data frames, TDQN. In addition, the same strategy was</p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">2.14 times more lucrative than the second best-evaluated benchmark in iShares S&amp;P500 ETF,</p><p class="s5" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">Buy-and-Hold<span class="p">.</span></p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Regarding the mean 6-month rolling Sharpe ratio metric, the previously mentioned strategy outperformed every benchmark in both iShares S&amp;P500 ETF and Western Digital Corporation data frames. The algorithm achieved better 1.01 and 3.33 times risk-return relation compared to TDQN and <i>Buy-and-Hold </i>benchmarks. Unfortunately, even though the agent’s developed strategy achieved a 7.3 maximum rolling Sharpe ratio in the ATOM/USDT, it did not outperform the <i>Buy-and-Hold </i>benchmark concerning the mean in this data frame.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Concerning maximum drawdown, ETDQN had 10.7% less risk of ruin in iShares S&amp;P500 ETF than the second-best evaluated benchmark, TDQN strategy. In Western Digital Corporation, it had 8.8% less risk of ruin than the second-best considered benchmark, the random action strategy. However, in the ATOM/USDT data frame, the policy did have 2.7% more risk of ruin than the <i>Buy-and-Hold </i>benchmark.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">Finally, the proposed strategy outperformed every benchmark regarding the AAR metric. In iShares S&amp;P500 ETF, the ETDQN had an AAR of 17.07% against 12.34% produced by TDQN. In addition, it showed more aggressive behavior, delivering higher and more volatile returns with a 66.14% higher standard deviation than TDQN. Regarding the Western Digital Corporation, the same algorithm had an AAR from 2010 to 2022 of 47.27%, against 13.09% generated by TDQN from 2010 to 2022. Once again, it showed aggressive behavior with the highest standard</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: justify;">deviation of 65.93, compared to 48.44 from TDQN. Finally, in ATOM/USDT data frame, the ETDQN had the best performance regarding AAR from 2019 to 2021 with 309.25%, showing the most aggressive and volatile behavior, having 33.24% and 18.61% higher standard deviation than the TDQN and <i>Buy-and-Hold </i>benchmarks.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">This work considered a single position size only; hence the algorithm trades a single amount of the asset. Future works may extend it to a position size more significant than one. In addition, Sentiment analysis could also be explored to enhance the information in the states. Natural language processing techniques can be used to read texts and predict sentiment classes, serving as input to the state.</p><p style="padding-left: 17pt;text-indent: 42pt;text-align: justify;">Regarding learning with non-frequent feedback from the environment, it is difficult to guide an agent to its objective depending on how delayed its reward is provided. Temporal- difference learning creates bias. This fact is even more severe when rewards are sparse. Reward decomposition techniques could be explored, attributing compensation once a sub-goal is completed and maintaining the generalization capability.</p><p style="padding-left: 16pt;text-indent: 42pt;text-align: justify;">Concerning distributional learning, the implemented approach uses discrete distributions, which have limitations. The number of atoms and bounds of the support requires domain knowledge of the task; hence a different approach could be explored to change the proposed parameterization.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 7pt;padding-left: 45pt;text-indent: 0pt;text-align: center;"><a name="bookmark123">REFERENCES</a><a name="bookmark124">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Andrychowicz, M., Crow, D., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W. (2017). Hindsight experience replay. In <i>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</i>, pages 5048–5058, Long Beach, USA.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Ané, T. and Geman, H. (2000). Order flow, transaction clock, and normality of asset returns. <i>The Journal of Finance</i>, 55(5):2259–2284.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Bellemare, M. G., Dabney, W., and Munos, R. (2017). A distributional perspective on reinforcement learning. In <i>Proceedings of the 34th International Conference on Machine Learning</i>, pages 449–458, Sydney, Australia. PMLR.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">de Prado, M. L. (2018). <i>Advances in Financial Machine Learning</i>. Wiley Publishing, Hoboken, New Jersey, 1st edition.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Fortunato, M., Azar, M. G., Piot, B., Menick, J., Hessel, M., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S. (2018). Noisy networks for exploration. In <i>6th International Conference on Learning Representations Conference Track Proceedings</i>, Vancouver, Canada. OpenReview.net.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Hasselt, H. v., Guez, A., and Silver, D. (2016). Deep reinforcement learning with double q-learning. In <i>Proceedings of the 30th AAAI Conference on Artificial Intelligence</i>, AAAI’16, page 2094–2100, Phoenix, Arizona. AAAI Press.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M. G., and Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. In <i>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</i>, pages 3215–3222, New Orleans, USA. AAAI Press.</p><p style="padding-top: 8pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Kissell, R. L. (2021). <i>Algorithmic Trading Methods</i>. Academic Press, Cambridge, Massachusetts, 2nd edition.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Lakatos, E. M. and Marconi, M. d. A. (2003). <i>Fundamentos de Metodologia Científica</i>. Atlas, São Paulo, SP. OCLC: 53849497.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: left;">Levine, S., Finn, C., Darrell, T., and Abbeel, P. (2016). End-to-end training of deep visuomotor policies. <i>Journal of Machine Learning Research</i>, 17(1):1334–1373.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: left;">Li, Y., Zheng, W., and Zheng, Z. (2019). Deep robust reinforcement learning for practical algorithmic trading. <i>IEEE Access</i>, 7:108014–108022.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: left;">Lin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and teaching. <i>Machine Learning</i>, 8(3–4):293–321.</p><p style="padding-top: 9pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller,</p><p style="padding-left: 28pt;text-indent: 0pt;text-align: left;">M. (2013). Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop. Lake Tahoe, USA.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;"><a name="bookmark125">Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control through deep reinforcement learning. </a><i>Nature</i>, 518(7540):529–533.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Russell, S. J. and Norcvig, P. (2016). <i>Artificial intelligence: a modern approach</i>. Malaysia; Pearson Education Limited.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Schaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal value function approximators. In <i>Proceedings of the 32nd International Conference on Machine Learning</i>, volume 37 of <i>Proceedings of Machine Learning Research</i>, pages 1312–1320, Lille, France. PMLR.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Prioritized experience replay. In <i>4th International Conference on Learning Representations Conference Track Proceedings</i>, San Juan, Puerto Rico.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Théate, T. and Ernst, D. (2021). An application of deep reinforcement learning to algorithmic trading. <i>Expert Systems with Applications</i>, 173:114632.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016). Dueling network architectures for deep reinforcement learning. In <i>Proceedings of the 33nd International Conference on Machine Learning, ICML</i>, volume 48 of <i>JMLR Workshop and Conference Proceedings</i>, pages 1995–2003, New York, USA. JMLR.org.</p><p style="padding-top: 9pt;padding-left: 28pt;text-indent: -11pt;text-align: justify;">Watkins, C. J. C. H. and Dayan, P. (1992). Technical note q-learning. <i>Machine Learning</i>, 8:279–292.</p></body></html>
